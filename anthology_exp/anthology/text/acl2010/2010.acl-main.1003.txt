        Bitext Dependency Parsing with Bilingual Subtree Constraints


                   Wenliang Chen, Jun’ichi Kazama and Kentaro Torisawa
                        Language Infrastructure Group, MASTAR Project
                National Institute of Information and Communications Technology
                 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
                     {chenwl, kazama, torisawa}@nict.go.jp




                     Abstract                                  accurate bitext parsing (Smith and Smith, 2004;
    This paper proposes a dependency parsing                   Burkett and Klein, 2008; Huang et al., 2009).
    method that uses bilingual constraints to                     This paper proposes a dependency parsing
    improve the accuracy of parsing bilingual                  method, which uses the bilingual constraints that
    texts (bitexts). In our method, a target-                  we call bilingual subtree constraints and statistics
    side tree fragment that corresponds to a                   concerning the constraints estimated from large
    source-side tree fragment is identified via                unlabeled monolingual corpora. Basically, a (can-
    word alignment and mapping rules that                      didate) dependency subtree in a source-language
    are automatically learned. Then it is ver-                 sentence is mapped to a subtree in the correspond-
    ified by checking the subtree list that is                 ing target-language sentence by using word align-
    collected from large scale automatically                   ment and mapping rules that are automatically
    parsed data on the target side. Our method,                learned. The target subtree is verified by check-
    thus, requires gold standard trees only on                 ing the subtree list that is collected from unla-
    the source side of a bilingual corpus in                   beled sentences in the target language parsed by
    the training phase, unlike the joint parsing               a usual monolingual parser. The result is used as
    model, which requires gold standard trees                  additional features for the source side dependency
    on the both sides. Compared to the re-                     parser. In this paper, our task is to improve the
    ordering constraint model, which requires                  source side parser with the help of the translations
    the same training data as ours, our method                 on the target side.
    achieved higher accuracy because of richer                    Many researchers have investigated the use
    bilingual constraints. Experiments on the                  of bilingual constraints for parsing (Burkett and
    translated portion of the Chinese Treebank                 Klein, 2008; Zhao et al., 2009; Huang et al.,
    show that our system outperforms mono-                     2009). For example, Burkett and Klein (2008)
    lingual parsers by 2.93 points for Chinese                 show that parsing with joint models on bitexts im-
    and 1.64 points for English.                               proves performance on either or both sides. How-
                                                               ever, their methods require that the training data
1   Introduction
                                                               have tree structures on both sides, which are hard
Parsing bilingual texts (bitexts) is crucial for train-        to obtain. Our method only requires dependency
ing machine translation systems that rely on syn-              annotation on the source side and is much sim-
tactic structures on either the source side or the             pler and faster. Huang et al. (2009) proposes a
target side, or the both (Ding and Palmer, 2005;               method, bilingual-constrained monolingual pars-
Nakazawa et al., 2006). Bitexts could provide                  ing, in which a source-language parser is extended
more information, which is useful in parsing, than             to use the re-ordering of words between two sides’
a usual monolingual texts that can be called “bilin-           sentences as additional information. The input of
gual constraints”, and we expect to obtain more                their method is the source trees with their trans-
accurate parsing results that can be effectively               lation on the target side as ours, which is much
used in the training of MT systems. With this mo-              easier to obtain than trees on both sides. However,
tivation, there are several studies aiming at highly           their method does not use any tree structures on


                                                          21
         Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21–29,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


the target side that might be useful for ambiguity
resolution. Our method achieves much greater im-
provement because it uses the richer subtree con-                     He ate   the   meat with   a   fork   .
straints.
   Our approach takes the same input as Huang
et al. (2009) and exploits the subtree structure on               Ԇ(He) ⭘(use) ৹ᆀ(fork) ਲ਼(eat) 㚹(meat) Ǆ(.)
the target side to provide the bilingual constraints.
The subtrees are extracted from large-scale auto-                   Figure 1: Example for disambiguation
parsed monolingual data on the target side. The
main problem to be addressed is mapping words                  tion.
on the source side to the target subtree because                  There are two candidates “ate” and “meat” to be
there are many to many mappings and reordering                 the head of “with” as the dashed directed links in
problems that often occur in translation (Koehn et             Figure 1 show. By adding “fork”, we have two
al., 2003). We use an automatic way for generat-               possible dependency relations, “meat-with-fork”
ing mapping rules to solve the problems. Based                 and “ate-with-fork”, to be verified.
on the mapping rules, we design a set of features                 First, we check the possible relation of “meat”,
for parsing models. The basic idea is as follows: if           “with”, and “fork”. We obtain their corresponding
the words form a subtree on one side, their corre-             words “肉(meat)”, “用(use)”, and “叉子(fork)” in
sponding words on the another side will also prob-             Chinese via the word alignment links. We ver-
ably form a subtree.                                           ify that the corresponding words form a subtree
   Experiments on the translated portion of the                by looking up a subtree list in Chinese (described
Chinese Treebank (Xue et al., 2002; Bies et al.,               in Section 4.1). But we can not find a subtree for
2007) show that our system outperforms state-of-               them.
the-art monolingual parsers by 2.93 points for Chi-               Next, we check the possible relation of “ate”,
nese and 1.64 points for English. The results also             “with”, and “fork”. We obtain their correspond-
show that our system provides higher accuracies                ing words “吃(ate)”, “用(use)”, and “叉子(fork)”.
than the parser of Huang et al. (2009).                        Then we verify that the words form a subtree by
   The rest of the paper is organized as follows:              looking up the subtree list. This time we can find
Section 2 introduces the motivation of our idea.               the subtree as shown in Figure 2.
Section 3 introduces the background of depen-
dency parsing. Section 4 proposes an approach
of constructing bilingual subtree constraints. Sec-
tion 5 explains the experimental results. Finally, in                    ⭘(use) ৹ᆀ(fork) ਲ਼(eat)
Section 6 we draw conclusions and discuss future
work.
                                                                   Figure 2: Example for a searched subtree
2   Motivation
                                                                  Finally, the parser may assign “ate” to be the
In this section, we use an example to show the                 head of “with” based on the verification results.
idea of using the bilingual subtree constraints to             This simple example shows how to use the subtree
improve parsing performance.                                   information on the target side.
   Suppose that we have an input sentence pair as
                                                               3 Dependency parsing
shown in Figure 1, where the source sentence is in
English, the target is in Chinese, the dashed undi-            For dependency parsing, there are two main types
rected links are word alignment links, and the di-             of parsing models (Nivre and McDonald, 2008;
rected links between words indicate that they have             Nivre and Kubler, 2006): transition-based (Nivre,
a (candidate) dependency relation.                             2003; Yamada and Matsumoto, 2003) and graph-
   In the English side, it is difficult for a parser to        based (McDonald et al., 2005; Carreras, 2007).
determine the head of word “with” because there                Our approach can be applied to both parsing mod-
is a PP-attachment problem. However, in Chinese                els.
it is unambiguous. Therefore, we can use the in-                  In this paper, we employ the graph-based MST
formation on the Chinese side to help disambigua-              parsing model proposed by McDonald and Pereira


                                                          22


(2006), which is an extension of the projec-                 4 Bilingual subtree constraints
tive parsing algorithm of Eisner (1996). To use
richer second-order information, we also imple-              In this section, we propose an approach that uses
ment parent-child-grandchild features (Carreras,             the bilingual subtree constraints to help parse
2007) in the MST parsing algorithm.                          source sentences that have translations on the tar-
                                                             get side.
3.1 Parsing with monolingual features                           We use large-scale auto-parsed data to obtain
                                                             subtrees on the target side. Then we generate the
Figure 3 shows an example of dependency pars-                mapping rules to map the source subtrees onto the
ing. In the graph-based parsing model, features are          extracted target subtrees. Finally, we design the
represented for all the possible relations on single         bilingual subtree features based on the mapping
edges (two words) or adjacent edges (three words).           rules for the parsing model. These features in-
The parsing algorithm chooses the tree with the              dicate the information of the constraints between
highest score in a bottom-up fashion.                        bilingual subtrees, that are called bilingual subtree
                                                             constraints.

                                                             4.1 Subtree extraction

   ROOT     He ate   the   meat with    a   fork   .         Chen et al. (2009) propose a simple method to ex-
                                                             tract subtrees from large-scale monolingual data
      Figure 3: Example of dependency tree                   and use them as features to improve monolingual
                                                             parsing. Following their method, we parse large
   In our systems, the monolingual features in-              unannotated data with a monolingual parser and
clude the first- and second- order features pre-             obtain a set of subtrees (STt ) in the target lan-
sented in (McDonald et al., 2005; McDonald                   guage.
and Pereira, 2006) and the parent-child-grandchild              We encode the subtrees into string format that is
features used in (Carreras, 2007). We call the               expressed as st = w : hid(−w : hid)+1 , where w
parser with the monolingual features monolingual             refers to a word in the subtree and hid refers to the
parser.                                                      word ID of the word’s head (hid=0 means that this
                                                             word is the root of a subtree). Here, word ID refers
3.2 Parsing with bilingual features                          to the ID (starting from 1) of a word in the subtree
                                                             (words are ordered based on the positions of the
In this paper, we parse source sentences with the            original sentence). For example, “He” and “ate”
help of their translations. A set of bilingual fea-          have a left dependency arc in the sentence shown
tures are designed for the parsing model.                    in Figure 3. The subtree is encoded as “He:2-
                                                             ate:0”. There is also a parent-child-grandchild re-
3.2.1 Bilingual subtree features                             lation among “ate”, “with”, and “fork”. So the
We design bilingual subtree features, as described           subtree is encoded as “ate:0-with:1-fork:2”. If a
in Section 4, based on the constraints between the           subtree contains two nodes, we call it a bigram-
source subtrees and the target subtrees that are ver-        subtree. If a subtree contains three nodes, we call
ified by the subtree list on the target side. The            it a trigram-subtree.
source subtrees are from the possible dependency                From the dependency tree of Figure 3, we ob-
relations.                                                   tain the subtrees, as shown in Figure 4 and Figure
                                                             5. Figure 4 shows the extracted bigram-subtrees
3.2.2 Bilingual reordering feature                           and Figure 5 shows the extracted trigram-subtrees.
                                                             After extraction, we obtain a set of subtrees. We
Huang et al. (2009) propose features based on
                                                             remove the subtrees occurring only once in the
reordering between languages for a shift-reduce
                                                             data. Following Chen et al. (2009), we also group
parser. They define the features based on word-
                                                             the subtrees into different sets based on their fre-
alignment information to verify that the corre-
                                                             quencies.
sponding words form a contiguous span for resolv-
ing shift-reduce conflicts. We also implement sim-              1
                                                                  + refers to matching the preceding element one or more
ilar features in our system.                                 times and is the same as a regular expression in Perl.


                                                        23


        ate                                         meat                                            4.2.1 Reordering and MtoN mapping in
                 He:1:2-ate:2:0                                     the:1:2-meat:2:0
   He                                           the
                                                                                                            translation
  ate
                   ate:1:0-meat:2:1 with
                                                                                                    Both Chinese and English are classified as SVO
                                                                    with:1:0-fork:2:1
   meat                                              fork
                                                                                                    languages because verbs precede objects in simple
  ate                                               fork                                            sentences. However, Chinese has many character-
                   ate:1:0-with:2:1                                 a:1:2-fork:2:0
    with                                            a                                               istics of such SOV languages as Japanese. The
                                                                                                    typical cases are listed below:
         Figure 4: Examples of bigram-subtrees                                                         1) Prepositional phrases modifying a verb pre-
                                                                                                    cede the verb. Figure 6 shows an example. In En-
                                                                                                    glish the prepositional phrase “at the ceremony”
  ate
                  ate:1:0-meat:2:1-with:3:1
                                                    ate
                                                                    ate:1:0-with:2:1-.:3:1
                                                                                                    follows the verb “said”, while its corresponding
   meat with
     ate
                                                     with .
                                                    ate                                             prepositional phrase “在(NULL) 仪式(ceremony)
                                                                   ate:1:0-NULL:2:1-meat:3:1
 He NULL
                 He:1:3-NULL:2:3-ate:3:0
                                                    NULL meat                                       上(at)” precedes the verb “说(say)” in Chinese.
         the:1:3-NULL:2:3-meat:3:0                        with:1:0-NULL:2:1-fork:3:1
         a:1:3-NULL:2:3-fork:3:0

                                              (a)

         ate:1:0-the:2:3-meat:3:1                         ate:1:0-with:2:1-fork:3:2

         with:1:0-a:2:3-fork:3:1                          NULL:1:2-He:2:3-ate:3:0
                                                                                                                ൘        Ԛᔿ      к     䈤
         He:1:3-NULL:2:1-ate:3:0                          ate:1:0-meat:2:1-NULL:3:2
         ate:1:0-NULL:2:3-with:3:1                        with:1:0-fork:2:1-NULL:3:2
         NULL:1:2-a:2:3-fork:3:0                          a:1:3-NULL:2:1-fork:3:0
         ate:1:0-NULL:2:3-.:3:1                           ate:1:0-.:2:1-NULL:3:2

         NULL:1:2-the:2:3-meat:3:0                        the:1:3-NULL:2:1-meat:3:0

                                              (b)
                                                                                                                Said at the ceremony
         Figure 5: Examples of trigram-subtrees

                                                                                                    Figure 6: Example for prepositional phrases mod-
                                                                                                    ifying a verb
4.2 Mapping rules
                                                                                                       2) Relative clauses precede head noun. Fig-
To provide bilingual subtree constraints, we need                                                   ure 7 shows an example. In Chinese the relative
to find the characteristics of subtree mapping for                                                  clause “今天(today) 签字(signed)” precedes the
the two given languages. However, subtree map-                                                      head noun “项目(project)”, while its correspond-
ping is not easy. There are two main problems:                                                      ing clause “signed today” follows the head noun
MtoN (words) mapping and reordering, which of-                                                      “projects” in English.
ten occur in translation. MtoN (words) map-
ping means that a source subtree with M words
is mapped onto a target subtree with N words. For
                                                                                                              Ӻཙ ㆮᆇ Ⲵ й њ 亩ⴞ
example, 2to3 means that a source bigram-subtree
is mapped onto a target trigram-subtree.
   Due to the limitations of the parsing algo-
rithm (McDonald and Pereira, 2006; Carreras,
2007), we only use bigram- and trigram-subtrees                                                               The 3 projects signed today
in our approach. We generate the mapping rules
for the 2to2, 2to3, 3to3, and 3to2 cases. For
trigram-subtrees, we only consider the parent-                                                      Figure 7: Example for relative clauses preceding
child-grandchild type. As for the use of other                                                      the head noun
types of trigram-subtrees, we leave it for future
                                                                                                       3) Genitive constructions precede head noun.
work.
                                                                                                    For example, “汽 车(car) 轮 子(wheel)” can be
   We first show the MtoN and reordering prob-                                                      translated as “the wheel of the car”.
lems by using an example in Chinese-English                                                            4) Postposition in many constructions rather
translation. Then we propose a method to auto-                                                      than prepositions. For example, “桌 子(table)
matically generate mapping rules.                                                                   上(on)” can be translated as “on the table”.


                                                                                               24


   We can find the MtoN mapping problem occur-                 tree pair: “社 会(society):2-边 缘(fringe):0” and
ring in the above cases. For example, in Figure 6,             “fringe(W 2):0-of:1-society(W 1):2”.
trigram-subtree “在(NULL):3-上(at):1-说(say):0”                      The extracted subtree pairs indicate the trans-
is mapped onto bigram-subtree “said:0-at:1”.                   lation characteristics between Chinese and En-
   Since asking linguists to define the mapping                glish. For example, the pair “社 会(society):2-
rules is very expensive, we propose a simple                   边 缘(fringe):0” and “fringes:0-of:1-society:2”
method to easily obtain the mapping rules.                     is a case where “Genitive constructions pre-
                                                               cede/follow the head noun”.
4.2.2 Bilingual subtree mapping
                                                               4.2.3 Generalized mapping rules
To solve the mapping problems, we use a bilingual
corpus, which includes sentence pairs, to automat-             To increase the mapping coverage, we general-
ically generate the mapping rules. First, the sen-             ize the mapping rules from the extracted sub-
tence pairs are parsed by monolingual parsers on               tree pairs by using the following procedure. The
both sides. Then we perform word alignment us-                 rules are divided by “=>” into two parts: source
ing a word-level aligner (Liang et al., 2006; DeN-             (left) and target (right). The source part is
ero and Klein, 2007). Figure 8 shows an example                from the source subtree and the target part is
of a processed sentence pair that has tree structures          from the target subtree. For the source part,
on both sides and word alignment links.                        we replace nouns and verbs using their POS
                                                               tags (coarse grained tags). For the target part,
                                                               we use the word alignment information to rep-
                                                               resent the target words that have correspond-
    ROOT    ԆԜ     ༴Ҿ    ⽮Պ     䗩㕈    Ǆ                        ing source words. For example, we have the
                                                               subtree pair: “社 会(society):2-边 缘(fringe):0”
                                                               and “fringes(W 2):0-of:1-society(W 1):2”, where
   ROOT    They are on the fringes of society .                “of” does not have a corresponding word, the POS
                                                               tag of “社会(society)” is N, and the POS tag of
                                                               “边缘(fringe)” is N. The source part of the rule
                                                               becomes “N:2-N:0” and the target part becomes
Figure 8: Example of auto-parsed bilingual sen-                “W 2:0-of:1-W 1:2”.
tence pair                                                        Table 1 shows the top five mapping rules of
                                                               all four types ordered by their frequencies, where
   From these sentence pairs, we obtain subtree                W 1 means that the target word is aligned to the
pairs. First, we extract a subtree (sts ) from a               first word of the source subtree, W 2 means that
source sentence. Then through word alignment                   the target word is aligned to the second word, and
links, we obtain the corresponding words of the                W 3 means that the target word is aligned to the
words of sts . Because of the MtoN problem, some               third word. We remove the rules that occur less
words lack of corresponding words in the target                than three times. Finally, we obtain 9,134 rules
sentence. Here, our approach requires that at least            for 2to2, 5,335 for 2to3, 7,450 for 3to3, and 1,244
two words of sts have corresponding words and                  for 3to2 from our data. After experiments with dif-
nouns and verbs need corresponding words. If not,              ferent threshold settings on the development data
it fails to find a subtree pair for sts . If the corre-        sets, we use the top 20 rules for each type in our
sponding words form a subtree (stt ) in the target             experiments.
sentence, sts and stt are a subtree pair. We also                 The generalized mapping rules might generate
keep the word alignment information in the tar-                incorrect target subtrees. However, as described in
get subtree. For example, we extract subtree “社                Section 4.3.1, the generated subtrees are verified
会(society):2-边缘(fringe):0” on the Chinese side                 by looking up list STt before they are used in the
and get its corresponding subtree “fringes(W 2):0-             parsing models.
of:1-society(W 1):2” on the English side, where
W 1 means that the target word is aligned to the               4.3 Bilingual subtree features
first word of the source subtree, and W 2 means                Informally, if the words form a subtree on the
that the target word is aligned to the second word             source side, then the corresponding words on the
of the source subtree. That is, we have a sub-                 target side will also probably form a subtree. For


                                                          25


 # rules                                         freq         2to2, 3to3, 3to2, and 2to3 mappings. In the 2to2,
 2to2 mapping
 1 N:2 N:0 => W 1:2 W 2:0                        92776        3to3, and 3to2 cases, the target subtrees do not add
 2 V:0 N:1 => W 1:0 W 2:1                        62437        new words. We represent features in a direct way.
 3 V:0 V:1 => W 1:0 W 2:1                        49633        For the 2to3 case, we represent features using a
 4 N:2 V:0 => W 1:2 W 2:0                        43999
 5 的:2 N:0 => W 2:0 W 1:2                        25301        different strategy.
 2to3 mapping
 1 N:2-N:0 => W 2:0-of:1-W 1:2                   10361        4.3.1 Features for 2to2, 3to3, and 3to2
 2 V:0-N:1 => W 1:0-of:1-W 2:2                   4521
 3 V:0-N:1 => W 1:0-to:1-W 2:2                   2917
                                                              We design the features based on the mapping
 4 N:2-V:0 => W 2:0-of:1-W 1:2                   2578         rules of 2to2, 3to3, and 3to2. For example, we
 5 N:2-N:0 => W 1:2-’:3-W 2:0                    2316         design features for a 3to2 case from Figure 9.
 3to2 mapping
 1 V:2-的/DEC:3-N:0 => W 1:0-W 3:1                873
                                                              The possible relation to be verified forms source
 2 V:2-的/DEC:3-N:0 => W 3:2-W 1:0                634          subtree “签 字(signed)/VV:2-的(NULL)/DEC:3-
 3 N:2-的/DEG:3-N:0 => W 1:0-W 3:1                319          项 目(project)/NN:0” in which “项 目(project)”
 4 N:2-的/DEG:3-N:0 => W 3:2-W 1:0                301
                                                              is aligned to “projects” and “签 字(signed)” is
 5 V:0-的/DEG:3-N:1 => W 3:0-W 1:1                247
 3to3 mapping                                                 aligned to “signed” as shown in Figure 9. The
 1 V:0-V:1-N:2 => W 1:0-W 2:1-W 3:2              9580         procedure of generating the features is shown in
 2 N:2-的/DEG:3-N:0 => W 3:0-W 2:1-W 1:2          7010         Figure 10. We explain Steps (1), (2), (3), and (4)
 3 V:0-N:3-N:1 => W 1:0-W 2:3-W 3:1              5642
 4 V:0-V:1-V:2 => W 1:0-W 2:1-W 3:2              4563         as follows:
 5 N:2-N:3-N:0 => W 1:2-W 2:3-W 3:0              3570
                                                                ㆮᆇ/VV:2-Ⲵ/DEC:3-亩ⴞ/NN:0
Table 1: Top five mapping rules of 2to3 and 3to2
                                                                 projects(W_3) signed(W_1)



example, in Figure 8, words “他 们(they)” and                                   (1)
“处于(be on)” form a subtree , which is mapped
onto the words “they” and “are” on the target side.                V:2-Ⲵ/DEC:3-N:0


These two target words form a subtree. We now
develop this idea as bilingual subtree features.                              (2)
   In the parsing process, we build relations for
                                                                     W_3:0-W_1:1
two or three words on the source side. The con-                      W_3:2-W_1:0
                                                                     W 3:2 W 1:0

ditions of generating bilingual subtree features are
that at least two of these source words must have                               (3)
corresponding words on the target side and nouns                   projects:0-signed:1
                                                                                             STt
                                                                   projects:2-signed:0
and verbs must have corresponding words.
   At first, we have a possible dependency relation                           (4)
(represented as a source subtree) of words to be
                                                                       3to2:YES
verified. Then we obtain the corresponding target
subtree based on the mapping rules. Finally, we
verify that the target subtree is included in STt . If        Figure 10: Example of feature generation for 3to2
yes, we activate a positive feature to encourage the          case
dependency relation.
                                                                (1) Generate source part from the source
                                                              subtree. We obtain “V:2-的/DEC:3-N:0” from
        䘉 ᱟ Ӻཙ          ㆮᆇ Ⲵ й њ 亩ⴞ                           “签       字(signed)/VV:2-的(NULL)/DEC:3-项
                                                              目(project)/NN:0”.
                                                                (2) Obtain target parts based on the matched
                                                              mapping rules, whose source parts equal
                                                              “V:2-的/DEC:3-N:0”. The matched rules are
   Those are the 3 projects signed today                      “V:2-的/DEC:3-N:0 =>W 3:0-W 1:1” and
                                                              “V:2-的/DEC:3-N:0 => W 3:2-W 1:0”. Thus,
    Figure 9: Example of features for parsing                 we have two target parts “W 3:0-W 1:1” and
                                                              “W 3:2-W 1:0”.
  We consider four types of features based on                   (3) Generate possible subtrees by consider-


                                                         26


ing the dependency relation indicated in the                 tive, building relations is very likely among source
target parts. We generate a possible subtree                 words. If both are inactive, this is a strong negative
“projects:0-signed:1” from the target part “W 3:0-           signal for their relations.
W 1:1”, where “projects” is aligned to “项
目(project)(W 3)” and “signed” is aligned to “签               5 Experiments
字(signed)(W 1)”. We also generate another pos-               All the bilingual data were taken from the trans-
sible subtree “projects:2-signed:0” from “W 3:2-             lated portion of the Chinese Treebank (CTB)
W 1:0”.                                                      (Xue et al., 2002; Bies et al., 2007), articles
   (4) Verify that at least one of the generated             1-325 of CTB, which have English translations
possible subtrees is a target subtree, which is in-          with gold-standard parse trees. We used the tool
cluded in STt . If yes, we activate this feature. In         “Penn2Malt”2 to convert the data into dependency
the figure, “projects:0-signed:1” is a target subtree        structures. Following the study of Huang et al.
in STt . So we activate the feature “3to2:YES”               (2009), we used the same split of this data: 1-270
to encourage dependency relations among “签                   for training, 301-325 for development, and 271-
字(signed)”, “的(NULL)”, and “项目(project)”.                    300 for test. Note that some sentence pairs were
4.3.2 Features for 2to3                                      removed because they are not one-to-one aligned
                                                             at the sentence level (Burkett and Klein, 2008;
In the 2to3 case, a new word is added on the target
                                                             Huang et al., 2009). Word alignments were gen-
side. The first two steps are identical as those in
                                                             erated from the Berkeley Aligner (Liang et al.,
the previous section. For example, a source part
                                                             2006; DeNero and Klein, 2007) trained on a bilin-
“N:2-N:0” is generated from “汽车(car)/NN:2-轮
                                                             gual corpus having approximately 0.8M sentence
子(wheel)/NN:0”. Then we obtain target parts
                                                             pairs. We removed notoriously bad links in {a,
such as “W 2:0-of/IN:1-W 1:2”, “W 2:0-in/IN:1-
                                                             an, the}×{的(DE), 了(LE)} following the work of
W 1:2”, and so on, according to the matched map-
                                                             Huang et al. (2009).
ping rules.
                                                                For Chinese unannotated data, we used the
   The third step is different. In the target parts,
                                                             XIN CMN portion of Chinese Gigaword Version
there is an added word. We first check if the added
                                                             2.0 (LDC2009T14) (Huang, 2009), which has ap-
word is in the span of the corresponding words,
                                                             proximately 311 million words whose segmenta-
which can be obtained through word alignment
                                                             tion and POS tags are given. To avoid unfair com-
links. We can find that “of” is in the span “wheel
                                                             parison, we excluded the sentences of the CTB
of the car”, which is the span of the corresponding
                                                             data from the Gigaword data. We discarded the an-
words of “汽 车(car)/NN:2-轮 子(wheel)/NN:0”.
                                                             notations because there are differences in annota-
Then we choose the target part “W 2:0-of/IN:1-
                                                             tion policy between CTB and this corpus. We used
W 1:2” to generate a possible subtree. Finally,
                                                             the MMA system (Kruengkrai et al., 2009) trained
we verify that the subtree is a target subtree in-
                                                             on the training data to perform word segmentation
cluded in STt . If yes, we say feature “2to3:YES”
                                                             and POS tagging and used the Baseline Parser to
to encourage a dependency relation between “汽
                                                             parse all the sentences in the data. For English
车(car)” and “轮子(wheel)”.
                                                             unannotated data, we used the BLLIP corpus that
4.4 Source subtree features                                  contains about 43 million words of WSJ text. The
                                                             POS tags were assigned by the MXPOST tagger
Chen et al. (2009) shows that the source sub-
                                                             trained on training data. Then we used the Base-
tree features (Fsrc−st ) significantly improve per-
                                                             line Parser to parse all the sentences in the data.
formance. The subtrees are obtained from the
                                                                We reported the parser quality by the unlabeled
auto-parsed data on the source side. Then they are
                                                             attachment score (UAS), i.e., the percentage of to-
used to verify the possible dependency relations
                                                             kens (excluding all punctuation tokens) with cor-
among source words.
                                                             rect HEADs.
   In our approach, we also use the same source
subtree features described in Chen et al. (2009).            5.1 Main results
So the possible dependency relations are verified
                                                             The results on the Chinese-source side are shown
by the source and target subtrees. Combining two
                                                             in Table 2, where “Baseline” refers to the systems
types of features together provides strong discrim-
                                                                2
ination power. If both types of features are ac-                    http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html


                                                        27


with monolingual features, “Baseline2” refers to           (OURS) were significant in McNemar’s Test (p <
adding the reordering features to the Baseline,            10−3 ).
“FBI ” refers to adding all the bilingual subtree
features to “Baseline2”, “Fsrc−st ” refers to the                             Order-1         Order-2
monolingual parsing systems with source subtree                Baseline        86.41           87.37
features, “Order-1” refers to the first-order mod-             Baseline2       86.86           87.66
els, and “Order-2” refers to the second-order mod-             +2to2           87.23           87.87
els. The results showed that the reordering fea-               +2to3           87.35           87.96
tures yielded an improvement of 0.53 and 0.58                  +3to3             –             88.25
points (UAS) for the first- and second-order mod-              +3to2             –             88.37
els respectively. Then we added four types of                  FBI          87.35(+0.94)    88.37(+1.00)
bilingual constraint features one by one to “Base-             Fsrc−st      87.25(+0.84)    88.57(+1.20)
line2”. Note that the features based on 3to2 and               OURS         87.71(+1.30)    89.01(+1.64)
3to3 can not be applied to the first-order models,
because they only consider single dependencies             Table 3: Dependency parsing results of English-
(bigram). That is, in the first model, FBI only in-        source case
cludes the features based on 2to2 and 2to3. The
results showed that the systems performed better           5.2 Comparative results
and better. In total, we obtained an absolute im-          Table 4 shows the performance of the system we
provement of 0.88 points (UAS) for the first-order         compared, where Huang2009 refers to the result of
model and 1.36 points for the second-order model           Huang et al. (2009). The results showed that our
by adding all the bilingual subtree features. Fi-          system performed better than Huang2009. Com-
nally, the system with all the features (OURS) out-        pared with the approach of Huang et al. (2009),
performed the Baseline by an absolute improve-             our approach used additional large-scale auto-
ment of 3.12 points for the first-order model and          parsed data. We did not compare our system with
2.93 points for the second-order model. The im-            the joint model of Burkett and Klein (2008) be-
provements of the final systems (OURS) were sig-           cause they reported the results on phrase struc-
nificant in McNemar’s Test (p < 10−4 ).                    tures.

                   Order-1         Order-2                                        Chinese     English
    Baseline        84.35           87.20                          Huang2009        86.3       87.5
    Baseline2       84.88           87.78                          Baseline        87.20       87.37
    +2to2           85.08           88.07                          OURS            90.13       89.01
    +2to3           85.23           88.14
    +3to3             –             88.29                             Table 4: Comparative results
    +3to2             –             88.56
    FBI          85.23(+0.88)    88.56(+1.36)              6 Conclusion
    Fsrc−st      86.54(+2.19)    89.49(+2.29)
    OURS         87.47(+3.12)    90.13(+2.93)              We presented an approach using large automati-
                                                           cally parsed monolingual data to provide bilingual
Table 2: Dependency parsing results of Chinese-            subtree constraints to improve bitexts parsing. Our
source case                                                approach remains the efficiency of monolingual
                                                           parsing and exploits the subtree structure on the
   We also conducted experiments on the English-           target side. The experimental results show that the
source side. Table 3 shows the results, where ab-          proposed approach is simple yet still provides sig-
breviations are the same as in Table 2. As in the          nificant improvements over the baselines in pars-
Chinese experiments, the parsers with bilingual            ing accuracy. The results also show that our sys-
subtree features outperformed the Baselines. Fi-           tems outperform the system of previous work on
nally, the systems (OURS) with all the features            the same data.
outperformed the Baselines by 1.30 points for the             There are many ways in which this research
first-order model and 1.64 for the second-order            could be continued. First, we may attempt to ap-
model. The improvements of the final systems               ply the bilingual subtree constraints to transition-


                                                      28


based parsing models (Nivre, 2003; Yamada and                 P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
Matsumoto, 2003). Here, we may design new fea-                   phrase-based translation. In Proceedings of NAACL,
                                                                 page 54. Association for Computational Linguistics.
tures for the models. Second, we may apply the
proposed method for other language pairs such as              Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Japanese-English and Chinese-Japanese. Third,                   Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
larger unannotated data can be used to improve the              Isahara. 2009. An error-driven word-character hy-
                                                                brid model for joint Chinese word segmentation and
performance further.                                            POS tagging. In Proceedings of ACL-IJCNLP2009,
                                                                pages 513–521, Suntec, Singapore, August. Associ-
                                                                ation for Computational Linguistics.
References
                                                              Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
Ann Bies, Martha Palmer, Justin Mott, and Colin                 ment by agreement. In Proceedings of the Human
  Warner. 2007. English Chinese translation treebank            Language Technology Conference of the NAACL,
  v 1.0. In LDC2007T02.                                         Main Conference, pages 104–111, New York City,
                                                                USA, June. Association for Computational Linguis-
David Burkett and Dan Klein. 2008. Two languages                tics.
  are better than one (for syntactic parsing). In Pro-
  ceedings of the 2008 Conference on Empirical Meth-          R. McDonald and F. Pereira. 2006. Online learning
  ods in Natural Language Processing, pages 877–                of approximate dependency parsing algorithms. In
  886, Honolulu, Hawaii, October. Association for               Proc. of EACL2006.
  Computational Linguistics.
                                                              R. McDonald, K. Crammer, and F. Pereira. 2005. On-
X. Carreras. 2007. Experiments with a higher-order               line large-margin training of dependency parsers. In
  projective dependency parser. In Proceedings of                Proc. of ACL 2005.
  the CoNLL Shared Task Session of EMNLP-CoNLL
  2007, pages 957–961.                                        T. Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.
                                                                 2006. Example-based machine translation based on
WL. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.               deeper nlp. In Proceedings of IWSLT 2006, pages
 2009. Improving dependency parsing with subtrees                64–70, Kyoto, Japan.
 from auto-parsed data. In Proceedings of the 2009
 Conference on Empirical Methods in Natural Lan-              J. Nivre and S. Kubler. 2006. Dependency parsing:
 guage Processing, pages 570–579, Singapore, Au-                 Tutorial at Coling-ACL 2006. In CoLING-ACL.
 gust. Association for Computational Linguistics.             J. Nivre and R. McDonald. 2008. Integrating graph-
                                                                 based and transition-based dependency parsers. In
John DeNero and Dan Klein. 2007. Tailoring word
                                                                 Proceedings of ACL-08: HLT, Columbus, Ohio,
  alignments to syntactic machine translation. In Pro-
                                                                 June.
  ceedings of the 45th Annual Meeting of the Asso-
  ciation of Computational Linguistics, pages 17–24,          J. Nivre. 2003. An efficient algorithm for projective
  Prague, Czech Republic, June. Association for Com-             dependency parsing. In Proceedings of IWPT2003,
  putational Linguistics.                                        pages 149–160.
Yuan Ding and Martha Palmer. 2005. Machine trans-             David A. Smith and Noah A. Smith. 2004. Bilingual
  lation using probabilistic synchronous dependency             parsing with factored estimation: Using English to
  insertion grammars. In ACL ’05: Proceedings of the            parse Korean. In Proceedings of EMNLP.
  43rd Annual Meeting on Association for Computa-
  tional Linguistics, pages 541–548, Morristown, NJ,          Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
  USA. Association for Computational Linguistics.               2002. Building a large-scale annotated Chinese cor-
                                                                pus. In Coling.
J. Eisner. 1996. Three new probabilistic models for
   dependency parsing: An exploration. In Proc. of            H. Yamada and Y. Matsumoto. 2003. Statistical de-
   the 16th Intern. Conf. on Computational Linguistics          pendency analysis with support vector machines. In
   (COLING), pages 340–345.                                     Proceedings of IWPT2003, pages 195–206.

Liang Huang, Wenbin Jiang, and Qun Liu. 2009.                 Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
  Bilingually-constrained (monolingual) shift-reduce            2009. Cross language dependency parsing us-
  parsing. In Proceedings of the 2009 Conference on             ing a bilingual lexicon. In Proceedings of ACL-
  Empirical Methods in Natural Language Process-                IJCNLP2009, pages 55–63, Suntec, Singapore, Au-
  ing, pages 1222–1231, Singapore, August. Associ-              gust. Association for Computational Linguistics.
  ation for Computational Linguistics.

Chu-Ren Huang. 2009. Tagged Chinese Gigaword
  Version 2.0, LDC2009T14. Linguistic Data Con-
  sortium.


                                                         29

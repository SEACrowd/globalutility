                         The Human Language Project:
              Building a Universal Corpus of the World’s Languages

                     Steven Abney                                          Steven Bird
                 University of Michigan                            University of Melbourne and
                 abney@umich.edu                                    University of Pennsylvania
                                                                   sbird@unimelb.edu.au


                     Abstract                                  we have non-negligible quantities of machine-
                                                               readable data for only about 20–30 of the world’s
    We present a grand challenge to build a                    6,900 languages (Maxwell and Hughes, 2006).
    corpus that will include all of the world’s                Linguistics as a field is awake to the crisis. There
    languages, in a consistent structure that                  has been a tremendous upsurge of interest in doc-
    permits large-scale cross-linguistic pro-                  umentary linguistics, the field concerned with the
    cessing, enabling the study of universal                   the “creation, annotation, preservation, and dis-
    linguistics. The focal data types, bilin-                  semination of transparent records of a language”
    gual texts and lexicons, relate each lan-                  (Woodbury, 2010). However, documentary lin-
    guage to one of a set of reference lan-                    guistics alone is not equal to the task. For example,
    guages. We propose that the ability to train               no million-word machine-readable corpus exists
    systems to translate into and out of a given               for any endangered language, even though such a
    language be the yardstick for determin-                    quantity would be necessary for wide-ranging in-
    ing when we have successfully captured a                   vestigation of the language once no speakers are
    language. We call on the computational                     available. The chances of constructing large-scale
    linguistics community to begin work on                     resources will be greatly improved if computa-
    this Universal Corpus, pursuing the many                   tional linguists contribute their expertise.
    strands of activity described here, as their                  This collaboration between linguists and com-
    contribution to the global effort to docu-                 putational linguists will extend beyond the con-
    ment the world’s linguistic heritage before                struction of the Universal Corpus to its exploita-
    more languages fall silent.                                tion for both theoretical and technological ends.
                                                               We envisage a new paradigm of universal linguis-
1   Introduction
                                                               tics, in which grammars of individual languages
The grand aim of linguistics is the construction of            are built from the ground up, combining expert
a universal theory of human language. To a com-                manual effort with the power tools of probabilis-
putational linguist, it seems obvious that the first           tic language models and grammatical inference.
step is to collect significant amounts of primary              A universal grammar captures redundancies which
data for a large variety of languages. Ideally, we             exist across languages, constituting a “universal
would like a complete digitization of every human              linguistic prior,” and enabling us to identify the
language: a Universal Corpus.                                  distinctive properties of specific languages and
   If we are ever to construct such a corpus, it must          families. The linguistic prior and regularities due
be now. With the current rate of language loss, we             to common descent enable a new economy of scale
have only a small window of opportunity before                 for technology development: cross-linguistic tri-
the data is gone forever. Linguistics may be unique            angulation can improve performance while reduc-
among the sciences in the crisis it faces. The next            ing per-language data requirements.
generation will forgive us for the most egregious                 Our aim in the present paper is to move beyond
shortcomings in theory construction and technol-               generalities to a concrete plan of attack, and to
ogy development, but they will not forgive us if we            challenge the field to a communal effort to cre-
fail to preserve vanishing primary language data in            ate a Universal Corpus of the world’s languages,
a form that enables future research.                           in consistent machine-readable format, permitting
   The scope of the task is enormous. At present,              large-scale cross-linguistic processing.


                                                          88
         Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 88–97,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


2       Human Language Project                               resource. We contrast the proposed effort with
                                                             general efforts to develop open resources, stan-
2.1      Aims and scope
                                                             dards, and best practices. We do not aim to be all-
Although language endangerment provides ur-                  inclusive. The project does require large-scale col-
gency, the corpus is not intended primarily as               laboration, and a task definition that is simple and
a Noah’s Ark for languages. The aims go be-                  compelling enough to achieve buy-in from a large
yond the current crisis: we wish to support cross-           number of data providers. But we do not need and
linguistic research and technology development at            do not attempt to create consensus across the en-
the largest scale. There are existing collections            tire community. (Although one can hope that what
that contain multiple languages, but it is rare to           proves successful for a project of this scale will
have consistent formats and annotation across lan-           provide a good foundation for future standards.)
guages, and few such datasets contain more than a
                                                                Moreover, we do not aim to collect data
dozen or so languages.
                                                             merely in the vague hope that it will prove use-
   If we think of a multi-lingual corpus as con-
                                                             ful. Although we strive for maximum general-
sisting of an array of items, with columns repre-
                                                             ity, we also propose a specific driving “use case,”
senting languages and rows representing resource
                                                             namely, machine translation (MT), (Hutchins and
types, the usual focus is on “vertical” processing.
                                                             Somers, 1992; Koehn, 2010). The corpus pro-
Our particular concern, by contrast, is “horizontal”
                                                             vides a testing ground for the development of MT
processing that cuts indiscriminately across lan-
                                                             system-construction methods that are dramatically
guages. Hence we require an unusual degree of
                                                             “leaner” in their resource requirements, and which
consistency across languages.
                                                             take advantage of cross-linguistic bootstrapping.
   The kind of processing we wish to enable is
                                                             The large engineering question is how one can
much like the large-scale systematic research that
                                                             turn the size of the task—constructing MT systems
motivated the Human Genome Project.
                                                             for all the world’s languages simultaneously—to
         One of the greatest impacts of having               one’s advantage, and thereby consume dramati-
         the sequence may well be in enabling                cally less data per language.
         an entirely new approach to biological                The choice of MT as the use case is also driven
         research. In the past, researchers stud-            by scientific considerations. To explain, we re-
         ied one or a few genes at a time. With              quire a bit of preamble.
         whole-genome sequences . . . they can
                                                                We aim for a digitization of each human lan-
         approach questions systematically and
                                                             guage. What exactly does it mean to digitize an
         on a grand scale. They can study . . .
                                                             entire language? It is natural to think in terms
         how tens of thousands of genes and pro-
                                                             of replicating the body of resources available for
         teins work together in interconnected
                                                             well-documented languages, and the pre-eminent
         networks to orchestrate the chemistry of
                                                             resource for any language is a treebank. Producing
         life. (Human Genome Project, 2007)
                                                             a treebank involves a staggering amount of man-
We wish to make it possible to investigate human             ual effort. It is also notoriously difficult to obtain
language equally systematically and on an equally            agreement about how parse trees should be defined
grand scale: a Human Linguome Project, as it                 in one language, much less in many languages si-
were, though we have chosen the “Human Lan-                  multaneously. The idea of producing treebanks for
guage Project” as a more inviting title for the un-          6,900 languages is quixotic, to put it mildly. But
dertaking. The product is a Universal Corpus,1 in            is a treebank actually necessary?
two senses of universal: in the sense of including              Let us suppose that the purpose of a parse
(ultimately) all the world’s languages, and in the           tree is to mediate interpretation. A treebank, ar-
sense of enabling software and processing meth-              guably, represents a theoretical hypothesis about
ods that are language-universal.                             how interpretations could be constructed; the pri-
   However, we do not aim for a collection that              mary data is actually the interpretations them-
is universal in the sense of encompassing all lan-           selves. This suggests that we annotate sentences
guage documentation efforts. Our goal is the con-            with representations of meanings instead of syn-
struction of a specific resource, albeit a very large        tactic structures. Now that seems to take us out of
    1
        http://universalcorpus.org/                          the frying pan into the fire. If obtaining consen-


                                                        89


sus on parse trees is difficult, obtaining consensus         allomorphs, reducing the amount of data required
on meaning representations is impossible. How-               for training an MT system. This most-refined
ever, if the language under consideration is any-            target annotation corresponds to the interlinear
thing other than English, then a translation into            glossed texts that are the de facto standard of anno-
English (or some other reference language) is for            tation in the documentary linguistics community.
most purposes a perfectly adequate meaning rep-                 We postulate that interlinear glossed text is suf-
resentation. That is, we view machine translation            ficiently fine-grained to serve our purposes. It
as an approximation to language understanding.               invites efforts to enrich it by automatic means:
   Here is another way to put it. One measure of             for example, there has been work on parsing the
adequacy of a language digitization is the abil-             English translations and using the word-by-word
ity of a human—already fluent in a reference                 glosses to transfer the parse tree to the object lan-
language—to acquire fluency in the digitized lan-            guage, effectively creating a treebank automati-
guage using only archived material. Now it would             cally (Xia and Lewis, 2007). At the same time, we
be even better if we could use a language digiti-            believe that interlinear glossed text is sufficiently
zation to construct an artificial speaker of the lan-        simple and well-understood to allow rapid con-
guage. Importantly, we do not need to solve the AI           struction of resources, and to make cross-linguistic
problem: the speaker need not decide what to say,            consistency a realistic goal.
only how to translate from meanings to sentences                Each of these layers—primary text, translations,
of the language, and from sentences back to mean-            alignments, and morphological glosses—seems to
ings. Taking sentences in a reference language as            be an unavoidable piece of the overall solution.
the meaning representation, we arrive back at ma-            The fact that these layers will exist in diminishing
chine translation as the measure of success. In              quantity is also unavoidable. However, there is an
short, we have successfully captured a language if           important consequence: the primary texts will be
we can translate into and out of the language.               permanently subject to new translation initiatives,
   The key resource that should be built for each            which themselves will be subject to new align-
language, then, is a collection of primary texts             ment and glossing initiatives, in which each step
with translations into a reference language. “Pri-           is an instance of semisupervised learning (Abney,
mary text” includes both written documents and               2007). As time passes, our ability to enhance the
transcriptions of recordings. Large volumes of pri-          quantity and quality of the annotations will only
mary texts will be useful even without translation           increase, thanks to effective combinations of auto-
for such tasks as language modeling and unsuper-             matic, professional, and crowd-sourced effort.
vised learning of morphology. Thus, we antici-               2.2   Principles
pate that the corpus will have the usual “pyrami-
dal” structure, starting from a base layer of unan-          The basic principles upon which the envisioned
notated text, some portion of which is translated            corpus is based are the following:
into a reference language at the document level to           Universality. Covering as many languages as
make the next layer. Note that, for maximally au-            possible is the first priority. Progress will be
thentic primary texts, we assume the direction of            gauged against concrete goals for numbers of lan-
translation will normally be from primary text to            guages, data per language, and coverage of lan-
reference language, not the other way around.                guage families (Whalen and Simons, 2009).
   Another layer of the corpus consists of sentence
and word alignments, required for training and               Machine readability and consistency. “Cover-
evaluating machine translation systems, and for              ing” languages means enabling machine process-
extracting bilingual lexicons. Curating such anno-           ing seamlessly across languages. This will sup-
tations is a more specialized task than translation,         port new types of linguistic inquiry and the devel-
and so we expect it will only be done for a subset           opment and testing of inference methods (for mor-
of the translated texts.                                     phology, parsers, machine translation) across large
                                                             numbers of typologically diverse languages.
   In the last and smallest layer, morphology is an-
notated. This supports the development of mor-               Community effort. We cannot expect a single
phological analyzers, to preprocess primary texts            organization to assemble a resource on this scale.
to identify morpheme boundaries and recognize                It will be necessary to get community buy-in, and


                                                        90


many motivated volunteers. The repository will              For both written and spoken text. (1) Trans-
not be the sole possession of any one institution.          lations of primary documents into a refer-
                                                            ence language (possibly including commentary).
Availability. The content of the corpus will be
                                                            (2) Sentence-level segmentation and transla-
available under one or more permissive licenses,
                                                            tion. (3) Word-level segmentation and glossing.
such as the Creative Commons Attribution Li-
                                                            (4) Morpheme-level segmentation and glossing.
cense (CC-BY), placing as few limits as possible
                                                               All documents will be included in primary
on community members’ ability to obtain and en-
                                                            form, but the percentage of documents with man-
hance the corpus, and redistribute derivative data.
                                                            ual annotation, or manually corrected annotation,
Utility. The corpus aims to be maximally use-               decreases at increasingly fine-grained levels of an-
ful, and minimally parochial. Annotation will be            notation. Where manual fine-grained annotation is
as lightweight as possible; richer annotations will         unavailable, automatic methods for creating it (at a
will emerge bottom-up as they prove their utility           lower quality) are desirable. Defining such meth-
at the large scale.                                         ods for a large range of resource-poor languages is
                                                            an interesting computational challenge.
Centrality of primary data. Primary texts and
recordings are paramount. Secondary resources               Secondary resources. Although it is possible to
such as grammars and lexicons are important, but            base descriptive analyses exclusively on a text cor-
no substitute for primary data. It is desirable that        pus (Himmelmann, 2006, p. 22), the following
secondary resources be integrated with—if not de-           secondary resources should be secured if they are
rived from—primary data in the corpus.                      available: (1) A lexicon with glosses in a reference
2.3   What to include                                       language. Ideally, everything should be attested in
                                                            the texts, but as a practical matter, there will be
What should be included in the corpus? To some              words for which we have only a lexical entry and
extent, data collection will be opportunistic, but          no instances of use. (2) Paradigms and phonol-
it is appropriate to have a well-defined target in          ogy, for the construction of a morphological ana-
mind. We consider the following essential.                  lyzer. Ideally, they should be inducible from the
Metadata. One means of resource identification              texts, but published grammatical information may
is to survey existing documentation for the lan-            go beyond what is attested in the text.
guage, including bibliographic references and lo-
cations of web resources. Provenance and proper             2.4   Inadequacy of existing efforts
citation of sources should be included for all data.        Our key desideratum is support for automatic pro-
                                                            cessing across a large range of languages. No data
For written text. (1) Primary documents in
                                                            collection effort currently exists or is proposed, to
original printed form, e.g. scanned page images or
                                                            our knowledge, that addresses this desideratum.
PDF. (2) Transcription. Not only optical charac-
                                                            Traditional language archives such as the Audio
ter recognition output, but also the output of tools
                                                            Archive of Linguistic Fieldwork (UC Berkeley),
that extract text from PDF, will generally require
                                                            Documentation of Endangered Languages (Max
manual editing.
                                                            Planck Institute, Nijmegen), the Endangered Lan-
For spoken text. (1) Audio recordings. Both                 guages Archive (SOAS, University of London),
elicited and spontaneous speech should be in-               and the Pacific And Regional Archive for Digi-
cluded. It is highly desirous to have some con-             tal Sources in Endangered Cultures (Australia) of-
nected speech for every language. (2) Slow speech           fer broad coverage of languages, but the majority
“audio transcriptions.” Carefully respeaking a              of their offerings are restricted in availability and
spoken text can be much more efficient than writ-           do not support machine processing. Conversely,
ten transcription, and may one day yield to speech          large-scale data collection efforts by the Linguis-
recognition methods. (3) Written transcriptions.            tic Data Consortium and the European Language
We do not impose any requirements on the form               Resources Association cover less than one percent
of transcription, though orthographic transcription         of the world’s languages, with no evident plans for
is generally much faster to produce than phonetic           major expansion of coverage. Other efforts con-
transcription, and may even be more useful as               cern the definition and aggregation of language
words are represented by normalized forms.                  resource metadata, including OLAC, IMDI, and


                                                       91


CLARIN (Simons and Bird, 2003; Broeder and                       We take an aligned text to be composed of a
Wittenburg, 2006; Váradi et al., 2008), but this is          series of aligned sentences, each consisting of a
not the same as collecting and disseminating data.            small set of attributes and values, e.g.:
   Initiatives to develop standard formats for lin-           ID: europarl/swedish/ep-00-01-17/18
guistic annotations are orthogonal to our goals.              LANGS: swd eng
                                                              SENT: det gäller en ordningsfråga
The success of the project will depend on con-                TRANS: this is a point of order
tributed data from many sources, in many differ-              ALIGN: 1-1 2-2 3-3 4-4 4-5 4-6
ent formats. Converting all data formats to an                PROVENANCE: pharaoh-v1.2, ...
                                                              REV: 8947 2010-05-02 10:35:06 leobfld12
official standard, such as the RDF-based models               RIGHTS: Copyright (C) 2010 Uni...; CC-BY
being developed by ISO Technical Committee 37
Sub-committee 4 Working Group 2, is simply im-                The value of ID identifies the document and sen-
practical. These formats have onerous syntactic               tence, and any collection to which the document
and semantic requirements that demand substan-                belongs. Individual components of the identi-
tial further processing together with expert judg-            fier can be referenced or retrieved. The LANGS
ment, and threaten to crush the large-scale collab-           attribute identifies the source and reference lan-
orative data collection effort we envisage, before            guage using ISO 639 codes.2 The SENT attribute
it even gets off the ground. Instead, we opt for a            contains space-delimited tokens comprising a sen-
very lightweight format, sketched in the next sec-            tence. Optional attributes TRANS and ALIGN
tion, to minimize the effort of conversion and en-            hold the translation and alignment, if these are
able an immediate start. This does not limit the              available; they are omitted in monolingual text.
options of community members who desire richer                A provenance attribute records any automatic or
formats, since they are free to invest the effort in          manual processes which apply to the record, and
enriching the existing data. Such enrichment ef-              a revision attribute contains the version number,
forts may gain broad support if they deliver a tan-           timestamp, and username associated with the most
gible benefit for cross-language processing.                  recent modification of the record, and a rights at-
                                                              tribute contains copyright and license information.
3   A Simple Storage Model                                       When morphological annotation is available, it
                                                              is represented by two additional attributes, LEX
Here we sketch a simple approach to storage of
                                                              and AFF. Here is a monolingual example:
texts (including transcribed speech), bitexts, inter-
linear glossed text, and lexicons. We have been               ID: example/001
                                                              LANGS: eng
deliberately schematic since the goal is just to give         SENT: the dogs are barking
grounds for confidence that there exists a general,           LEX: the dog be bark
                                                              AFF: - PL PL ING
scalable solution.
   For readability, our illustrations will include            Note that combining all attributes of these
space-separated sequences of tokens. However,                 two examples—that is, combining word-by-word
behind the scenes these could be represented as               translation with morphological analysis—yields
a sequence of pairs of start and end offsets into a           interlinear glossed text.
primary text or speech signal, or as a sequence of               A bilingual lexicon is an indispensable re-
integers that reference an array of strings. Thus,            source, whether provided as such, induced from
when we write (1a), bear in mind it may be imple-             a collection of aligned text, or created by merg-
mented as (1b) or (1c).                                       ing contributed and induced lexicons. A bilin-
                                                              gual lexicon can be viewed as an inventory of
(1) a.    This is a point of order .
                                                              cross-language correspondences between words
    b.    (0,4), (5,7), (8,9), (10,15), (16,18), . . .        or groups of words. These correspondences are
    c.    9347, 3053, 0038, 3342, 3468, . . .                 just aligned text fragments, albeit much smaller
                                                              than a sentence. Thus, we take a bilingual lexicon
   In what follows, we focus on the minimal re-
                                                              to be a kind of text in which each record contains
quirements for storing and disseminating aligned
                                                              a single lexeme and its translation, represented us-
text, not the requirements for efficient in-memory
                                                              ing the LEX and TRANS attributes we have already
data structures. Moreover, we are agnostic about
                                                              introduced, e.g.:
whether the normalized, tokenized format is stored
                                                                 2
entire or computed on demand.                                        http://www.sil.org/iso639-3/


                                                         92


ID: swedishlex/v3.2/0419                                    4.1   Motivation for data providers
LANGS: swd eng
LEX: ordningsfråga                                         We hope that potential contributors of data will
TRANS: point of order                                       be motivated to participate primarily by agree-
                                                            ment with the goals of the project. Even some-
   In sum, the Universal Corpus is represented as           one who has specialized in a particular language
a massive store of records, each representing a             or language family maintains an interest, we ex-
single sentence or lexical entry, using a limited           pect, in the universal question—the exploration of
set of attributes. The store is indexed for effi-           Language writ large.
cient access, and supports access to slices identi-            Data providers will find benefit in the availabil-
fied by language, content, provenance, rights, and          ity of volunteers for crowd-sourcing, and tools for
so forth. Many component collections would be               (semi-)automated quality control, refinement, and
“unioned” into this single, large Corpus, with only         presentation of data. For example, a data holder
the record identifiers capturing the distinction be-        should be able to contribute recordings and get
tween the various data sources.                             help in transcribing them, through a combination
   Special cases of aligned text and wordlists,             of volunteer labor and automatic processing.
spanning more than 1,000 languages, are Bible                  Documentary linguists and computational lin-
translations and Swadesh wordlists (Resnik et al.,          guists have much to gain from collaboration. In re-
1999; Swadesh, 1955). Here there are obvious                turn for the data that documentary linguistics can
use-cases for accessing a particular verse or word          provide, computational linguistics has the poten-
across all languages. However, it is not neces-             tial to revolutionize the tools and practice of lan-
sary to model n-way language alignments. In-                guage documentation.
stead, such sources are implicitly aligned by virtue           We also seek collaboration with communities of
of their structure. Extracting all translations of          language speakers. The corpus provides an econ-
a verse, or all cognates of a Swadesh wordlist              omy of scale for the development of literacy mate-
item, is an index operation that returns monolin-           rials and tools for interactive language instruction,
gual records, e.g.:                                         in support of language preservation and revitaliza-
    ID: swadesh/47         ID: swadesh/47                   tion. For small languages, literacy in the mother
    LANGS: fra             LANGS: eng                       tongue is often defended on the grounds that it pro-
    LEX: chien             LEX: dog
                                                            vides the best route to literacy in the national lan-
                                                            guage (Wagner, 1993, ch. 8). An essential ingredi-
4   Building the Corpus
                                                            ent of any local literacy program is to have a sub-
                                                            stantial quantity of available texts that represent
Data collection on this scale is a daunting
                                                            familiar topics including cultural heritage, folk-
prospect, yet it is important to avoid the paraly-
                                                            lore, personal narratives, and current events. Tran-
sis of over-planning. We can start immediately by
                                                            sition to literacy in a language of wider commu-
leveraging existing infrastructure, and the volun-
                                                            nication is aided when transitional materials are
tary effort of interested members of the language
                                                            available (Waters, 1998, pp. 61ff). Mutual bene-
resources community. One possibility is to found
                                                            fits will also flow from the development of tools
a “Language Commons,” an open access reposi-
                                                            for low-cost publication and broadcast in the lan-
tory of language resources hosted in the Internet
                                                            guage, with copies of the published or broadcast
Archive, with a lightweight method for commu-
                                                            material licensed to and archived in the corpus.
nity members to contribute data sets.
   A fully processed and indexed version of se-             4.2   Roles
lected data can be made accessible via a web ser-
                                                            The enterprise requires collaboration of many in-
vices interface to a major cloud storage facility,
                                                            dividuals and groups, in a variety of roles.
such as Amazon Web Services. A common query
interface could be supported via APIs in multi-             Editors. A critical group are people with suffi-
ple NLP toolkits such as NLTK and GATE (Bird                cient engagement to serve as editors for particular
et al., 2009; Cunningham et al., 2002), and also            language families, who have access to data or are
in generic frameworks such as UIMA and SOAP,                able to negotiate redistribution rights, and oversee
leaving developers to work within their preferred           the workflow of transcription, translation, and an-
environment.                                                notation.


                                                       93


CL Research. All manual annotation steps need                 Data agencies. The LDC and ELRA have a cen-
to be automated. Each step presents a challeng-               tral role to play, given their track record in obtain-
ing semi-supervised learning and cross-linguistic             ing, curating, and publishing data with licenses
bootstrapping problem. In addition, the overall               that facilitate language technology development.
measure of success—induction of machine trans-                We need to identify key resources where negoti-
lation systems from limited resources—pushes the              ation with the original data provider, and where
state of the art (Kumar et al., 2007). Numerous               payment of all preparation costs plus compensa-
other CL problems arise: active learning to im-               tion for lost revenue, leads to new material for the
prove the quality of alignments and bilingual lex-            Corpus. This is a new publication model and a
icons; automatic language identification for low-             new business model, but it can co-exist with the
density languages; and morphology learning.                   existing models.
                                                              Language archives. Language archives have a
Tool builders. We need tools for annotation, for-             special role to play as holders of unique materi-
mat conversion, spidering and language identifica-            als. They could contribute existing data in its na-
tion, search, archiving, and presentation. Innova-            tive format, for other participants to process. They
tive crowd-sourcing solutions are of particular in-           could give bilingual texts a distinct status within
terest, e.g. web-based functionality for transcrib-           their collections, to facilitate discovery.
ing audio and video of oral literature, or setting up
a translation service based on aligned texts for a            Funding agencies. To be successful, the Human
low-density language, and collecting the improved             Language Project would require substantial funds,
translations suggested by users.                              possibly drawing on a constellation of public and
                                                              private agencies in many countries. However, in
Volunteer annotators. An important reason for                 the spirit of starting small, and starting now, agen-
keeping the data model as lightweight as possible             cies could require that sponsored projects which
is to enable contributions from volunteers with lit-          collect texts and build lexicons contribute them to
tle or no linguistic training. Two models are the             the Language Commons. After all, the most effec-
volunteers who scan documents and correct OCR                 tive time to do translation, alignment, and lexicon
output in Project Gutenberg, or the undergraduate             work is often at the point when primary data is
volunteers who have constructed Greek and Latin               first collected, and this extra work promises direct
treebanks within Project Perseus (Crane, 2010).               benefits to the individual project.
Bilingual lexicons that have been extracted from              4.3   Early tasks
aligned text collections might be corrected using
crowd-sourcing, leading to improved translation               Seed corpus. The central challenge, we believe,
models and improved alignments. We also see the               is getting critical mass. Data attracts data, and if
Universal Corpus as an excellent opportunity for              one can establish a sufficient seed, the effort will
undergraduates to participate in research, and for            snowball. We can make some concrete proposals
native speakers to participate in the preservation of         as to how to collect a seed. Language resources
their language.                                               on the web are one source—the Crúbadán project
                                                              has identified resources for 400 languages, for ex-
                                                              ample (Scannell, 2008); the New Testament of the
Documentary linguists. The collection proto-                  Bible exists in about 1200 languages and contains
col known as Basic Oral Language Documentation                of the order of 100k words. We hope that exist-
(BOLD) enables documentary linguists to collect               ing efforts that are already well-disposed toward
2–3 orders of magnitude more oral discourse than              electronic distribution will participate. We partic-
before (Bird, 2010). Linguists can equip local                ularly mention the Language and Culture Archive
speakers to collect written texts, then to carefully          of the Summer Institute of Linguistics, and the
“respeak” and orally translate the texts into a refer-        Rosetta Project. The latter is already distributed
ence language. With suitable tools, incorporating             through the Internet Archive and contains material
active learning, local speakers could further curate          for 2500 languages.
bilingual texts and lexicons. An early need is pi-
lot studies to determine costings for different cat-          Resource discovery. Existing language re-
egories of language.                                          sources need to be documented, a large un-


                                                         94


dertaking that depends on widely distributed                Audio protocol. The challenge posed by lan-
knowledge. Existing published corpora from the              guages with no written literature should not be
LDC, ELRA and dozens of other sources—a total               underestimated. A promising collection method
of 85,000 items—are already documented in the               is Basic Oral Language Documentation, which
combined catalog of the Open Language Archives              calls for inexpensive voice recorders and net-
Community,3 so there is no need to recreate this            books, project-specific software for transcription
information. Other resources can be logged by               and sentence-aligned translation, network band-
community members using a public access wiki,               width for upload to the repository, and suitable
with a metadata template to ensure key fields are           training and support throughout the process.
elicited such as resource owner, license, ISO 639
language code(s), and data type. This information           Corpus readers. Software developers will in-
can itself be curated and stored in the form of an          spect the file formats and identify high priority for-
OLAC archive, to permit search over the union of            mats based on information about resource priori-
the existing and newly documented items. Work               ties and sizes. They will code a corpus reader, an
along these lines has already been initiated by             open source reference implementation for convert-
LDC and ELRA (Cieri et al., 2010).                          ing between corpus formats and the storage model
                                                            presented in section 3.
Resource classification. Editors with knowl-
edge of particular language families will catego-           4.4   Further challenges
rize documented resources relative to the needs of          There are many additional difficulties that could
the project, using controlled vocabularies. This            be listed, though we expect they can be addressed
involves examining a resource, determining the              over time, once a sufficient seed corpus is estab-
granularity and provenance of the segmentation              lished. Two particular issues deserve further com-
and alignment, checking its ISO 639 classifi-               ment, however.
cations, assigning it to a logarithmic size cate-
gory, documenting its format and layout, collect-           Licenses. Intellectual property issues surround-
ing sample files, and assigning a priority score.           ing linguistic corpora present a complex and
                                                            evolving landscape (DiPersio, 2010). For users, it
Acquisition. Where necessary, permission will               would be ideal for all materials to be available un-
be sought to lodge the resource in the repository.          der a single license that permits derivative works,
Funding may be required to buy the rights to the            commercial use, and redistribution, such as the
resource from its owner, as compensation for lost           Creative Commons Attribution License (CC-BY).
revenue from future data sales. Funding may be              There would be no confusion about permissible
required to translate the source into a reference           uses of subsets and aggregates of the collected cor-
language. The repository’s ingestion process is             pora, and it would be easy to view the Universal
followed, and the resource metadata is updated.             Corpus as a single corpus. But to attract as many
                                                            data contributors as possible, we cannot make such
Text collection. Languages for which the avail-
                                                            a license a condition of contribution.
able resources are inadequate are identified, and
the needs are prioritized, based on linguistic and             Instead, we propose to distinguish between:
geographical diversity. Sponsorship is sought               (1) a digital Archive of contributed corpora that
for collecting bilingual texts in high priority lan-        are stored in their original format and made avail-
guages. Workflows are developed for languages               able under a range of licenses, offering preserva-
based on a variety of factors, such as availability         tion and dissemination services to the language
of educated people with native-level proficiency            resources community at large (i.e. the Language
in their mother tongue and good knowledge of                Commons); and (2) the Universal Corpus, which
a reference language, internet access in the lan-           is embodied as programmatic access to an evolv-
guage area, availability of expatriate speakers in a        ing subset of materials from the archive under
first-world context, and so forth. A classification         one of a small set of permissive licenses, licenses
scheme is required to help predict which work-              whose unions and intersections are understood
flows will be most successful in a given situation.         (e.g. CC-BY and its non-commercial counterpart
                                                            CC-BY-NC). Apart from being a useful service in
   3
       http://www.language-archives.org/                    its own right, the Archive would provide a staging


                                                       95


ground for the Universal Corpus. Archived cor-                tional linguistics is yet to participate substantially.
pora having restrictive licenses could be evaluated              The first half century of research in compu-
for their potential as contributions to the Corpus,           tational linguistics—from circa 1960 up to the
making it possible to prioritize the work of nego-            present—has touched on less than 1% of the
tiating more liberal licenses.                                world’s languages. For a field which is justly
   There are reasons to distinguish Archive and               proud of its empirical methods, it is time to apply
Corpus even beyond the license issues. The Cor-               those methods to the remaining 99% of languages.
pus, but not the Archive, is limited to the formats           We will never have the luxury of richly annotated
that support automatic cross-linguistic processing.           data for these languages, so we are forced to ask
Conversely, since the primary interface to the Cor-           ourselves: can we do more with less?
pus is programmatic, it may include materials that               We believe the answer is “yes,” and so we chal-
are hosted in many different archives; it only needs          lenge the computational linguistics community to
to know how to access and deliver them to the user.           adopt a scalable computational approach to the
Incidentally, we consider it an implementation is-            problem. We need leaner methods for building
sue whether the Corpus is provided as a web ser-              machine translation systems; new algorithms for
vice, a download service with user-side software,             cross-linguistic bootstrapping via multiple paths;
user-side software with data delivered on physical            more effective techniques for leveraging human
media, or a cloud application with user programs              effort in labeling data; scalable ways to get bilin-
executed server-side.                                         gual text for unwritten languages; and large scale
                                                              social engineering to make it all happen quickly.
Expenses of conversion and editing. We do not
                                                                 To believe we can build this Universal Corpus is
trivialize the work involved in converting docu-
                                                              certainly audacious, but not to even try is arguably
ments to the formats of section 3, and in manu-
                                                              irresponsible. The initial step parallels earlier ef-
ally correcting the results of noisy automatic pro-
                                                              forts to create large machine-readable text collec-
cesses such as optical character recognition. In-
                                                              tions which began in the 1960s and reverberated
deed, the amount of work involved is one moti-
                                                              through each subsequent decade. Collecting bilin-
vation for the lengths to which we have gone to
                                                              gual texts is an orthodox activity, and many alter-
keep the data format simple. For example, we have
                                                              native conceptions of a Human Language Project
deliberately avoided specifying any particular to-
                                                              would likely include this as an early task.
kenization scheme. Variation will arise as a con-
                                                                 The undertaking ranks with the largest data-
sequence, but we believe that it will be no worse
                                                              collection efforts in science today. It is not achiev-
than the variability in input that current machine
                                                              able without considerable computational sophis-
translation training methods routinely deal with,
                                                              tication and the full engagement of the field of
and will not greatly injure the utility of the Corpus.
                                                              computational linguistics. Yet we require no fun-
The utter simplicity of the formats also widens the
                                                              damentally new technologies. We can build on
pool of potential volunteers for doing the manual
                                                              our strengths in corpus-based methods, linguis-
work that is required. By avoiding linguistically
                                                              tic models, human- and machine-supplied annota-
delicate annotation, we can take advantage of mo-
                                                              tions, and learning algorithms. By rising to this,
tivated but untrained volunteers such as students
                                                              the greatest language challenge of our time, we
and members of speaker communities.
                                                              enable multi-lingual technology development at a
5   Conclusion                                                new scale, and simultaneously lay the foundations
                                                              for a new science of empirical universal linguis-
Nearly twenty years ago, the linguistics commu-               tics.
nity received a wake-up call, when Hale et al.
(1992) predicted that 90% of the world’s linguis-             Acknowledgments
tic diversity would be lost or moribund by the year
                                                              We are grateful to Ed Bice, Doug Oard, Gary
2100, and warned that linguistics might “go down
                                                              Simons, participants of the Language Commons
in history as the only science that presided oblivi-
                                                              working group meeting in Boston, students in
ously over the disappearance of 90 per cent of the
                                                              the “Digitizing Languages” seminar (University of
very field to which it is dedicated.” Today, lan-
                                                              Michigan), and anonymous reviewers, for feed-
guage documentation is a high priority in main-
                                                              back on an earlier version of this paper.
stream linguistics. However, the field of computa-


                                                         96


References                                                     Shankar Kumar, Franz J. Och, and Wolfgang
                                                                 Macherey. 2007. Improving word alignment with
Steven Abney. 2007. Semisupervised Learning for                  bridge languages. In Proceedings of the 2007 Joint
   Computational Linguistics. Chapman & Hall/CRC.                Conference on Empirical Methods in Natural Lan-
Steven Bird, Ewan Klein, and Edward Loper.                       guage Processing and Computational Natural Lan-
   2009. Natural Language Processing with Python.                guage Learning (EMNLP-CoNLL), pages 42–50,
   O’Reilly Media. http://nltk.org/book.                         Prague, Czech Republic. Association for Computa-
                                                                 tional Linguistics.
Steven Bird. 2010. A scalable method for preserving
   oral literature from small languages. In Proceedings        Mike Maxwell and Baden Hughes. 2006. Frontiers
   of the 12th International Conference on Asia-Pacific          in linguistic annotation for lower-density languages.
   Digital Libraries, pages 5–14.                                In Proceedings of the Workshop on Frontiers in Lin-
                                                                 guistically Annotated Corpora 2006, pages 29–37,
Daan Broeder and Peter Wittenburg. 2006. The IMDI                Sydney, Australia, July. Association for Computa-
  metadata framework, its current application and fu-            tional Linguistics.
  ture direction. International Journal of Metadata,
                                                               Philip Resnik, Mari Broman Olsen, and Mona Diab.
  Semantics and Ontologies, 1:119–132.
                                                                 1999. The Bible as a parallel corpus: Annotating
Christopher Cieri, Khalid Choukri, Nicoletta Calzo-              the ‘book of 2000 tongues’. Computers and the Hu-
  lari, D. Terence Langendoen, Johannes Leveling,                manities, 33:129–153.
  Martha Palmer, Nancy Ide, and James Pustejovsky.
                                                               Kevin Scannell. 2008. The Crúbadán Project: Corpus
  2010. A road map for interoperable language re-
                                                                 building for under-resourced languages. In Cahiers
  source metadata. In Proceedings of the 7th Interna-
                                                                 du Cental 5: Proceedings of the 3rd Web as Corpus
  tional Conference on Language Resources and Eval-
                                                                 Workshop.
  uation (LREC).
                                                               Gary Simons and Steven Bird. 2003. The Open Lan-
Gregory R. Crane. 2010. Perseus Digital Library:                 guage Archives Community: An infrastructure for
  Research in 2008/09. http://www.perseus.                       distributed archiving of language resources. Liter-
  tufts.edu/hopper/research/current.                             ary and Linguistic Computing, 18:117–128.
  Accessed Feb. 2010.
                                                               Morris Swadesh. 1955. Towards greater accuracy
Hamish Cunningham, Diana Maynard, Kalina                        in lexicostatistic dating. International Journal of
  Bontcheva, and Valentin Tablan. 2002. GATE: an                American Linguistics, 21:121–137.
  architecture for development of robust HLT appli-
  cations. In Proceedings of 40th Annual Meeting               Tamás Váradi, Steven Krauwer, Peter Wittenburg,
  of the Association for Computational Linguistics,              Martin Wynne, and Kimmo Koskenniemi. 2008.
  pages 168–175. Association for Computational                   CLARIN: common language resources and technol-
  Linguistics.                                                   ogy infrastructure. In Proceedings of the Sixth Inter-
                                                                 national Language Resources and Evaluation Con-
Denise DiPersio. 2010. Implications of a permis-                 ference. European Language Resources Association.
  sions culture on the development and distribution
  of language resources. In FLaReNet Forum 2010.               Daniel A. Wagner. 1993. Literacy, Culture, and Devel-
  Fostering Language Resources Network. http:                    opment: Becoming Literate in Morocco. Cambridge
  //www.flarenet.eu/.                                            University Press.
Hale, M. Krauss, L. Watahomigie, A. Yamamoto, and              Glenys Waters. 1998. Local Literacies: Theory and
  C. Craig. 1992. Endangered languages. Language,                Practice. Summer Institute of Linguistics, Dallas.
  68(1):1–42.
                                                               Douglas H. Whalen and Gary Simons. 2009. En-
Nikolaus P. Himmelmann. 2006. Language documen-                  dangered language families. In Proceedings of the
  tation: What is it and what is it good for? In                 1st International Conference on Language Docu-
  Jost Gippert, Nikolaus Himmelmann, and Ulrike                  mentation and Conservation. University of Hawaii.
  Mosel, editors, Essentials of Language Documenta-              http://hdl.handle.net/10125/5017.
  tion, pages 1–30. Mouton de Gruyter.
                                                               Anthony C. Woodbury. 2010. Language documenta-
Human Genome Project.   2007.   The science                      tion. In Peter K. Austin and Julia Sallabank, edi-
  behind the Human Genome Project.  http:                        tors, The Cambridge Handbook of Endangered Lan-
  //www.ornl.gov/sci/techresources/                              guages. Cambridge University Press.
  Human_Genome/project/info.shtml.
  Accessed Dec. 2007.                                          Fei Xia and William D. Lewis. 2007. Multilingual
                                                                 structural projection across interlinearized text. In
W. John Hutchins and Harold L. Somers. 1992. An In-              Proceedings of the Meeting of the North American
  troduction to Machine Translation. Academic Press.             Chapter of the Association for Computational Lin-
                                                                 guistics (NAACL). Association for Computational
Philipp Koehn. 2010. Statistical Machine Translation.            Linguistics.
  Cambridge University Press.


                                                          97

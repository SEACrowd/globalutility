          Bilingual Lexicon Generation Using Non-Aligned Signatures

                     Daphna Shezaf                                        Ari Rappoport
             Institute of Computer Science                        Institute of Computer Science
            Hebrew University of Jerusalem                       Hebrew University of Jerusalem
             daphna.shezaf@mail.huji.ac.il                               arir@cs.huji.ac.il



                     Abstract                                 Section 2). However, such lexicons are noisy. In
    Bilingual lexicons are fundamental re-                    this paper we present a method for generating a
    sources. Modern automated lexicon gen-                    high quality lexicon given such a noisy one. Our
    eration methods usually require parallel                  evaluation focuses on the pivot language case.
    corpora, which are not available for most                    Pivot language approaches deal with the
    language pairs. Lexicons can be gener-                    scarcity of bilingual data for most language pairs
    ated using non-parallel corpora or a pivot                by relying on the availability of bilingual data for
    language, but such lexicons are noisy.                    each of the languages in question with a third,
    We present an algorithm for generating                    pivot, language. In practice, this third language
    a high quality lexicon from a noisy one,                  is often English.
    which only requires an independent cor-                      A naive method for pivot-based lexicon genera-
    pus for each language. Our algorithm in-                  tion goes as follows. For each source headword1 ,
    troduces non-aligned signatures (NAS), a                  take its translations to the pivot language using the
    cross-lingual word context similarity score               source-to-pivot lexicon, then for each such transla-
    that avoids the over-constrained and inef-                tion take its translations to the target language us-
    ficient nature of alignment-based methods.                ing the pivot-to-target lexicon. This method yields
    We use NAS to eliminate incorrect transla-                highly noisy (‘divergent’) lexicons, because lexi-
    tions from the generated lexicon. We eval-                cons are generally intransitive. This intransitivity
    uate our method by improving the quality                  stems from polysemy in the pivot language that
    of noisy Spanish-Hebrew lexicons gener-                   does not exist in the source language. For ex-
    ated from two pivot English lexicons. Our                 ample, take French-English-Spanish. The English
    algorithm substantially outperforms other                 word spring is the translation of the French word
    lexicon generation methods.                               printemps, but only in the season of year sense.
                                                              Further translating spring into Spanish yields both
1   Introduction                                              the correct translation primavera and an incorrect
Bilingual lexicons are useful for both end users              one, resorte (the elastic object).
and computerized language processing tasks.                      To cope with the issue of divergence due to lex-
They provide, for each source language word or                ical intransitivity, we present an algorithm for as-
phrase, a set of translations in the target language,         sessing the correctness of candidate translations.
and thus they are a basic component of dictio-                The algorithm is quite simple to understand and
naries, which also include syntactic information,             to implement and is computationally efficient. In
sense division, usage examples, semantic fields,              spite of its simplicity, we are not aware of previous
usage guidelines, etc.                                        work applying it to our problem.
   Traditionally, when bilingual lexicons are not                The algorithm utilizes two monolingual cor-
compiled manually, they are extracted from par-               pora, comparable in their domain but otherwise
allel corpora. However, for most language pairs               unrelated, in the source and target languages. It
parallel bilingual corpora either do not exist or are         does not need a pivot language corpus. The al-
at best small and unrepresentative of the general             gorithm comprises two stages: signature genera-
language.                                                        1
                                                                  In this paper we focus on single word head entries.
   Bilingual lexicons can be generated using non-             Multi-word expressions form a major topic in NLP and their
parallel corpora or pivot language lexicons (see              handling is deferred to future work.


                                                         98
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98–107,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


tion and signature ranking. The signature of word             intersection for a correct translation pair printemps
w is the set of words that co-occur with w most               and primavera may include two synonym words,
strongly. While co-occurrence scores are used                 spring and springtime. Variations of this method
to compute signatures, signatures, unlike context             were proposed by (Kaji and Aizono, 1996; Bond
vectors, do not contain the score values. For                 et al., 2001; Paik et al., 2004; Ahn and Frampton,
each given source headword we compute its sig-                2006).
nature and the signatures of all of its candidate                One weakness of IC is that it relies on pivot lan-
translations. We present the non-aligned signa-               guage synonyms to identify correct translations.
tures (NAS) similarity score for signature and use            In the above example, if the relatively rare spring-
it to rank these translations. NAS is based on the            time had not existed or was missing from the input
number of headword signature words that may be                lexicons, IC would not have been able to discern
translated using the input noisy lexicon into words           that primavera is a correct translation. This may
in the signature of a candidate translation.                  result in low recall.
   We evaluate our algorithm by generating a
bilingual lexicon for Hebrew and Spanish using                2.2.2   Multiple Pivot Languages
pivot Hebrew-English and English-Spanish lexi-                Mausam et al. (2009) used many input bilingual
cons compiled by a professional publishing house.             lexicons to create bilingual lexicons for new lan-
We show that the algorithm outperforms exist-                 guage pairs. They represent the multiple input
ing algorithms for handling divergence induced by             lexicons in a single undirected graph, with words
lexical intransitivity.                                       from all the lexicons as nodes. The input lexi-
                                                              cons translation pairs define the edges in the graph.
2     Previous Work                                           New translation pairs are inferred based on cycles
                                                              in the graph, that is, the existence of multiple paths
2.1    Parallel Corpora
                                                              between two words in different languages.
Parallel corpora are often used to infer word-                   In a sense, this is a generalization of the pivot
oriented machine-readable bilingual lexicons. The             language idea, where multiple pivots are used. In
texts are aligned to each other, at chunk- and/or             the example above, if both English and German
word-level. Alignment is generally evaluated by               are used as pivots, printemps and primavera would
consistency (source words should be translated to             be accepted as correct because they are linked by
a small number of target words over the entire cor-           both English spring and German Fruehling, while
pus) and minimal shifting (in each occurrence, the            printemps and resorte are not linked by any Ger-
source should be aligned to a translation nearby).            man pivot. This multiple-pivot idea is similar to
For a review of such methods see (Lopez, 2008).               Inverse Consultation in that multiple pivots are re-
The limited availability of parallel corpora of suffi-        quired, but using multiple pivot languages frees it
cient size for most language pairs restricts the use-         from the dependency on rich input lexicons that
fulness of these methods.                                     contain a variety of synonyms. This is replaced,
2.2    Pivot Language Without Corpora                         however, with the problem of coming up with mul-
                                                              tiple suitable input lexicons.
2.2.1 Inverse Consultation
Tanaka and Umemura (1994) generated a bilin-                  2.2.3   Micro-Structure of Dictionary Entries
gual lexicon using a pivot language. They ap-                 Dictionaries published by a single publishing
proached lexical intransitivity divergence using              house tend to partition the semantic fields of head-
Inverse Consultation (IC). IC examines the inter-             words in the same way. Thus the first translation
section of two pivot language sets: the set of pivot          of some English headword in the English-Spanish
translations of a source-language word w, and the             and in the English-Hebrew dictionaries would cor-
set of pivot translations of each target-language             respond to the same sense of the headword, and
word that is a candidate for being a translation              would therefore constitute translations of each
to w. IC generally requires that the intersection             other. The applicability of this method is lim-
set contains at least two words, which are syn-               ited by the availability of machine-readable dic-
onyms. For example, the intersection of the En-               tionaries produced by the same publishing house.
glish translations of French printemps and Spanish            Not surprisingly, this method has been proposed
resorte contains only a single word, spring. The              by lexicographers working in such companies (Sk-


                                                         99


oumalova, 2001).                                              relies on alignments of 3-word cliques in each
                                                              language, every pair of which frequently co-
2.3   Cross-lingual Co-occurrences in Lexicon                 occurring. This is a relatively rare occurrence,
      Construction                                            which may explain the low recall rates of their re-
Rapp (1999) and Fung (1998) discussed seman-                  sults.
tic similarity estimation using cross-lingual con-
text vector alignment. Both works rely on a                   3     Algorithm
pre-existing large (16-20K entries), correct, one-            Our algorithm transforms a noisy lexicon into a
to-one lexicon between the source and target                  high quality one. As explained above, in this paper
languages, which is used to align context vec-                we focus on noisy lexicons generated using pivot
tors between languages. The context vector                    language lexicons. Other methods for obtaining
data was extracted from comparable (monolingual               an initial noisy lexicon could be used as well; their
but domain-related) corpora. Koehn and Knight                 evaluation is deferred to future work.
(2002) were able to do without the initial large lex-
                                                                 In the setting evaluated in this paper, we first
icon by limiting themselves to related languages
                                                              generate an initial noisy lexicon iLex possibly
that share a writing system, and using identically-
                                                              containing many translation candidates for each
spelled words as context words. Garera et al.
                                                              source headword. iLex is computed from two
(2009) and Pekar et al. (2006) suggested different
                                                              pivot-language lexicons, and is the only place in
methods for improving the context vectors data in
                                                              which the algorithm utilizes the pivot language.
each language before aligning them. Garera et al.
                                                              Afterwards, for each source headword, we com-
(2009) replaced the traditional window-based co-
                                                              pute its signature and the signatures of each of its
occurrence counting with dependency-tree based
                                                              translation candidates. Signature computation uti-
counting, while Pekar et al. (2006) predicted miss-
                                                              lizes a monolingual corpus to discover the words
ing co-occurrence values based on similar words
                                                              that are most strongly related to the word. We now
in the same language. In the latter work, the one-
                                                              rank the candidates according to the non-aligned
to-one lexicon assumption was not made: when
                                                              signatures (NAS) similarity score, which assesses
a context word had multiple equivalents, it was
                                                              the similarity between each candidate’s signature
mapped into all of them, with the original prob-
                                                              and that of the headword. For each headword,
ability equally distributed between them.
                                                              we select the t translations with the highest NAS
Pivot Language. Using cross-lingual co-                       scores as correct translations.
occurrences to improve a lexicon generated using
                                                              3.1    Input Resources
a pivot language was suggested by Tanaka and
Iwasaki (1996). Schafer and Yarowsky (2002)                   The resources required by our algorithm as evalu-
created lexicons between English and a target                 ated in this paper are: (a) two bilingual lexicons,
local language (e.g. Gujarati) using a related                one from the source to the pivot language and the
language (e.g. Hindi) as pivot. An English pivot              other from the pivot to the target language. In
lexicon was used in conjunction with pivot-target             principle, these two pivot lexicons can be noisy,
cognates. Cross-lingual co-occurrences were used              although in our evaluation we use manually com-
to remove errors, together with other cues such as            piled lexicons; (b) two monolingual corpora, one
edit distance and Inverse Document Frequencies                for each of the source and target languages. We
(IDF) scores. It appears that this work assumed a             have tested the method with corpora of compa-
single alignment was possible from English to the             rable domains, but not covering the same well-
target language.                                              defined subjects (the corpora contain news from
   Kaji et al. (2008) used a pivot English lexicon            different countries and over non-identical time pe-
to generate initial Japanese-Chinese and Chinese-             riods).
Japanese lexicons, then used co-occurrences in-
formation, aligned using the initial lexicon, to              3.2    Initial Lexicon Construction
identify correct translations. Unlike other works,            We create an initial lexicon from the source to the
which require alignments of pairs (i.e., two co-              target language using the pivot language: we look
occurring words in one language translatable into             up each source language word s in the source-
two co-occurring words in the other), this method             pivot lexicon, and obtain the set Ps of its pivot


                                                        100


translations. We then look up each of the mem-                      Language    Sites                       Tokens
bers of Ps in the pivot-target lexicon, and obtain                  Hebrew      haartz.co.il, ynet.co.il,   510M
a set Ts of candidate target translations. iLex is                              nrg.co.il
therefore a mapping from the set of source head-                    Spanish     elpais.com,                 560M
words to the set of candidate target translations.                              elmundo.com, abc.es
Note that it is possible that not all target lexicon
words appear as translation candidates. To create                        Table 1: Hebrew corpus data.
a target to source lexicon, we repeat the process
with the directions reversed.                                lexicon, iLex, throughout this work, we usually
                                                             omit the L subscript when referring to NAS.
3.3   Signatures
The signature of a word w in a language is the               4     Lexicon Generation Experiments
set of N words most strongly related to w. There
are various possible ways to formalize this notion.          We tested our algorithm by generating bilingual
We use a common and simple one, the words hav-               lexicons for Hebrew and Spanish, using English
ing the highest tendency to co-occur with w in a             as a pivot language. We chose a language pair for
corpus. We count co-occurrences using a sliding              which basically no parallel corpora exist2 , and that
fixed-length window of size k. We compute, for               do not share ancestry or writing system in a way
each pair of words, their Pointwise Mutual Infor-            that can provide cues for alignment.
mation (PMI), that is:                                          We conducted the test twice: once creating
                                                             a Hebrew-Spanish lexicon, and once creating a
                                P r(w1 , w2 )                Spanish-Hebrew one.
      P M I(w1 , w2 ) = log
                              P r(w1 )P r(w2 )
                                                             4.1    Experimental Setup
where P r(w1 , w2 ) is the co-occurrence count, and
P r(wi ) is the total number of appearance of wi             4.1.1 Corpora
in the corpus (Church and Hanks, 1990). We de-               The Hebrew and Spanish corpora were extracted
fine the signature G(w)N,k of w to be the set of N           from Israeli and Spanish newspaper websites re-
words with the highest PMI with w.                           spectively (see table 1 for details). Crawling a
   Note that a word’s signature includes words in            small number of sites allowed us to use special-
the same language. Therefore, two signatures of              tailored software to extract the textual data from
words in different languages cannot be directly              the web pages, thus improving the quality of the
compared; we compare them using a lexicon L as               extracted texts. Our two corpora are comparable
explained below.                                             in their domains, news and news commentary.
   Signature is a function of w parameterized by                No kind of preprocessing was used for the Span-
N and k. We discuss the selection of these param-            ish corpus. For Hebrew, closed-class words that
eters in section 4.1.5.                                      are attached to the succeeding word (e.g., ‘the’,
                                                             ‘and’, ‘in’) were segmented using a simple un-
3.4   Non-aligned Signatures (NAS) Similarity
                                                             supervised method (Dinur et al., 2009). This
      Scoring
                                                             method compares the corpus frequencies of the
The core strength of our method lies in the way              non-prefixed form x and the prefixed form wx. If x
in which we evaluate similarity between words in             is frequent enough, it is assumed to be the correct
the source and target languages. For a lexicon L,            form, and all the occurrences of wx are segmented
a source word s and a target word t, N ASL (s, t)            into two tokens, w x. This method was chosen for
is defined as the number of words in the signature           being simple and effective. However, the segmen-
G(s)N,k of s that may be translated, using L, to             tation it produces is not perfect. It is context insen-
words in the signature G(t)N,k of t, normalized by           sitive, segmenting all appearances of a token in the
dividing it by N. Formally,                                  same way, while many wx forms are actually am-
                 N ASL (s, t) =                              biguous. Even unambiguous token segmentations
              |{w∈G(s)|L(w)∩G(t)6=∅}|                        may fail when the non-segmented form is very fre-
                        N                                    quent in the domain.
  Where L(x) is the set of candidate translations               2
                                                                  Old testament corpora are for biblical Hebrew, which is
of x under the lexicon L. Since we use a single              very different from modern Hebrew.


                                                       101


               Lexicon    # headwords   BF                    the NAS score is replaced by other scores.
               Eng-Spa      55057       2.4                      IC (see section 2.2.1) is a corpus-less method.
                                                              It ranks t1 , t2 , ..., the candidate translations of a
               Spa-Eng      44349       2.9
                                                              source word s, by the size of the intersections of
              Eng-Heb       48857       2.5
                                                              the sets of pivot translations of ti and s. Note that
              Heb-Eng       33439       3.7                   IC ranking is a partial order, as the intersection
              Spa-Heb       34077       12.6                  size may be the same for many candidate transla-
              Heb-Spa       27591       14.8                  tions. IC is a baseline for our algorithm as a whole.
                                                                 Cosine and city block distances are widely
Table 2: Number of words in lexicons, and branch-             used methods for calculating distances of vectors
ing factors (BF).                                             within the same vector space. They are defined
                                                              here as4
   Hebrew orthography presents additional diffi-                                                    P
                                                                                                       vi ui
culties: there are relatively many homographs, and                     Cosine(v, u) = 1 − √P             P
                                                                                                       vi ui
spelling is not quite standardized. These consid-
erations lead us to believe that our choice of lan-
guage pair is more challenging than, for example,
                                                                                                  X
                                                                      CityBlock(v, u) = −              |vi − ui |
a pair of European languages.                                                                      i

4.1.2     Lexicons                                               In the case of context vectors, the vector in-
The source of the Hebrew-English lexicon was the              dices, or keys, are words, and their values are co-
Babylon on-line dictionary3 . For Spanish-English,            occurrence based scores. We used the words in
we used the union of Babylon with the Oxford                  our signatures as context vector keys, and PMI
English-Spanish lexicon. Since the corpus was                 scores as values. In this way, the two scores are
segmented to words using spaces, lexicon entries              ‘plugged’ into our method and serve as baselines
containing spaces were discarded.                             for our NAS similarity score.
   Lexicon directionality was ignored. All trans-                Since the context vectors are in different lan-
lation pairs extracted for Hebrew-Spanish via En-             guages, we had to translate, or align, the baseline
glish, were also reversed and added to the Spanish-           context vectors for the source and target words.
Hebrew lexicon, and vice-versa. Therefore, every              Our initial lexicon is a many-to-many relation, so
L1-L2 lexicon we mention is identical to the cor-             multiple alignments were possible; in fact, the
responding L2-L1 lexicon in the set of translation            number of possible alignments tends to be very
pairs it contains. Our lexicon is thus the ‘noisi-            large5 . We therefore generated M random possible
est’ that can be generated using a pivot language             alignments, and used the average distance metric
and two source-pivot-target lexicons, but it also             across these alignments.
provides the most complete candidate set possible.            4.1.4   Test Sets and Gold Standard
Ignoring directionality is also in accordance with
                                                              Following other works (e.g. (Rapp, 1999)), and to
the reversibility principle of the lexicographic lit-
                                                              simplify the experimental setup, we focused in our
erature (Tomaszczyk, 1998).
                                                              experiments on nouns.
   Table 2 details the sizes and branching factors
                                                                 A p-q frequency range in a corpus is the set of
(BF) (the average number of translations for head-
                                                              tokens in the places between p and q in the list of
word) of the input lexicons, as well as those of the
                                                              corpus tokens, sorted by frequency from high to
generated initial noisy lexicon.
                                                              low. Two types of test sets were used. The first
4.1.3     Baseline                                            (R1) includes all the singular, correctly segmented
                                                              (in Hebrew) nouns among the 500 words in the
The performance of our method was compared to
                                                              1001-1500 frequency range. The 1000 highest-
three baselines: Inverse Consultation (IC), average
                                                              frequency tokens were discarded, as a large num-
cosine distance, and average city block distance.
                                                              ber of these are utilized as auxiliary syntactic
The first is a completely different algorithm, and
the last two are a version of our algorithm in which             4
                                                                    We modified the standard cosine and city block metrics
                                                              so that for all measures higher values would be better.
   3                                                              5
       www.babylon.com.                                             This is another advantage of our NAS score.


                                                        102


                          R1                    R2                                          R1                    R2
                 Precision     Recall   Precision    Recall                        Precision     Recall   Precision    Recall

      NAS         82.1%        100%       56%        100%               NAS         87.6%        100%       80%        100%
      Cosine      60.7%        100%       28%        100%               Cosine       68%         100%       44%        100%
  City block      56.3%        100%       32%        100%             City block    69.8%        100%       36%        100%
       IC         55.2%        85.7%      52%        88%                  IC        76.4%        100%       48%        92%

Table 3: Hebrew-Spanish lexicon generation:                         Table 4: Spanish-Hebrew Lexicon Generation:
highest-ranking translation.                                        highest-ranking translation.


words. This yielded a test set of 112 Hebrew                        test words whose selected translation was one of
nouns and 169 Spanish nouns. The second (R2),                       the translations in the gold standard.
contains 25 words for each of the two languages,                       IC translations ranking is a partial order, as usu-
obtained by randomly selecting 5 singular cor-                      ally many translations are scored equally. When
rectly segmented nouns from each of the 5 fre-                      all translations have the same score, IC is effec-
quency ranges 1-1000 to 4001-5000.                                  tively undecided. We calculate recall as the per-
   For each of the test words, the correct transla-                 centage of cases in which there was more than one
tions were extracted from a modern professional                     score rank. A result was counted as precise if any
concise printed Hebrew-Spanish-Hebrew dictio-                       of the highest-ranking translations was in the gold-
nary (Prolog, 2003). This dictionary almost al-                     standard, even if other translations were equally
ways provides a single Spanish translation for He-                  ranked, creating a bias in favor of IC.
brew headwords. Spanish headwords had 1.98 He-
                                                                       In both of the Hebrew-Spanish and the Spanish-
brew translations on the average. In both cases
                                                                    Hebrew cases, our method significantly outper-
this is a small number of correct translation com-
                                                                    formed all baselines in generating a precise lexi-
paring to what we might expect with other evalu-
                                                                    con on the highest-ranking translations.
ation methods; therefore this evaluation amounts
                                                                       All methods performed better in R1 than in
to a relatively high standard of correctness. Our
                                                                    R2, which included also lower-frequency words,
score comparison experiments (section 5) extend
                                                                    and this was more noticeable with the corpus-
the evaluation beyond this gold standard.
                                                                    based methods (Hebrew-Spanish) than with IC.
4.1.5 Parameters                                                    This suggests, not surprisingly, that the perfor-
The following parameter values were used. The                       mance of corpus-based methods is related to the
window size for co-occurrence counting, k, was 4.                   amount of information in the corpus.
This value was chosen in a small pre-test. Signa-                      That the results for the Spanish-Hebrew lexi-
ture size N was 200 (see Section 6.1). The number                   con are higher may arise from the difference in the
of alignments M for the baseline scores was 100.                    gold standard. As mentioned, Hebrew words only
The number of translations selected for each head-                  had one “correct” Spanish translation, while Span-
word, t, was set to 1 for ease of testing, but see                  ish had 1.98 correct translations on the average.
further notes under results.                                        If we had used a more comprehensive resource to
                                                                    test against, the precision of the method would be
4.2    Results                                                      higher than shown here.
Tables 3 and 4 summarize the results of the                            In translation pairs generation, the results be-
Hebrew-Spanish and Spanish-Hebrew lexicon                           yond the top-ranking pair are also of importance.
generation respectively, for both the R1 and R2                     Tables 5 and 6 present the accuracy of the first
test sets.                                                          three translation suggestions, for the three co-
   In the three co-occurrence based methods, NAS                    occurrence based scores, calculated for the R1 test
similarity, cosine distance and and city block dis-                 set. IC results are not included, as they are incom-
tance, the highest ranking translation was selected.                parable to those of the other methods: IC tends to
Recall is always 100% as a translation from the                     score many candidate translations identically, and
candidate set is always selected, and all of this set               in practice, the three highest-scoring sets of trans-
is valid. Precision is computed as the number of                    lation candidates contained on average 77% of all


                                                              103


                   1st    2nd      3rd      total              rect since our gold standard gives only a small set
       NAS       82.1%    6.3%    1.8%     90.2%               of translations. The set of possible translations in
                                                               iLex tends to include, besides the “correct” transla-
      Cosine     60.7%    9.8%    2.7%     73.2%
                                                               tion of the gold standard, other translations that are
    City block   56.3%    4.5%    10.7%    71.4%
                                                               suitable in certain contexts or are semantically re-
Table 5: Hebrew-Spanish lexicon generation: ac-                lated. For example, for one Hebrew word, kvuza,
curacy of 3 best translations for the R1 condition.            the gold standard translation was grupo (group),
The table shows how many of the 2nd and 3rd                    while our method chose equipo (team), which was
translations are correct. Note that NAS is always              at least as plausible given the amount of sports
a better solution, even though its numbers for 2nd             news in the corpus.
and 3rd are smaller, because its accumulative per-                Thus to better compare the capability of NAS to
centage, shown in the last column, is higher.                  distinguish correct and incorrect translations with
                                                               that of other scores, we performed two more ex-
                  1st     2nd      3rd      total              periments. In the first score comparison experi-
                                                               ment (SCE1), we used the two R1 test sets, He-
      NAS        87.6%   77.5%    16%     163.9%
                                                               brew and Spanish, from the lexicon generation test
     Cosine      68%     66.3%    10.1%    144.4%
                                                               (section 4.1.4). For each word in the test set, we
    City block   69.8%   64.5%    7.7%     142%                used our method to select between one of two
                                                               translations: a correct translation, from the gold
Table 6: Spanish-Hebrew lexicon generation: ac-
                                                               standard, and a random translation, chosen ran-
curacy of 3 best translations for the R1 condition.
                                                               domly among all the nouns similar in frequency
The total exceeds 100% because Spanish words
                                                               to the correct translation.
had more than one correct translation. See also
the caption of Table 5.                                           The second score comparison experiment
                                                               (SCE2) was designed to test the score with a more
                                                               extensive test set. For each of the two languages,
the candidates, thus necessarily yielding mostly               we randomly selected 1000 nouns, and used our
incorrect translations. Recall was omitted from the            method to select between a possibly correct trans-
tables as it is always 100%.                                   lation, chosen randomly among the translations
   For all methods, many of the correct translations           suggested in iLex, and a random translation, cho-
that do not rank first, rank as second or third. For           sen randomly among nouns similar in frequency
both languages, NAS ranks highest for total ac-                to the possibly correct translation. This test, while
curacy of the three translations, with considerable            using a more extensive test set, is less accurate
advantage.                                                     because it is not guaranteed that any of the input
                                                               translations is correct.
5   Score Comparison Experiments
                                                                  In both SCE1 and SCE2, cosine and city block
Lexicon generation, as defined in our experiment,              distance were used as baselines. Inverse Consul-
is a relatively high standard for cross-linguistic se-         tation is irrelevant here because it can only score
mantic distance evaluation. This is especially cor-            translation pairs that appear in iLex.
                                                                  Table 7 presents the results of the two score
                                                               comparison experiments, each of them for each of
                    Heb-Spa           Spa-Heb
                                                               the translation directions. Recall is by definition
                 SCE1     SCE2     SCE1     SCE2
                                                               100% and is omitted.
       NAS       93.8%   76.2%    94.1%    83.7%                  Again, NAS performs better than the baselines
      Cosine     74.1%   57.1%    70.7%    63.2%               in all cases. With all scores, precision values in
    City block   74.1%   68.3%    78,1%    75.2%               SCE1 are higher than in the lexicon generation
                                                               experiment. This is consistent with the expecta-
Table 7: Precision of score comparison experi-                 tion that selection between a correct and a ran-
ments. The percentage of cases in which each                   dom, probably incorrect, translation is easier than
of the scoring methods was able to successfully                selecting among the translations in iLex. The pre-
distinguish the correct (SCE1) or possible correct             cision in SCE2 is lower than that in SCE1. This
(SCE2) translation from the random translation.                may be a result of both translations in SCE2 being


                                                         104


                                                             6.2    Dependency on Alignment Lexicon
                                                             N ASL values depend on L, the lexicon in use.
                                                             Clearly again, in the extremes, an almost empty
                                                             lexicon or a lexicon containing every possible pair
                                                             of words (a Cartesian product), this score would
                                                             not be useful. In the first case, it would yield 0
                                                             for every pair, and in the second, 1. However as
                                                             our experiments show, it performed well with real-
                                                             world examples of a noisy lexicon, with branching
                                                             factors of 12.6 and 14.8 (see table 2).
Figure 1: NAS values (not algorithm precision) for
                                                             6.3    Lemmatization
various N sizes. NAS is not sensitive to the value
of N (see text).                                             Lemmatization is the process of extracting the
                                                             lemmas of words in the corpus. Our experiments
                                                             show that good results can be achieved without
in some cases incorrect. Yet this may also reflect a         lemmatization, at least for nouns in the pair of lan-
weakness of all three scores with lower-frequency            guages tested (aside from the simple prefix seg-
words, which are represented in the 1000-word                mentation we used for Hebrew, see section 4.1.1).
samples but not in the ones used in SCE1.                    For other language pairs lemmatization may be
                                                             needed. In general, correct lemmatization should
                                                             improve results, since the signatures would con-
6     NAS Score Properties                                   sist of more meaningful information. If automatic
                                                             lemmatization introduces noise, it may reduce the
6.1    Signature Size                                        results’ quality.
NAS values are in the range [0, 1]. The values de-           6.4    Alternative Models for Relatedness
pend on N, the size of the signature used. With an
extremely small N, NAS values would usually be               Cosine and city block, as well as other related dis-
0, and would tend to be noisy, due to accidental             tance metrics, rely on context vectors. The context
inclusion of high-frequency or highly ambiguous              vector of a word w collects words and maps them
words in the signature. As N approaches the size             to some score of their “relatedness” to w; in this
of the lexicon used for alignment, NAS values ap-            case, we used PMI. NAS, in contrast, relies on the
proach 1 for all word pairs.                                 signature, the set of N words most related to w.
                                                             That is, it requires a Boolean relatedness indica-
   This suggests that choosing a suitable value of
                                                             tion, rather than a numeric relatedness score. We
N is critical for effectively using NAS. Yet an em-
                                                             used PMI to generate this Boolean indication, and
pirical test has shown that NAS may be useful for
                                                             naturally, other similar measures could be used as
a wide range of N values: we computed NAS val-
                                                             well. More significantly, it may be possible to use
ues for the correct and random translations used
                                                             it with corpus-less sources of “relatedness”, such
in the Hebrew-Spanish SCE1 experiment (section
                                                             as WordNet or search result snippets.
5), using N values between 50 and 2000.
   Figure 1 shows the average score values (note             7     Conclusion
that these are not precision values) for the correct
and random translations across that N range. The             We presented a method to create a high quality
scores for the correct translations are consistently         bilingual lexicon given a noisy one. We focused
higher than those of the random translations, even           on the case in which the noisy lexicon is created
while there is a discernible decline in the differ-          using two pivot language lexicons. Our algorithm
ence between them. In fact, the precision of the se-         uses two unrelated monolingual corpora. At the
lection between the correct and random translation           heart of our method is the non-aligned signatures
is persistent throughout the range. This suggests            (NAS) context similarity score, used for remov-
that while extreme N values should be avoided, the           ing incorrect translations using cross-lingual co-
selection of N is not a major issue.                         occurrences.


                                                       105


   Words in one language tend to have multiple                    tionary. In MT Summit VIII: Machine Translation in
translations in another. The common method for                    the Information Age, Proceedings, pages 53–58.
context similarity scoring utilizes some algebraic
                                                                Kenneth W. Church and Patrick Hanks. 1990. Word
distance between context vectors, and requires a                  association norms, mutual information, and lexicog-
single alignment of context vectors in one lan-                   raphy. Computational Linguistics, 16:22–29.
guage into the other. Finding a single correct
alignment is unrealistic even when a perfectly cor-             Elad Dinur, Dmitry Davidov, and Ari Rappoport. 2009.
                                                                  Unsupervised concept discovery in hebrew using
rect lexicon is available. For example, alignment                 simple unsupervised word prefix segmentation for
forces us to choose one correct translation for each              hebrew and arabic. In EACL 2009 Workshop on
context word, while in practice a few possible                    Computational Approaches to Semitic Languages.
terms may be used interchangeably in the other
                                                                Pascale Fung. 1998. A statistical view on bilin-
language. In our task, moreover, the lexicon used                 gual lexicon extraction:from parallel corpora to non-
for alignment was automatically generated from                    parallel corpora. In The Third Conference of the As-
pivot language lexicons and was expected to con-                  sociation for Machine Translation in the Americas.
tain errors.
                                                                Nikesh Garera, Chris Callison-Burch, and David
   NAS does not depend on finding a single correct                Yarowsky. 2009. Improving translation lexi-
alignment. While it measures how well the sets of                 con induction from monolingual corpora via depen-
words that tend to co-occur with these two words                  dency contexts and part-of-speech equivalences. In
align to each other, its strength may lie in bypass-              CoNLL.
ing the question of which word in one language                  Hiroyuki Kaji and Toshiko Aizono. 1996. Extracting
should be aligned to a certain context word in the                word correspondences from bilingual corpora based
other language. Therefore, unlike other scoring                   on word co-occurrence information. In COLING.
methods, it is not effected by incorrect alignments.
                                                                Hiroyuki Kaji, Shin’ichi Tamamura, and Dashtseren
   We have shown that NAS outperforms the more                    Erdenebat. 2008. Automatic construction of a
traditional distance metrics, which we adapted to                 japanese-chinese dictionary via english. In LREC.
the many-to-many scenario by amortizing across
multiple alignments. Our results confirm that                   Philipp Koehn and Kevin Knight. 2002. Learn-
                                                                  ing a translation lexicon from monolingual corpora.
alignment is problematic in using co-occurrence                   In Proceedings of ACL Workshop on Unsupervised
methods across languages, at least in our settings.               Lexical Acquisition.
NAS constitutes a way to avoid this problem.
   While the purpose of this work was to discern                Adam Lopez. 2008. Statistical machine translation.
                                                                  ACM Computing Surveys, 40(3):1–49.
correct translations from incorrect one, it is worth
noting that our method actually ranks translation               Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
correctness. This is a stronger property, which                  Weld, Michael Skinner, and Jeff Bilmes. 2009.
may render it useful in a wider range of scenarios.              Compiling a massive, multilingual dictionary via
                                                                 probabilistic inference. In Proceedings of the 47th
   In fact, NAS can be viewed as a general mea-
                                                                 Annual Meeting of the Association for Computa-
sure for word similarity between languages. It                   tional Linguistics and 4th International Joint Con-
would be interesting to further investigate this ob-             ference on Natural Language Processing.
servation with other sources of lexicons (e.g., ob-
tained from parallel or comparable corpora) and                 Kyonghee Paik, Satoshi Shirai, and Hiromi Nakaiwa.
                                                                  2004. Automatic construction of a transfer dictio-
for other tasks, such as cross-lingual word sense                 nary considering directionality. In COLING, Multi-
disambiguation and information retrieval.                         lingual Linguistic Resources Workshop.

                                                                Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
References                                                        drea Mulloni. 2006. Finding translations for low-
                                                                  frequency words in comparable corpora. Machine
Kisuh Ahn and Matthew Frampton. 2006. Automatic                   Translation, 20:247 – 266.
  generation of translation dictionaries using interme-
  diary languages. In EACL 2006 Workshop on Cross-              Prolog.   2003.  Practical Bilingual Dictionary:
  Language Knowledge Induction.                                   Spanish-Hebew/Hebrew-Spanish. Israel.

Francis Bond, Ruhaida Binti Sulong, Takefumi Ya-                Reinhard Rapp. 1999. Automatic identification of
  mazaki, and Kentaro Ogura. 2001. Design and con-                word translations from unrelated english and german
  struction of a machine-tractable japanese-malay dic-            corpora. In ACL.


                                                          106


Charles Schafer and David Yarowsky. 2002. Inducing
  translation lexicons via diverse similarity measures
  and bridge languages. In CoNLL.
Hana Skoumalova. 2001. Bridge dictionaries as
  bridges between languages. International Journal
  of Corpus Linguistics, 6:95–105.
Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction
  of lexical translations from non-aligned corpora. In
  Conference on Computational linguistics.

Kumiko Tanaka and Kyoji Umemura. 1994. Construc-
  tion of a bilingual dictionary intermediated by a third
  language. In Conference on Computational Linguis-
  tics.
Jerzy Tomaszczyk. 1998. The bilingual dictionary un-
   der review. In ZuriLEX’86.




                                                            107

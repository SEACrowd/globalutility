    Automatic Evaluation Method for Machine Translation using
                     Noun-Phrase Chunking
               Hiroshi Echizen-ya                                       Kenji Araki
             Hokkai-Gakuen University                                Hokkaido University
          S 26-Jo, W 11-chome, Chuo-ku,                         N 14-Jo, W 9-Chome, Kita-ku,
             Sapporo, 064-0926 Japan                              Sapporo, 060-0814 Japan
           echi@eli.hokkai-s-u.ac.jp                          araki@media.eng.hokudai.ac.jp


                    Abstract                                  and IMPACT(Echizen-ya and Araki, 2007))
                                                              calculate matching scores using only common
    As described in this paper, we propose                    words between MT outputs and references
    a new automatic evaluation method for                     from bilingual humans. However, these meth-
    machine translation using noun-phrase                     ods cannot determine the correct word corre-
    chunking. Our method correctly deter-                     spondences suﬃciently because they fail to fo-
    mines the matching words between two                      cus solely on phrase correspondences. More-
    sentences using corresponding noun                        over, various methods using syntactic analyt-
    phrases. Moreover, our method deter-                      ical tools(Pozar and Charniak, 2006; Mutton
    mines the similarity between two sen-                     et al., 2007; Mehay and Brew, 2007) are pro-
    tences in terms of the noun-phrase or-                    posed to address the sentence structure. Nev-
    der of appearance. Evaluation experi-                     ertheless, those methods depend strongly on
    ments were conducted to calculate the                     the quality of the syntactic analytical tools.
    correlation among human judgments,                           As described herein, for use with MT sys-
    along with the scores produced us-                        tems, we propose a new automatic evaluation
    ing automatic evaluation methods for                      method using noun-phrase chunking to obtain
    MT outputs obtained from the 12 ma-                       higher sentence-level correlations. Using noun
    chine translation systems in NTCIR-                       phrases produced by chunking, our method
    7. Experimental results show that                         yields the correct word correspondences and
    our method obtained the highest cor-                      determines the similarity between two sen-
    relations among the methods in both                       tences in terms of the noun phrase order of ap-
    sentence-level adequacy and ﬂuency.                       pearance. Evaluation experiments using MT
                                                              outputs obtained by 12 machine translation
1    Introduction                                             systems in NTCIR-7(Fujii et al., 2008) demon-
High-quality automatic evaluation has be-                     strate that the scores obtained using our sys-
come increasingly important as various ma-                    tem yield the highest correlation with the hu-
chine translation systems have developed. The                 man judgments among the automatic evalua-
scores of some automatic evaluation meth-                     tion methods in both sentence-level adequacy
ods can obtain high correlation with human                    and ﬂuency. Moreover, the diﬀerences be-
judgment in document-level automatic evalua-                  tween correlation coeﬃcients obtained using
tion(Coughlin, 2007). However, sentence-level                 our method and other methods are statisti-
automatic evaluation is insuﬃcient. A great                   cally signiﬁcant at the 5% or lower signiﬁ-
gap exists between language processing of au-                 cance level for adequacy. Results conﬁrmed
tomatic evaluation and the processing by hu-                  that our method using noun-phrase chunking
mans. Therefore, in recent years, various au-                 is eﬀective for automatic evaluation for ma-
tomatic evaluation methods particularly ad-                   chine translation.
dressing sentence-level automatic evaluations
                                                              2   Automatic Evaluation Method
have been proposed. Methods based on word
                                                                  using Noun-Phrase Chunking
strings (e.g., BLEU(Papineni et al., 2002),
NIST(NIST, 2002), METEOR(Banerjee and                         The system based on our method has four pro-
Lavie., 2005), ROUGE-L(Lin and Och, 2004),                    cesses. First, the system determines the corre-


                                                        108
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 108–117,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


spondences of noun phrases between MT out-                            part”, “the amount” and “crowning drop” are
puts and references using chunking. Secondly,                         obtained in the reference by chunking. Next,
the system calculates word-level scores based                         the system determines the corresponding noun
on the correct matched words using the deter-                         phrases from these noun phrases between the
mined correspondences of noun phrases. Next,                          MT output and reference. The score between
the system calculates phrase-level scores based                       “the end” and “the end part” is the highest
on the noun-phrase order of appearance. The                           among the scores between “the end” in the
system calculates the ﬁnal scores combining                           MT output and “it”, “the end part”, “the
word-level scores and phrase-level scores.                            amount”, and “crowning drop” in the refer-
                                                                      ence. Moreover, the score between “the end
2.1    Correspondence of Noun Phrases                                 part” and “the end” is the highest among the
       by Chunking                                                    scores between “the end part” in reference
The system obtains the noun phrases from                              and “the amount”, “the crowning fall”, “the
each sentence by chunking. It then determines                         end” in the MT output. Consequently, “the
corresponding noun phrases between MT out-                            end” and “the end part” are selected as noun
puts and references calculating the similarity                        phrases with the highest mutual scores: “the
for two noun phrases by the PER score(Su et                           end” and “the end part” are determined as one
al., 1992). In that case, PER scores of two                           corresponding noun phrase. In Fig. 1, “the
kinds are calculated. One is the ratio of the                         amount” in the MT output and “the amount”
number of match words between an MT out-                              in reference, and “the crowning fall” in the
put and reference for the number of all words                         MT output and “crowning drop” in the ref-
of the MT output. The other is the ratio of the                       erence also are determined as the respective
number of match words between the MT out-                             corresponding noun phrases. The noun phrase
put and reference for the number of all words                         for which the score between it and other noun
of the reference. The similarity is obtained as                       phrases is 0.0 (e.g., “it” in reference) has no
an F -measure between two PER scores. The                             corresponding noun phrase. The use of the
high score represents that the similarity be-                         noun phrases is eﬀective because the frequency
tween two noun phrases is high. Figure 1                              of the noun phrases is higher than those of
presents an example of the determination of                           other phrases. The verb phrases are not used
the corresponding noun phrases.                                       for this study, but they can also be generated
                                                                      by chunking. It is diﬃcult to determine the
 (1) Use of noun phrase chunking                                      corresponding verb phrases correctly because
   MT output :                                                        the words in each verb phrase are often fewer
    in general , [NP the amount ] of [NP the crowning fall ]
    is large like [NP the end ] .
                                                                      than the noun phrases.

  Reference :
   generally , the closer [NP it ] is to [NP the end part ] ,
                                                                      2.2   Word-level Score
   the larger [NP the amount ] of [NP crowning drop ] is .

 (2) Determination of corresponding noun phrases                      The system calculates the word-level scores
   MT output :
                                                                      between MT output and reference using the
    in general , [NP the amount ] of [NP the crowning fall ]
    is large like [NP the end ] .                                     corresponding noun phrases. First, the sys-
                                                0.3714                tem determines the common words based on
  Reference :      1.0000       0.7429
                                                                      Longest Common Subsequence (LCS). The
   generally , the closer [NP it ] is to [NP the end part ] ,
   the larger [NP the amount ] of [NP crowning drop ] is .            system selects only one LCS route when sev-
                                                                      eral LCS routes exist. In such cases, the sys-
                                                                      tem calculates the Route Score (RS) using the
Figure 1: Example of determination of corre-
                                                                      following Eqs. (1) and (2):
sponding noun phrases.

   In Fig. 1, “the amount”, “the crowning fall”                                                                  β
and “the end” are obtained as noun phrases                                                     
                                                                             RS =                     weight(w)        (1)
in MT output by chunking, and “it”, “the end                                        c∈LCS       w∈c


                                                                109


                                                               (1) First process for determination of common parts :
              ⎧                                                LCS = 7
              ⎪
              ⎪   words in corresponding                       Our method
              ⎪
              ⎨ 2    noun phrase                               MT output :
weight(w) =                                                     in general , [NP1 the amount ] of [NP2 the crowning fall ]
              ⎪
              ⎪   words in non
              ⎪
              ⎩ 1                                               is large like [NP3 the end ] .
                     corresponding noun phrase
                                             (2)               Reference : 12.0 (2+2+1)2.0 12.0          12.0     22.0
                                                                generally , the closer [NP it ] is to [NP3 the end part ] , the
   In Eq. (1), β is a parameter for length                      larger [NP1 the amount ] of [NP2 crowning drop ] is .
weighting of common parts; it is greater than
                                                               IMPACT
1.0. Figure 2 portrays an example of deter-                    MT output :
mination of the common parts. In the ﬁrst                       in general , [NP1 the amount ] of [NP2 the crowning fall ]
process of Fig. 2, LCS is 7. In this example,                   is large like [NP3 the end ] .
several LCS routes exist. The system selects                   Reference : (1+1)2.0(2+1)2.0 22.0 12.0 12.0
                                                                generally , the closer [NP it ] is to [NP3 the end part ] , the
the LCS route which has “,”, “the amount                        larger [NP1 the amount ] of [NP2 crowning drop ] is .
of”, “crowning”, “is”, and “.” as the com-
mon parts. The common part is the part                        (2) Second process for determination of common parts :
for which the common words appear contin-                     LCS=3
                                                              Our method
uously. In contrast, IMPACT selects a diﬀer-                  MT output :
ent LCS route that includes “, the”, “amount                   in general , [NP1 the amount ] of [NP2 the crowning fall ]
of”, “crowning”, “is”, and “.” as the com-                     is large like [NP3 the end ] .
mon parts. In IMPACT, using no analytical                     Reference :
knowledge, the LCS route is determined using                   generally , the closer [NP it ] is to [NP3 the end part ] , the
                                                               larger [NP1 the amount ] of [NP2 crowning drop ] is .
the information of the number of words in the
common parts and the position of the com-
mon parts. The RS for LCS route selected                      Figure 2: Example of common-part determi-
using our method is 32 (= 12.0 + (2 + 2 +                     nation.
1)2.0 + 22.0 + 12.0 + 12.0 ) when β is 2.0. The
RS for LCS route selected by IMPACT is 19
(= (1 + 1)2.0 + (2 + 1)2.0 + 22.0 + 12.0 + 12.0 ).                                        (1 + γ 2 )Rwd Pwd
In the LCS route selected by IMPACT, the                                  scorewd =                                        (5)
                                                                                           Rwd + γ 2 Pwd
weight of “the” in the common part “, the”
                                                                 Equation (3) represents recall and Eq. (4)
is 1 because “the” in the reference is not in-
cluded in the corresponding noun phrase. In                   represents precision. Therein, m signiﬁes the
                                                              word number of the reference in Eq. (3), and
the LCS route selected using our method, the
weight of “the” in “the amount of” is 2 because               n stands for the word number of the MT out-
                                                              put in Eq. (4). Here, RN denotes the repe-
“the” in MT output and “the” in the reference
are included in the corresponding noun phrase                 tition number of the determination process of
“NP1”. Therefore, the system based on our                     the LCS route, and i, which has initial value 0,
method can select the correct LCS route.                      is the counter for RN . In Eqs. (3) and (4), α
   Moreover, the word-level score is calculated               is a parameter for the repetition process of the
using the common parts in the selected LCS                    determination of LCS route, and is less than
route as the following Eqs. (3), (4), and (5).                1.0. Therefore, Rwd and Pwd becomes small
                                                              as the appearance order of the common parts
                                                              between MT output and reference is diﬀerent.
         ⎛                                    ⎞ β1
              RN
              i=0   αi   c∈LCS   length(c)β                   Moreover, length(c) represents the number of
 Rwd = ⎝                                      ⎠               words in each common part; β is a param-
                          mβ
                                                              eter related to the length weight of common
                                                  (3)         parts, as in Eq. (1). In this case, the weight
                                                              of each common word in the common part is
         ⎛                                    ⎞ β1            1. The system calculates scorewd as the word-
              RN
                    αi           length(c)β
 Pwd = ⎝
              i=0        c∈LCS
                                              ⎠               level score in Eq. (5). In Eq. (5), γ is deter-
                           nβ                                 mined as Pwd /Rwd. The scorewd is between
                                                  (4)         0.0 and 1.0.


                                                        110


   In the ﬁrst process of Fig.                  2,
αi c∈LCS length(c)β is 13.0 (=0.50 ×
(12.0 + 32.0 + 12.0 + 12.0 + 12.0 )) when α and                                        (1 + γ 2Rwd multi )Pwd multi
                                                            scorewd    multi   =
β are 0.5 and 2.0, respectively. In this case,                                          Rwd multi + γ 2 Pwd multi
                                                                                                                  (8)
the counter i is 0. Moreover, in the second
process of Fig. 2, αi c∈LCS length(c)β is 2.5              2.3    Phrase-level Score
(=0.51 × (12.0 + 22.0 )) using two common parts
                                                           The system calculates the phrase-level score
“the” and “the end”, except the common
                                                           using the noun phrases obtained by chunking.
parts determined using the ﬁrst process.
                                                           First, the system extracts only noun phrases
In Fig. 2, RN is 1 because the system
                                                           from sentences. Then it generalizes each noun
ﬁnishes calculating αi c∈LCS length(c)β
                                                           phrase as each word. Figure 3 presents exam-
when counter i became 1: this means that
                                                           ples of generalization by noun phrases.
all common parts were processed until
the second process. As a result,√ Rwd is
                                 2.0 =                      (1) Corresponding noun phrases
0.1969 (= (13.0 + 2.5)/20                0.0388),
                                                             MT output :
and Pwd is 0.2625 (= (13.0 + 2.5)/15 2.0 =
√                                                             in general , [NP1 the amount ] of [NP2 the crowning fall ]
  0.0689). Consequently, scorewd is 0.2164                    is large like [NP3 the end ] .
             2)×0.1969×0.2625
(= (1+1.3332
      0.1969+1.3332 2×0.2625
                               ). In this case, γ            Reference :
                        0.2625                                generally , the closer [NP it ] is to [NP3 the end part ] ,
becomes 1.3332 (= 0.1969 ). The system can
                                                              the larger [NP1 the amount ] of [NP2 crowning drop ] is .
determine the matching words correctly using
                                                            (2) Generalization by noun phrases
the corresponding noun phrases between the                    MT output :
MT output and the reference.                                   NP1 NP2 NP3
   The system calculates scorewd multi using                  Reference :
Rwd multi and Pwd multi which are, respec-                     NP NP3 NP1 NP2
tively, maximum Rwd and Pwd when multiple
references are used as the following Eqs. (6),             Figure 3: Example of generalization by noun
(7) and (8). In Eq. (8), γ is determined as                phrases.
Pwd multi /Rwd multi . The scorewd multi is be-
tween 0.0 and 1.0.                                            Figure 3 presents three corresponding noun
                                                           phrases between the MT output and the refer-
                                                           ence. The noun phrase “it”, which has no cor-
                                                           responding noun phrase, is expressed as “NP”
Rwd   multi   =
        ⎛⎛                             ⎞ 1 ⎞           in the reference. Consequently, the MT output
             RN                              β
        ⎜⎜                                     ⎟
                                                           is generalized as “NP1 NP2 NP3”; the refer-
        ⎜⎜         αi
                            length(c) β
                                           ⎟
        ⎜                                  ⎟ ⎟             ence is generalized as “NP NP3 NP1 NP2”.
         ⎜   i=0      c∈LCS               j⎟ ⎟
maxuj=1 ⎜
        ⎜⎜                                 ⎟ ⎟             Subsequently, the system obtains the phrase-
        ⎜⎜                m β              ⎟ ⎟
        ⎝⎝                  j              ⎠ ⎟ ⎠           level score between the generalized MT output
                                                           and reference as the following Eqs. (9), (10),
                                              (6)          and (11).

                                                                   ⎛                                                     ⎞1
                                                                                                                             β
                                                                 ⎜
                                                                         RN
                                                                         i=0   αi        cnpp∈LCS length(cnpp)
                                                                                                               β
                                                                                                                 ⎟
                                                           Rnp = ⎝                                               ⎠
                                                                                              √         β
                                                                                       mcnp × mno cnp
Pwd   multi   =
       ⎛⎛                             ⎞ 1 ⎞                                                                (9)
            RN                              β
       ⎜⎜         αi
                           length(c) β
                                          ⎟ ⎟
       ⎜⎜                                 ⎟ ⎟
       ⎜⎜   i=0      c∈LCS               j⎟ ⎟                     ⎛                                                      ⎞1
   u   ⎜
maxj=1 ⎜⎜                                 ⎟ ⎟
                                          ⎟ ⎟
                                                                                                                             β
       ⎜⎜                n β                                            RN
                                                                               α   i
                                                                                                      length(cnpp)   β
       ⎝⎝                  j              ⎠ ⎟ ⎠                  ⎜
                                                           Pnp = ⎝
                                                                        i=0              cnpp∈LCS                        ⎟
                                                                                                                         ⎠
                                                                                                √               β
                                                                                       ncnp ×       nno   cnp
                                              (7)                                                                   (10)


                                                     111


                                Table 1: Machine translation system types.
         System No. 1        System No. 2      System No. 3          System No. 4    System No. 5    System No. 6
  Type       SMT                 SMT              RBMT                    SMT             SMT             SMT
         System No. 7        System No. 8      System No. 9          System No. 10   System No. 11   System No. 12
  Type       SMT                 SMT              EBMT                    SMT             SMT            RBMT



                                                                 3     Experiments
                          (1 + γ 2 )RnpPnp
          scorenp       =                           (11)
                           Rnp + γ 2 Pnp                         3.1     Experimental Procedure

   In Eqs. (9) and (10), cnpp denotes the                        We calculated the correlation between the
common noun phrase parts; mcnp and ncnp                          scores obtained using our method and scores
respectively signify the quantities of common                    produced by human judgment. The system
noun phrases in the reference and MT output.                     based on our method obtained the evaluation
Moreover, mno cnp and nno cnp are the quanti-                    scores for 1,200 English output sentences re-
ties of noun phrases except the common noun                      lated to the patent sentences. These English
phrases in the reference and MT output. The                      output sentences are sentences that 12 ma-
values of mno cnp and nno cnp are processed                      chine translation systems in NTCIR-7 trans-
as 1 when no non-corresponding noun phrases                      lated from 100 Japanese sentences. Moreover,
exist. The square root used for mno cnp and                      the number of references to each English sen-
nno cnp is to decrease the weight of the non-                    tence in 100 English sentences is four. These
corresponding noun phrases. In Eq. (11), γ is                    references were obtained from four bilingual
determined as Pnp  /Rnp. In Fig. 3, Rnp and                     humans. Table 1 presents types of the 12 ma-
                         2.0 +0.5×12.0                           chine translation systems.
Pnp are 0.7071 (= 1×2 (3×1)     2.0    ) when α is
                                                                    Moreover, three human judges evaluated
0.5 and β is 2.0. Therefore, scorenp is 0.7071.
                                                                 1,200 English output sentences from the per-
   The system obtains scorenp multi calculat-
                                                                 spective of adequacy and ﬂuency on a scale of
ing the average of scorenp when multiple ref-
                                                                 1–5. We used the median value in the evalua-
erences are used as the following Eq. (12).
                                                                 tion results of three human judges as the ﬁnal
                                                                 scores of 1–5. We calculated Pearson’s correla-
                             u                                   tion eﬃcient and Spearman’s rank correlation
                             j=0 (scorenp )j
      scorenp   multi   =                           (12)         eﬃcient between the scores obtained using our
                                  u
                                                                 method and the scores by human judgments in
2.4    Final Score                                               terms of sentence-level adequacy and ﬂuency.
The system calculates the ﬁnal score by com-                        Additionally, we calculated the correlations
bining the word-level score and the phrase-                      between the scores using seven other methods
level score as shown in the following Eq. (13).                  and the scores by human judgments to com-
                                                                 pare our method with other automatic evalua-
                                                                 tion methods. The other seven methods were
                 scorewd + δ × scorenp
      score =                                       (13)         IMPACT, ROUGE-L, BLEU1 , NIST, NMG-
                         1+δ                                     WN(Ehara, 2007; Echizen-ya et al., 2009),
   Therein, δ represents a parameter for the                     METEOR2 , and WER(Leusch et al., 2003).
weight of scorenp : it is between 0.0 and 1.0.                   Using our method, 0.1 was used as the value of
The ratio of scorewd to scorenp is 1:1 when δ is                 the parameter α in Eqs. (3)-(10) and 1.1 was
1.0. Moreover, scorewd multi and scorenp multi                   used as the value of the parameter β in Eqs.
are used for Eq. (13) in multiple references.                    (1)–(10). Moreover, 0.3 was used as the value
In Figs. 2 and 3, the ﬁnal score between                         of the parameter δ in Eq. (13). These val-
the MT output and the reference is 0.4185                           1
                                                                      BLEU was improved to perform sentence-level
(= 0.2164+0.7×0.7071
         1+0.7       ) when δ is 0.7. The system                 evaluation: the maximum N value between MT output
can realize high-quality automatic evaluation                    and reference is used(Echizen-ya et al., 2009).
                                                                    2
                                                                      The matching modules of METEOR are the exact
using both word-level information and phrase-                    and stemmed matching module, and a WordNet-based
level information.                                               synonym-matching module.


                                                           112


            Table 2: Pearson’s correlation coeﬃcient for sentence-level adequacy.
                         No. 1     No. 2     No. 3     No. 4     No. 5     No. 6             No. 7
      Our method        0.7862 0.4989 0.5970 0.5713 0.6581 0.6779                           0.7682
       IMPACT           0.7639     0.4487    0.5980   0.5371    0.6371     0.6255           0.7249
       ROUGE-L          0.7597     0.4264 0.6111 0.5229         0.6183     0.5927           0.7079
         BLEU           0.6473     0.2463    0.4230   0.4336    0.3727     0.4124           0.5340
         NIST           0.5135     0.2756    0.4142   0.3086    0.2553     0.2300           0.3628
       NMG-WN           0.7010     0.3432    0.6067   0.4719    0.5441     0.5885           0.5906
       METEOR           0.4509     0.0892    0.3907   0.2781    0.3120     0.2744           0.3937
         WER            0.7464     0.4114    0.5519   0.5185    0.5461     0.5970           0.6902
     Our method II      0.7870     0.5066    0.5967   0.5191    0.6529     0.6635           0.7698
   BLEU with our method 0.7244     0.3935    0.5148   0.5231    0.4882     0.5554           0.6459
                         No. 8     No. 9     No. 10 No. 11 No. 12           Avg.              All
      Our method        0.7664 0.7208 0.6355 0.7781 0.5707 0.6691                           0.6846
       IMPACT           0.7007     0.7125    0.5981   0.7621    0.5345     0.6369           0.6574
       ROUGE-L          0.6834     0.7042    0.5691   0.7480    0.5293     0.6228           0.6529
         BLEU           0.5188     0.5884    0.3697   0.5459    0.4357     0.4607           0.4722
         NIST           0.4218     0.4092    0.1721   0.3521    0.4769     0.3493           0.3326
       NMG-WN           0.6658     0.6068    0.6116   0.6770 0.5740 0.5818                  0.5669
       METEOR           0.3881     0.4947    0.3127   0.2987    0.4162     0.3416           0.2958
         WER            0.6656     0.6570    0.5740   0.7491    0.5301     0.6031           0.5205
     Our method II      0.7676     0.7217    0.6343   0.7917    0.5474     0.6632           0.6774
   BLEU with our method 0.6395     0.6696    0.5139   0.6611    0.5079     0.5698           0.5790


ues of the parameter are determined using En-           12 machine translation systems in respective
glish sentences from Reuters articles(Utiyama           automatic evaluation methods, and “All” are
and Isahara, 2003). Moreover, we obtained               the correlation coeﬃcients using the scores of
the noun phrases using a shallow parser(Sha             1,200 output sentences obtained using the 12
and Pereira, 2003) as the chunking tool. We             machine translation systems.
revised some erroneous results that were ob-
tained using the chunking tool.                         3.3   Discussion
                                                        In Tables 2–5, the “Avg.” score of our method
3.2   Experimental Results
                                                        is shown to be higher than those of other meth-
As described in this paper, we performed com-           ods. Especially in terms of the sentence-level
parison experiments using our method and                adequacy shown in Tables 2 and 4, “Avg.”
seven other methods. Tables 2 and 3 respec-             of our method is about 0.03 higher than that
tively show Pearson’s correlation coeﬃcient for         of IMPACT. Moreover, in system No. 8 and
sentence-level adequacy and ﬂuency. Tables 4            “All” of Tables 2 and 4, the diﬀerences be-
and 5 respectively show Spearman’s rank cor-            tween correlation coeﬃcients obtained using
relation coeﬃcient for sentence-level adequacy          our method and IMPACT are statistically sig-
and ﬂuency. In Tables 2–5, bold typeface                niﬁcant at the 5% signiﬁcance level.
signiﬁes the maximum correlation coeﬃcients                Moreover, we investigated the correlation of
among eight automatic evaluation methods.               machine translation systems of every type. Ta-
Underlining in our method signiﬁes that the             ble 6 shows “All” of Pearson’s correlation co-
diﬀerences between correlation coeﬃcients ob-           eﬃcient and Spearman’s rank correlation coef-
tained using our method and IMPACT are                  ﬁcient in SMT (i.e., system Nos. 1–2, system
statistically signiﬁcant at the 5% signiﬁcance          Nos. 4–8 and system Nos. 10–11) and RBMT
level. Moreover, “Avg.” signiﬁes the aver-              (i.e., system Nos. 3 and 12). The scores of
age of the correlation coeﬃcients obtained by           900 output sentences obtained by 9 machine


                                                  113


              Table 3: Pearson’s correlation   coeﬃcient for sentence-level ﬂuency.
                          No. 1     No. 2        No. 3    No. 4    No. 5      No. 6         No. 7
      Our method         0.5853 0.3782          0.5689   0.4673    0.5739 0.5344           0.7193
       IMPACT            0.5581     0.3407      0.5821   0.4586 0.5768 0.4852              0.6896
       ROUGE-L           0.5551     0.3056      0.5925 0.4391      0.5666     0.4475       0.6756
         BLEU            0.4793     0.0963      0.4488   0.3033    0.4690     0.3602       0.5272
         NIST            0.4139     0.0257      0.4987   0.1682    0.3923     0.2236       0.3749
       NMG-WN            0.5782     0.3090      0.5434 0.4680 0.5070          0.5234       0.5363
       METEOR            0.4050     0.1405      0.4420   0.1825    0.4259     0.2336       0.4873
         WER             0.5143     0.3031      0.5220   0.4262    0.4936     0.4405       0.6351
     Our method II       0.5831     0.3689      0.5753   0.3991    0.5610     0.5445       0.7186
   BLEU with our method  0.5425     0.2304      0.5115   0.3770    0.5358     0.4741       0.6142
                          No. 8     No. 9       No. 10 No. 11 No. 12           Avg.          All
      Our method         0.5796 0.6424          0.3241   0.5920    0.4321 0.5331           0.5574
       IMPACT            0.5612     0.6320      0.3492   0.6034    0.4166     0.5211       0.5469
       ROUGE-L           0.5414     0.6347      0.3231   0.5889    0.4127     0.5069       0.5387
         BLEU            0.5040     0.5521      0.2134   0.4783    0.4078     0.4033       0.4278
         NIST            0.3682     0.3811      0.1682   0.3116 0.4484 0.3146              0.3142
       NMG-WN            0.5526     0.5799      0.4509 0.6308 0.4124          0.5007       0.5074
       METEOR            0.2511     0.4153      0.1376   0.3351    0.2902     0.3122       0.2933
         WER             0.5492     0.6421      0.3962   0.6228    0.4063     0.4960       0.4478
     Our method II       0.5774     0.6486      0.3428   0.5975    0.4197     0.5280       0.5519
   BLEU with our method  0.5660     0.6247      0.2536   0.5495    0.4550     0.4770       0.5014


translation systems in SMT and the scores of           for BLEU with our method are higher than
200 output sentences obtained by 2 machine             those of BLEU in any machine translation sys-
translation systems in RBMT are used respec-           tem, “Avg.” and “All” in Tables 2–5. More-
tively. However, EBMT is not included in Ta-           over, for sentence-level adequacy, BLEU with
ble 6 because EBMT is only system No. 9.               our method is signiﬁcantly better than BLEU
In Table 6, our method obtained the highest            in almost all machine translation systems and
correlation among the eight methods, except            “All” in Tables 2 and 4. These results indicate
in terms of the adequacy of RBMT in Pear-              that our method using noun-phrase chunking
son’s correlation coeﬃcient. The diﬀerences            is eﬀective for some methods and that it is
between correlation coeﬃcients obtained us-            statistically signiﬁcant in each machine trans-
ing our method and IMPACT are statistically            lation system, not only “All”, which has large
signiﬁcant at the 5% signiﬁcance level for ad-         sentences.
equacy of SMT.                                            Subsequently, we investigated the precision
   To conﬁrm the eﬀectiveness of noun-phrase           of the determination process of the corre-
chunking, we performed the experiment using            sponding noun phrases described in section
a system combining BLEU with our method.               2.1: in the results of system No. 1, we cal-
In this case, BLEU scores were used as scorewd         culated the precision as the ratio of the num-
in Eq. (13). This experimental result is shown         ber of the correct corresponding noun phrases
as “BLEU with our method” in Tables 2–5. In            for the number of all noun-phrase correspon-
the results of “BLEU with our method” in Ta-           dences obtained using the system based on our
bles 2–5, underlining signiﬁes that the diﬀer-         method. Results show that the precision was
ences between correlation coeﬃcients obtained          93.4%, demonstrating that our method can de-
using BLEU with our method and BLEU alone              termine the corresponding noun phrases cor-
are statistically signiﬁcant at the 5% signif-         rectly.
icance level. The coeﬃcients of correlation              Moreover, we investigated the relation be-


                                                 114


       Table 4: Spearman’s rank correlation coeﬃcient for sentence-level adequacy.
                        No. 1   No. 2      No. 3    No. 4     No. 5      No. 6    No. 7
     Our method        0.7456 0.5049 0.5837 0.5146 0.6514 0.6557 0.6746
      IMPACT           0.7336   0.4881    0.5992   0.4741     0.6382    0.5841   0.6409
      ROUGE-L          0.7304   0.4822 0.6092 0.4572          0.6135    0.5365   0.6368
        BLEU           0.5525   0.2206    0.4327   0.3449     0.3230    0.2805   0.4375
        NIST           0.5032   0.2438    0.4218   0.2489     0.2342    0.1534   0.3529
      NMG-WN           0.7541 0.3829      0.5579   0.4472     0.5560    0.5828   0.6263
      METEOR           0.4409   0.1509    0.4018   0.2580     0.3085    0.1991   0.4115
        WER            0.6566   0.4147    0.5478   0.4272     0.5524    0.4884   0.5539
    Our method II      0.7478   0.4972    0.5817   0.4892     0.6437    0.6428   0.6707
  BLEU with our method 0.6644   0.3926    0.5065   0.4522     0.4639    0.4715   0.5460
                        No. 8   No. 9     No. 10 No. 11 No. 12           Avg.      All
     Our method        0.7298 0.7258 0.5961 0.7633 0.6078 0.6461 0.6763
      IMPACT           0.6703   0.7067    0.5617   0.7411     0.5583    0.6164   0.6515
      ROUGE-L          0.6603   0.6983    0.5340   0.7280     0.5281    0.6012   0.6435
        BLEU           0.4571   0.5827    0.3220   0.4987     0.4302    0.4069   0.4227
        NIST           0.4255   0.4424    0.1313   0.2950     0.4785    0.3276   0.3062
      NMG-WN           0.6863   0.6524 0.6412 0.7015          0.5728    0.5968   0.5836
      METEOR           0.4242   0.4776    0.3335   0.2861     0.4455    0.3448   0.2887
        WER            0.6234   0.6480    0.5463   0.7131     0.5684    0.5617   0.4797
    Our method II      0.7287   0.7255    0.5936   0.7761     0.5798    0.6397   0.6699
  BLEU with our method 0.5850   0.6757    0.4596   0.6272     0.5452    0.5325   0.5474


tween the correlation obtained by our method         same as the common parts of Fig. 2 when “the
and the quality of chunking. In “Our method”         crowning fall” in the MT output and “crown-
shown in Tables 2–5, noun phrases for which          ing drop” in the reference are not determined
some erroneous results obtained using the            as the noun phrases. Other common parts are
chunking tool were revised. “Our method II”          determined correctly because the weight of the
of Tables 2–5 used noun phrases that were            common part “the amount of” is higher than
given as results obtained using the chunk-           those of other common parts by Eqs. (1) and
ing tool. Underlining in “Our method II” of          (2). Consequently, the determination of the
Tables 2–5 signiﬁes that the diﬀerences be-          common parts except “the amount of” is not
tween correlation coeﬃcients obtained using          diﬃcult.
our method II and IMPACT are statistically              In other language sentences, we already per-
signiﬁcant at the 5% signiﬁcance level. Fun-         formed the experiments using Japanese sen-
damentally, in both “Avg.” and “All” of Ta-          tences from Reuters articles(Oyamada et al.,
bles 2–5, the correlation coeﬃcients of our          2010). Results show that the correlation co-
method II without the revised noun phrases           eﬃcients of IMPACT with our method, for
are lower than those of our method using the         which IMPACT scores were used as scorewd in
revised noun phrases. However, the diﬀerence         Eq. (13), were highest among some methods.
between our method and our method II in              Therefore, our method might not be language-
“Avg.” and “All” of Tables 2–5 is not large.         dependent. Nevertheless, experiments using
The performance of the chunking tool has no          various language data are necessary to eluci-
great inﬂuence on the results of our method          date this point.
because scorewd in Eqs. (3), (4), and (5) do
not depend strongly on the performance of            4   Conclusion
the chunking tool. For example, in sentences
                                                     As described herein, we proposed a new auto-
shown in Fig. 2, all common parts are the
                                                     matic evaluation method for machine transla-


                                               115


         Table 5: Spearman’s rank correlation coeﬃcient for sentence-level ﬂuency.
                         No. 1   No. 2      No. 3    No. 4     No. 5     No. 6     No. 7
      Our method        0.5697 0.3299      0.5446   0.4199     0.5733    0.5060 0.6459
       IMPACT           0.5481   0.3285    0.5572   0.3976 0.5960 0.4317           0.6334
       ROUGE-L          0.5470   0.3041 0.5646 0.3661          0.5638    0.3879    0.6255
         BLEU           0.4157   0.0559    0.4286   0.2018     0.4475    0.2569    0.4909
         NIST           0.4209   0.0185    0.4559   0.1093     0.3186    0.1898    0.3634
       NMG-WN           0.5569 0.3461 0.5381 0.4300 0.5052 0.5264 0.5328
       METEOR           0.4608   0.1429    0.4438   0.1783     0.4073    0.1596    0.4821
         WER            0.4469   0.2395    0.5087   0.3292     0.4995    0.3482    0.5637
     Our method II      0.5659   0.3216    0.5484   0.3773     0.5638    0.5211    0.6343
   BLEU with our method 0.5188   0.1534    0.4793   0.3005     0.5255    0.3942    0.5676
                         No. 8   No. 9     No. 10 No. 11 No. 12           Avg.       All
      Our method        0.5646 0.6617 0.3319        0.6256     0.4485 0.5185 0.5556
       IMPACT           0.5471   0.6454    0.3222   0.6319     0.4358    0.5062    0.5489
       ROUGE-L          0.5246   0.6428    0.2949   0.6159     0.3928    0.4858    0.5359
         BLEU           0.4882   0.5419    0.1407   0.4740     0.4176    0.3633    0.3971
         NIST           0.4150   0.4193    0.0889   0.3006 0.4752 0.2980           0.2994
       NMG-WN           0.5684 0.5850 0.4451 0.6502 0.4387               0.5102    0.5156
       METEOR           0.2911   0.4267    0.1735   0.3264     0.3512    0.3158    0.2886
         WER            0.5320   0.6505    0.3828   0.6501     0.4003    0.4626    0.4193
     Our method II      0.5609   0.6687    0.3629   0.6223     0.4384    0.5155    0.5531
   BLEU with our method 0.5470   0.6213    0.2184   0.5808     0.4870    0.4495    0.4825


                     Table 6: Correlation coeﬃcient for SMT and RBMT.

                  Pearson’s correlation coeﬃcient             Spearman’s rank correlation coeﬃcient
                   Adequacy             Fluency                  Adequacy             Fluency
                 SMT     RBMT        SMT     RBMT              SMT    RBMT       SMT        RBMT
 Our method     0.7054 0.5840 0.5477 0.5016                   0.6710 0.5961 0.5254         0.5003
  IMPACT        0.6721   0.5650     0.5364   0.4960           0.6397  0.5811    0.5162      0.4951
 ROUGE-L        0.6560   0.5691     0.5179   0.4988           0.6225  0.5701    0.4942      0.4783
  NMG-WN        0.5958 0.5850 0.5201         0.4732           0.6129  0.5755    0.5238      0.4959


tion. Our method calculates the scores for MT           Acknowledgements
outputs using noun-phrase chunking. Conse-
quently, the system obtains scores using the            This work was done as research under the
correctly matched words and phrase-level in-            AAMT/JAPIO Special Interest Group on
formation based on the corresponding noun               Patent Translation. The Japan Patent In-
phrases. Experimental results demonstrate               formation Organization (JAPIO) and the Na-
that our method yields the highest correlation          tional Institute of Informatics (NII) provided
among eight methods in terms of sentence-               corpora used in this work. The author grate-
level adequacy and ﬂuency.                              fully acknowledges JAPIO and NII for their
                                                        support. Moreover, this work was partially
  Future studies will improve our method,               supported by Grants from the High-Tech Re-
enabling it to achieve high correlation in              search Center of Hokkai-Gakuen University
sentence-level ﬂuency. Future studies will also         and the Kayamori Foundation of Informa-
include experiments using data of various lan-          tional Science Advancement.
guages.


                                                  116


References                                               NIST.       2002.       Automatic Evaluation
                                                           of    Machine    Translation    Quality    Us-
Satanjeev Banerjee and Alon Lavie. 2005. ME-               ing    N-gram     Co-Occurrence     Statistics.
  TEOR: An Automatic Metric for MT Eval-                   http://www.nist.gov/speech/tests/mt/doc/
  uation with Improved Correlation with Hu-                ngram-study.pdf.
  man Judgments. In Proc. of ACL Workshop
  on Intrinsic and Extrinsic Evaluation Measures         Takashi Oyamada, Hiroshi Echizen-ya and Kenji
  for Machine Translation and/or Summariza-                Araki. 2010. Automatic Evaluation of Machine
  tion, 65–72.                                             Translation Using both Words Information and
                                                           Comprehensive Phrases Information. In IPSJ
Deborah Coughlin. 2003. Correlating Automated              SIG Technical Report, Vol.2010-NL-195, No. 3
  and Human Assessments of Machine Translation             (in Japanese).
  Quality. In Proc. of MT Summit IX, 63–70.
                                                         Kishore Papineni, Salim Roukos, Todd Ward and
Hiroshi Echizen-ya and Kenji Araki. 2007. Auto-            Wei-Jing Zhu. 2002. BLEU: a Method for Au-
  matic Evaluation of Machine Translation based            tomatic Evaluation of Machine Translation. In
  on Recursive Acquisition of an Intuitive Com-            Proc. of ACL’02, 311–318.
  mon Parts Continuum. In Proc. of MT Summit
  XII, 151–158.                                          Michael Pozar and Eugene Charniak. 2006. Bllip:
                                                           An Improved Evaluation Metric for Machine
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shi-            Translation. Brown University Master Thesis.
  mohata, Atsushi Fujii, Masao Utiyama, Mikio
  Yamamoto, Takehito Utsuro and Noriko Kando.            Fei Sha and Fernando Pereira. 2003. Shallow Pars-
                                                           ing with Conditional Random Fields. In Proc.
  2009. Meta-Evaluation of Automatic Evaluation
  Methods for Machine Translation using Patent             of HLT-NAACL 2003, 134–141.
  Translation Data in NTCIR-7. In Proc. of the           Keh-Yih Su, Ming-Wen Wu and Jing-Shin Chang.
  3rd Workshop on Patent Translation, 9–16.                1992. A New Quantitative Quality Measure for
                                                           Machine Translation Systems. In Proc. of GOL-
Terumasa Ehara. 2007. Rule Based Machine                   ING’92, 433–439.
  Translation Combined with Statistical Post Ed-
  itor for Japanese to English Patent Transla-           Masao Utiyama and Hitoshi Isahara. 2003. Re-
  tion. In Proc. of MT Summit XII Workshop                liable Measures for Aligning Japanese–English
  on Patent Translation, 13–18.                           News Articles and Sentences. In Proc. of the
                                                          ACL’03, pp.72–79.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto
  and Takehito Utsuro. 2008. Overview of the
  Patent Translation Task at the NTCIR-7 Work-
  shop. In Proc. of 7th NTCIR Workshop Meeting
  on Evaluation of Information Access Technolo-
  gies: Information Retrieval, Question Answer-
  ing and Cross-lingual Information Access, 389–
  400.

Gregor Leusch, Nicola Ueﬃng and Hermann Ney.
  2003. A Novel String-to-String Distance Mea-
  sure with Applications to Machine Translation
  Evaluation. In Proc. of MT Summit IX, 240–
  247.

Chin-Yew Lin and Franz Josef Och. 2004. Auto-
  matic Evaluation of Machine Translation Qual-
  ity Using Longest Common Subsequence and
  Skip-Bigram Statistics. In Proc. of ACL’04,
  606–613.

Dennis N. Mehay and Chris Brew.            2007.
  BLEUÂTRE: Flattening Syntactic Dependen-
  cies for MT Evaluation. In Proc. of MT Summit
  XII, 122–131.

Andrew Mutton, Mark Dras, Stephen Wan and
  Robert Dale. 2007. GLEU: Automatic Eval-
  uation of Sentence-Level Fluency. In Proc. of
  ACL’07, 344–351.


                                                   117

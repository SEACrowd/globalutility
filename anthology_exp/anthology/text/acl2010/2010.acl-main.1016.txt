              Pseudo-word for Phrase-based Machine Translation


              Xiangyu Duan                Min Zhang               Haizhou Li
                     Institute for Infocomm Research, A-STAR, Singapore
                   {Xduan, mzhang, hli}@i2r.a-star.edu.sg




                                                             are there in such phrasal equivalences. Moreover,
                     Abstract                                should basic translational unit be word or coarse-
                                                             grained multi-word is an open problem for opti-
                                                             mizing SMT models.
                                                                Some researchers have explored coarse-
    The pipeline of most Phrase-Based Statistical            grained translational unit for machine translation.
    Machine Translation (PB-SMT) systems starts              Marcu and Wong (2002) attempted to directly
    from automatically word aligned parallel cor-
                                                             learn phrasal alignments instead of word align-
    pus. But word appears to be too fine-grained
    in some cases such as non-compositional
                                                             ments. But computational complexity is prohibi-
    phrasal equivalences, where no clear word                tively high for the exponentially large number of
    alignments exist. Using words as inputs to PB-           decompositions of a sentence pair into phrase
    SMT pipeline has inborn deficiency. This pa-             pairs. Cherry and Lin (2007) and Zhang et al.
    per proposes pseudo-word as a new start point            (2008) used synchronous ITG (Wu, 1997) and
    for PB-SMT pipeline. Pseudo-word is a kind               constraints to find non-compositional phrasal
    of basic multi-word expression that character-           equivalences, but they suffered from intractable
    izes minimal sequence of consecutive words in            estimation problem. Blunsom et al. (2008; 2009)
    sense of translation. By casting pseudo-word             induced phrasal synchronous grammar, which
    searching problem into a parsing framework,
                                                             aimed at finding hierarchical phrasal equiva-
    we search for pseudo-words in a monolingual
    way and a bilingual synchronous way. Ex-
                                                             lences.
    periments show that pseudo-word significantly               Another direction of questioning word as basic
    outperforms word for PB-SMT model in both                translational unit is to directly question word
    travel translation domain and news translation           segmentation on languages where word bounda-
    domain.                                                  ries are not orthographically marked. In Chinese-
                                                             to-English translation task where Chinese word
1    Introduction                                            boundaries are not marked, Xu et al. (2004) used
                                                             word aligner to build a Chinese dictionary to re-
The pipeline of most Phrase-Based Statistical                segment Chinese sentence. Xu et al. (2008) used
Machine Translation (PB-SMT) systems starts                  a Bayesian semi-supervised method that com-
from automatically word aligned parallel corpus              bines Chinese word segmentation model and
generated from word-based models (Brown et al.,              Chinese-to-English translation model to derive a
1993), proceeds with step of induction of phrase             Chinese segmentation suitable for machine trans-
table (Koehn et al., 2003) or synchronous gram-              lation. There are also researches focusing on the
mar (Chiang, 2007) and with model weights tun-               impact of various segmentation tools on machine
ing step. Words are taken as inputs to PB-SMT at             translation (Ma et al. 2007; Chang et al. 2008;
the very beginning of the pipeline. But there is a           Zhang et al. 2008). Since there are many 1-to-n
deficiency in such manner that word is too fine-             phrasal equivalences in Chinese-to-English trans-
grained in some cases such as non-compositional              lation (Ma and Way. 2009), only focusing on
phrasal equivalences, where clear word align-                Chinese word as basic translational unit is not
ments do not exist. For example in Chinese-to-               adequate to model 1-to-n translations. Ma and
English translation, “ 想 ” and “would like to”               Way (2009) tackle this problem by using word
constitute a 1-to-n phrasal equivalence, “多 少                aligner to bootstrap bilingual segmentation suit-
钱” and “how much is it” constitute a m-to-n                  able for machine translation. Lambert and
phrasal equivalence. No clear word alignments                Banchs (2005) detect bilingual multi-word ex-


                                                         148
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148–156,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


pressions by monotonically segmenting a given            where X denotes the sentence, Y denotes a de-
Spanish-English sentence pair into bilingual             composition of X. Sig function acts as potential
units, where word aligner is also used.                  function on each multi-word yk, and ZX acts as
   IBM model 3, 4, 5 (Brown et al., 1993) and            partition function. Note that the number of yk is
Deng and Byrne (2005) are another kind of re-            not fixed given X because X can be decomposed
lated works that allow 1-to-n alignments, but            into various number of multi-words.
they rarely questioned if such alignments exist in           Given X, ZX is fixed, so searching for optimal
word units level, that is, they rarely questioned        decomposition is as below:
word as basic translational unit. Moreover, m-to-
n alignments were not modeled.
                                                          Yˆ = ARGMAX P(Y | X ) = ARGMAX Sig
                                                                   Y                           Y1K
                                                                                                      ∑   yk   (2)
                                                                                                      k
   This paper focuses on determining the basic           where Y1K denotes K multi-word units from de-
translational units on both language sides without       composition of X. A multi-word sequence with
using word aligner before feeding them into PB-          maximal sum of Sig function values is the search
SMT pipeline. We call such basic translational           target — pseudo-word sequence. From (2) we
unit as pseudo-word to differentiate with word.          can see that Sig function is vital for pseudo-word
Pseudo-word is a kind of multi-word expression           searching. In this paper Sig function calculates
(includes both unary word and multi-word).               sequence significance which is proposed to char-
Pseudo-word searching problem is the same to             acterize pseudo-word as minimal sequence of
decomposition of a given sentence into pseudo-           consecutive words in sense of translation. The
words. We assume that such decomposition is in           detail of sequence significance is described in the
the Gibbs distribution. We use a measurement,            following section.
which characterizes pseudo-word as minimal
sequence of consecutive words in sense of trans-         2.1     Sequence Significance
lation, as potential function in Gibbs distribution.
                                                         Two kinds of definitions of sequence signifi-
Note that the number of decomposition of one
                                                         cance are proposed. One is monolingual se-
sentence into pseudo-words grows exponentially
                                                         quence significance. X and Y are monolingual
with sentence length. By fitting decomposition
                                                         sentence and monolingual multi-words respec-
problem into parsing framework, we can find
                                                         tively in this monolingual scenario. The other is
optimal pseudo-word sequence in polynomial
                                                         bilingual sequence significance. X and Y are sen-
time. Then we feed pseudo-words into PB-SMT
                                                         tence pair and multi-word pairs respectively in
pipeline, and find that pseudo-words as basic
                                                         this bilingual scenario.
translational units improve translation perform-
ance over words as basic translational units. Fur-       2.1.1    Monolingual Sequence Significance
ther experiments of removing the power of
higher order language model and longer max               Given a sentence w1, …, wn, where wi denotes
phrase length, which are inherent in pseudo-             unary word, monolingual sequence significance
words, show that pseudo-words still improve              is defined as:
translational performance significantly over                                           Freqi , j
                                                                        Sigi , j =                             (3)
unary words.                                                                         Freqi −1, j +1
   This paper is structured as follows: In section
                                                         where Freqi, j (i≤j) represents frequency of word
2, we define the task of searching for pseudo-
                                                         sequence wi, …, wj in the corpus, Sigi, j repre-
words and its solution. We present experimental
                                                         sents monolingual sequence significance of a
results and analyses of using pseudo-words in
                                                         word sequence wi, …, wj. We also denote word
PB-SMT model in section 3. The conclusion is
                                                         sequence wi, …, wj as span[i, j], whole sentence
presented at section 4.
                                                         as span[1, n]. Each span is also a multi-word ex-
2    Searching for Pseudo-words                          pression.
                                                            Monolingual sequence significance of span[i, j]
Pseudo-word searching problem is equal to de-            is proportional to span[i, j]’s frequency, while is
composition of a given sentence into pseudo-             inversely proportion to frequency of expanded
words. We assume that the distribution of such           span (span[i-1, j+1]). Such definition character-
decomposition is in the form of Gibbs distribu-          izes minimal sequence of consecutive words
tion as below:                                           which we are looking for. Our target is to find
                         1                               pseudo-word sequence which has maximal sum
         P (Y | X ) =      exp( ∑ Sig y k )     (1)
                        ZX      k
                                                         of spans’ significances:


                                                       149


                                   ∑                           (4)     the sentence grows exponentially with the sen-
                                       K
        pw 1K = ARGMAX
                    K                  k =1
                                              Sig   span   k
                       span   1
                                                                       tence length in both monolingual scenario and
where pw denotes pseudo-word, K is equal to or                         bilingual scenario. By casting such decomposi-
less than sentence’s length. spank is the kth span                     tion problem into parsing framework, we can
of K spans span1K. Equation (4) is the rewrite of                      find pseudo-word sequence in polynomial time.
equation (2) in monolingual scenario. Searching                        According to the two scenarios, searching for
for pseudo-words pw1K is the same to finding                           pseudo-words can be performed in a monolin-
optimal segmentation of a sentence into K seg-                         gual way and a synchronous way. Details of the
ments span1K (K is a variable too). Details of                         two kinds of searching algorithms are described
searching algorithm are described in section                           in the following two sections.
2.2.1.
    We firstly search for monolingual pseudo-                          2.2.1      Algorithm of Searching for Monolin-
words on source and target side individually.                                     gual Pseudo-words (SMP)
Then we apply word alignment techniques to                             Searching for monolingual pseudo-words is
build pseudo-word alignments. We argue that                            based on the computation of monolingual se-
word alignment techniques will work fine if non-                       quence significance. Figure 1 presents the search
existent word alignments in such as non-                               algorithm. It is performed in a way similar to
compositional phrasal equivalences have been                           CKY (Cocke-Kasami-Younger) parser.
filtered by pseudo-words.
2.1.2     Bilingual Sequence Significance                                      Initialization: Wi, i = Sigi, i;
                                                                                               Wi, j = 0, (i≠j);
Bilingual sequence significance is proposed to                                 1: for d = 2 … n do
characterize pseudo-word pairs. Co-occurrence                                  2: for all i, j s.t. j-i=d-1 do
of sequences on both language sides is used to                                 3:       for k = i … j – 1 do
define bilingual sequence significance. Given a                                4:          v = Wi, k + Wk+1, j
bilingual sequence pair: span-pair[is, js, it, jt]                             5:          if v > Wi, j then
(source side span[is, js] and target side span[it, jt]),                       6:             Wi, j = v;
                                                                               7:       u = Sigi, j
bilingual sequence significance is defined as be-
                                                                               8:       if u > Wi, j then
low:                                                                           9:          Wi, j = u;
                                Freqis , j s ,it , jt                      Figure 1. Algorithm of searching for monolingual
       Sigis , j s ,it , jt =                                (5)
                              Freqis −1, j s +1,it −1, jt +1                            pseudo-words (SMP).
where Freq denotes the frequency of a span-pair.
Bilingual sequence significance is an extension                           In this algorithm, Wi, j records maximal sum of
of monolingual sequence significance. Its value                        monolingual sequence significances of sub spans
is proportional to frequency of span-pair[is, js, it,                  of span[i, j]. During initialization, Wi, i is initial-
jt], while is inversely proportional to frequency                      ized as Sigi,i (note that this sequence is word wi
of expanded span-pair[is-1, js+1, it-1, jt+1].                         only). For all spans that have more than one
Pseudo-word pairs of one sentence pair are such                        word (i≠j), Wi, j is initialized as zero.
pairs that maximize the sum of span-pairs’ bilin-                         In the main algorithm, d represents span’s
gual sequence significances:                                           length, ranging from 2 to n, i represents start po-
                                                                       sition of a span, j represents end position of a
                       ∑k =1 Sigspan− pairk
                                           K
        pwp1K = ARGMAX
                    K
                                                               (6)     span, k represents decomposition position of
                    span − pair1
                                                                       span[i,j]. For span[i, j], Wi, j is updated if higher
pwp represents pseudo-word pair. Equation (6) is                       sum of monolingual sequence significances is
the rewrite of equation (2) in bilingual scenario.                     found.
Searching for pseudo-word pairs pwp1K is equal                            The algorithm is performed in a bottom-up
to bilingual segmentation of a sentence pair into                      way. Small span’s computation is first. After
optimal span-pair1K. Details of searching algo-                        maximal sum of significances is found in small
rithm are presented in section 2.2.2.                                  spans, big span’s computation, which uses small
2.2     Algorithms of Searching for Pseudo-                            spans’ maximal sum, is continued. Maximal sum
        words                                                          of significances for whole sentence (W1,n, n is
                                                                       sentence’s length) is guaranteed in this way, and
Pseudo-word searching problem is equal to de-                          optimal decomposition is obtained correspond-
composition of a sentence into pseudo-words.                           ingly.
But the number of possible decompositions of

                                                                     150


   The method of fitting the decomposition prob-                                                         js/jt is the end position of a span-pair on
lem into CKY parsing framework is located at                                                             source/target side, ks/kt is the decomposition po-
steps 7-9. After steps 3-6, all possible decompo-                                                        sition of a span-pair[is, js, it, jt] on source/target
sitions of span[i, j] are explored and Wi, j of op-                                                      side.
timal decomposition of span[i, j] is recorded.                                                               Update steps in Figure 2 are similar to that of
Then monolingual sequence significance Sigi,j of                                                         Figure 1, except that the update is about span-
span[i, j] is computed at step 7, and it is com-                                                         pairs, not monolingual spans. Reversed and non-
pared to Wi, j at step 8. Update of Wi, j is taken at                                                    reversed alignments inside a span-pair are com-
step 9 if Sigi,j is bigger than Wi, j, which indicates                                                   pared at step 4. For span-pair[is, js, it, jt],
that span[i, j] is non-decomposable. Thus                                                                    Wis , js ,it , jt is updated at step 6 if higher sum of
whether span[i, j] should be non-decomposable
or not is decided through steps 7-9.                                                                     bilingual sequence significances is found.
                                                                                                            Fitting the bilingually searching for pseudo-
2.2.2    Algorithm of Synchronous Searching                                                              words into ITG framework is located at steps 7-9.
         for Pseudo-words (SSP)                                                                          Steps 3-6 have explored all possible decomposi-
Synchronous searching for pseudo-words utilizes                                                          tions of span-pair[is, js, it, jt] and have recorded
bilingual sequence significance. Figure 2 pre-                                                           maximal Wi , j , i , j of these decompositions. Then
                                                                                                                               s   s   t   t


sents the search algorithm. It is similar to ITG                                                         bilingual sequence significance of span-pair[is, js,
(Wu, 1997), except that it has no production                                                             it, jt] is computed at step 7. It is compared to
rules and non-terminal nodes of a synchronous                                                            W i , j , i , j at step 8. Update is taken at step 9 if
                                                                                                               s   s   t   t

grammar. What it cares about is the span-pairs                                                           bilingual sequence significance of span-pair[is, js,
that maximize the sum of bilingual sequence sig-                                                         it, jt] is bigger than Wi , j , i , j , which indicates that
nificances.                                                                                                                                    s   s   t   t


                                                                                                         span-pair[is, js, it, jt] is non-decomposable.
 Initialization: if is = js or it = jt then                                                              Whether the span-pair[is, js, it, jt] should be non-
                          W i s , j s , i t , j t = Sig                                    ;             decomposable or not is decided through steps 7-
                                                                      i s , j s ,it , jt
                                                                                                         9.
                 else
                                                                                                            In addition to the initialization step, all span-
                          W i s , j s , it , jt = 0 ;
                                                                                                         pairs’ bilingual sequence significances are com-
 1: for ds = 2 … ns, dt = 2 … nt do                                                                      puted. Maximal sum of bilingual sequence sig-
 2: for all is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do                                             nificances for one sentence pair is guaranteed
 3:        for ks = is … js – 1, kt = it … jt – 1 do
                                                                                                         through this bottom-up way, and the optimal de-
 4:              v = max{ Wis ,ks ,it ,kt                         +Wks +1, js ,kt +1, jt ,               composition of the sentence pair is obtained cor-
                                          W i s , k s , k t + 1 , j t + W k s + 1 , j s , it , k t }     respondingly.

 5:              if v >        Wis , js ,it , jt            then                                         z             Algorithm of Excluded Synchronous
                                                                                                                       Searching for Pseudo-words (ESSP)
 6:                       W i s , j s , it , j t = v;
 7:          u = Sig i                                                                                   The algorithm of SSP in Figure 2 explores all
                         s   , j s , it , j t
                                                                                                         span-pairs, but it neglects NULL alignments,
 8:          if u >   Wis , js ,it , jt             then                                                 where words and “empty” word are aligned. In
 9:                                                                                                      fact, SSP requires that all parts of a sentence pair
                 W i s , j s , it , j t = u;
                                                                                                         should be aligned. This requirement is too strong
 Figure 2. Algorithm of Synchronous Searching for                                                        because NULL alignments are very common in
               Pseudo-words(SSP).
                                                                                                         many language pairs. In SSP, words that should
                                                                                                         be aligned to “empty” word are programmed to
  In the algorithm,                    Wis , js ,it , jt records maximal                                 be aligned to real words.
sum of bilingual sequence significances of sub                                                              Unlike most word alignment methods (Och
span-pairs of span-pair[is, js, it, jt]. For 1-to-m                                                      and Ney, 2003) that add “empty” word to ac-
span-pairs, Ws are initialized as bilingual se-                                                          count for NULL alignment entries, we propose a
quence significances of such span-pairs. For                                                             method to naturally exclude such NULL align-
other span-pairs, Ws are initialized as zero.                                                            ments. We call this method as Excluded Syn-
   In the main algorithm, ds/dt denotes the length                                                       chronous Searching for Pseudo-words (ESSP).
of a span on source/target side, ranging from 2 to                                                          The main difference between ESSP and SSP is
ns/nt (source/target sentence’s length). is/it is the                                                    in steps 3-6 in Figure 3. We illustrate Figure 3’s
start position of a span-pair on source/target side,                                                     span-pair configuration in Figure 4.


                                                                                                       151


                                                                                                                               solid boxes are shrunk into void, algorithm of
Initialization: if is = js or it = jt then                                                                                     ESSP is the same to SSP.
                                W i s , j s , i t , j t = Sig                i s , j s ,it , jt
                                                                                                  ;                               Generally, span length of NULL alignment is
                     else                                                                                                      not very long, so we can set a length threshold
                                W i s , j s , it , jt = 0 ;                                                                    for NULL alignments, eg. ks2-ks1≤EL, where EL
1: for ds = 2 … ns, dt = 2 … nt do                                                                                             denotes Excluded Length threshold. Computa-
2:     for all is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do                                                                tional complexity of the ESSP remains the same
3:         for ks1=is+1 … js, ks2=ks1-1 … js-1                                                                                 to SSP’s complexity O(ns3.nt3), except multiply a
              kt1=it+1 … jt, kt2=kt1-1 … jt-1 do                                                                               constant EL2.
4:             v = max{ W i , k −1 ,i , k −1 + W k +1, j , k +1, j ,                                                              There is one kind of NULL alignments that
                                                        s   s1     t   t1                   s2          s   t2      t


                                                W i s , k s 1 − 1 , k t 2 + 1 , j t + W k s 2 + 1 , j s , it , k t 1 − 1 }     ESSP can not consider. Since we limit excluded
                                                                                                                               parts in the middle of a span-pair, the algorithm
5:                   if v >           Wis , js ,it , jt            then                                                        will end without considering boundary parts of a
6:                              W i s , j s , it , j t = v;                                                                    sentence pair as NULL alignments.
7:            u = Sig i
                                 s   , j s , it , j t                                                                          3       Experiments and Results
8:            if u >        Wis , js ,it , jt               then                                                               In our experiments, pseudo-words are fed into
9:                   Wi s , j s , it , j t     = u;                                                                            PB-SMT pipeline. The pipeline uses GIZA++
      Figure 3. Algorithm of Excluded Synchronous                                                                              model 4 (Brown et al., 1993; Och and Ney, 2003)
         Searching for Pseudo-words (ESSP).                                                                                    for pseudo-word alignment, uses Moses (Koehn
                                                                                                                               et al., 2007) as phrase-based decoder, uses the
    The solid boxes in Figure 4 represent excluded                                                                             SRI Language Modeling Toolkit to train lan-
parts of span-pair[is, js, it, jt] in ESSP. Note that,                                                                         guage model with modified Kneser-Ney smooth-
in SSP, there is no excluded part, that is, ks1=ks2                                                                            ing (Kneser and Ney 1995; Chen and Goodman
and kt1=kt2.                                                                                                                   1998). Note that MERT (Och, 2003) is still on
    We can see that in Figure 4, each monolingual                                                                              original words of target language. In our experi-
span is configured into three parts, for example:                                                                              ments, pseudo-word length is limited to no more
span[is, ks1-1], span[ks1, ks2] and span[ks2+1, js]                                                                            than six unary words on both sides of the lan-
on source language side. ks1 and ks2 are two new                                                                               guage pair.
variables gliding between is and js, span[ks1, ks2]                                                                               We conduct experiments on Chinese-to-
is source side excluded part of span-pair[is, js, it,                                                                          English machine translation. Two data sets are
jt]. Bilingual sequence significance is computed                                                                               adopted, one is small corpus of IWSLT-2008
only on pairs of blank boxes, solid boxes are ex-                                                                              BTEC task of spoken language translation in
cluded in this computation to represent NULL                                                                                   travel domain (Paul, 2008), the other is large
alignment cases.                                                                                                               corpus in news domain, which consists Hong
                                                                                                                               Kong News (LDC2004T08), Sinorama Magazine
                is                                ks1            ks2                               js                          (LDC2005T10), FBIS (LDC2003E14), Xinhua
                                                                                                                               (LDC2002E18), Chinese News Translation
                                                                                                                               (LDC2005T06),           Chinese        Treebank
                                                                                                                               (LDC2003E07), Multiple Translation Chinese
                it                                kt1            kt2                               jt                          (LDC2004T07). Table 1 lists statistics of the
                                     a) non-reversed                                                                           corpus used in these experiments.

                is                                ks1            ks2                               js                                                 small             large
                                                                                                                                                   Ch → En           Ch → En
                                                                                                                                        Sent.          23k             1,239k
                                                                                                                                        word 190k 213k 31.7m                35.5m
                it                                kt1            kt2                               jt
                                                                                                                                         ASL       8.3     9.2     25.6       28.6
                                           b) reversed
                                                                                                                                   Table 1. Statistics of corpora, “Ch” denotes Chinese,
     Figure 4. Illustration of excluded configuration.                                                                             “En” denotes English, “Sent.” row is the number of
                                                                                                                                   sentence pairs, “word” row is the number of words,
   Note that, in Figure 4, solid box on either lan-                                                                                      “ASL” denotes average sentence length.
guage side can be void (i.e., length is zero) if
there is no NULL alignment on its side. If all


                                                                                                                             152


   For small corpus, we use CSTAR03 as devel-             3.3   Pseudo-word Performances on Small
opment set, use IWSLT08 official test set for test.             Corpus
A 5-gram language model is trained on English
                                                          Table 3 presents performances of SMP, SSP,
side of parallel corpus. For large corpus, we use
                                                          ESSP on small data set. pwchpwen denotes that
NIST02 as development set, use NIST03 as test
                                                          pseudo-words are on both language side of train-
set. Xinhua portion of the English Gigaword3
                                                          ing data, and they are input strings during devel-
corpus is used together with English side of large
                                                          opment and testing, and translations are also
corpus to train a 4-gram language model.
                                                          pseudo-words, which will be converted to words
   Experimental results are evaluated by case-
                                                          as final output. wchpwen/pwchwen denotes that
insensitive BLEU-4 (Papineni et al., 2001).
                                                          pseudo-words are adopted only on Eng-
Closest reference sentence length is used for
                                                          lish/Chinese side of the data set.
brevity penalty. Additionally, NIST score (Dod-
                                                             We can see from table 3 that, ESSP attains the
dington, 2002) and METEOR (Banerjee and La-
                                                          best performance, while SSP attains the worst
vie, 2005) are also used to check the consistency
                                                          performance. This shows that excluding NULL
of experimental results. Statistical significance in
                                                          alignments in synchronous searching for pseudo-
BLEU score differences was tested by paired
                                                          words is effective. SSP puts overly strong align-
bootstrap re-sampling (Koehn, 2004).
                                                          ment constraints on parallel corpus, which im-
3.1   Baseline Performance                                pacts performance dramatically. ESSP is superior
                                                          to SMP indicating that bilingually motivated
Our baseline system feeds word into PB-SMT                searching for pseudo-words is more effective.
pipeline. We use GIZA++ model 4 for word                  Both SMP and ESSP outperform baseline consis-
alignment, use Moses for phrase-based decoding.           tently in BLEU, NIST and METEOR.
The setting of language model order for each                 There is a common phenomenon among SMP,
corpus is not changed. Baseline performances on           SSP and ESSP. wchpwen always performs better
test sets of small corpus and large corpus are re-        than the other two cases. It seems that Chinese
ported in table 2.                                        word prefers to have English pseudo-word
                                                          equivalence which has more than or equal to one
                       small           Large
                                                          word. pwchpwen in ESSP performs similar to the
       BLEU           0.4029          0.3146
                                                          baseline, which reflects that our direct pseudo-
       NIST           7.0419          8.8462
     METEOR           0.5785          0.5335              word pairs do not work very well with GIZA++
 Table 2. Baseline performances on test sets of small     alignments. Such disagreement is weakened by
              corpus and large corpus.                    using pseudo-words on only one language side
                                                          (wchpwen or pwchwen), while the advantage of
3.2   Pseudo-word Unpacking                               pseudo-words is still leveraged in the alignments.
Because pseudo-word is a kind of multi-word                  Best ESSP (wchpwen) is significantly better
expression, it has inborn advantage of higher             than baseline (p<0.01) in BLEU score, best SMP
language model order and longer max phrase                (wchpwen) is significantly better than baseline
length over unary word. To see if such inborn             (p<0.05) in BLEU score. This indicates that
advantage is the main contribution to the per-            pseudo-words, through either monolingual
formance or not, we unpack pseudo-word into               searching or synchronous searching, are more
words after GIZA++ aligning. Aligned pseudo-              effective than words as to being basic transla-
words are unpacked into m×n word alignments.              tional units.
PB-SMT pipeline is executed thereafter. The ad-              Figure 5 illustrates examples of pseudo-words
vantage of longer max phrase length is removed            of one Chinese-to-English sentence pair. Gold
during phrase extraction, and the advantage of            standard word alignments are shown at the bot-
higher order of language model is also removed            tom of figure 5. We can see that “front desk” is
during decoding since we use language model               recognized as one pseudo-word in ESSP. Be-
trained on unary words. Performances of pseudo-           cause SMP performs monolingually, it can not
word unpacking are reported in section 3.3.1 and          consider “前台” and “front desk” simultaneously.
3.4.1. Ma and Way (2009) used the unpacking               SMP only detects frequent monolingual multi-
after phrase extraction, then re-estimated phrase         words as pseudo-words. SSP has a strong con-
translation probability and lexical reordering            straint that all parts of a sentence pair should be
model. The advantage of longer max phrase                 aligned, so source sentence and target sentence
length is still used in their method.                     have same length after merging words into


                                                        153


                       SMP                             SSP                           ESSP                 baseline
            pwchpwen   wchpwen pwchwen pwchpwen wchpwen pwchwen pwchpwen wchpwen                pwchwen
 BLEU        0.3996    0.4155 0.4024       0.3184     0.3661 0.3552       0.3998     0.4229     0.4147    0.4029
 NIST        7.4711    7.6452 7.6186       6.4099     6.9284 6.8012       7.1665     7.4373     7.4235    7.0419
METEOR       0.5900    0.6008 0.6000       0.5255     0.5569 0.5454       0.5739     0.5963     0.5891    0.5785
                           Table 3. Performance of using pseudo-words on small data.

 pseudo-words. We can see that too many pseudo-            pseudo-word itself as basic translational unit,
 words are detected by SSP.                                does not rely very much on higher language
                                                           model order or longer max phrase length setting.

     前台 的 那个 人 真 粗鲁 。
                                                           3.4     Pseudo-word Performances on Large
                                                  ESSP             Corpus
     The guy at the front_desk is pretty rude .
                                                           Table 5 lists the performance of using pseudo-
                                                           words on large corpus. We apply SMP on this
     前台 的 那个 人 真 粗鲁 。                              SMP     task. ESSP is not applied because of its high
     The guy at the front desk is pretty rude .
                                                           computational complexity. Table 5 shows that all
                                                           three configurations (pwchpwen, wchpwen, pwchwen)
     前台 的 那个 人 真 粗鲁 。                                      of SMP outperform the baseline. If we go back to
                                                   SSP
     The guy_at the front_desk is pretty_rude .            the definition of sequence significance, we can
                                                           see that it is a data-driven definition that utilizes
                                                           corpus frequencies. Corpus scale has an influ-
     前台 的 那个 人 真                    粗鲁 。                   ence on computation of sequence significance in
                                                           long sentences which appear frequently in news
     The guy at the front desk is pretty rude .
                                                           domain. SMP benefits from large corpus, and
                                                           wchpwen is significantly better than baseline
         Gold standard word alignments                     (p<0.01). Similar to performances on small cor-
                                                           pus, wchpwen always performs better than the
    Figure 5. Outputs of the three algorithms ESSP,        other two cases, which indicates that Chinese
 SMP and SSP on one sentence pair and gold standard        word prefers to have English pseudo-word
 word alignments. Words in one pseudo-word are con-        equivalence which has more than or equal to one
                  catenated by “_”.                        word.

                                                                                   SMP                baseline
 3.3.1    Pseudo-word Unpacking               Perform-                  pwchpwen wchpwen pwchwen
          ances on Small Corpus                             BLEU         0.3185    0.3230    0.3166    0.3146
 We test pseudo-word unpacking in ESSP. Table               NIST         8.9216    9.0447    8.9210    8.8462
 4 presents its performances on small corpus.              METEOR        0.5402    0.5489    0.5435    0.5335
                                                           Table 5. Performance of using pseudo-words on large
                     unpackingESSP         baseline                              corpus.
             pwchpwen wchpwen pwchwen
                                                           3.4.1    Pseudo-word Unpacking            Perform-
  BLEU        0.4097     0.4182    0.4031   0.4029
                                                                    ances on Large Corpus
  NIST        7.5547     7.2893    7.2670   7.0419
 METEOR       0.5951     0.5874    0.5846   0.5785         Table 6 presents pseudo-word unpacking per-
 Table 4. Performances of pseudo-word unpacking on         formances on large corpus. All three configura-
                    small corpus.                          tions improve performance over baseline after
                                                           pseudo-word unpacking. pwchpwen attains the
    We can see that pseudo-word unpacking sig-             best BLEU among the three configurations, and
 nificantly outperforms baseline. wchpwen is sig-          is significantly better than baseline (p<0.03).
 nificantly better than baseline (p<0.04) in BLEU          wchpwen is also significantly better than baseline
 score. Unpacked pseudo-word performs com-                 (p<0.04). By comparing table 6 with table 5, we
 paratively with pseudo-word without unpacking.            can see that unpacked pseudo-word performs
 There is no statistical difference between them. It       comparatively with pseudo-word without un-
 shows that the improvement derives from                   packing. There is no statistical difference be-

                                                         154


tween them. It shows that the improvement de-              proved correlation with human judgments. In
rives from pseudo-word itself as basic transla-            Proceedings of the ACL Workshop on Intrinsic and
tional unit, does not rely very much on higher             Extrinsic Evaluation Measures for Machine Trans-
language model order or longer max phrase                  lation and/or Summarization (ACL’05). 65–72.
length setting. In fact, slight improvement in         P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009. A
pwchpwen and pwchwen is seen after pseudo-word             Gibbs Sampler for Phrasal Synchronous
unpacking, which indicates that higher language            Grammar Induction. In Proceedings of ACL-
model order and longer max phrase length im-               IJCNLP, Singapore.
pact the performance in these two configurations.
                                                       P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian
                     UnpackingSMP         Baseline        synchronous grammar induction. In Proceed-
                                                          ings of NIPS 21, Vancouver, Canada.
             pwchpwen wchpwen pwchwen
 BLEU         0.3219     0.3192 0.3187     0.3146      P. Brown, S. Della Pietra, V. Della Pietra, and R.
  NIST        8.9458     8.9325 8.9801     8.8462         Mercer. 1993. The mathematics of machine
METEOR        0.5429     0.5424 0.5411     0.5335         translation: Parameter estimation. Computa-
 Table 6. Performance of pseudo-word unpacking on         tional Linguistics, 19:263–312.
                    large corpus.                      P.-C. Chang, M. Galley, and C. D. Manning. 2008.
3.5    Comparison to English Chunking                      Optimizing Chinese word segmentation for
                                                           machine translation performance. In Proceed-
English chunking is experimented to compare                ings of the 3rd Workshop on Statistical Machine
with pseudo-word. We use FlexCRFs (Xuan-                   Translation (SMT’08). 224–232.
Hieu Phan et al., 2005) to get English chunks.         Chen, Stanley F. and Joshua Goodman. 1998. An
Since there is no standard Chinese chunking data           empirical study of smoothing techniques for
and code, only English chunking is executed.               language modeling. Technical Report TR-10-98,
The experimental results show that English                 Harvard University Center for Research in Com-
chunking performs far below baseline, usually 8            puting Technology.
absolute BLEU points below. It shows that sim-
ple chunks are not suitable for being basic trans-     C. Cherry, D. Lin. 2007. Inversion transduction
lational units.                                            grammar for joint phrasal translation model-
                                                           ing. In Proc. of the HLTNAACL Workshop on
4     Conclusion                                           Syntax and Structure in Statistical Translation
                                                           (SSST 2007), Rochester, USA.
We have presented pseudo-word as a novel ma-
                                                       D. Chiang. 2007. Hierarchical phrase-based
chine translational unit for phrase-based machine
                                                         translation.Computational Linguistics, 33(2):201–
translation. It is proposed to replace too fine-         228.
grained word as basic translational unit. Pseudo-
word is a kind of basic multi-word expression          Y. Deng and W. Byrne. 2005. HMM word and
that characterizes minimal sequence of consecu-            phrase alignment for statistical machine trans-
tive words in sense of translation. By casting             lation. In Proc. of HLT-EMNLP, pages 169–176.
pseudo-word searching problem into a parsing           G. Doddington. 2002. Automatic evaluation of ma-
framework, we search for pseudo-words in poly-             chine translation quality using n-gram cooc-
nomial time. Experimental results of Chinese-to-           currence statistics. In Proceedings of the 2nd In-
English translation task show that, in phrase-             ternational Conference on Human Language Tech-
based machine translation model, pseudo-word               nology (HLT’02). 138–145.
performs significantly better than word in both
spoken language translation domain and news            Kneser, Reinhard and Hermann Ney. 1995. Improved
domain. Removing the power of higher order               backing-off for M-gram language modeling. In
                                                         Proceedings of the IEEE International Conference
language model and longer max phrase length,
                                                         on Acoustics, Speech, and Signal Processing,
which are inherent in pseudo-words, shows that           pages 181–184, Detroit, MI.
pseudo-words still improve translational per-
formance significantly over unary words.               P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
                                                          Federico, N. Bertoldi, B. Cowan,W. Shen, C.
References                                                Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
                                                          E. Herbst. 2007. Moses: Open source toolkit for
S. Banerjee, and A. Lavie. 2005. METEOR: An               statistical machine translation. In Proc. of the
    automatic metric for MT evaluation with im-

                                                     155


  45th Annual Meeting of the ACL (ACL-2007),             D. Wu. 1997. Stochastic inversion transduction
  Prague.                                                    grammars and bilingual parsing of parallel
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical             corpora. Computational Linguistics, 23(3):377–
   phrasebased translation. In Proc. of the 3rd In-          403.
   ternational conference on Human Language Tech-        J. Xu, Zens., and H. Ney. 2004. Do we need Chi-
   nology Research and 4th Annual Meeting of the             nese word segmentation for statistical ma-
   NAACL (HLT-NAACL 2003), 81–88, Edmonton,                  chine translation? In Proceedings of the ACL
   Canada.                                                   Workshop on Chinese Language          Processing
P. Koehn. 2004. Statistical Significance Tests for           SIGHAN’04). 122–128.
   Machine Translation Evaluation. In Proceed-           J. Xu, J. Gao, K. Toutanova, and H. Ney. 2008.
   ings of EMNLP.                                            Bayesian semi-supervised chinese word seg-
P. Lambert and R. Banchs. 2005. Data Inferred                mentation for statistical machine translation.
  Multi-word Expressions for Statistical Ma-                 In Proceedings of the 22nd International Confer-
  chine Translation. In Proceedings of MT Summit             ence on Computational Linguistics (COLING’08).
  X.                                                         1017–1024.
                                                         H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008.
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrap-
                                                             Bayesian learning of non-compositional
  ping word alignment via word packing. In Pro-
  ceedings of the 45th Annual Meeting of the Asso-           phrases with synchronous parsing. In Proc. of
  ciation of Computational Linguistics (ACL’07).             the 46th Annual Conference of the Association for
  304–311.                                                   Computational Linguistics: Human Language
                                                             Technologies (ACL-08:HLT), 97–105, Columbus,
Y. Ma, and A. Way. 2009. Bilingually Motivated               Ohio.
  Word Segmentation for Statistical Machine
                                                         R. Zhang, K. Yasuda, and E. Sumita. 2008. Improved
  Translation. In ACM Transactions on Asian Lan-             statistical machine translation by multiple
  guage Information Processing, 8(2).
                                                             Chinese word segmentation. In Proceedings of
D. Marcu,W.Wong. 2002. A phrase-based, joint                 the 3rd Workshop on Statistical Machine Transla-
  probability model for statistical machine                  tion (SMT’08). 216–223.
  translation. In Proc. of the 2002 Conference on
  Empirical Methods in Natural Language Process-
  ing (EMNLP-2002), 133–139, Philadelphia. Asso-
  ciation for Computational Linguistics.
F. J. Och. 2003. Minimum error rate training in
   statistical machine translation. In Proc. of ACL,
   pages 160–167.
F. J. Och and H. Ney. 2003. A systematic compari-
  son of various statistical alignment models.
  Computational Linguistics, 29(1):19–51.
Xuan-Hieu Phan, Le-Minh Nguyen, and Cam-Tu
  Nguyen. 2005. FlexCRFs: Flexible Conditional
  Random Field Toolkit, http://flexcrfs.sourceforge.
  net
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu:
  a method for automatic evaluation of machine
  translation, 2001.
M. Paul, 2008. Overview of the IWSLT 2008
  evaluation campaign. In Proc. of Internationa
  Workshop on Spoken Language Translation, 20-21
  October 2008.
A. Stolcke. (2002). SRILM - an extensible lan-
  guage modeling toolkit. In Proceedings of
  ICSLP, Denver, Colorado.



                                                       156

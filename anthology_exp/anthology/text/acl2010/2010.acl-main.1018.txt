                            “Was it good? It was provocative.”
                         Learning the meaning of scalar adjectives
     Marie-Catherine de Marneffe, Christopher D. Manning and Christopher Potts
                               Linguistics Department
                                 Stanford University
                                 Stanford, CA 94305
                      {mcdm,manning,cgpotts}@stanford.edu


                      Abstract                                 ‘yes’ or ‘no’ word, 44% of which they regard as
                                                               failing to convey a clear ‘yes’ or ‘no’ response. In
    Texts and dialogues often express infor-                   some cases, interpreting the answer is straightfor-
    mation indirectly. For instance, speak-                    ward (Was it bad? It was terrible.), but in others,
    ers’ answers to yes/no questions do not                    what to infer from the answer is unclear (Was it
    always straightforwardly convey a ‘yes’                    good? It was provocative.). It is even common
    or ‘no’ answer. The intended reply is                      for the speaker to explicitly convey his own uncer-
    clear in some cases (Was it good? It was                   tainty with such answers.
    great!) but uncertain in others (Was it                       In this paper, we focus on the interpretation
    acceptable? It was unprecedented.). In                     of answers to a particular class of polar ques-
    this paper, we present methods for inter-                  tions: ones in which the main predication in-
    preting the answers to questions like these                volves a gradable modifier (e.g., highly unusual,
    which involve scalar modifiers. We show                    not good, little) and the answer either involves an-
    how to ground scalar modifier meaning                      other gradable modifier or a numerical expression
    based on data collected from the Web. We                   (e.g., seven years old, twenty acres of land). Inter-
    learn scales between modifiers and infer                   preting such question–answer pairs requires deal-
    the extent to which a given answer conveys                 ing with modifier meanings, specifically, learning
    ‘yes’ or ‘no’. To evaluate the methods,                    context-dependent scales of expressions (Horn,
    we collected examples of question–answer                   1972; Fauconnier, 1975) that determine how, and
    pairs involving scalar modifiers from CNN                  to what extent, the answer as a whole resolves the
    transcripts and the Dialog Act corpus and                  issue raised by the question.
    use response distributions from Mechani-                      We propose two methods for learning the
    cal Turk workers to assess the degree to                   knowledge necessary for interpreting indirect an-
    which each answer conveys ‘yes’ or ‘no’.                   swers to questions involving gradable adjectives,
    Our experimental results closely match the                 depending on the type of predications in the ques-
    Turkers’ response data, demonstrating that                 tion and the answer. The first technique deals
    meanings can be learned from Web data                      with pairs of modifiers: we hypothesized that on-
    and that such meanings can drive prag-                     line, informal review corpora in which people’s
    matic inference.                                           comments have associated ratings would provide
                                                               a general-purpose database for mining scales be-
1   Introduction
                                                               tween modifiers. We thus use a large collection of
An important challenge for natural language pro-               online reviews to learn orderings between adjec-
cessing is how to learn not only basic linguistic              tives based on contextual entailment (good < ex-
meanings but also how those meanings are system-               cellent), and employ this scalar relationship to in-
atically enriched when expressed in context. For               fer a yes/no answer (subject to negation and other
instance, answers to polar (yes/no) questions do               qualifiers). The second strategy targets numeri-
not always explicitly contain a ‘yes’ or ‘no’, but             cal answers. Since it is unclear what kind of cor-
rather give information that the hearer can use to             pora would contain the relevant information, we
infer such an answer in a context with some degree             turn to the Web in general: we use distributional
of certainty. Hockey et al. (1997) find that 27% of            information retrieved via Web searches to assess
answers to polar questions do not contain a direct             whether the numerical measure counts as a posi-


                                                         167
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 167–176,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


tive or negative instance of the adjective in ques-                     a. The speaker is certain of ‘yes’ or ‘no’ and
tion. Both techniques exploit the same approach:                           conveys that directly and successfully to the
using texts (the Web) to learn meanings that can                           hearer.
drive pragmatic inference in dialogue. This paper                       b. The speaker is certain of ‘yes’ or ‘no’ but
demonstrates to some extent that meaning can be                            conveys this only partially to the hearer.
grounded from text in this way.                                         c. The speaker is uncertain of ‘yes’ or ‘no’ and
                                                                           conveys this uncertainty to the hearer.
2     Related work
                                                                        d. The speaker is uncertain of ‘yes’ or ‘no’,
Indirect speech acts are studied by Clark (1979),                          but the hearer infers one of those with con-
Perrault and Allen (1980), Allen and Perrault                              fidence.
(1980) and Asher and Lascarides (2003), who
identify a wide range of factors that govern how                       The uncertainty is especially pressing for pred-
speakers convey their intended messages and how                     ications built around scalar modifiers, which are
hearers seek to uncover those messages from                         inherently vague and highly context-dependent
uncertain and conflicting signals. In the com-                      (Kamp and Partee, 1995; Kennedy and McNally,
putational literature, Green and Carberry (1994,                    2005; Kennedy, 2007). For example, even if we
1999) provide an extensive model that interprets                    fix the basic sense for little to mean ‘young for a
and generates indirect answers to polar questions.                  human’, there is a substantial amount of gray area
They propose a logical inference model which                        between the clear instances (babies) and the clear
makes use of discourse plans and coherence rela-                    non-instances (adults). This is the source of un-
tions to infer categorical answers. However, to ad-                 certainty in (3), in which B’s children fall into the
equately interpret indirect answers, the uncertainty                gray area.
inherent in some answers needs to be captured (de                   (3)     A: Are your kids little?
Marneffe et al., 2009). While a straightforward                             B: I have a seven year-old and a ten
‘yes’ or ‘no’ response is clear in some indirect an-                           year-old.
swers, such as in (1), the intended answer is less
certain in other cases (2):1                                        3     Corpus description
(1) A: Do you think that’s a good idea, that we
                                                                    Since indirect answers are likely to arise in in-
           just begin to ignore these numbers?
                                                                    terviews, to gather instances of question–answer
       B: I think it’s an excellent idea.                           pairs involving gradable modifiers (which will
                                                                    serve to evaluate the learning techniques), we use
(2)    A: Is he qualified?
                                                                    online CNN interview transcripts from five dif-
       B: I think he’s young.                                       ferent shows aired between 2000 and 2008 (An-
   In (2), it might be that the answerer does not                   derson Cooper, Larry King Live, Late Edition,
know about qualifications or does not want to talk                  Lou Dobbs Tonight, The Situation Room). We
about these directly, and therefore shifts the topic                also searched the Switchboard Dialog Act corpus
slightly. As proposed by Zeevat (1994) in his work                  (Jurafsky et al., 1997). We used regular expres-
on partial answers, the speaker’s indirect answer                   sions and manual filtering to find examples of two-
might indicate that he is deliberately leaving the                  utterance dialogues in which the question and the
original question only partially addressed, while                   reply contain some kind of gradable modifier.
giving a fully resolving answer to another one.
The hearer must then interpret the answer to work                   3.1    Types of question–answer pairs
out the other question. In (2) in context, we get a                 In total, we ended up with 224 question–answer
sense that the speaker would resolve the issue to                   pairs involving gradable adjectives. However
‘no’, but that he is definitely not committed to that               our collection contains different types of answers,
in any strong sense. Uncertainty can thus reside                    which naturally fall into two categories: (I) in
both on the speaker and the hearer sides, and the                   205 dialogues, both the question and the answer
four following scenarios are attested in conversa-                  contain a gradable modifier; (II) in 19 dialogues,
tion:                                                               the reply contains a numerical measure (as in (3)
   1
     Here and throughout, the examples come from the corpus         above and (4)).
described in section 3.


                                                              168


                Modification in answer        Example     Count         3.2    Answer assignment
      I         Other adjective                (1), (2)     125
                Adverb - same adjective          (5)         55         To assess the degree to which each answer con-
                Negation - same adjective      (6), (7)      21         veys ‘yes’ or ‘no’ in context, we use response dis-
                Omitted adjective                (8)          4
      II        Numerical measure              (3), (4)      19         tributions from Mechanical Turk workers. Given a
                                                                        written dialogue between speakers A and B, Turk-
Table 1: Types of question–answer pairs, and                            ers were asked to judge what B’s answer conveys:
counts in the corpus.                                                   ‘definite yes’, ‘probable yes’, ‘uncertain’, ‘proba-
                                                                        ble no’, ‘definite no’. Within each of the two ‘yes’
           I      Modification in answer         Mean     SD            and ‘no’ pairs, there is a scalar relationship, but
                  Other adjective                 1.1     0.6
                  Adverb - same adjective         0.8     0.6           the pairs themselves are not in a scalar relationship
                  Negation - same adjective       1.0     0.5           with each other, and ‘uncertain’ is arguably a sep-
                  Omitted adjective               1.1     0.2           arate judgment. Figure 1 shows the exact formu-
           II     Numerical measure               1.5     0.2
                                                                        lation used in the experiment. For each dialogue,
Table 2: Mean entropy values and standard devi-                         we got answers from 30 Turkers, and we took the
ation obtained in the Mechanical Turk experiment                        dominant response as the correct one though we
for each question–answer pair category.                                 make extensive use of the full response distribu-
                                                                        tions in evaluating our approach.2 We also com-
                                                                        puted entropy values for the distribution of an-
                                                                        swers for each item. Overall, the agreement was
                                                                        good: 21 items have total agreement (entropy of
(4)        A: Have you been living there very long?                     0.0 — 11 in the “adjective” category, 9 in the
           B: I’m in here right now about twelve and                    “adverb-adjective” category and 1 in the “nega-
              a half years.                                             tion” category), and 80 items are such that a single
                                                                        response got chosen 20 or more times (entropy <
   Category I, which consists of pairs of modifiers,                    0.9). The dialogues in (1) and (9) are examples of
can be further divided. In most dialogues, the an-                      total agreement. In contrast, (10) has response en-
swer contains another adjective than the one used                       tropy of 1.1, and item (11) has the highest entropy
in the question, such as in (1). In others, the an-                     of 2.2.
swer contains the same adjective as in the ques-                        (9)    A: Advertisements can be good or bad.
tion, but modified by an adverb (e.g., very, basi-                                Was it a good ad?
cally, quite) as in (5) or a negation as in (6).
                                                                               B: It was a great ad.
(5)        A: That seems to be the biggest sign of
              progress there. Is that accurate?                         (10)    A: Am I clear?
           B: That’s absolutely accurate.                                       B: I wish you were a little more forthright.

(6)        A: Are you bitter?                                           (11)    A: 91 percent of the American people still
           B: I’m not bitter because I’m a soldier.                                express confidence in the long-term
The negation can be present in the main clause                                     prospect of the U.S. economy; only 8
when the adjectival predication is embedded, as in                                 percent are not confident. Are they
example (7).                                                                       overly optimistic, in your professional
                                                                                   assessment?
(7)        A: [. . . ] Is that fair?
                                                                            2
                                                                              120 Turkers were involved (the median number of items
           B: I don’t think that’s a fair statement.                    done was 28 and the mean 56.5). The Fleiss’ Kappa score for
In a few cases, when the question contains an ad-                       the five response categories is 0.46, though these categories
                                                                        are partially ordered. For the three-category response system
jective modifying a noun, the adjective is omitted                      used in section 5, which arguably has no scalar ordering, the
in the answer:                                                          Fleiss’ Kappa is 0.63. Despite variant individual judgments,
                                                                        aggregate annotations done with Mechanical Turk have been
(8)        A: Is that a huge gap in the system?                         shown to be reliable (Snow et al., 2008; Sheng et al., 2008;
           B: It is a gap.                                              Munro et al., 2010). Here, the relatively low Kappa scores
                                                                        also reflect the uncertainty inherent in many of our examples,
  Table 1 gives the distribution of the types ap-                       uncertainty that we seek to characterize and come to grips
pearing in the corpus.                                                  with computationally.



                                                                  169


 Indirect Answers to Yes/No Questions                                the question and the main predication PA in the an-
 In the following dialogue, speaker A asks a simple Yes/No
 question, but speaker B answers with something more in-
                                                                     swer is the primary factor in determining whether,
 direct and complicated.                                             and to what extent, ‘yes’ or ‘no’ was intended. If
 dialogue here                                                       PA is at least as strong as PQ , the intended answer
                                                                     is ‘yes’; if PA is weaker than PQ , the intended an-
 Which of the following best captures what speaker B
 meant here:                                                         swer is ‘no’; and, where no reliable entailment re-
                                                                     lationship exists between PA and PQ , the result is
      • B definitely meant to convey “Yes”.
                                                                     uncertainty.
      • B probably meant to convey “Yes”.                               For example, good is weaker (lower on the rel-
      • B definitely meant to convey “No”.                           evant scale) than excellent, and thus speakers in-
      • B probably meant to convey “No”.                             fer that the reply in example (1) above is meant to
      • (I really can’t tell whether B meant to convey “Yes”         convey ‘yes’. In contrast, if we reverse the order
        or “No”.)
                                                                     of the modifiers — roughly, Is it a great idea?;
                                                                     It’s a good idea — then speakers infer that the
Figure 1: Design of the Mechanical Turk experi-                      answer conveys ‘no’. Had B replied with It’s a
ment.                                                                complicated idea in (1), then uncertainty would
                                                                     likely have resulted, since good and complicated
                                                                     are not in a reliable scalar relationship. Negation
                                                                     reverses scales (Horn, 1972; Levinson, 2000), so it
                                                                     flips ‘yes’ and ‘no’ in these cases, leaving ‘uncer-
        B: I think it shows how wise the American
                                                                     tain’ unchanged. When both the question and the
           people are.
                                                                     answer contain a modifier (such as in (9–11)), the
   Table 2 shows the mean entropy values for the                     yes/no response should correlate with the extent to
different categories identified in the corpus. Inter-                which the pair of modifiers can be put into a scale
estingly, the pairs involving an adverbial modifi-                   based on contextual entailment.
cation in the answer all received a positive answer                     To ground such scales from text, we collected a
(‘yes’ or ‘probable yes’) as dominant response.                      large corpus of online reviews from IMDB. Each
All 19 dialogues involving a numerical measure                       of the reviews in this collection has an associated
had either ‘probable yes’ or ‘uncertain’ as domi-                    star rating: one star (most negative) to ten stars
nant response. There is thus a significant bias for                  (most positive). Table 3 summarizes the distribu-
positive answers: 70% of the category I items and                    tion of reviews as well as the number of words and
74% of the category II items have a positive an-                     vocabulary across the ten rating categories.
swer as dominant response. Examining a subset                           As is evident from table 3, there is a signif-
of the Dialog Act corpus, we found that 38% of                       icant bias for ten-star reviews. This is a com-
the yes/no questions receive a direct positive an-                   mon feature of such corpora of informal, user-
swers, whereas 21% have a direct negative answer.                    provided reviews (Chevalier and Mayzlin, 2006;
This bias probably stems from the fact that people                   Hu et al., 2006; Pang and Lee, 2008). However,
are more likely to use an overt denial expression                    since we do not want to incorporate the linguis-
where they need to disagree, whether or not they                     tically uninteresting fact that people tend to write
are responding indirectly.                                           a lot of ten-star reviews, we assume uniform pri-
                                                                     ors for the rating categories. Let count(w, r) be
4     Methods                                                        the number of tokens of word w in reviews in rat-
In this section, we present the methods we propose                   ing category r, and let count(r) be the total word
for grounding the meanings of scalar modifiers.                      count for all words in rating category r. The prob-
                                                                     ability of w given a rating category r is simply
4.1    Learning modifier scales and inferring                        Pr(w|r) = count(w, r)/ count(r). Then under the
       yes/no answers                                                assumption of uniform priors, we get Pr(r|w) =
                                                                     Pr(w|r)/ r0 ∈R Pr(w|r0 ).
                                                                               P
The first technique targets items such as the ones
in category I of our corpus. Our central hypothesis                     In reasoning about our dialogues, we rescale
is that, in polar question dialogues, the semantic                   the rating categories by subtracting 5.5 from each,
relationship between the main predication PQ in                      to center them at 0. This yields the scale R =



                                                               170


                                                                 Rating                   Reviews                          Words                             Vocabulary                   Average words per review
                                                                      1                    124,587                     25,389,211                               192,348                                     203.79
                                                                      2                     51,390                     11,750,820                               133,283                                     228.66
                                                                      3                     58,051                     13,990,519                               148,530                                     241.00
                                                                      4                     59,781                     14,958,477                               156,564                                     250.22
                                                                      5                     80,487                     20,382,805                               188,461                                     253.24
                                                                      6                    106,145                     27,408,662                               225,165                                     258.22
                                                                      7                    157,005                     40,176,069                               282,530                                     255.89
                                                                      8                    195,378                     48,706,843                               313,046                                     249.30
                                                                      9                    170,531                     40,264,174                               273,266                                     236.11
                                                                     10                    358,441                     73,929,298                               381,508                                     206.25
                                                                  Total                  1,361,796                    316,956,878                             1,160,072                                     206.25

Table 3: Numbers of reviews, words and vocabulary size per rating category in the IMDB review corpus,
as well as the average number of words per review.


                                             enjoyable                                                                       best                                                                    great                                                             superb
                  0.4




                                                                                           0.4




                                                                                                                                                                   0.4




                                                                                                                                                                                                                                          0.4
                               ER = 0.74                                                                ER = 1.08                                                                ER = 1.1                                                              ER = 2.18
                  0.3




                                                                                           0.3




                                                                                                                                                                   0.3




                                                                                                                                                                                                                                          0.3
                  0.2




                                                                                           0.2




                                                                                                                                                                   0.2




                                                                                                                                                                                                                                          0.2
                  0.1




                                                                                           0.1




                                                                                                                                                                   0.1




                                                                                                                                                                                                                                          0.1
Pr(Rating|Word)
                  0.0




                                                                                           0.0




                                                                                                                                                                   0.0




                                                                                                                                                                                                                                          0.0
                        -4.5
                               -3.5
                                      -2.5
                                             -1.5
                                                    -0.5
                                                           0.5
                                                                 1.5
                                                                       2.5
                                                                             3.5
                                                                                   4.5




                                                                                                 -4.5
                                                                                                        -3.5
                                                                                                               -2.5
                                                                                                                      -1.5
                                                                                                                             -0.5
                                                                                                                                    0.5
                                                                                                                                          1.5
                                                                                                                                                2.5
                                                                                                                                                      3.5
                                                                                                                                                            4.5




                                                                                                                                                                         -4.5
                                                                                                                                                                                -3.5
                                                                                                                                                                                       -2.5
                                                                                                                                                                                              -1.5
                                                                                                                                                                                                     -0.5
                                                                                                                                                                                                            0.5
                                                                                                                                                                                                                  1.5
                                                                                                                                                                                                                        2.5
                                                                                                                                                                                                                              3.5
                                                                                                                                                                                                                                    4.5




                                                                                                                                                                                                                                                -4.5
                                                                                                                                                                                                                                                       -3.5
                                                                                                                                                                                                                                                              -2.5
                                                                                                                                                                                                                                                                     -1.5
                                                                                                                                                                                                                                                                            -0.5
                                                                                                                                                                                                                                                                                   0.5
                                                                                                                                                                                                                                                                                         1.5
                                                                                                                                                                                                                                                                                               2.5
                                                                                                                                                                                                                                                                                                     3.5
                                                                                                                                                                                                                                                                                                           4.5
                                      disappointing                                                                          bad                                                                     awful                                                                  worst
                  0.4




                                                                                           0.4




                                                                                                                                                                   0.4




                                                                                                                                                                                                                                          0.4
                                                           ER = -1.1                                                                ER = -1.47                                                              ER = -2.5                                                              ER = -2.56
                  0.3




                                                                                           0.3




                                                                                                                                                                   0.3




                                                                                                                                                                                                                                          0.3
                  0.2




                                                                                           0.2




                                                                                                                                                                   0.2




                                                                                                                                                                                                                                          0.2
                  0.1




                                                                                           0.1




                                                                                                                                                                   0.1




                                                                                                                                                                                                                                          0.1
                  0.0




                                                                                           0.0




                                                                                                                                                                   0.0




                                                                                                                                                                                                                                          0.0
                        -4.5
                               -3.5
                                      -2.5
                                             -1.5
                                                    -0.5
                                                           0.5
                                                                 1.5
                                                                       2.5
                                                                             3.5
                                                                                   4.5




                                                                                                 -4.5
                                                                                                        -3.5
                                                                                                               -2.5
                                                                                                                      -1.5
                                                                                                                             -0.5
                                                                                                                                    0.5
                                                                                                                                          1.5
                                                                                                                                                2.5
                                                                                                                                                      3.5
                                                                                                                                                            4.5




                                                                                                                                                                         -4.5
                                                                                                                                                                                -3.5
                                                                                                                                                                                       -2.5
                                                                                                                                                                                              -1.5
                                                                                                                                                                                                     -0.5
                                                                                                                                                                                                            0.5
                                                                                                                                                                                                                  1.5
                                                                                                                                                                                                                        2.5
                                                                                                                                                                                                                              3.5
                                                                                                                                                                                                                                    4.5




                                                                                                                                                                                                                                                -4.5
                                                                                                                                                                                                                                                       -3.5
                                                                                                                                                                                                                                                              -2.5
                                                                                                                                                                                                                                                                     -1.5
                                                                                                                                                                                                                                                                            -0.5
                                                                                                                                                                                                                                                                                   0.5
                                                                                                                                                                                                                                                                                         1.5
                                                                                                                                                                                                                                                                                               2.5
                                                                                                                                                                                                                                                                                                     3.5
                                                                                                                                                                                                                                                                                                           4.5
                                                                                                                                      Rating (centered at 0)


Figure 2: The distribution of some scalar modifiers across the ten rating categories. The vertical lines
mark the expected ratings, defined as a weighted sum of the probability values (black dots).


h−4.5, −3.5, −2.5, −1.5, −0.5, 0.5, 1.5, 2.5, 3.5, 4.5i.                                                                                                          and negative, across the rescaled ratings, with
Our rationale for this is that modifiers at the neg-                                                                                                              the vertical lines marking their ER values. The
ative end of the scale (bad, awful, terrible) are                                                                                                                 weak scalar modifiers all the way on the left are
not linguistically comparable to those at the                                                                                                                     most common near the middle of the scale, with
positive end of the scale (good, excellent, superb).                                                                                                              a slight positive bias in the top row and a slight
Each group forms its own qualitatively different                                                                                                                  negative bias in the bottom row. As we move
scale (Kennedy and McNally, 2005). Rescaling                                                                                                                      from left to right, the bias for one end of the scale
allows us to make a basic positive vs. negative                                                                                                                   grows more extreme, until the words in question
distinction. Once we have done that, an increase                                                                                                                  are almost never used outside of the most extreme
in absolute value is an increase in strength. In                                                                                                                  rating category. The resulting scales correspond
our experiments, we use expected rating values                                                                                                                    well with linguistic intuitions and thus provide
to characterize the polarity and strength of mod-                                                                                                                 an initial indication that the rating categories
ifiers. The expected rating value for a word w                                                                                                                    are a reliable guide to strength and polarity for
is ER(w) = r∈R r Pr(r|w). Figure 2 plots these
             P
                                                                                                                                                                  scalar modifiers. We put this information to use
values for a number of scalar terms, both positive                                                                                                                in our dialogue corpus via the decision procedure


                                                                                                                                                        171


 Let D be a dialogue consisting of (i) a polar question                tract ages from the positive and negative snippets
 whose main predication is based on scalar predicate PQ
 and (ii) an indirect answer whose main predication is
                                                                       obtained, and we fit a logistic regression to these
 based on scalar predicate PA . Then:                                  data. To remove noise, we discard low counts
                                                                       (positive and negative instances for a given unit
      1. if PA or PQ is missing from our data, infer ‘Uncer-
         tain’;                                                        < 5). Also, for some adjectives, such as little or
                                                                       young, there is an inherent ambiguity between ab-
      2. else if ER(PQ ) and ER(PA ) have different signs, in-
         fer ‘No’;
                                                                       solute and relative uses. Ideally, a word sense dis-
                                                                       ambiguation system would be used to filter these
      3. else if abs(ER(PQ )) 6 abs(ER(PA )), infer ‘Yes’;             cases. For now, we extract the largest contiguous
      4. else infer ‘No’.                                              range for which the data counts are over the noise
                                                                       threshold.3 When not enough data is retrieved for
      5. In the presence of negation, map ‘Yes’ to ‘No’, ‘No’
         to ‘Yes’, and ‘Uncertain’ to ‘Uncertain’.                     the negative examples, we expand the query by
                                                                       moving the negation outside the search phrase. We
                                                                       also replace the negation and the adjective by the
Figure 3: Decision procedure for using the word                        antonyms given in WordNet (using the first sense).
frequencies across rating categories in the review                        The logistic regression thus has only one fac-
corpus to decide what a given answer conveys.                          tor — the unit of measure (age in the case of lit-
                                                                       tle kids). For a given answer, the model assigns a
                                                                       probability indicating the extent to which the ad-
                                                                       jectival property applies to that answer. If the fac-
described in figure 3.                                                 tor is a significant predictor, we can use the prob-
                                                                       abilities from the model to decide whether the an-
4.2     Interpreting numerical answers                                 swer qualifies as a positive or negative instance of
The second technique aims at determining                               the adjective in the question, and thus interpret the
whether a numerical answer counts as a positive                        indirect response as a ‘yes’ or a ‘no’. The prob-
or negative instance of the adjective in the ques-                     abilistic nature of this technique adheres perfectly
tion (category II in our corpus).                                      to the fact that indirect answers are intimately tied
   Adjectives that can receive a conventional unit                     up with uncertainty.
of measure, such as little or long, inherently pos-
                                                                       5    Evaluation and results
sess a degree of vagueness (Kamp and Partee,
1995; Kennedy, 2007): while in the extreme cases,                      Our primary goal is to evaluate how well we can
judgments are strong (e.g., a six foot tall woman                      learn the relevant scalar and entailment relation-
can clearly be called “a tall woman” whereas a                         ships from the Web. In the evaluation, we thus ap-
five foot tall woman cannot), there are borderline                     plied our techniques to a manually coded corpus
cases for which it is difficult to say whether the                     version. For the adjectival scales, we annotated
adjectival predication can truthfully be ascribed                      each example for its main predication (modifier, or
to them. A logistic regression model can capture                       adverb–modifier bigram), including whether that
these facts. To build this model, we gather distri-                    predication was negated. For the numerical cases,
butional information from the Web.                                     we manually constructed the initial queries: we
   For instance, in the case of (3), we can retrieve                   identified the adjective and the modified entity in
from the Web positive and negative examples of                         the question, and the unit of measure in the answer.
age in relation to the adjective and the modified en-                  However, we believe that identifying the requisite
tity “little kids”. The question contains the adjec-                   predications and recognizing the presence of nega-
tive and the modified entity. The reply contains the                   tion or embedding could be done automatically us-
unit of measure (here “year-old”) and the numer-                       ing dependency graphs.4
ical answer. Specifically we query the Web using                           3
                                                                             Otherwise, our model is ruined by references to “young
Yahoo! BOSS (Academic) for “little kids” year-                         80-year olds”, using the relative sense of young, which are
old (positive instances) as well as for “not little                    moderately frequent on the Web.
                                                                           4
                                                                             As a test, we transformed our corpus into the Stanford
kids” year-old (negative instances). Yahoo! BOSS                       dependency representation (de Marneffe et al., 2006), using
is an open search services platform that provides a                    the Stanford parser (Klein and Manning, 2003) and were able
query API for Yahoo! Web search. We then ex-                           to automatically retrieve all negated modifier predications,
                                                                       except one (We had a view of it, not a particularly good one),


                                                                 172


                 Modification in answer      Precision        Recall                          Response    Precision   Recall    F1
    I            Other adjective                    60           60           WordNet-based        Yes       82        83      82.5
                 Adverb - same adjective            95           95           (items I)            No        60        56        58
                 Negation - same adjective        100           100
                 Omitted adjective                100           100          Table 6: Precision, recall, and F1 (%) per response
    II           Numerical                          89           40
    Total                                           75           71
                                                                             category for the WordNet-based approach.

Table 4: Summary of precision and recall (%) by
type.
                    Response    Precision    Recall      F1                  somewhat mixed for the “Other adjective” cate-
            I            Yes       87         76         81
                          No       57         71         63
                                                                             gory.
            II           Yes       100        36         53                     Inferring the relation between scalar adjectives
                    Uncertain      67         40         50                  has some connection with work in sentiment de-
                                                                             tection. Even though most of the research in that
Table 5: Precision, recall, and F1 (%) per response
                                                                             domain focuses on the orientation of one term us-
category. In the case of the scalar modifiers exper-
                                                                             ing seed sets, techniques which provide the ori-
iment, there were just two examples whose dom-
                                                                             entation strength could be used to infer a scalar
inant response from the Turkers was ‘Uncertain’,
                                                                             relation between adjectives. For instance, Blair-
so we have left that category out of the results. In
                                                                             Goldensohn et al. (2008) use WordNet to develop
the case of the numerical experiment, there were
                                                                             sentiment lexicons in which each word has a posi-
not any ‘No’ answers.
                                                                             tive or negative value associated with it, represent-
                                                                             ing its strength. The algorithm begins with seed
                                                                             sets of positive, negative, and neutral terms, and
                                                                             then uses the synonym and antonym structure of
   To evaluate the techniques, we pool the Me-                               WordNet to expand those initial sets and refine
chanical Turk ‘definite yes’ and ‘probable yes’                              the relative strength values. Using our own seed
categories into a single category ‘Yes’, and we                              sets, we built a lexicon using Blair-Goldensohn
do the same for ‘definite no’ and ‘probable no’.                             et al. (2008)’s method and applied it as in figure
Together with ‘uncertain’, this makes for three-                             3 (changing the ER values to sentiment scores).
response categories. We count an inference as                                Both approaches achieve similar results: for the
successful if it matches the dominant Turker re-                             “Other adjective” category, the WordNet-based
sponse category. To use the three-response scheme                            approach yields 56% accuracy, which is not signif-
in the numerical experiment, we simply catego-                               icantly different from our performance (60%); for
rize the probabilities as follows: 0–0.33 = ‘No’,                            the other types in category I, there is no difference
0.33–0.66 = ‘Uncertain’, 0.66–1.00 = ‘Yes’.                                  in results between the two methods. Table 6 sum-
   Table 4 gives a breakdown of our system’s per-                            marizes the results per response category for the
formance on the various category subtypes. The                               WordNet-based approach (which can thus be com-
overall accuracy level is 71% (159 out of 224 in-                            pared to the category I results in table 5). However
ferences correct). Table 5 summarizes the results                            in contrast to the WordNet-based approach, we re-
per response category, for the examples in which                             quire no hand-built resources: the synonym and
both the question and answer contain a gradable                              antonym structures, as well as the strength values,
modifier (category I), and for the numerical cases                           are learned from Web data alone. In addition, the
(category II).                                                               WordNet-based approach must be supplemented
                                                                             with a separate method for the numerical cases.
6       Analysis and discussion                                                 In the “Other adjective” category, 31 items
                                                                             involve oppositional terms: canonical antonyms
Performance is extremely good on the “Adverb –
                                                                             (e.g., right/wrong, good/bad) as well as terms
same adjective” and “Negation – same adjective”
                                                                             that are “statistically oppositional” (e.g., ready/
cases because the ‘Yes’ answer is fairly direct for
                                                                             premature, true/preposterous, confident/nervous).
them (though adverbs like basically introduce an
                                                                             “Statistically oppositional” terms are not opposi-
interesting level of uncertainty). The results are
                                                                             tional by definition, but as a matter of contingent
because of a parse error which led to wrong dependencies.                    fact. Our technique accurately deals with most


                                                                       173


                                                    little kids                                                                     young kids                                                                                                                      warm weather

                                0.8




                                                                                                                0.8




                                                                                                                                                                                                                                              0.8
                                                                                                                                                                                                                                              0.7
                                                                                 Probability of being "young"




                                                                                                                                                                                                                Probability of being "warm"
                                0.6
Probability of being "little"




                                                                                                                0.6




                                                                                                                                                                                                                                              0.6
                                0.4




                                                                                                                0.4




                                                                                                                                                                                                                                              0.5
                                0.2




                                                                                                                                                                                                                                              0.4
                                                                                                                0.2




                                                                                                                                                                                                                                              0.3
                                0.0




                                      0   10   20      30         40   50   60                                        0   10   20      30                                                     40     50    60                                        0    20       40         60   80   100     120

                                                       Age                                                                             Age                                                                                                                                Degree




Figure 4: Probabilities of being appropriately described as “little”, “young” or “warm”, fitted on data
retrieved when querying the Web for “little kids”, “young kids” and “warm weather”.


of the canonical antonyms, and also finds some
contingent oppositions (qualified/young, wise/
neurotic) that are lacking in antonymy resources or
automatically generated antonymy lists (Moham-
mad et al., 2008). Out of these 31 items, our tech-
nique correctly marks 18, whereas Mohammad et
                                                                                                                                                                                               1




al.’s list of antonyms only contains 5 and Blair-
                                                                                                                                                                                               0.9
                                                                                                                                             Probability of correct inference by our system




Goldensohn et al. (2008)’s technique finds 11. Our
                                                                                                                                                                                               0.8




technique is solely based on unigrams, and could
                                                                                                                                                                                               0.7




be improved by adding context: making use of de-
                                                                                                                                                                                               0.6




pendency information, as well as moving beyond
                                                                                                                                                                                               0.5




unigrams.
                                                                                                                                                                                               0.4




   In the numerical cases, precision is high but re-
                                                                                                                                                                                               0.3




call is low. For roughly half of the items, not
                                                                                                                                                                                               0.2




enough negative instances can be gathered from
                                                                                                                                                                                               0.1




the Web and the model lacks predictive power (as
                                                                                                                                                                                               0




for items (4) or (12)).
                                                                                                                                                                                                     0.0                                       0.5                      1.0               1.5
(12) A: Do you happen to be working for a
                                                                                                                                                                                                                                              Entropy of response distribution
            large firm?
        B: It’s about three hundred and fifty
            people.                                                                                                                         Figure 5: Correlation between agreement among
                                                                                                                                            Turkers and whether the system gets the correct
   Looking at the negative hits for item (12), one
                                                                                                                                            answer. For each dialogue, we plot a circle at
sees that few give an indication about the num-
                                                                                                                                            Turker response entropy and either 1 = correct
ber of people in the firm, but rather qualifications
                                                                                                                                            inference or 0 = incorrect inference, except the
about colleagues or employees (great people, peo-
                                                                                                                                            points are jittered a little vertically to show where
ple’s productivity), or the hits are less relevant:
                                                                                                                                            the mass of data lies. As the entropy rises (i.e., as
“Most of the people I talked to were actually pretty
                                                                                                                                            agreement levels fall), the system’s inferences be-
optimistic. They were rosy on the job market
                                                                                                                                            come less accurate. The fitted logistic regression
and many had jobs, although most were not large
                                                                                                                                            model (black line) has a statistically significant co-
firm jobs”. The lack of data comes from the fact
                                                                                                                                            efficient for response entropy (p < 0.001).
that the queries are very specific, since the adjec-
tive depends on the product (e.g., “expensive ex-
ercise bike”, “deep pond”). However when we
do get a predictive model, the probabilities corre-


                                                                                                                                    174


late almost perfectly with the Turkers’ responses.            levels drop.
This happens for 8 items: “expensive to call (50
cents a minute)”, “little kids (7 and 10 year-old)”,          7   Conclusion
“long growing season (3 months)”, “lot of land                We set out to find techniques for grounding ba-
(80 acres)”, “warm weather (80 degrees)”, “young              sic meanings from text and enriching those mean-
kids (5 and 2 year-old)”, “young person (31 year-             ings based on information from the immediate lin-
old)” and “large house (2450 square feet)”. In                guistic context. We focus on gradable modifiers,
the latter case only, the system output (uncer-               seeking to learn scalar relationships between their
tain) doesn’t correlate with the Turkers’ judgment            meanings and to obtain an empirically grounded,
(where the dominant answer is ‘probable yes’ with             probabilistic understanding of the clear and fuzzy
15 responses, and 11 answers are ‘uncertain’).                cases that they often give rise to (Kamp and Partee,
   The logistic curves in figure 4 capture nicely the         1995). We show that it is possible to learn the req-
intuitions that people have about the relation be-            uisite scales between modifiers using review cor-
tween age and “little kids” or “young kids”, as               pora, and to use that knowledge to drive inference
well as between Fahrenheit degrees and “warm                  in indirect responses. When the relation in ques-
weather”. For “little kids”, the probabilities of be-         tion is not too specific, we show that it is also pos-
ing little or not are clear-cut for ages below 7 and          sible to learn the strength of the relation between
above 15, but there is a region of vagueness in be-           an adjective and a numerical measure.
tween. In the case of “young kids”, the probabil-
ities drop less quickly with age increasing (an 18            Acknowledgments
year-old can indeed still be qualified as a “young
                                                              This paper is based on work funded in part by
kid”). In sum, when the data is available, this
                                                              ONR award N00014-10-1-0109 and ARO MURI
method delivers models which fit humans’ intu-
                                                              award 548106, as well as by the Air Force Re-
itions about the relation between numerical mea-
                                                              search Laboratory (AFRL) under prime contract
sure and adjective, and can handle pragmatic in-
                                                              no. FA8750-09-C-0181. Any opinions, findings,
ference.
                                                              and conclusion or recommendations expressed in
   If we restrict attention to the 66 examples on
                                                              this material are those of the authors and do not
which the Turkers completely agreed about which
                                                              necessarily reflect the view of the Air Force Re-
of these three categories was intended (again pool-
                                                              search Laboratory (AFRL), ARO or ONR.
ing ‘probable’ and ‘definite’), then the percent-
age of correct inferences rises to 89% (59 cor-
rect inferences). Figure 5 plots the relation-                References
ship between the response entropy and the accu-
                                                              James F. Allen and C. Raymond Perrault. 1980. Ana-
racy of our decision procedure, along with a fit-               lyzing intention in utterances. Artificial Intelligence,
ted logistic regression model using response en-                15:143–178.
tropy to predict whether our system’s inference
was correct. The handful of empirical points in               Nicholas Asher and Alex Lascarides. 2003. Logics of
                                                                Conversation. Cambridge University Press, Cam-
the lower left of the figure show cases of high                 bridge.
agreement between Turkers but incorrect infer-
ence from the system. The few points in the up-               Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
                                                                ald, Tyler Neylon, George A. Reis, and Jeff Reynar.
per right indicate low agreement between Turk-                  2008. Building a sentiment summarizer for local
ers and correct inference from the system. Three                service reviews. In WWW Workshop on NLP in the
of the high-agreement/incorrect-inference cases                 Information Explosion Era (NLPIX).
involve the adjectives right–correct. For low-
                                                              Judith A. Chevalier and Dina Mayzlin. 2006. The
agreement/correct-inference, the disparity could                effect of word of mouth on sales: Online book re-
trace to context dependency: the ordering is clear              views. Journal of Marketing Research, 43(3):345–
in the context of product reviews, but unclear in               354.
a television interview. The analysis suggests that            Herbert H. Clark. 1979. Responding to indirect speech
overall agreement is positively correlated with our             acts. Cognitive Psychology, 11:430–477.
system’s chances of making a correct inference:
our system’s accuracy drops as human agreement                Marie-Catherine de Marneffe, Bill MacCartney, and
                                                               Christopher D. Manning. 2006. Generating typed


                                                        175


  dependency parses from phrase structure parses. In           Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
  Proceedings of the 5th International Conference on             2008. Computing word-pair antonymy. In Proceed-
  Language Resources and Evaluation (LREC-2006).                 ings of the Conference on Empirical Methods in Nat-
                                                                 ural Language Processing and Computational Nat-
Marie-Catherine de Marneffe, Scott Grimm, and                    ural Language Learning (EMNLP-2008).
 Christopher Potts. 2009. Not a simple ‘yes’ or
 ‘no’: Uncertainty in indirect answers. In Proceed-            Robert Munro, Steven Bethard, Victor Kuperman,
 ings of the 10th Annual SIGDIAL Meeting on Dis-                 Vicky Tzuyin Lai, Robin Melnick, Christopher
 course and Dialogue.                                            Potts, Tyler Schnoebelen, and Harry Tily. 2010.
                                                                 Crowdsourcing and language studies: The new gen-
Gilles Fauconnier. 1975. Pragmatic scales and logical            eration of linguistic data. In NAACL 2010 Workshop
  structure. Linguistic Inquiry, 6(3):353–375.                   on Creating Speech and Language Data With Ama-
                                                                 zon’s Mechanical Turk.
Nancy Green and Sandra Carberry. 1994. A hybrid
  reasoning model for indirect answers. In Proceed-            Bo Pang and Lillian Lee. 2008. Opinion mining and
  ings of the 32nd Annual Meeting of the Association             sentiment analysis. Foundations and Trends in In-
  for Computational Linguistics, pages 58–65.                    formation Retrieval, 2(1):1–135.
Nancy Green and Sandra Carberry. 1999. Interpret-              C. Raymond Perrault and James F. Allen. 1980. A
  ing and generating indirect answers. Computational             plan-based analysis of indirect speech acts. Amer-
  Linguistics, 25(3):389–435.                                    ican Journal of Computational Linguistics, 6(3-
                                                                 4):167–182.
Beth Ann Hockey, Deborah Rossen-Knill, Beverly
  Spejewski, Matthew Stone, and Stephen Isard.                 Victor S. Sheng, Foster Provost, and Panagiotis G.
  1997. Can you predict answers to Y/N questions?                Ipeirotis. 2008. Get another label? improving data
  Yes, No and Stuff. In Proceedings of Eurospeech                quality and data mining using multiple, noisy label-
  1997, pages 2267–2270.                                         ers. In Proceedings of KDD-2008.
Laurence R Horn. 1972. On the Semantic Properties of           Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
  Logical Operators in English. Ph.D. thesis, UCLA,              Andrew Y. Ng. 2008. Cheap and fast – but is it
  Los Angeles.                                                   good? evaluating non-expert annotations for natural
                                                                 language tasks. In Proceedings of the Conference on
Nan Hu, Paul A. Pavlou, and Jennifer Zhang. 2006.                Empirical Methods in Natural Language Process-
  Can online reviews reveal a product’s true quality?:           ing and Computational Natural Language Learning
  Empirical findings and analytical modeling of online           (EMNLP-2008).
  word-of-mouth communication. In Proceedings of
  Electronic Commerce (EC), pages 324–330.                     Henk Zeevat. 1994. Questions and exhaustivity in up-
                                                                 date semantics. In Harry Bunt, Reinhard Muskens,
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-               and Gerrit Rentier, editors, Proceedings of the In-
  asca. 1997. Switchboard SWBD-DAMSL shallow-                    ternational Workshop on Computational Semantics,
  discourse-function annotation coders manual, draft             pages 211–221.
  13. Technical Report 97-02, University of Colorado,
  Boulder Institute of Cognitive Science.
Hans Kamp and Barbara H. Partee. 1995. Prototype
  theory and compositionality. Cognition, 57(2):129–
  191.
Christopher Kennedy and Louise McNally. 2005.
  Scale structure and the semantic typology of grad-
  able predicates. Language, 81(2):345–381.
Christopher Kennedy. 2007. Vagueness and grammar:
  The semantics of relative and absolute gradable ad-
  jectives. Linguistics and Philosophy, 30(1):1–45.
Dan Klein and Christopher D. Manning. 2003. Ac-
  curate unlexicalized parsing. In Proceedings of the
  41st Meeting of the Association of Computational
  Linguistics.
Stephen C. Levinson. 2000. Presumptive Meanings:
   The Theory of Generalized Conversational Implica-
   ture. MIT Press, Cambridge, MA.




                                                         176

             Syntactic and Semantic Factors in Processing Difficulty:
                             An Integrated Measure
                Jeff Mitchell, Mirella Lapata, Vera Demberg and Frank Keller
                                     University of Edinburgh
                                  Edinburgh, United Kingdom
                      jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk,
                        v.demberg@ed.ac.uk, keller@inf.ed.ac.uk

                      Abstract                                 hear the word eat (Altmann and Kamide 1999).
                                                               The second type of prediction is syntactic predic-
    The analysis of reading times can pro-                     tion. Comprehenders are faster at naming words
    vide insights into the processes that under-               that are syntactically compatible with prior con-
    lie language comprehension, with longer                    text, even when they bear no semantic relationship
    reading times indicating greater cognitive                 to the context (Wright and Garrett 1984). Another
    load. There is evidence that the language                  instance of syntactic prediction has been reported
    processor is highly predictive, such that                  by Staub and Clifton (2006): following the word
    prior context allows upcoming linguistic                   either, readers predict or and the complement that
    material to be anticipated. Previous work                  follows it, and process it faster compared to a con-
    has investigated the contributions of se-                  trol condition without either.
    mantic and syntactic contexts in isolation,                   Thus, human language processing takes advan-
    essentially treating them as independent                   tage of the constraints imposed by the preceding
    factors. In this paper we analyze reading                  semantic and syntactic context to derive expecta-
    times in terms of a single predictive mea-                 tions about the upcoming input. Much recent work
    sure which integrates a model of seman-                    has focused on developing computational mea-
    tic composition with an incremental parser                 sures of these constraints and expectations. Again,
    and a language model.                                      the literature is split into syntactic and semantic
                                                               models. Probably the best known measure of syn-
1   Introduction                                               tactic expectation is surprisal (Hale 2001) which
Psycholinguists have long realized that language               can be coarsely defined as the negative log proba-
comprehension is highly incremental, with readers              bility of word wt given the preceding words, typ-
and listeners continuously extracting the meaning              ically computed using a probabilistic context-free
of utterances on a word-by-word basis. As soon                 grammar.
as they encounter a word in a sentence, they inte-                Modeling work on semantic constraint focuses
grate it as fully as possible into a representation            on the degree to which a word is related to its
of the sentence thus far (Marslen-Wilson 1973;                 preceding context. Pynte et al. (2008) use La-
Konieczny 2000; Tanenhaus et al. 1995; Sturt and               tent Semantic Analysis (LSA, Landauer and Du-
Lombardo 2005). Recent research suggests that                  mais 1997) to assess the degree of contextual con-
language comprehension can also be highly pre-                 straint exerted on a word by its context. In this
dictive, i.e., comprehenders are able to anticipate            framework, word meanings are represented as vec-
upcoming linguistic material. This is beneficial as            tors in a high dimensional space and distance in
it gives them more time to keep up with the in-                this space is interpreted as an index of process-
put, and predictions can be used to compensate for             ing difficulty. Other work (McDonald and Brew
problems with noise or ambiguity.                              2004) models contextual constraint in information
   Two types of prediction have been observed in               theoretic terms. The assumption is that words
the literature. The first type is semantic predic-             carry prior semantic expectations which are up-
tion, as evidenced in semantic priming: a word                 dated upon seeing the next word. Expectations are
that is preceded by a semantically related prime               represented by a vector of probabilities which re-
or a semantically congruous sentence fragment is               flects the likely location in semantic space of the
processed faster (Stanovich and West 1981; van                 upcoming word.
Berkum et al. 1999; Clifton et al. 2007). Another                 The measures discussed above are typically
example is argument prediction: listeners are able             computed automatically on real-language corpora
to launch eye-movements to the predicted argu-                 using data-driven methods and their predictions
ment of a verb before having encountered it, e.g.,             are verified through analysis of eye-movements
they will fixate an edible object as soon as they              that people make while reading. Ample evidence


                                                         196
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196–206,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


(Rayner 1998) demonstrates that eye-movements                 accounted for by costs associated with low-level
are related to the moment-to-moment cognitive ac-             features of the stimuli, e.g.. relating to orthography
tivities of readers. They also provide an accurate            and eye-movement control (Rayner 1998). In ad-
temporal record of the on-line processing of nat-             dition, there may also be costs associated with the
ural language, and through the analysis of eye-               integration of new input into an incremental rep-
movement measurements (e.g., the amount of time               resentation. Dependency Locality Theory (DLT,
spent looking at a word) can give insight into the            Gibson 2000) is essentially a distance-based mea-
processing difficulty involved in reading.                    sure of the amount of processing effort required
   In this paper, we investigate a model of predic-           when the head of a phrase is integrated with its
tion that is incremental and takes into account syn-          syntactic dependents. We do not consider integra-
tactic as well as semantic constraint. The model              tion costs here (as they have not been shown to
essentially integrates the predictions of an incre-           correlate reliably with reading times; see Demberg
mental parser (Roark 2001) together with those                and Keller 2008 for details) and instead focus on
of a semantic space model (Mitchell and Lap-                  the costs associated with semantic and syntactic
ata 2009). The latter creates meaning representa-             constraint and low-level features, which appear to
tions compositionally, and therefore builds seman-            make the most substantial contributions.
tic expectations for word sequences (e.g., phrases,               In the following subsections we describe the
sentences, even documents) rather than isolated               various features which contribute to the process-
words. Some existing models of sentence process-              ing costs of a word in context. We begin by look-
ing integrate semantic information into a prob-               ing at the low-level costs and move on to con-
abilistic parser (Narayanan and Jurafsky 2002;                sider the costs associated with syntactic and se-
Padó et al. 2009); however, the semantic compo-              mantic constraint. For readers unfamiliar with the
nent of these models is limited to semantic role in-          methodology involved in modeling eye-tracking
formation, rather than attempting to build a full se-         data, we note that regression analysis (or the more
mantic representation for a sentence. Furthermore,            general mixed effects models) is typically used to
the models of Narayanan and Jurafsky (2002) and               study the relationship between dependent and in-
Padó et al. (2009) do not explicitly model pre-              dependent variables. The independent variables
diction, but rather focus on accounting for garden            are the various costs of processing effort and
path effects. The proposed model simultaneously               the dependent variables are measurements of eye-
captures semantic and syntactic effects in a sin-             movements, three of which are routinely used in
gle measure which we empirically show is predic-              the literature: first fixation duration (the duration
tive of processing difficulty as manifested in eye-           of the first fixation on a word regardless of whether
movements.                                                    it is the first fixation on a word or the first of mul-
                                                              tiple fixations on the same word), first pass dura-
2   Models of Processing Difficulty                           tion, also known as gaze duration, (the sum of all
                                                              fixations made on a word prior to looking at an-
As described in Section 1, reading times provide              other word), and total reading time (the sum of
an insight into the various cognitive activities that         all fixations on a word including refixations after
contribute to the overall processing difficulty in-           moving on to other words).
volved in comprehending a written text. To quan-
tify and understand the overall cognitive load asso-          2.1   Low-level Costs
ciated with processing a word in context, we will             Low-level features include word frequency (more
break that load down into a sum of terms repre-               frequent words are read faster), word length
senting distinct computational costs (semantic and            (shorter words are read faster), and the position
syntactic). For example, surprisal can be thought             of the word in the sentence (later words are read
of as measuring the cost of dealing with unex-                faster). Oculomotor variables have also been
pected input. When a word conforms to the lan-                found to influence reading times. These include
guage processor’s expectations, surprisal is low,             previous fixation (indicating whether or not the
and the cognitive load associated with processing             previous word has been fixated), launch distance
that input will also be low. In contrast, unexpected          (how many characters intervene between the cur-
words will have a high surprisal and a high cogni-            rent fixation and the previous fixation), and land-
tive cost.                                                    ing position (which letter in the word the fixation
   However, high-level syntactic and semantic fac-            landed on).
tors are only one source of cognitive costs. A siz-              Information about the sequential context of a
able proportion of the variance in reading times is           word can also influence reading times. Mc-


                                                        197


Donald and Shillcock (2003) show that forward                            (e.g., left-to-right vs. top-down, PCFGs vs de-
and backward transitional probabilities are pre-                         pendency parsing) and different degrees of lexi-
dictive of first fixation and first pass durations:                      calization (see Roark et al. 2009 for an overview) .
the higher the transitional probability, the shorter                     For instance, unlexicalized surprisal can be easily
the fixation time. Backward transitional prob-                           derived by substituting the words in Equation (1)
ability is essentially the conditional probabil-                         with parts of speech (Demberg and Keller 2008).
ity of a word given its immediately preceding                            Surprisal could be also defined using a vanilla
word, P(wk |wk−1 ). Analogously, forward proba-                          language model that does not take any structural
bility is the conditional probability of the current                     or grammatical information into account (Frank
word given the next word, P(wk |wk+1 ).                                  2009).

2.2    Syntactic Constraint                                              2.3   Semantic Constraint
As mentioned earlier, surprisal (Hale 2001; Levy                         Distributional models of meaning have been com-
2008) is one of the best known models of process-                        monly used to quantify the semantic relation be-
ing difficulty associated with syntactic constraint,                     tween a word and its context in computational
and has been previously applied to the modeling of                       studies of lexical processing. These models are
reading times (Demberg and Keller 2008; Ferrara                          based on the idea that words with similar mean-
Boston et al. 2008; Roark et al. 2009; Frank 2009).                      ings will be found in similar contexts. In putting
The basic idea is that the processing costs relating                     this idea into practice, the meaning of a word is
to the expectations of the language processor can                        then represented as a vector in a high dimensional
be expressed in terms of the probabilities assigned                      space, with the vector components relating to the
by some form of language model to the input.                             strength on occurrence of that word in various
These processing costs are assumed to arise from                         types of context. Semantic similarities are then
the change in the expectations of the language pro-                      modeled in terms of geometric similarities within
cessor as new input arrives. If we express these ex-                     the space.
pectations in terms of a distribution over all possi-                       To give a concrete example, Latent Semantic
ble continuations of the input seen so far, then we                      Analysis (LSA, Landauer and Dumais 1997) cre-
can measure the magnitude of this change in terms                        ates a meaning representation for words by con-
of the Kullback-Leibler divergence of the old dis-                       structing a word-document co-occurrence matrix
tribution to the updated distribution. This measure                      from a large collection of documents. Each row in
of processing cost for an input word, wk+1 , given                       the matrix represents a word, each column a doc-
the previous context, w1 . . . wk , can be expressed                     ument, and each entry the frequency with which
straightforwardly in terms of its conditional prob-                      the word appeared within that document. Because
ability as:                                                              this matrix tends to be quite large it is often trans-
                                                                         formed via a singular value decomposition (Berry
              S = − log P(wk+1 |w1 . . . wk )               (1)          et al. 1995) into three component matrices: a ma-
                                                                         trix of word vectors, a matrix of document vectors,
                                                                         and a diagonal matrix containing singular values.
That is, the processing cost for a word decreases as
                                                                         Re-multiplying these matrices together using only
its probability increases, with zero processing cost
                                                                         the initial portions of each (corresponding to the
incurred for words which must appear in a given
                                                                         use of a lower dimensional spatial representation)
context, as these do not result in any change in the
                                                                         produces a tractable approximation to the original
expectations of the language processor.
                                                                         matrix. In this framework, the similarity between
   The original formulation of surprisal (Hale
                                                                         two words can be easily quantified, e.g., by mea-
2001) used a probabilistic parser to calculate these
                                                                         suring the cosine of the angle of the vectors repre-
probabilities, as the emphasis was on the process-
                                                                         senting them.
ing costs incurred when parsing structurally am-
                                                                            As LSA is one the best known semantic space
biguous garden path sentences.1 Several variants
                                                                         models it comes as no surprise that it has been
of calculating surprisal have been developed in the
                                                                         used to analyze semantic constraint. Pynte et al.
literature since using different parsing strategies
                                                                         (2008) measure the similarity between the next
    1 While hearing a sentence like The horse raced past the             word and its preceding context under the assump-
barn fell (Bever 1970), English speakers are inclined to in-             tion that high similarity indicates high semantic
terpreted horse as the subject of raced expecting the sentence           constraint (i.e., the word was expected) and analo-
to end at the word barn. So upon hearing the word fell they
are forced to revise their analysis of the sentence thus far and         gously low similarity indicates low semantic con-
adopt a reduced relative reading.                                        straint (i.e., the word was unexpected). They oper-


                                                                   198


ationalize preceding contexts in two ways, either              general framework for studying vector composi-
as the word immediately preceding the next word                tion, which they formulate as a function f of two
as the sentence fragment preceding it. Sentence                vectors u and v:
fragments are represented as the average of the
words they contain independently of their order.                                   h = f (u, v)                  (2)
The model takes into account only content words,
function words are of little interest here as they can         where h denotes the composition of u and v. Dif-
be found in any context.                                       ferent composition models arise, depending on
                                                               how f is chosen. Assuming that h is a linear func-
   Pynte et al. (2008) analyze reading times on the
                                                               tion of the Cartesian product of u and v allows to
French part of the Dundee corpus (Kennedy and
                                                               specify additive models which are by far the most
Pynte 2005) and find that word-level LSA similar-
                                                               common method of vector combination in the lit-
ities are predictive of first fixation and first pass
                                                               erature:
durations, whereas sentence-level LSA is only
                                                                                   hi = ui + vi               (3)
predictive of first pass duration (i.e., for a mea-
sure that includes refixation). This latter finding            Alternatively, we can assume that h is a linear
is somewhat counterintuitive, one would expect                 function of the tensor product of u and v, and thus
longer contexts to have an immediate effect as                 derive models based on multiplication:
they are presumably more constraining. One rea-
son why sentence-level influences are only visible                                  hi = ui · vi                 (4)
on first pass duration may be due to LSA itself,
which is syntax-blind. Another reason relates to               Mitchell and Lapata (2008) show that several ad-
the way sentential context was modeled as vec-                 ditive and multiplicative models can be formu-
tor addition (or averaging). The idea of averag-               lated under this framework, including the well-
ing is not very attractive from a linguistic perspec-          known tensor products (Smolensky 1990) and cir-
tive as it blends the meanings of individual words             cular convolution (Plate 1995). Importantly, com-
together. Ideally, the combination of simple el-               position models are not defined with a specific se-
ements onto more complex ones must allow the                   mantic space in mind, they could easily be adapted
construction of novel meanings which go beyond                 to LSA, or simple co-occurrence vectors, or more
those of the individual elements (Pinker 1994).                sophisticated semantic representations (e.g., Grif-
                                                               fiths et al. 2007), although admittedly some com-
   The only other model of semantic constraint we
                                                               position functions may be better suited for partic-
are aware of is Incremental Contextual Distinc-
                                                               ular semantic spaces.
tiveness (ICD, McDonald 2000; McDonald and
                                                                  Composition models can be straightforwardly
Brew 2004). ICD assumes that words carry prior
                                                               used as predictors of processing difficulty, again
semantic expectations which are updated upon
                                                               via measuring the cosine of the angle between a
seeing the next word. Context is represented by
                                                               vector w representing the upcoming word and a
a vector of probabilities which reflects the likely
                                                               vector h representing the words preceding it:
location in semantic space of the upcoming word.
When the latter is observed, the prior expecta-                                                w·h
tion is updated using a Bayesian inference mecha-                              sim(w, h) =                       (5)
                                                                                              |w||h|
nism to reflect the newly arrived information. Like
LSA, ICD is based on word co-occurrence vectors,               where h is created compositionally, via some (ad-
however it does not employ singular value decom-               ditive or multiplicative) function f .
position, and constructs a word-word rather than a                In this paper we evaluate additive and compo-
word-document co-occurrence matrix. Although                   sitional models in their ability to capture seman-
this model has been shown to successfully simu-                tic prediction. We also examine the influence of
late single- and multiple-word priming (McDon-                 the underlying meaning representations by com-
ald and Brew 2004), it failed to predict processing            paring a simple semantic space similar to Mc-
costs in the Embra eye-tracking corpus (McDon-                 Donald (2000) against Latent Dirichlet Allocation
ald and Shillcock 2003).                                       (Blei et al. 2003; Griffiths et al. 2007). Specif-
   In this work we model semantic constraint us-               ically, the simpler space is based on word co-
ing the representational framework put forward in              occurrence counts; it constructs the vector repre-
Mitchell and Lapata (2008). Their aim is not so                senting a given target word, t, by identifying all the
much to model processing difficulty, but to con-               tokens of t in a corpus and recording the counts of
struct vector-based meaning representations that               context words, ci (within a specific window). The
go beyond individual words. They introduce a                   context words, ci , are limited to a set of the n most


                                                         199


common content words and each vector compo-                   obvious solution is to derive some form of seman-
nent is given by the ratio of the probability of a ci         tic surprisal rather than sticking with similarity.
given t to the overall probability of ci .                    This can be achieved by turning a vector model
                                                              of semantic similarity into a probabilistic language
                           p(ci |t)                           model.
                    vi =                         (6)
                            p(ci )                               There are in fact a number of approaches to de-
                                                              riving language models from distributional mod-
Despite its simplicity, the above semantic space              els of semantics (e.g., Bellegarda 2000; Coccaro
(and variants thereof) has been used to success-              and Jurafsky 1998; Gildea and Hofmann 1999).
fully simulate lexical priming (e.g., McDonald                We focus here on the model of Mitchell and La-
2000), human judgments of semantic similarity                 pata (2009) which tackles the issue of the compo-
(Bullinaria and Levy 2007), and synonymy tests                sition of semantic vectors and also integrates the
(Padó and Lapata 2007) such as those included in             output of an incremental parser. The core of their
the Test of English as Foreign Language (TOEFL).              model is based on the product of a trigram model
   LDA is a probabilistic topic model offering an             p(wn |wn−1
                                                                      n−2 ) and a semantic component ∆(wn , h)
alternative to spatial semantic representations. It           which determines the factor by which this proba-
is similar in spirit to LSA, it also operates on a            bility should be scaled up or down given the prior
word-document co-occurrence matrix and derives                semantic context h:
a reduced dimensionality description of words and
documents. Whereas in LSA words are repre-                              p(wn ) = p(wn |wn−1
                                                                                        n−2 ) · ∆(wn , h)     (7)
sented as points in a multi-dimensional space,
LDA represents words using topics. Specifically,              The factor ∆(wn , h) is essentially based on a com-
each document in a corpus is modeled as a distri-             parison between the vector representing the cur-
bution over K topics, which are themselves char-              rent word wn and the vector representing the prior
acterized as distribution over words. The individ-            history h. Varying the method for constructing
ual words in a document are generated by repeat-              word vectors (e.g., using LDA or a simpler seman-
edly sampling a topic according to the topic distri-          tic space model) and for combining them into a
bution and then sampling a single word from the               representation of the prior context h (e.g., using
chosen topic. Under this framework, word mean-                additive or multiplicative functions) produces dis-
ing is represented as a probability distribution over         tinct models of semantic composition.
a set of latent topics, essentially a vector whose               The calculation of ∆ is then based on a weighted
dimensions correspond to topics and values to the             dot product of the vector representing the upcom-
probability of the word given these topics. Topic             ing word w, with the vector representing the prior
models have been recently gaining ground as a                 context h:
more structured representation of word meaning
                                                                            ∆(w, h) = ∑ wi hi p(ci )          (8)
(Griffiths et al. 2007; Steyvers and Griffiths 2007).                                   i
In contrast to more standard semantic space mod-
els where word senses are conflated into a single             As shown in Equation (7) this semantic factor then
representation, topics have an intuitive correspon-           modulates the trigram probabilities, to take ac-
dence to coarse-grained sense distinctions.                   count of the effect of the semantic content outside
                                                              the n-gram window.
3   Integrating Semantic Constraint into                         Mitchell and Lapata (2009) show that a com-
    Surprisal                                                 bined semantic-trigram language model derived
                                                              from this approach and trained on the Wall Street
The treatment of semantic and syntactic constraint
                                                              Journal outperforms a baseline trigram model in
in models of processing difficulty has been some-
                                                              terms of perplexity on a held out set. They also
what inconsistent. While surprisal is a theo-
                                                              linearly interpolate this semantic language model
retically well-motivated measure, formalizing the
                                                              with the output of an incremental parser, which
idea of linguistic processing being highly predic-
                                                              computes the following probability:
tive in terms of probabilistic language models, the
measurement of semantic constraint in terms of                      p(w|h) = λp1 (w|h) + (1 − λ)p2 (w|h)      (9)
vector similarities lacks a clear motivation. More-
over, the two approaches, surprisal and similarity,           where p1 (w|h) is computed as in Equation (7)
produce mathematically different types of mea-                and p2 (w|h) is computed by the parser. Their im-
sures. Formally, it would be preferable to have               plementation uses Roark’s (2001) top-down incre-
a single approach to capturing constraint and the             mental parser which estimates the probability of


                                                        200


the next word based upon the previous words of                         Model Implementation All elements of our
the sentence. These prefix probabilities are calcu-                    model were trained on the BLLIP corpus, a col-
lated from a grammar, by considering the likeli-                       lection of texts from the Wall Street Journal
hood of seeing the next word given the possible                        (years 1987–89). The training corpus consisted of
grammatical relations representing the prior con-                      38,521,346 words. We used a development cor-
text.                                                                  pus of 50,006 words and a test corpus of similar
   Equation (9) essentially defines a language                         size. All words were converted to lowercase and
model which combines semantic, syntactic and                           numbers were replaced with the symbol hnumi. A
n-gram structure, and Mitchell and Lapata (2009)                       vocabulary of 20,000 words was chosen and the
demonstrate that it improves further upon a se-                        remaining tokens were replaced with hunki.
mantic language model in terms of perplexity. We                          Following Mitchell and Lapata (2009), we con-
argue that the probabilities from this model give                      structed a simple semantic space based on co-
us a means to model the incrementally and predic-                      occurrence statistics from the BLLIP training set.
tivity of the language processor in a manner that                      We used the 2,000 most frequent word types as
integrates both syntactic and semantic constraints.                    contexts and a symmetric five word window. Vec-
Converting these probabilities to surprisal should                     tor components were defined as in Equation (6).
result in a single measure of the processing cost as-                  We also trained the LDA model on BLLIP, using
sociated with semantic and syntactic expectations.                     the Gibb’s sampling procedure discussed in Grif-
                                                                       fiths et al. (2007). We experimented with different
4    Method                                                            numbers of topics on the development set (from 10
                                                                       to 1,000) and report results on the test set with 100
Data The models discussed in the previous sec-
                                                                       topics. In our experiments, the hyperparameter α
tion were evaluated against an eye-tracking cor-
                                                                       was initialized to .5, and the β word probabilities
pus. Specifically, we used the English portion
                                                                       were initialized randomly.
of the Dundee Corpus (Kennedy and Pynte 2005)
which contains 20 texts taken from The Indepen-                           We integrated our compositional models with a
dent newspaper. The corpus consists of 51,502                          trigram model which we also trained on BLLIP.
tokens and 9,776 types in total. It is annotated                       The model was built using the SRILM toolkit
with the eye-movement records of 10 English na-                        (Stolcke 2002) with backoff and Kneser-Ney
tive speakers, who each read the whole corpus.                         smoothing. As our incremental parser we used
The eye-tracking data was preprocessed following                       Roark’s (2001) parser trained on sections 2–21 of
the methodology described in Demberg and Keller                        the Penn Treebank containing 936,017 words. The
(2008). From this data, we computed total reading                      parser produces prefix probabilities for each word
time for each word in the corpus. Our statistical                      of a sentence which we converted to conditional
analyses were based on actual reading times, and                       probabilities by dividing each current probability
so we only included words that were not skipped.                       by the previous one.
We also excluded words for which the previous                          Statistical Analysis The statistical analyses in
word had been skipped, and words on which the                          this paper were carried out using linear mixed
normal left-to-right movement of gaze had been                         effects models (LME, Pinheiro and Bates 2000).
interrupted, i.e., by blinks, regressions, etc. Fi-                    The latter can be thought of as generalization of
nally, because our focus is the influence of seman-                    linear regression that allows the inclusion of ran-
tic context, we selected only content words whose                      dom factors (such as participants or items) as well
prior sentential context contained at least two fur-                   as fixed factors (e.g., word frequency). In our
ther content words. The resulting data set con-                        analyses, we treat participant as a random factor,
sisted of 53,704 data points, which is about 10%                       which means that our models contain an intercept
of the theoretically possible total.2                                  term for each participant, representing the individ-
    2 The total of all words read by all subjects is 515,020.          ual differences in the rates at which they read.3
The pre-processing recommended by Demberg and Keller’s                    We evaluated the effect of adding a factor to a
(2008) results in a data sets containing 436,000 data points.          model by comparing the likelihoods of the mod-
Removing non-content words leaves 205,922 data points. It
only makes sense to consider words that were actually fixated          els with and without that factor. If a χ2 test on the
(the eye-tracking measures used are not defined on skipped
words), which leaves 162,129 data points. Following Pynte                  3 Other random factors that are appropriate for our anal-
et al. (2008), we require that the previous word was fixated,          yses are word and sentence; however, due to the large num-
with 70,051 data points remaining. We exclude words on                 ber of instances for these factors (given that the Dundee cor-
which the normal left to right movement of gaze had been               pus contains 51,502 tokens), we were not able to include
interrupted, e.g., by blinks and regressions, which results in         them: the model fitting algorithm we used (implemented in
the final total to 53,704 data points.                                 the R package lme4) does not converge for such large models.


                                                                 201


    Factor                            Coefficient                      Model     Composition       Coefficient
    Intercept                             −.011                                  Additive          −.03820∗∗∗
                                                                       SSS
    Word Length                             .264                                 Multiplicative    −.00895∗∗∗
    Launch Distance                         .109                                 Additive          −.02500∗∗∗
                                                                       LDA
    Landing Position                        .612                                 Multiplicative    −.00262∗∗∗
    Word Frequency                        −.010
    Reading Time of Last Word               .151                 Table 2: Coefficients of LME models including
                                                                 simple semantic space (SSS) or Latent Dirichlet
Table 1: Coefficients of the baseline LME model                  Allocation (LDA) as factors; ∗∗∗ p < .001
for total reading time

                                                                 quency, launch distance, landing position, and the
likelihood ratio is significant, then this indicates             reading time for the last fixated word, and its pa-
that the new factor significantly improves model                 rameter estimates are given in Table 1. To further
fit. We also experimented with adding random                     reduce collinearity, we also centered all fixed fac-
slopes for participant to the model (in addition to              tors, both in the baseline model, and in the models
the random intercept); however, this either led to               fitted on the residuals that we report in the follow-
non-convergence of the model fitting procedure, or               ing. Note that some intercorrelations remain be-
failed to result in an increase in model fit accord-             tween the factors, which we will discuss at the end
ing to the likelihood ratio test. Therefore, all mod-            of Section 5.
els reported in the rest of this paper contain ran-                  Before investigating whether an integrated
dom intercept of participants as the sole random                 model of semantic and syntactic constraint im-
factor.                                                          proves the goodness of fit over the baseline, we ex-
    Rather than model raw reading times, we model                amined the influence of semantic constraint alone.
times on the log scale. This is desirable for a                  This was necessary as compositional models have
number of reasons. Firstly, the raw reading times                not been previously used to model processing
tend to have a skew distribution and taking logs                 difficulty. Besides, replicating Pynte et al.’s
produces something closer to normal, which is                    (2008) finding, we were also interested in assess-
preferable for modeling. Secondly, the regres-                   ing whether the underlying semantic representa-
sion equation makes more sense on the log scale                  tion (simple semantic space or LDA) and com-
as the contribution of each term to raw reading                  position function (additive versus multiplicative)
time is multiplicative rather than additive. That is,            modulate reading times differentially.
log(t) = ∑i βi xi implies t = ∏i eβi xi . In particular,
                                                                     We built an LME model that predicted the resid-
the intercept term for each participant now repre-
                                                                 ual reading times of the baseline model using the
sents a multiplicative factor by which that partici-
                                                                 similarity scores from our composition models as
pant is slower or faster.
                                                                 factors. We then carried out a χ2 test on the like-
                                                                 lihood ratio of a model only containing the ran-
5   Results
                                                                 dom factor and the intercept, and a model also
We computed separate mixed effects models for                    containing the semantic factor (cosine similarity).
three dependent variables, namely first fixation du-             The addition of the semantic factor significantly
ration, first pass duration, and total reading time.             improves model fit for both the simple semantic
We report results for total times throughout, as                 space and LDA. This result is observed for both
the results of the other two dependent variables                 additive and multiplicative composition functions.
are broadly similar. Our strategy was to first con-              Our results are summarized in Table 2 which re-
struct a baseline model of low-level factors influ-              ports the coefficients of the four LME models fit-
encing reading time, and then to take the resid-                 ted against the residuals of the baseline model, to-
uals from that model as the dependent variable                   gether with the p-values of the χ2 test.
in subsequent analyses. In this way we removed                       Before evaluating our integrated surprisal mea-
the effects of low-level factors before investigating            sure, we evaluated its components individually in
the factors associated with syntactic and semantic               order to tease their contributions apart. For ex-
constraint. This avoids problems with collinear-                 ample, it may be the case that syntactic surprisal
ity between low-level factors and the factors we                 is an overwhelmingly better predictor of reading
are interested in (e.g., trigram probability is highly           time than semantic surprisal, however we would
correlated with word frequency). The baseline                    not be able to detect this by simply adding a factor
model contained the factors word length, word fre-               based on Equation (9) to the baseline model. The


                                                           202


              Factor               SSS Coef LDA Coef                           Model     Composition       Coefficient
              − log(p)             .00760∗∗∗ .00760∗∗∗                                   Additive           .00804∗∗∗
                                                                               SSS
              − log(∆)             .03810∗∗∗ .00622∗∗∗                                   Multiplicative     .00819∗∗∗
  Mult Add
                              p2
              log(λ + (1 − λ) p1 ) .00953∗∗∗ .00943∗∗∗                                   Additive           .00817∗∗∗
                                                                               LDA
              − log(∆)             .01110∗∗∗ −.00033                                     Multiplicative     .00640∗∗∗
                              p2
              log(λ + (1 − λ) p1 ) .00882∗∗∗ .00133
                                                                         Table 4: Coefficients of LME models with inte-
Table 3: Coefficients of nested LME models with                          grated surprisal measure (based on SSS or LDA)
the components of SSS or LDA surprisal as fac-                           as factor
tors; only the coefficient of the additional factor at
each step are shown

                                                                         all four variants of our semantic constraint mea-
integrated surprisal measure can be written as:
                                                                         sure.
                   S = − log(λp1 + (1 − λ)p2 )             (10)
                                                                            Finally, we built a separate LME model where
Where p2 is the incremental parser probability and                       we added the integrated surprisal measure (see
p1 is the product of the semantic component, ∆,                          Equation (9)) to the model only containing the ran-
and the trigram probability, p. This can be broken                       dom factor and the intercept (see Table 4). We
down into the sum of two terms:                                          did this separately for all four versions of the in-
                                                                         tegrated surprisal measure (SSS, LDA; additive,
                                                p2                       multiplicative). We find that model fit improved
             S = − log(p1 ) − log(λ + (1 − λ)      )       (11)
                                                p1                       significantly all versions of integrated surprisal.
Since the first term, − log(p1 ) is itself a product it                      One technical issue that remains to be discussed
can also be broken down further:                                         is collinearity, i.e., intercorrelations between the
                                                       p2                factors in a model. The presence of collinearity
 S = − log(p)−log(∆)−log(λ+(1−λ)                          ) (12)         is problematic, as it can render the model fitting
                                                       p1
                                                                         procedure unstable; it can also affect the signifi-
Thus, to evaluate the contribution of the three                          cance of individual factors. As mentioned in Sec-
components to the integrated surprisal measure we                        tion 4 we used two techniques to reduce collinear-
fitted nested LME models, i.e., we entered these                         ity: residualizing and centering. Table 5 gives
terms one at a time into a mixed effects model                           an overview of the correlation coefficients for all
and tested the significance of the improvement in                        pairs of factors. It becomes clear that collinear-
model fit for each additional term.                                      ity has mostly been removed; there is a remaining
    We again start with an LME model that only                           relationship between word length and word fre-
contains the random factor and the intercept, with                       quency, which is expected as shorter words tend to
the residuals of the baseline models as the depen-                       be more frequent. This correlation is not a prob-
dent variable. Considering the trigram model first,                      lem for our analysis, as it is confined to the base-
we find that adding this factor to the model gives a                     line model. Furthermore, word frequency and tri-
significant improvement in fit. Also adding the se-                      gram probability are highly correlated. Again this
mantic component (− log(∆)) improves fit further,                        is expected, given that the frequencies of unigrams
both for additive and multiplicative composition                         and higher-level n-grams tend to be related. This
functions using a simple semantic space. Finally,                        correlation is taken care of by residualizing, which
the addition of the parser probabilities (log(λ +                        isolates the two factors: word frequency is part
(1 − λ) pp21 )) again improves model fit significantly.                  of the baseline model, while trigram probability is
As far as LDA is concerned, the additive model                           part of the separate models that we fit on the resid-
significantly improves model fit, whereas the mul-                       uals. All other correlations are small (with coeffi-
tiplicative one does not. These results mirror                           cients of .27 or less), with one exception: there is
the findings of Mitchell and Lapata (2009), who                          a high correlation between the − log(∆) term and
report that a multiplicative composition function                        the log(λ + (1 − λ) pp21 ) term in the multiplicative
produced the lowest perplexity for the simple se-                        LDA model. This collinearity issue may explain
mantic space model, whereas an additive function                         the absence of a significant improvement in model
gave the best perplexity for the LDA space. Ta-                          fit when these two terms are added to the baseline
ble 3 lists the coefficients for the nested models for                   (see Table 3).


                                                                   203


                    Factor                  Len Freq −l(p) −l(∆)     An LME model containing separate factors, on the
                    Frequency             −.310                      other hand, requires a coefficient for each of them,
                    − log(p)               .230 −.700                and thus has more parameters.
                    − log(∆)               .016 −.120 .025              In evaluating our model, we adopted a broad
Mult Add Mult Add
LDA LDA SSS SSS



                    log(λ + (1 − λ) pp12 ) .024 .036 −.270 .065      coverage approach using the reading time data
                    − log(∆)              −.015 −.110 .035           from a naturalistic corpus rather than artificially
                    log(λ + (1 − λ) pp21 ) .020 .028 −.260 .160      constructed experimental materials. In doing so,
                    − log(∆)              −.024 −.130 .046           we were able to compare different syntactic and
                    log(λ + (1 − λ) pp12 ) .005 .014 −.250 .030      semantic costs on the same footing. Previous
                    − log(∆)              −.120 .006 −.046           analyses of semantic constraint have been con-
                    log(λ + (1 − λ) pp12 )−.089 −.005 −.180 .740     ducted on different eye-tracking corpora (Dundee
                                                                     and Embra Corpus) and on different languages
    Table 5: Intercorrelations between model factors                 (English, French). Moreover, comparisons of the
                                                                     individual contributions of syntactic and semantic
                                                                     factors were generally absent from the literature.
 6                  Discussion                                       Our analysis showed that both of these factors can
                                                                     be captured by our integrated surprisal measure
 In this paper we investigated the contributions of                  which is uniformly probabilistic and thus prefer-
 syntactic and semantic constraint in modeling pro-                  able to modeling semantic and syntactic costs dis-
 cessing difficulty. Our work departs from previ-                    jointly using a mixture of probabilistic and non-
 ous approaches in that we propose a single mea-                     probabilistic measures.
 sure which integrates syntactic and semantic fac-                      An interesting question is which aspects of se-
 tors. Evaluation on an eye-tracking corpus shows                    mantics our model is able to capture, i.e., why
 that our measure predicts reading time better than                  does the combination of LSA or LDA representa-
 a baseline model that captures low-level factors                    tions with an incremental parser yield a better fit of
 in reading (word length, landing position, etc.).                   the behavioral data. In the psycholinguistic liter-
 Crucially, we were able to show that the semantic                   ature, various types of semantic information have
 component of our measure improves reading time                      been investigated: lexical semantics (word senses,
 predictions over and above a model that includes                    selectional restrictions, thematic roles), senten-
 syntactic measures (based on a trigram model and                    tial semantics (scope, binding), and discourse se-
 incremental parser). This means that semantic                       mantics (coreference and coherence); see Keller
 costs are a significant predictor of reading time in                (2010) of a detailed discussion. We conjecture that
 addition to the well-known syntactic surprisal.                     our model is mainly capturing lexical semantics
    An open issue is whether a single, integrated                    (through the vector space representation of words)
 measure (as evaluated in Table 4) fits the eye-                     and sentential semantics (through the multiplica-
 movement data significantly better than separate                    tion or addition of words). However, discourse
 measures for trigram, syntactic, and semantic sur-                  coreference effects (such as the ones reported by
 prisal (as evaluated in Table 3. However, we are                    Altmann and Steedman (1988) and much subse-
 not able to investigate this hypothesis: our ap-                    quent work) are probably not amenable to a treat-
 proach to testing the significance of factors re-                   ment in terms of vector space semantics; an ex-
 quires nested models; the log-likelihood test (see                  plicit representation of discourse entities and co-
 Section 4) is only able to establish whether adding                 reference relations is required (see Dubey 2010
 a factor to a model improves its fit; it cannot com-                for a model of human sentence processing that can
 pare models with disjunct sets of factors (such as                  handle coreference).
 a model containing the integrated surprisal mea-                       A key objective for future work will be to in-
 sure and one containing the three separate ones).                   vestigate models that integrate semantic constraint
 However, we would argue that a single, integrated                   with syntactic predictions more tightly. For ex-
 measure that captures human predictive process-                     ample, we could envisage a parser that uses se-
 ing is preferable over a collection of separate mea-                mantic representations to guide its search, e.g., by
 sures. It is conceptually simpler (as it is more par-               pruning syntactic analyses that have a low seman-
 simonious), and is also easier to use in applica-                   tic probability. At the same time, the semantic
 tions (such as readability prediction). Finally, an                 model should have access to syntactic informa-
 integrated measure requires less parameters; our                    tion, i.e., the composition of word representations
 definition of surprisal in 12 is simply the sum of                  should take their syntactic relationships into ac-
 the trigram, syntactic, and semantic components.                    count, rather than just linear order.


                                                               204


References                                                     2008. Parsing costs as predictors of reading dif-
                                                               ficulty: An evaluation using the Potsdam Sen-
ACL. 2010. Proceedings of the 48th Annual Meet-
                                                               tence Corpus. Journal of Eye Movement Re-
 ing of the Association for Computational Lin-
                                                               search 2(1):1–12.
 guistics. Uppsala.
                                                             Frank, Stefan L. 2009. Surprisal-based compar-
Altmann, Gerry T. M. and Yuki Kamide. 1999.                    ison between a symbolic and a connectionist
  Incremental interpretation at verbs: Restricting             model of sentence processing. In Proceedings
  the domain of subsequent reference. Cognition                of the 31st Annual Conference of the Cognitive
  73:247–264.                                                  Science Society. Austin, TX, pages 139–1144.
Altmann, Gerry T. M. and Mark J. Steedman.                   Gibson, Edward. 2000. Dependency locality the-
  1988. Interaction with context during human                  ory: A distance-dased theory of linguistic com-
  sentence processing. Cognition 30(3):191–238.                plexity. In Alec Marantz, Yasushi Miyashita,
Bellegarda, Jerome R. 2000. Exploiting latent se-              and Wayne O’Neil, editors, Image, Language,
  mantic information in statistical language mod-              Brain: Papers from the First Mind Articulation
  eling. Proceedings of the IEEE 88(8):1279–                   Project Symposium, MIT Press, Cambridge,
  1296.                                                        MA, pages 95–126.
Berry, Michael W., Susan T. Dumais, and                      Gildea, Daniel and Thomas Hofmann. 1999.
  Gavin W. O’Brien. 1995. Using linear algebra                 Topic-based language models using EM. In
  for intelligent information retrieval. SIAM re-              Proceedings of the 6th European Conference
  view 37(4):573–595.                                          on Speech Communiation and Technology. Bu-
Bever, Thomas G. 1970. The cognitive basis for                 dapest, Hungary, pages 2167–2170.
  linguistic strutures. In J. R. Hayes, editor, Cog-         Griffiths, Thomas L., Mark Steyvers, and
  nition and the Development of Language, Wi-                  Joshua B. Tenenbaum. 2007. Topics in se-
  ley, New York, pages 279–362.                                mantic representation. Psychological Review
Blei, David M., Andrew Y. Ng, and Michael I. Jor-              114(2):211–244.
  dan. 2003. Latent Dirichlet allocation. Journal            Hale, John. 2001. A probabilistic Earley parser as
  of Machine Learning Research 3:993–1022.                     a psycholinguistic model. In Proceedings of the
Bullinaria, John A. and Joseph P. Levy. 2007. Ex-              2nd Conference of the North American Chap-
  tracting semantic representations from word co-              ter of the Association. Association for Compu-
  occurrence statistics: A computational study.                tational Linguistics, Pittsburgh, PA, volume 2,
  Behavior Research Methods 39:510–526.                        pages 159–166.
Clifton, Charles, Adrian Staub, and Keith Rayner.            Keller, Frank. 2010. Cognitively plausible models
  2007. Eye movement in reading words and sen-                 of human language processing. In ACL.
  tences. In R V Gompel, M Fisher, W Murray,                 Kennedy, Alan and Joel Pynte. 2005. Parafoveal-
  and R L Hill, editors, Eye Movements: A Win-                 on-foveal effects in normal reading. Vision Re-
  dow in Mind and Brain, Elsevier, pages 341–                  search 45:153–168.
  372.                                                       Konieczny, Lars. 2000. Locality and parsing com-
Coccaro, Noah and Daniel Jurafsky. 1998. To-                   plexity. Journal of Psycholinguistic Research
  wards better integration of semantic predictors              29(6):627–645.
  in satistical language modeling. In Proceedings            Landauer, Thomas K. and Susan T. Dumais. 1997.
  of the 5th International Conference on Spoken                A solution to Plato’s problem: the latent seman-
  Language Processing. Sydney, Australia, pages                tic analysis theory of acquisition, induction and
  2403–2406.                                                   representation of knowledge. Psychological Re-
Demberg, Vera and Frank Keller. 2008. Data from                view 104(2):211–240.
  eye-tracking corpora as evidence for theories              Levy, Roger. 2008. Expectation-based syntactic
  of syntactic processing complexity. Cognition                comprehension. Cognition 106(3):1126–1177.
  101(2):193–210.                                            Marslen-Wilson, William D. 1973. Linguistic
Dubey, Amit. 2010. The influence of discourse on               structure and speech shadowing at very short la-
  syntax: A psycholinguistic model of sentence                 tencies. Nature 244:522–523.
  processing. In ACL.                                        McDonald, Scott. 2000. Environmental Determi-
Ferrara Boston, Marisa, John Hale, Reinhold                    nants of Lexical Processing Effort. Ph.D. thesis,
  Kliegl, Umesh Patil, and Shravan Vasishth.                   University of Edinburgh.


                                                       205


McDonald, Scott and Chris Brew. 2004. A dis-               Roark, Brian. 2001. Probabilistic top-down pars-
 tributional model of semantic context effects in            ing and language modeling. Computational
 lexical processing. In Proceedings of the 42th              Linguistics 27(2):249–276.
 Annual Meeting of the Association for Com-                Roark, Brian, Asaf Bachrach, Carlos Cardenas,
 putational Linguistics. Barcelona, Spain, pages             and Christophe Pallier. 2009. Deriving lex-
 17–24.                                                      ical and syntactic expectation-based measures
McDonald, Scott A. and Richard C. Shillcock.                 for psycholinguistic modeling via incremental
 2003. Low-level predictive inference in read-               top-down parsing. In Proceedings of the 2009
 ing: The influence of transitional probabilities            Conference on Empirical Methods in Natural
 on eye movements. Vision Research 43:1735–                  Language Processing. Association for Compu-
 1751.                                                       tational Linguistics, Singapore, pages 324–333.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-           Smolensky, Paul. 1990. Tensor product vari-
 based models of semantic composition. In Pro-               able binding and the representation of symbolic
 ceedings of ACL-08: HLT. Columbus, OH,                      structures in connectionist systems. Artificial
 pages 236–244.                                              Intelligence 46:159–216.
Mitchell, Jeff and Mirella Lapata. 2009. Language          Stanovich, Kieth E. and Richard F. West. 1981.
 models based on semantic composition. In Pro-               The effect of sentence context on ongoing word
 ceedings of the 2009 Conference on Empirical                recognition: Tests of a two-pricess theory. Jour-
 Methods in Natural Language Processing. Sin-                nal of Experimental Psychology: Human Per-
 gapore, pages 430–439.                                      ception and Performance 7:658–672.
Narayanan, Srini and Daniel Jurafsky. 2002. A              Staub, Adrian and Charles Clifton. 2006. Syntac-
  Bayesian model predicts human parse prefer-                tic prediction in language comprehension: Evi-
  ence and reading time in sentence processing. In           dence from either . . .or. Journal of Experimen-
  Thomas G. Dietterich, Sue Becker, and Zoubin               tal Psychology: Learning, Memory, and Cogni-
  Ghahramani, editors, Advances in Neural In-                tion 32:425–436.
  formation Processing Systems 14. MIT Press,              Steyvers, Mark and Tom Griffiths. 2007. Proba-
  Cambridge, MA, pages 59–65.                                bilistic topic models. In T. Landauer, D. Mc-
Padó, Sebastian and Mirella Lapata. 2007.                   Namara, S Dennis, and W Kintsch, editors, A
  Dependency-based construction of semantic                  Handbook of Latent Semantic Analysis, Psy-
  space models.    Computational Linguistics                 chology Press.
  33(2):161–199.                                           Stolcke, Andreas. 2002. Srilm - an extensible lan-
Padó, Ulrike, Matthew W. Crocker, and Frank                 guage modeling toolkit. In Proceedings of the
  Keller. 2009. A probabilistic model of semantic            Internatinal Conference on Spoken Language
  plausibility in sentence processing. Cognitive             Processing. Denver, Colorado.
  Science 33(5):794–838.                                   Sturt, Patrick and Vincenzo Lombardo. 2005.
Pinheiro, Jose C. and Douglas M. Bates.                      Processing coordinated structures: Incremen-
  2000. Mixed-effects Models in S and S-PLUS.                tality and connectedness. Cognitive Science
  Springer, New York.                                        29(2):291–305.
                                                           Tanenhaus, Michael K., Michael J. Spivey-
Pinker, Steven. 1994. The Language Instinct: How
                                                             Knowlton, Kathleen M. Eberhard, and Julie C.
  the Mind Creates Language. HarperCollins,
                                                             Sedivy. 1995. Integration of visual and linguis-
  New York.
                                                             tic information in spoken language comprehen-
Plate, Tony A. 1995. Holographic reduced repre-              sion. Science 268:1632–1634.
  sentations. IEEE Transactions on Neural Net-
                                                           van Berkum, Jos J. A., Colin M. Brown, and Peter
  works 6(3):623–641.
                                                             Hagoort. 1999. Early referential context effects
Pynte, Joel, Boris New, and Alan Kennedy. 2008.              in sentence processing: Evidence from event-
  On-line contextual influences during reading               related brain potentials. Journal of Memory and
  normal text: A multiple-regression analysis. Vi-           Language 41:147–182.
  sion Research 48:2172–2183.                              Wright, Barton and Merrill F. Garrett. 1984. Lex-
Rayner, Keith. 1998. Eye movements in read-                 ical decision in sentences: Effects of syntactic
  ing and information processing: 20 years of re-           structure. Memory and Cognition 12:31–45.
  search. Psychological Bulletin 124(3):372–422.


                                                     206

                       Unsupervised Ontology Induction from Text


                                Hoifung Poon and Pedro Domingos
                            Department of Computer Science & Engineering
                                      University of Washington
                            hoifung,pedrod@cs.washington.edu




                      Abstract                                 chine learning approaches to a number of key sub-
                                                               problems have been developed (e.g., Snow et al.
    Extracting knowledge from unstructured                     (2006)), but to date there is no sufficiently auto-
    text is a long-standing goal of NLP. Al-                   matic end-to-end solution. Most saliently, super-
    though learning approaches to many of its                  vised learning requires labeled data, which itself is
    subtasks have been developed (e.g., pars-                  costly and infeasible for large-scale, open-domain
    ing, taxonomy induction, information ex-                   knowledge acquisition.
    traction), all end-to-end solutions to date                   Ideally, we would like to have an end-to-end un-
    require heavy supervision and/or manual                    supervised (or lightly supervised) solution to the
    engineering, limiting their scope and scal-                problem of knowledge acquisition from text. The
    ability. We present OntoUSP, a system that                 TextRunner system (Banko et al., 2007) can ex-
    induces and populates a probabilistic on-                  tract a large number of ground atoms from the
    tology using only dependency-parsed text                   Web using only a small number of seed patterns
    as input. OntoUSP builds on the USP                        as guidance, but it is unable to extract non-atomic
    unsupervised semantic parser by jointly                    formulas, and the mass of facts it extracts is un-
    forming ISA and IS-PART hierarchies of                     structured and very noisy. The USP system (Poon
    lambda-form clusters. The ISA hierar-                      and Domingos, 2009) can extract formulas and ap-
    chy allows more general knowledge to                       pears to be fairly robust to noise. However, it is
    be learned, and the use of smoothing for                   still limited to extractions for which there is sub-
    parameter estimation. We evaluate On-                      stantial evidence in the corpus, and in most cor-
    toUSP by using it to extract a knowledge                   pora most pieces of knowledge are stated only
    base from biomedical abstracts and an-                     once or a few times, making them very difficult to
    swer questions. OntoUSP improves on                        extract without supervision. Also, the knowledge
    the recall of USP by 47% and greatly                       extracted is simply a large set of formulas with-
    outperforms previous state-of-the-art ap-                  out ontological structure, and the latter is essential
    proaches.                                                  for compact representation and efficient reasoning
                                                               (Staab and Studer, 2004).
1   Introduction
                                                                  We propose OntoUSP (Ontological USP), a sys-
Knowledge acquisition has been a major goal of                 tem that learns an ISA hierarchy over clusters of
NLP since its early days. We would like comput-                logical expressions, and populates it by translat-
ers to be able to read text and express the knowl-             ing sentences to logical form. OntoUSP is en-
edge it contains in a formal representation, suit-             coded in a few formulas of higher-order Markov
able for answering questions and solving prob-                 logic (Domingos and Lowd, 2009), and can be
lems. However, progress has been difficult. The                viewed as extending USP with the capability to
earliest approaches were manual, but the sheer                 perform hierarchical (as opposed to flat) cluster-
amount of coding and knowledge engineering                     ing. This clustering is then used to perform hier-
needed makes them very costly and limits them to               archical smoothing (a.k.a. shrinkage), greatly in-
well-circumscribed domains. More recently, ma-                 creasing the system’s capability to generalize from


                                                         296
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 296–305,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


sparse data.                                                  formation among the three tasks, uncovers more
  We begin by reviewing the necessary back-                   implicit information from text, and can potentially
ground. We then present the OntoUSP Markov                    work well even in domains not well covered by
logic network and the inference and learning al-              existing resources like WordNet and Wikipedia.
gorithms used with it. Finally, experiments on                Furthermore, we leverage the ontology for hierar-
a biomedical knowledge acquisition and question               chical smoothing and incorporate this smoothing
answering task show that OntoUSP can greatly                  into the induction process. This facilitates more
outperform USP and previous systems.                          accurate parameter estimation and better general-
                                                              ization.
2       Background                                               Our approach can also leverage existing on-
2.1 Ontology Learning                                         tologies and knowledge bases to conduct semi-
                                                              supervised ontology induction (e.g., by incorpo-
In general, ontology induction (constructing an
                                                              rating existing structures as hard constraints or pe-
ontology) and ontology population (mapping tex-
                                                              nalizing deviation from them).
tual expressions to concepts and relations in the
ontology) remain difficult open problems (Staab               2.2   Markov Logic
and Studer, 2004). Recently, ontology learn-
                                                              Combining uncertainty handling and joint infer-
ing has attracted increasing interest in both NLP
                                                              ence is the hallmark of the emerging field of statis-
and semantic Web communities (Cimiano, 2006;
                                                              tical relational learning (a.k.a. structured predic-
Maedche, 2002), and a number of machine learn-
                                                              tion), where a plethora of approaches have been
ing approaches have been developed (e.g., Snow
                                                              developed (Getoor and Taskar, 2007; Bakir et al.,
et al. (2006), Cimiano (2006), Suchanek et al.
                                                              2007). In this paper, we use Markov logic (Domin-
(2008,2009), Wu & Weld (2008)). However, they
                                                              gos and Lowd, 2009), which is the leading unify-
are still limited in several aspects. Most ap-
                                                              ing framework, but other approaches can be used
proaches induce and populate a deterministic on-
                                                              as well. Markov logic is a probabilistic exten-
tology, which does not capture the inherent un-
                                                              sion of first-order logic and can compactly specify
certainty among the entities and relations. Be-
                                                              probability distributions over complex relational
sides, many of them either bootstrap from heuris-
                                                              domains. It has been successfully applied to un-
tic patterns (e.g., Hearst patterns (Hearst, 1992))
                                                              supervised learning for various NLP tasks such
or build on existing structured or semi-structured
                                                              as coreference resolution (Poon and Domingos,
knowledge bases (e.g., WordNet (Fellbaum, 1998)
                                                              2008) and semantic parsing (Poon and Domingos,
and Wikipedia1 ), thus are limited in coverage.
                                                              2009). A Markov logic network (MLN) is a set of
Moreover, they often focus on inducing ontology
                                                              weighted first-order clauses. Together with a set
over individual words rather than arbitrarily large
                                                              of constants, it defines a Markov network with one
meaning units (e.g., idioms, phrasal verbs, etc.).
                                                              node per ground atom and one feature per ground
Most importantly, existing approaches typically
                                                              clause. The weight of a feature is the weight of the
separate ontology induction from population and
                                                              first-order clause that originated it. The probabil-
knowledge extraction, and pursue each task in a
                                                              ity of a state x in such a network is given by the
standalone fashion. While computationally effi-                                                      P
                                                              log-linear model P (x) = Z1 exp ( i wi ni (x)),
cient, this is suboptimal. The resulted ontology
                                                              where Z is a normalization constant, wi is the
is disconnected from text and requires additional
                                                              weight of the ith formula, and ni is the number
effort to map between the two (Tsujii, 2004). In
                                                              of satisfied groundings.
addition, this fails to leverage the intimate connec-
tions between the three tasks for joint inference             2.3   Unsupervised Semantic Parsing
and mutual disambiguiation.                                   Semantic parsing aims to obtain a complete canon-
   Our approach differs from existing ones in two             ical meaning representation for input sentences. It
main aspects: we induce a probabilistic ontology              can be viewed as a structured prediction problem,
from text, and we do so by jointly conducting on-             where a semantic parse is formed by partitioning
tology induction, population, and knowledge ex-               the input sentence (or a syntactic analysis such as
traction. Probabilistic modeling handles uncer-               a dependency tree) into meaning units and assign-
tainty and noise. A joint approach propagates in-             ing each unit to the logical form representing an
    1
        http : //www.wikipedia.org                            entity or relation (Figure 1). In effect, a semantic


                                                        297


                                       INDUCE(e1)
    IL-4 protein                                                                                       Object Cluster: INDUCE
                             INDUCER(e1,e2)          INDUCED(e1,e3)
  induces CD11b
                                  IL-4(e2)                 CD11B(e3)                 Core Form          Property Cluster: INDUCER

                                                                                      induces 0.1   nsubj 0.5    IL-4       0.2   None 0.1
   Structured prediction: Partition + Assignment                                     enhances 0.4   agent 0.4                     One 0.8
                                                                                                                                             ……
                                                                                                                 IL-8       0.1
           induces                INDUCE        induces




                                                                                           …




                                                                                                                        …




                                                                                                                                     …
                                                                                                       …
   nsubj             dobj   INDUCER   nsubj                dobj   INDUCED

   protein     CD11b                  protein             CD11b

   nn                                 nn                  CD11B
                                                                                  Figure 2: An example of object/property clusters:
                                       IL-4
   IL-4
                                                                                  INDUCE contains the core-form property cluster
                                       IL-4
                                                                                  and others, such as the agent argument INDUCER.

Figure 1: An example of semantic parsing. Top:                                    form,4 and an assignment of each subexpression
semantic parsing converts an input sentence into                                  to a lambda-form cluster.
logical form in Davidsonian semantics. Bottom: a                                     The lambda-form clusters naturally form an IS-
semantic parse consists of a partition of the depen-                              PART hierarchy (Figure 2). An object cluster cor-
dency tree and an assignment of its parts.                                        responds to semantic concepts or relations such as
                                                                                  INDUCE, and contains a variable number of prop-
parser extracts knowledge from input text and con-                                erty clusters. A special property cluster of core
verts them into logical form (the semantic parse),                                forms maintains a distribution over variations in
which can then be used in logical and probabilistic                               lambda forms for expressing this concept or rela-
inference and support end tasks such as question                                  tion. Other property clusters correspond to modi-
answering.                                                                        fiers or arguments such as INDUCER (the agent ar-
   A major challenge to semantic parsing is syn-                                  gument of INDUCE), each of which in turn con-
tactic and lexical variations of the same mean-                                   tains three subclusters of property values: the
ing, which abound in natural languages. For ex-                                   argument-object subcluster maintains a distribu-
ample, the fact that IL-4 protein induces CD11b                                   tion over object clusters that may occur in this
can be expressed in a variety of ways, such                                       argument (e.g., IL − 4), the argument-form sub-
as, “Interleukin-4 enhances the expression of                                     cluster maintains a distribution over lambda forms
CD11b”, “CD11b is upregulated by IL-4”, etc.                                      that corresponds to syntactic variations for this ar-
Past approaches either manually construct a gram-                                 gument (e.g., nsubj in active voice and agent in
mar or require example sentences with meaning                                     passive voice), and the argument-number subclus-
annotation, and do not scale beyond restricted do-                                ter maintains a distribution over total numbers of
mains.                                                                            this argument that may occur in a sentence (e.g.,
   Recently, we developed the USP system (Poon                                    zero if the argument is not mentioned).
and Domingos, 2009), the first unsupervised ap-                                      Effectively, USP simultaneously discovers the
proach for semantic parsing.2 USP inputs de-                                      lambda-form clusters and an IS-PART hierarchy
pendency trees of sentences and first transforms                                  among them. It does so by recursively combining
them into quasi-logical forms (QLFs) by convert-                                  subexpressions that are composed with or by sim-
ing each node to a unary atom and each depen-                                     ilar subexpressions. The partition breaks a sen-
dency edge to a binary atom (e.g., the node for                                   tence into subexpressions that are meaning units,
“induces” becomes induces(e1 ) and the subject                                    and the clustering abstracts away syntactic and
dependency becomes nsubj(e1 , e2 ), where ei ’s                                   lexical variations for the same meaning. This
are Skolem constants indexed by the nodes.).3                                     novel form of relational clustering is governed by
For each sentence, a semantic parse comprises of                                  a joint probability distribution P (T, L) defined in
a partition of its QLF into subexpressions, each                                  higher-order5 Markov logic, where T are the input
of which has a naturally corresponding lambda                                     dependency trees, and L the semantic parses. The
                                                                                     4
                                                                                       The lambda form is derived by replacing every Skolem
   2
      In this paper, we use a slightly different formulation of                   constant ei that does not appear in any unary atom in the
USP and its MLN to facilitate the exposition of OntoUSP.                          subexpression with a lambda variable xi that is uniquely in-
    3                                                                             dexed by the corresponding node i. For example, the lambda
      We call these QLFs because they are not true logical
form (the ambiguities are not yet resolved). This is related                      form for nsubj(e1 , e2 ) is λx1 λx2 .nsubj(x1 , x2 ).
                                                                                     5
to but not identical with the definition in Alshawi (1990).                            Variables can range over arbitrary lambda forms.


                                                                            298


main predicates are:                                              Each time, USP executes the highest-scored op-
e ∈ c: expression e is assigned to cluster c;                  erator and reparses affected sentences using the
                                                               new parameters. The output contains the optimal
SubExpr(s, e): s is a subexpression of e;
                                                               lambda-form clusters and parameters, as well as
HasValue(s, v): s is of value v;                               the MAP semantic parses of input sentences.
IsPart(c, i, p): p is the property cluster in ob-
    ject cluster c uniquely indexed by i.                      3 Unsupervised Ontology Induction with
                                                                 Markov Logic
In USP, property clusters in different object clus-
ters use distinct index i’s. As we will see later,             A major limitation of USP is that it either merges
in OntoUSP, property clusters with ISA relation                two object clusters into one, or leaves them sepa-
share the same index i, which corresponds to a                 rate. This is suboptimal, because different object
generic semantic frame such as agent and patient.              clusters may still possess substantial commonali-
   The probability model of USP can be captured                ties. Modeling these can help extract more gen-
by two formulas:                                               eral knowledge and answer many more questions.
                                                               The best way to capture such commonalities is
          x ∈ +p ∧ HasValue(x, +v)                             by forming an ISA hierarchy among the clusters.
         e ∈ c ∧ SubExpr(x, e) ∧ x ∈ p                         For example, INDUCE and INHIBIT are both sub-
              ⇒ ∃1 i.IsPart(c, i, p).                          concepts of REGULATE. Learning these ISA rela-
                                                               tions helps answer questions like “What regulates
   All free variables are implicitly universally               CD11b?”, when the text states that “IL-4 induces
quantified. The “+” notation signifies that the                CD11b” or “AP-1 suppresses CD11b”.
MLN contains an instance of the formula, with
                                                                  For parameter learning, this is also undesirable.
a separate weight, for each value combination of
                                                               Without the hierarchical structure, each cluster es-
the variables with a plus sign. The first formula is
                                                               timates its parameters solely based on its own ob-
the core of the model and represents the mixture
                                                               servations, which can be extremely sparse. The
of property values given the cluster. The second
                                                               better solution is to leverage the hierarchical struc-
formula ensures that a property cluster must be a
                                                               ture for smoothing (a.k.a. shrinkage (McCallum et
part in the corresponding object cluster; it is a hard
                                                               al., 1998; Gelman and Hill, 2006)). For example,
constraint, as signified by the period at the end.
                                                               if we learn that “super-induce” is a verb and that in
   To encourage clustering, USP imposes an expo-
                                                               general verbs have active and passive voices, then
nential prior over the number of parameters.
                                                               even though “super-induce” only shows up once
   To parse a new sentence, USP starts by parti-
                                                               in the corpus as in “AP-1 is super-induced by IL-
tioning the QLF into atomic forms, and then hill-
                                                               4”, by smoothing we can still infer that this proba-
climbs on the probability using a search operator
                                                               bly means the same as “IL-4 super-induces AP-1”,
based on lambda reduction until it finds the max-
                                                               which in turn helps answer questions like “What
imum a posteriori (MAP) parse. During learn-
                                                               super-induces AP-1”.
ing, USP starts with clusters of atomic forms,
                                                                  OntoUSP overcomes the limitations of USP by
maintains the optimal semantic parses according
                                                               replacing the flat clustering process with a hier-
to current parameters, and hill-climbs on the log-
                                                               archical clustering one, and learns an ISA hier-
likelihood of observed QLFs using two search op-
                                                               archy of lambda-form clusters in addition to the
erators:
                                                               IS-PART one. The output of OntoUSP consists
MERGE(c1 , c2 ) merges clusters c1 , c2 into a larger          of an ontology, a semantic parser, and the MAP
    cluster c by merging the core-form clusters                parses. In effect, OntoUSP conducts ontology in-
    and argument clusters of c1 , c2 , respectively.           duction, population, and knowledge extraction in a
    E.g., c1 = {“induce”}, c2 = {“enhance”},                   single integrated process. Specifically, given clus-
    and c = {“induce”, “enhance”}.                             ters c1 , c2 , in addition to merge vs. separate, On-
COMPOSE(c1 , c2 ) creates a new lambda-form                    toUSP evaluates a third option called abstraction,
    cluster c formed by composing the lambda                   in which a new object cluster c is created, and ISA
    forms in c1 , c2 into larger ones. E.g., c1 =              links are added from ci to c; the argument clusters
    {“amino”}, c2 = {“acid”}, and c =                          in c are formed by merging that of ci ’s.
    {“amino acid”}.                                               In the remainder of the section, we describe the


                                                         299


details of OntoUSP. We start by presenting the                 Algorithm 1 OntoUSP-Parse(MLN, T )
OntoUSP MLN. We then describe our inference                      Initialize semantic parse L with individual
algorithm and how to parse a new sentence us-                    atoms in the QLF of T
ing OntoUSP. Finally, we describe the learning al-               repeat
gorithm and how OntoUSP induces the ontology                        for all subexpressions e in L do
while learning the semantic parser.                                    Evaluate all semantic parses that are
                                                                       lambda-reducible from e
3.1 The OntoUSP MLN                                                 end for
The OntoUSP MLN can be obtained by modifying                        L ← the new semantic parse with the highest
the USP MLN with three simple changes. First,                       gain in probability
we introduce a new predicate IsA(c1 , c2 ), which                until none of these improve the probability
is true if cluster c1 is a subconcept of c2 . For con-           return L
venience, we stipulate that IsA is reflexive (i.e.,                      P
IsA(c, c) is true for any c). Second, we add two               arg maxL i wi ni (T, L).        Directly enumer-
formulas to the MLN:                                           ating all L’s is intractable. OntoUSP uses the
                                                               same inference algorithm as USP by hill-climbing
    IsA(c1 , c2 ) ∧ IsA(c2 , c3 ) ⇒ IsA(c1 , c3 ).             on the probability of L; in each step, OntoUSP
     IsPart(c1 , i1 , p1 ) ∧ IsPart(c2 , i2 , p2 )             evaluates the alternative semantic parses that
      ∧ IsA(c1 , c2 ) ⇒ (i1 = i2 ⇔ IsA(p1 , p2 )).             can be formed by lambda-reducing a current
                                                               subexpression with one of its arguments. The only
The first formula simply enforces the transitivity             difference is that OntoUSP uses a different MLN
of ISA relation. The second formula states that if             and so the probabilities and resulting semantic
the ISA relation holds for a pair of object clusters,          parses may be different. Algorithm 1 gives
it also holds between their corresponding property             pseudo-code for OntoUSP’s inference algorithm.
clusters. Both are hard constraints. Third, we in-
troduce hierarchical smoothing into the model by               3.3   Learning
replacing the USP mixture formula
                                                               OntoUSP uses the same learning objective as USP,
           x ∈ +p ∧ HasValue(x, +v)                            i.e., to find parameters θ that maximizes the log-
                                                               likelihood of observing the dependency trees T ,
with a new formula                                             summing out the unobserved semantic parses L:

  ISA(p1 , +p2 ) ∧ x ∈ p1 ∧ HasValue(x, +v)                              Lθ (T ) = log Pθ (L)
                                                                                       P
                                                                                 = log L Pθ (T, L)
Intuitively, for each p2 , the weight corresponds to
the delta in log-probability of v comparing to the                However, the learning problem in OntoUSP is
prediction according to all ancestors of p2 . The              distinct in two important aspects. First, OntoUSP
effect of this change is that now the value v of               learns in addition an ISA hierarchy among the
a subexpression x is not solely determined by its              lambda-form clusters. Second and more impor-
property cluster p1 , but is also smoothed by statis-          tantly, OntoUSP leverages this hierarchy during
tics of all p2 that are super clusters of p1 .                 learning to smooth the parameter estimation of in-
   Shrinkage takes place via interaction among the             dividual clusters, as embodied by the new ISA
weights of the ISA mixture formula. In particular,             mixture formula in the OntoUSP MLN.
if the weights for some property cluster p are all                OntoUSP faces several new challenges unseen
zero, it means that values in p are completely pre-            in previous hierarchical-smoothing approaches.
dicted by p’s ancestors. In effect, p is backed off            The ISA hierarchy in OntoUSP is not known in
to its parent.                                                 advance, but needs to be learned as well. Simi-
                                                               larly, OntoUSP has no known examples of pop-
3.2 Inference                                                  ulated facts and rules in the ontology, but has to
Given the dependency tree T of a sentence, the                 infer that in the same joint learning process. Fi-
conditional probability of a semantic parse L is               nally, OntoUSP does not start from well-formed
                                P
given by P r(L|T ) ∝ exp ( i wi ni (T, L)).                    structured input like relational tuples, but rather
The MAP semantic parse is simply                               directly from raw text. In sum, OntoUSP tackles a


                                                         300


Algorithm 2 OntoUSP-Learn(MLN, T’s)                                merge it with a property cluster in an exist-
  Initialize with a flat ontology, along with clus-                ing child of c1 ; create ISA link from it to
  ters and semantic parses                                         an abstract property cluster in c; leave it un-
  Merge clusters with the same core form                           changed.
  Agenda ← ∅
  repeat                                                         For efficiency, in both operators, the best option
     for all candidate operations O do                        is chosen greedily for each property cluster in c2 ,
        Score O by log-likelihood improvement                 in descending order of cluster size.
        if score is above a threshold then                       Notice that once an abstract cluster is created,
           Add O to agenda                                    it could be merged with an existing cluster using
        end if                                                MERGE. Thus with the new operators, OntoUSP
     end for                                                  is capable of inducing any ISA hierarchy among
     Execute the highest scoring operation O∗ in              abstract and existing clusters. (Of course, the ISA
     the agenda                                               hierarchy it actually induces depends on the data.)
     Regenerate MAP parses for affected trees and                Learning the shrinkage weights has been ap-
     update agenda and candidate operations                   proached in a variety of ways; examples include
  until agenda is empty                                       EM and cross-validation (McCallum et al., 1998),
  return the learned ontology and MLN, and the                hierarchical Bayesian methods (Gelman and Hill,
  semantic parses                                             2006), and maximum entropy with L1 priors
                                                              (Dudik et al., 2007). The past methods either only
very hard problem with exceedingly little aid from            learn parameters with one or two levels (e.g., in
user supervision.                                             hierarchical Bayes), or requires significant amount
   To combat these challenges, OntoUSP adopts                 of computation (e.g., in EM and in L1 -regularized
a novel form of hierarchical smoothing by inte-               maxent), while also typically assuming a given
grating it with the search process for identify-              hierarchy. In contrast, OntoUSP has to both in-
ing the hierarchy. Algorithm 2 gives pseudo-                  duce the hierarchy and populate it, with potentially
code for OntoUSP’s learning algorithm. Like                   many levels in the induced hierarchy, starting from
USP, OntoUSP approximates the sum over all                    raw text with little user supervision.
semantic parses with the most probable parse,                    Therefore, OntoUSP simplifies the weight
and searches for both θ and the MAP semantic                  learning problem by adopting standard m-
parses L that maximize Pθ (T, L). In addition to              estimation for smoothing. Namely, the weights
MERGE and COMPOSE, OntoUSP uses a new opera-                  for cluster c are set by counting its observations
tor ABSTRACT(c1 , c2 ), which does the following:             plus m fractional samples from its parent distribu-
                                                              tion. When c has few observations, its unreliable
  1. Create an abstract cluster c;
                                                              statistics can be significantly augmented via the
  2. Create ISA links from c1 , c2 to c;                      smoothing by its parent (and in turn to a gradually
  3. Align property clusters of c1 and c2 ; for each          smaller degree by its ancestors). m is a hyperpa-
     aligned pair p1 and p2 , either merge them               rameter that can be used to trade off bias towards
     into a single property cluster, or create an ab-         statistics for parent vs oneself.
     stract property cluster p in c and create ISA               OntoUSP also needs to balance between two
     links from pi to p, so as to maximize log-               conflicting aspects during learning. On one hand,
     likelihood.                                              it should encourage creating abstract clusters to
                                                              summarize intrinsic commonalities among the
Intuitively, c corresponds to a more abstract con-            children. On the other hand, this needs to be heav-
cept that summarizes similar properties in ci ’s.             ily regularized to avoid mistaking noise for the sig-
   To add a child cluster c2 to an existing ab-               nal. OntoUSP does this by a combination of priors
stract cluster c1 , OntoUSP also uses an operator             and thresholding. To encourage the induction of
ADDCHILD(c1 , c2 ) that does the following:                   higher-level nodes and inheritance, OntoUSP im-
                                                              poses an exponential prior β on the number of pa-
  1. Create an ISA link from c2 to c1 ;
                                                              rameter slots. Each slot corresponds to a distinct
  2. For each property cluster of c2 , maximize the           property value. A child cluster inherits its parent’s
     log-likelihood by doing one of the following:            slots (and thus avoids the penalty on them). On-


                                                        301


toUSP also stipulates that, in an ABSTRACT opera-             tains 2000 questions which were created by sam-
tion, a new property cluster can be created either as         pling verbs and entities according to their frequen-
a concrete cluster with full parameterization, or as          cies in GENIA. Sample questions include “What
an abstract cluster that merely serves for smooth-            regulates MIP-1alpha?”, “What does anti-STAT 1
ing purposes. To discourage overproposing clus-               inhibit?”. These simple question types were used
ters and ISA links, OntoUSP imposes a large ex-               to focus the evaluation on the knowledge extrac-
ponential prior γ on the number of concrete clus-             tion aspect, rather than engineering for handling
ters created by ABSTRACT. For abstract cluster, it            special question types and/or reasoning.
sets a cut-off tp and only allows storing a probabil-
ity value no less than tp . Like USP, it also rejects         4.2   Systems
MERGE and COMPOSE operations that improve log-
likelihood by less than to . These priors and cut-off         OntoUSP is the first unsupervised approach that
values can be tuned to control the granularity of             synergistically conducts ontology induction, pop-
the induced ontology and clusters.                            ulation, and knowledge extraction. The system
   Concretely, given semantic parses L, OntoUSP               closest in aim and capability is USP. We thus com-
computes the optimal parameters and evaluates                 pared OntoUSP with USP and all other systems
the regularized log-likelihood as follows. Let                evaluated in the USP paper (Poon and Domingos,
wp2 ,v denote the weight of the ISA mixture for-              2009). Below is a brief description of the systems.
mula ISA(p1 , +p2 ) ∧ x ∈ p1 ∧ HasValue(x, +v).               (For more details, see Poon & Domingos (2009).)
For convenience, for each pair of property clus-              Keyword is a baseline system based on keyword
ter c and value v, OntoUSP instead computes                   matching. It directly matches the question sub-
                       P
and stores wc,v0     = ISA(c, a) wa,v , which sums            string containing the verb and the available argu-
over all weights for c and its ancestors. (Thus               ment with the input text, ignoring case and mor-
wc,v = wc,v  0         0 , where p is the parent of
                   − wp,v                                     phology. Given a match, two ways to derive the
c.) Like USP, OntoUSP imposes local normal-                   answer were considered: KW simply returns the
ization constraints that enable closed-form esti-             rest of sentence on the other side of the verb,
mation of the optimal parameters and likelihood.              whereas KW-SYN is informed by syntax and ex-
Specifically, using m-estimation, the optimal wc,v 0          tracts the answer from the subject or object of the
                 0                                            verb, depending on the question (if the expected
is log((m · ewp,v + nc,v )/(m + nc )), where p is the
                                                              argument is absent, the sentence is ignored).
parent of c and n is the count. The log-likelihood
   P       0 · n , which is then augmented by the             TextRunner (Banko et al., 2007) is the state-of-
is c,v wc,v        c,v
                                                              the-art system for open-domain information ex-
priors.
                                                              traction. It inputs text and outputs relational triples
                                                              in the form (R, A1 , A2 ), where R is the relation
4   Experiments
                                                              string, and A1 , A2 the argument strings. To an-
4.1 Methodology                                               swer questions, each triple-question pair is consid-
                                                              ered in turn by first matching their relation strings,
Evaluating unsupervised ontology induction is dif-            and then the available argument strings. If both
ficult, because there is no gold ontology for com-            match, the remaining argument string in the triple
parison. Moreover, our ultimate goal is to aid                is returned as an answer. Results were reported
knowledge acquisition, rather than just inducing              when exact match is used (TR-EXACT), or when
an ontology for its own sake. Therefore, we                   the triple strings may contain the question ones as
used the same methodology and dataset as the                  substrings (TR-SUB).
USP paper to evaluate OntoUSP on its capabil-                 RESOLVER (Yates and Etzioni, 2009) inputs
ity in knowledge acquisition. Specifically, we ap-            TextRunner triples and collectively resolves coref-
plied OntoUSP to extract knowledge from the GE-               erent relation and argument strings. To answer
NIA dataset (Kim et al., 2003) and answer ques-               questions, the only difference from TextRunner is
tions, and we evaluated it on the number of ex-               that a question string can match any string in its
tracted answers and accuracy. GENIA contains                  cluster. As in TextRunner, results were reported
1999 PubMed abstracts.6 The question set con-                 for both exact match (RS-EXACT) and substring
  6
    http://www-tsujii.is.s.u-tokyo-                           (RS-SUB).
.ac.jp/GENIA/home/wiki.cgi.                                   DIRT (Lin and Pantel, 2001) resolves binary rela-


                                                        302


                                                                                              REGULATE
Table 1: Comparison of question answering re-                                       regulate, control, govern, modulate
sults on the GENIA dataset. Results for systems
other than OntoUSP are from Poon & Domingos                                     ISA                  ISA             ISA
(2009).
                                                                induce, enhance,         inhibit, block, suppress,        activate
                 # Total   # Correct   Accuracy                 trigger, augment,            prevent, abolish,        ACTIVATE
                                                                   up-regulate           abrogate, down-regulate
 KW                 150          67        45%
                                                                   INDUCE                        INHIBIT
 KW-SYN              87          67        77%
 TR-EXACT            29          23        79%
 TR-SUB             152          81        53%              Figure 3: A fragment of the induced ISA hierar-
 RS-EXACT            53          24        45%              chy, showing the core forms for each cluster (the
 RS-SUB             196          81        41%              cluster labels are added by the authors for illustra-
 DIRT               159          94        59%              tion purpose).
 USP                334         295        88%
                                                            recall. Compared to TextRunner (TR-SUB), On-
 OntoUSP            480         435       91%
                                                            toUSP gained on precision by 38 points and ex-
                                                            tracted more than five times of correct answers.
tions by inputting a dependency path that signifies
                                                               Manual inspection shows that the induced ISA
the relation and returns a set of similar paths. To
                                                            hierarchy is the key for the recall gain. Like
use DIRT in question answering, it was queried to
                                                            USP, OntoUSP discovered the following clusters
obtain similar paths for the relation of the ques-
                                                            (in core forms) that represent some of the core
tion, which were then used to match sentences.
                                                            concepts in biomedical research:
USP (Poon and Domingos, 2009) parses the in-
                                                                {regulate, control, govern, modulate}
put text using the Stanford dependency parser
                                                                {induce, enhance, trigger, augment, up-
(Klein and Manning, 2003; de Marneffe et al.,
                                                            regulate}
2006), learns an MLN for semantic parsing from
                                                                {inhibit, block, suppress, prevent, abolish, ab-
the dependency trees, and outputs this MLN and
                                                            rogate, down-regulate}
the MAP semantic parses of the input sentences.
                                                            However, USP formed these as separate clusters,
These MAP parses formed the knowledge base
                                                            whereas OntoUSP in addition induces ISA rela-
(KB). To answer questions, USP first parses the
                                                            tions from the INDUCE and INHIBIT clusters to
questions (with the question slot replaced by a
                                                            the REGULATE cluster (Figure 3). This allows
dummy word), and then matches the question
                                                            OntoUSP to answer many more questions that
parse to parses in the KB by testing subsumption.
                                                            are asked about general regulation events, even
OntoUSP uses a similar procedure as USP for ex-
                                                            though the text states them with specific regula-
tracting knowledge and answering questions, ex-
                                                            tion directions like “induce” or “inhibit”. Below
cept for two changes. First, USP’s learning and
                                                            is an example question-answer pair output by On-
parsing algorithms are replaced with OntoUSP-
                                                            toUSP; neither USP nor any other system were
Learn and OntoUSP-Parse, respectively. Second,
                                                            able to extract the necessary knowledge.
when OntoUSP matches a question to its KB, it
                                                                Q: What does IL-2 control?
not only considers the lambda-form cluster of the
                                                                A: The DEX-mediated IkappaBalpha induc-
question relation, but also all its sub-clusters.7
                                                            tion.
4.3 Results                                                     Sentence: Interestingly, the DEX-mediated
                                                            IkappaBalpha induction was completely inhibited
Table 1 shows the results comparing OntoUSP                 by IL-2, but not IL-4, in Th1 cells, while the re-
with other systems. While USP already greatly               verse profile was seen in Th2 cells.
outperformed other systems in both precision and               OntoUSP also discovered other interesting
recall, OntoUSP further substantially improved on           commonalities among the clusters. For exam-
the recall of USP, without any loss in precision.           ple, both USP and OntoUSP formed a singleton
In particular, OntoUSP extracted 140 more correct           cluster with core form “activate”. Although this
answers than USP, for a gain of 47% in absolute             cluster may appear similar to the INDUCE clus-
   7
     Additional details are available at                    ter, the data in GENIA does not support merg-
http : //alchemy.cs.washington.edu/papers/poon10.           ing the two. However, OntoUSP discovered that


                                                      303


the ACTIVATE cluster, while not completely resol-             ments. A relation is often manifested as verbs and
vent with INDUCE, shared very similar distribu-               has several arguments, whereas an entity typically
tions in their agent arguments. In fact, they are             appears as an argument of others and has few ar-
so similar that OntoUSP merges them into a sin-               guments of its own. As a result, in average, there
gle property cluster. It found that the patient ar-           is less information available for entities than rela-
guments of INDUCE and INHIBIT are very similar                tions. Presumably, we can address this limitation
and merged them. In turn, OntoUSP formed ISA                  by modeling longer-ranged dependencies such as
links from these three object clusters to REGULATE,           grandparents, siblings, etc. This is straightforward
as well as among their property clusters. In-                 to do in Markov logic.
tuitively, this makes sense. The positive- and                   OntoUSP also uses a rather elaborate scheme
negative-regulation events, as signified by INDUCE            for regularization. We hypothesize that this can
and INHIBIT, often target similar object entities             be much simplified and improved by adopting a
or processes. However, their agents tend to differ            principled framework such as Dudik et al. (2007).
since in one case they are inducers, and in the other
they are inhibitors. On the other hand, ACTIVATE              5    Conclusion
and INDUCE share similar agents since they both
                                                              This paper introduced OntoUSP, the first unsuper-
signify positive regulation. However, “activate”
                                                              vised end-to-end system for ontology induction
tends to be used more often when the patient ar-
                                                              and knowledge extraction from text. OntoUSP
gument is a concrete entity (e.g., cells, genes, pro-
                                                              builds on the USP semantic parser by adding the
teins), whereas “induce” and others are also used
                                                              capability to form hierarchical clusterings of logi-
with processes and events (e.g., expressions, inhi-
                                                              cal expressions, linked by ISA relations, and us-
bition, pathways).
                                                              ing them for hierarchical smoothing. OntoUSP
   USP was able to resolve common syntactic dif-              greatly outperformed USP and other state-of-the-
ferences such as active vs. passive voice. How-               art systems in a biomedical knowledge acquisition
ever, it does so on the basis of individual verbs,            task.
and there is no generalization beyond their clus-                Directions for future work include: exploiting
ters. OntoUSP, on the other hand, formed a high-              the ontological structure for principled handling of
level cluster with two abstract property clusters,            antonyms and (more generally) expressions with
corresponding to general agent argument and pa-               opposite meanings; developing and testing alter-
tient argument. The active-passive alternation is             nate methods for hierarchical modeling in On-
captured in these clusters, and is inherited by all           toUSP; scaling up learning and inference to larger
descendant clusters, including many rare verbs                corpora; investigating the theoretical properties of
like “super-induce” which only occur once in GE-              OntoUSP’s learning approach and generalizing it
NIA and for which there is no way that USP                    to other tasks; answering questions that require in-
could have learned about their active-passive al-             ference over multiple extractions; etc.
ternations. This illustrates the importance of dis-
covering ISA relations and performing hierarchi-              6    Acknowledgements
cal smoothing.
                                                              We give warm thanks to the anonymous reviewers for
                                                              their comments. This research was partly funded by ARO
4.4 Discussion                                                grant W911NF-08-1-0242, AFRL contract FA8750-09-C-
                                                              0181, DARPA contracts FA8750-05-2-0283, FA8750-07-D-
OntoUSP is a first step towards joint ontology in-            0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-
                                                              D030010, NSF grants IIS-0534881 and IIS-0803481, and
duction and knowledge extraction. The experi-                 ONR grant N00014-08-1-0670. The views and conclusions
mental results demonstrate the promise in this di-            contained in this document are those of the authors and
                                                              should not be interpreted as necessarily representing the offi-
rection. However, we also notice some limitations             cial policies, either expressed or implied, of ARO, DARPA,
in the current system. While OntoUSP induced                  NSF, ONR, or the United States Government.
meaningful ISA relations among relation clusters
like REGULATE, INDUCE, etc., it was less success-
                                                              References
ful in inducing ISA relations among entity clus-
ters such as specific genes and proteins. This is             Hiyan Alshawi. 1990. Resolving quasi logical forms. Com-
                                                                putational Linguistics, 16:133–144.
probably due to the fact that our model only con-
siders local features such as the parent and argu-            G. Bakir, T. Hofmann, B. B. Schölkopf, A. Smola, B. Taskar,


                                                        304


   S. Vishwanathan, and (eds.). 2007. Predicting Structured           Hoifung Poon and Pedro Domingos. 2009. Unsupervised
   Data. MIT Press, Cambridge, MA.                                      semantic parsing. In Proceedings of the 2009 Conference
                                                                        on Empirical Methods in Natural Language Processing,
Michele Banko, Michael J. Cafarella, Stephen Soderland,                 pages 1–10, Singapore. ACL.
  Matt Broadhead, and Oren Etzioni. 2007. Open informa-
  tion extraction from the web. In Proceedings of the Twen-           Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006. Seman-
  tieth International Joint Conference on Artificial Intelli-            tic taxonomy induction from heterogenous evidence. In
  gence, pages 2670–2676, Hyderabad, India. AAAI Press.                  Proceedings of COLING/ACL 2006.

Philipp Cimiano. 2006. Ontology learning and population               S. Staab and R. Studer. 2004. Handbook on ontologies.
   from text. Springer.                                                  Springer.

Marie-Catherine de Marneffe, Bill MacCartney, and Christo-            Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
  pher D. Manning. 2006. Generating typed dependency                    2008. Yago - a large ontology from Wikipedia and Word-
  parses from phrase structure parses. In Proceedings of the            Net. Journal of Web Semantics.
  Fifth International Conference on Language Resources
  and Evaluation, pages 449–454, Genoa, Italy. ELRA.                  Fabian Suchanek, Mauro Sozio, and Gerhard Weikum. 2009.
                                                                        Sofie: A self-organizing framework for information ex-
Pedro Domingos and Daniel Lowd. 2009. Markov Logic:                     traction. In Proceedings of the Eighteenth International
  An Interface Layer for Artificial Intelligence. Morgan &              Conference on World Wide Web.
  Claypool, San Rafael, CA.
                                                                      Jun-ichi Tsujii. 2004. Thesaurus or logical ontology, which
Miroslav Dudik, David Blei, and Robert Schapire. 2007. Hi-               do we need for mining text? In Proceedings of the Lan-
  erarchical maximum entropy density estimation. In Pro-                 guage Resources and Evaluation Conference.
  ceedings of the Twenty Fourth International Conference
  on Machine Learning.                                                Fei Wu and Daniel S. Weld. 2008. Automatically refining the
                                                                         wikipedia infobox ontology. In Proceedings of the Seven-
Christiane Fellbaum, editor. 1998. WordNet: An Electronic                teenth International Conference on World Wide Web, Bei-
  Lexical Database. MIT Press, Cambridge, MA.                            jing, China.

Andrew Gelman and Jennifer Hill. 2006. Data Analysis Us-              Alexander Yates and Oren Etzioni. 2009. Unsupervised
  ing Regression and Multilevel/Hierarchical Models. Cam-                methods for determining object and relation synonyms
  bridge University Press.                                               on the web. Journal of Artificial Intelligence Research,
                                                                         34:255–296.
Lise Getoor and Ben Taskar, editors. 2007. Introduction to
   Statistical Relational Learning. MIT Press, Cambridge,
   MA.

Marti Hearst. 1992. Automatic acquisition of hyponyms
  from large text corpora. In Proceedings of the 14th In-
  ternational Conference on Computational Linguistics.

Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsu-
   jii. 2003. GENIA corpus - a semantically annotated cor-
   pus for bio-textmining. Bioinformatics, 19:180–82.

Dan Klein and Christopher D. Manning. 2003. Accurate
  unlexicalized parsing. In Proceedings of the Forty First
  Annual Meeting of the Association for Computational Lin-
  guistics, pages 423–430.

Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of
  inference rules from text. In Proceedings of the Seventh
  ACM SIGKDD International Conference on Knowledge
  Discovery and Data Mining, pages 323–328, San Fran-
  cisco, CA. ACM Press.

Alexander Maedche. 2002. Ontology learning for the se-
   mantic Web. Kluwer Academic Publishers, Boston, Mas-
   sachusetts.

Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and
  Andrew Ng. 1998. Improving text classification by
  shrinkage in a hierarchy of classes. In Proceedings of the
  Fifteenth International Conference on Machine Learning.

Hoifung Poon and Pedro Domingos. 2008. Joint unsuper-
  vised coreference resolution with Markov logic. In Pro-
  ceedings of the 2008 Conference on Empirical Methods in
  Natural Language Processing, pages 649–658, Honolulu,
  HI. ACL.


                                                                305

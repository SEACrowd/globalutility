       Exploring Syntactic Structural Features for Sub-Tree Alignment
                       using Bilingual Tree Kernels

                  Jun Sun1,2              Min Zhang1              Chew Lim Tan2
    1
      Institute for Infocomm Research 2School of Computing, National University of Singapore
          sunjun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg




                        Abstract


     We propose Bilingual Tree Kernels (BTKs) to
     capture the structural similarities across a pair of
     syntactic translational equivalences and apply
     BTKs to sub-tree alignment along with some
     plain features. Our study reveals that the struc-
     tural features embedded in a bilingual parse tree
     pair are very effective for sub-tree alignment
     and the bilingual tree kernels can well capture
     such features. The experimental results show
     that our approach achieves a significant im-
     provement on both gold standard tree bank and
     automatically parsed tree pairs against a heuris-
     tic similarity based method. We further apply
     the sub-tree alignment in machine translation                        Figure 1: Sub-tree alignment as referred to
     with two methods. It is suggested that the sub-                                   Node alignment
     tree alignment benefits both phrase and syntax
     based systems by relaxing the constraint of the
     word alignment.
                                                                   (ii)  descendants of a source linked node may
1     Introduction                                                       only link to descendants of its target
Syntax based Statistical Machine Translation                             linked counterpart;
(SMT) systems allow the translation process to be                  (iii) ancestors of a source linked node may on-
more grammatically performed, which provides                             ly link to ancestors of its target linked
decent reordering capability. However, most of the                       counterpart.
syntax based systems construct the syntactic trans-                By sub-tree alignment, translational equivalent
lation rules based on word alignment, which not                 sub-tree pairs are coupled as aligned counterparts.
only suffers from the pipeline errors, but also fails           Each pair consists of both the lexical constituents
to effectively utilize the syntactic structural fea-            and their maximum tree structures generated over
tures. To address those deficiencies, Tinsley et al.            the lexical sequences in the original parse trees.
(2007) attempt to directly capture the syntactic                Due to the 1-to-1 mapping between sub-trees and
translational equivalences by automatically con-                tree nodes, sub-tree alignment can also be consi-
ducting sub-tree alignment, which can be defined                dered as node alignment by conducting multiple
as follows:                                                     links across the internal nodes as shown in Fig. 1.
   A sub-tree alignment process pairs up sub-tree                  Previous studies conduct sub-tree alignments by
pairs across bilingual parse trees whose contexts               either using a rule based method or conducting
are semantically translational equivalent. Accord-              some similarity measurement only based on lexi-
ing to Tinsley et al. (2007), a sub-tree aligned                cal features. Groves et al. (2004) conduct sub-tree
parse tree pair follows the following criteria:                 alignment by using some heuristic rules, lack of
   (i) a node can only be linked once;                          extensibility and generality. Tinsley et al. (2007)

                                                            306
           Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315,
                    Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


and Imamura (2001) propose some score functions             2     Bilingual Tree Kernels
based on the lexical similarity and co-occurrence.
These works fail to utilize the structural features,        In this section, we propose the two BTKs and
rendering the syntactic rich task of sub-tree align-        study their capability and complexity in modeling
ment less convincing and attractive. This may be            the bilingual structural similarity. Before elaborat-
due to the fact that the syntactic structures in a          ing the concepts of BTKs, we first illustrate some
parse tree pair are hard to describe using plain fea-       notations to facilitate further understanding.
tures. In addition, explicitly utilizing syntactic tree        Each sub-tree pair · can be explicitly de-
fragments results in exponentially high dimen-              composed into multiple sub-structures which be-
sional feature vectors, which is hard to compute.           long to the given sub-structure spaces.
Alternatively, convolution parse tree kernels (Col-            ,…, ,…,         refers to the source tree sub-
lins and Duffy, 2001), which implicitly explore             structure space; while            ,…, ,…,      refers
the tree structure information, have been success-          to the target sub-structure space. A sub-structure
fully applied in many NLP tasks, such as Semantic           pair    ,   refers to an element in the set of the
parsing (Moschitti, 2004) and Relation Extraction           Cartesian product of the two sub-structure spaces:
(Zhang et al. 2006). However, all those studies are            ,             .
carried out in monolingual tasks. In multilingual
tasks such as machine translation, tree kernels are         2.1    Independent             Bilingual         Tree         Kernel
seldom applied.                                                    (iBTK)
   In this paper, we propose Bilingual Tree Ker-            Given the sub-structure spaces and , we con-
nels (BTKs) to model the bilingual translational            struct two vectors using the integer counts of the
equivalences, in our case, to conduct sub-tree              source and target sub-structures:
alignment. This is motivated by the decent effec-
tiveness of tree kernels in expressing the similarity                              #       ,…,#          ,…,#   | |
between tree structures. We propose two kinds of                                   #       ,…,#          ,…,#
BTKs named dependent Bilingual Tree Kernel
(dBTK), which takes the sub-tree pair as a whole               where #     and #      are the numbers of oc-
and independent Bilingual Tree Kernel (iBTK),               currences of the sub-structures and . In order
which individually models the source and the tar-           to compute the dot product of the feature vectors
get sub-trees. Both kernels can be utilized within          in the exponentially high dimensional feature
different feature spaces using various representa-          space, we introduce the tree kernel functions as
tions of the sub-structures.                                follows:
   Along with BTKs, various lexical and syntactic                            · ,       ·                 ,            ,
structural features are proposed to capture the cor-
respondence between bilingual sub-trees using a                The iBTK is defined as a composite kernel con-
polynomial kernel. We then attempt to combine               sisting of a source tree kernel and a target tree
the polynomial kernel and BTKs to construct a               kernel which measures the source and the target
composite kernel. The sub-tree alignment task is            structural similarity respectively. Therefore, the
considered as a binary classification problem. We           composite kernel can be computed using the ordi-
employ a kernel based classifier with the compo-            nary monolingual tree kernels (Collins and Duffy,
site kernel to classify each candidate of sub-tree          2001).
pair as aligned or unaligned. Then a greedy search                       ,                      ,
algorithm is performed according to the three cri-                   ∑   | |
                                                                               ∑                        ·∑
teria of sub-tree alignment within the space of
candidates classified as aligned.                                    ∑          ∑           ∆       ,
   We evaluate the sub-tree alignment on both the              where     and      refer to the node sets of the
gold standard tree bank and an automatically                source sub-tree and         respectvely.        is an
parsed corpus. Experimental results show that the           indicator function which equals to 1 iff the sub-
proposed BTKs benefit sub-tree alignment on both            structure is rooted at the node          and 0 other-
corpora, along with the lexical features and the                                | |
                                                            wise. ∆ ,         ∑            ·         is the num-
plain structural features. Further experiments in
machine translation also suggest that the obtained          ber of identical sub-structures rooted at and .
sub-tree alignment can improve the performance              Then we compute the ∆ ,          function as follows:
of both phrase and syntax based SMT systems.



                                                          307


(1) If the production rule at and are different,                                                      It is infeasible to explicitly compute the kernel
∆ ,          0;                                                                                    function by expressing the sub-trees as feature
(2)else if both and are POS tags, ∆ ,               ;                                              vectors. In order to achieve convenient computa-
(3)else, ∆ ,           ∏       1 ∆      , ,   ,   .                                                tion, we deduce the kernel function as the above.
   where           is the child number of ,     ,                                                     The deduction from (1) to (2) is derived accord-
is the lth child of , is the decay factor used to                                                  ing to the fact that the number of identical sub-
make the kernel value less variable with respect to                                                structure pairs rooted in the node pairs    ,   and
the number of sub-structures.                                                                          ,     equals to the product of the respective
   Similarly, we can decompose the target kernel                                                   counts. As a result, the dBTK can be evaluated as
as       ,       ∑       ∑       ∆ ,    and run the                                                a product of two monolingual tree kernels. Here
                                                                                                   we verify the correctness of the kernel by directly
algorithm above as well.                                                                           constructing the feature space for the inner prod-
   The disadvantage of the iBTK is that it fails to                                                uct. Alternatively, Cristianini and Shawe-Taylor
capture the correspondence across the sub-                                                         (2000) prove the positive semi-definite characte-
structure pairs. However, the composite style of                                                   ristic of the tensor product of two kernels. The
constructing the iBTK helps keep the computa-                                                      decomposition benefits the efficient computation
tional complexity comparable to the monolingual                                                    to use the algorithm for the monolingual tree ker-
tree kernel, which is | | · | | | | · | | .                                                        nel in Section 2.1.
2.2       Dependent Bilingual Tree Kernel (dBTK)                                                      The computational complexity of the dBTK is
                                                                                                   still | | · | | | | · | | .
The iBTK explores the structural similarity of the
source and the target sub-trees respectively. As an                                                3     Sub-structure Spaces for BTKs
alternative, we further define a kernel to capture
the relationship across the counterparts without                                                   The syntactic translational equivalences under
increasing the computational complexity. As a                                                      BTKs are evaluated with respective to the sub-
result, we propose the dependent Bilingual Tree                                                    structures factorized from the candidate sub-tree
kernel (dBTK) to jointly evaluate the similarity                                                   pairs. In this section, we propose different sub-
across sub-tree pairs by enlarging the feature                                                     structures to facilitate the measurement of syntac-
space to the Cartesian product of the two sub-                                                     tic similarity for sub-tree alignment. Since the
structure sets.                                                                                    proposed BTKs can be computed by individually
   A dBTK takes the source and the target sub-                                                     evaluating the source and target monolingual tree
structure pair as a whole and recursively calculate                                                kernels, the definition of the sub-structure can be
over the joint sub-structures of the given sub-tree                                                simplified to base only on monolingual sub-trees.
pair. We define the dBTK as follows:
                                                                                                   3.1    Subset Tree
   Given the sub-structure space           , we con-
struct a vector using the integer counts of the sub-                                               Motivated from Collins and Duffy (2002) in mo-
structure pairs to represent a sub-tree pair:                                                      nolingual tree kernels, the Subset Tree (SST) can
                                                                                                   be employed as sub-structures. An SST is any sub-
                              #           ,       ,…,#        ,            ,#        ,       ,
          ·                                                                                        graph, which includes more than one non-terminal
                                  …,#             | |,    ,…,#            | |,                     node, with the constraint that the entire rule pro-
                                                                                                   ductions are included. Fig. 2 shows an example of
   where # ,      is the number of occurrences of
                                                                                                   the SSTs decomposed from the source sub-tree
the sub-structure pair , .                                                                         rooted at VP*.
                          · ,         ·                                                            3.2    Root directed Subset Tree
              ·           ,           ·
                                                                                                   Monolingual Tree kernels achieve decent perfor-
      |       |   ∑                   ∑                   ,           ·                            mance using the SSTs due to the rich exploration
  ∑                                                                                                of syntactic information. However, the sub-tree
                  ∑                   ∑                       ,
                                                                           ,     ,                 alignment task requires strong capability of dis-
  ∑               ∑               ∑               ∑       ∆                              (1)       criminating the sub-trees with their roots across
                                                                            ,
                                                                  ∆       ,      ·                 adjacent generations, because those candidates
  ∑               ∑               ∑               ∑                                      (2)       share many identical SSTs. As illustrated in Fig 2,
                                                                  ∆         ,
                                                                                                   the source sub-tree rooted at VP*, which should
  ∑               ∑               ∆           ,       ∑           ∑             ∆        ,         be aligned to the target sub-tree rooted at NP*,
          ,           ·           ,                                                                may be likely aligned to the sub-tree rooted at PP*,


                                                                                                 308


                               Figure 2: Illustration of SST, RdSST and RgSST


which shares quite a similar context with NP*. It         nel function can be simplified to only capture the
is also easy to show that the latter shares all the       sub-structure rooted at the root of the sub-tree.
SSTs that the former obtains. In consequence, the
                                                                                ,       ∆   ,
values of the SST based kernel function are quite
similar between the candidate sub-tree pair rooted           where and are the root nodes of the sub-
at (VP*,NP*) and (VP*,PP*).                               tree and respectively. The time complexity is
   In order to effectively differentiate the candi-       reduced to | | | | | | | | .
dates like the above, we propose the Root directed
                                                          3.4    Root only
Subset Tree (RdSST) by encapsulating each SST
with the root of the given sub-tree. As shown in          More aggressively, we can simplify the kernel to
Fig 2, a sub-structure is considered identical to the     only measure the common root node without con-
given examples, when the SST is identical and the         sidering the complex tree structures. Therefore the
root tag of the given sub-tree is NP. As a result,        kernel function is simplified to be a binary func-
the kernel function in Section 2.1 is re-defined as:      tion with time complexity 1 .
         ,      ∑       ∑      ∆    ,        ,                                  ,           ,
                    ,   ∑      ∑        ∆   ,
                                                          4     Plain features
   where and are the root nodes of the sub-
tree and respectively. The indicator function             Besides BTKs, we introduce various plain lexical
    ,   equals to 1 if and are identical, and 0           features and structural features which can be ex-
otherwise. Although defined for individual SST,           pressed as feature functions. The lexical features
the indicator function can be evaluated outside the       with directions are defined as conditional feature
summation, without increasing the computational           functions based on the conditional lexical transla-
complexity of the kernel function.                        tion probabilities. The plain syntactic structural
                                                          features can deal with the structural divergence of
3.3   Root generated Subset Tree                          bilingual parse trees in a more general perspective.
Some grammatical tags (NP/VP) may have iden-              4.1    Lexical and Word Alignment Features
tical tags as their parents or children which may
make RdSST less effective. Consequently, we step          In this section, we define seven lexical features to
further to propose the sub-structure of Root gener-       measure semantic similarity of a given sub-tree
ated Subset Tree (RgSST). An RgSST requires the           pair.
root node of the given sub-tree to be part of the            Internal Lexical Features: We define two lex-
sub-structure. In other words, all sub-structures         ical features with respective to the internal span of
should be generated from the root of the given            the sub-tree pair.
sub-tree as presented in Fig. 2. Therefore the ker-                |       ∏        ∑            |   |   |




                                                        309


                                                                                                 Thus the model will penalize the candidate sub-
                |           ∏           ∑                              |       |       |
                                                                                                 tree pairs with largely different length of spans.
   where       | refers to the lexical translation                                                                                    |       |   |       |
probability from the source word to the target                                                                               ,
                                                                                                                                      |       |   |   |
word within the sub-tree spans, while             |
                                                                                                      and refer to the entire source and target parse
refers to that from target to source;       refers to
                                                                                                 trees respectively. Therefore, |    | and |     | are
the word set for the internal span of the source
                                                                                                 the respective span length of the parse tree used for
sub-tree , while         refers to that of the target                                            normalization.
sub-tree .                                                                                          Number of Descendants: Similarly, the num-
   Internal-External Lexical Features: These                                                     ber of the root’s descendants of the aligned sub-
features are motivated by the fact that lexical                                                  trees should also correspond.
translation probabilities within the translational
                                                                                                                                          |   |   |   |
equivalence tend to be high, and that of the non-                                                                                ,
                                                                                                                                          |   |   |   |
equivalent counterparts tend to be low.
                                                                                                    where . refers to the descendant set of the
            |               ∏           ∑                              |           |       |
                                                                                                 root to a sub-tree.
            |               ∏           ∑                              |           |       |        Tree Depth difference: Intuitively, translation-
                                                                                                 al equivalent sub-tree pairs tend to have similar
   where           refers to the word set for the ex-                                            depth from the root of the parse tree. We allow the
ternal span of the source sub-tree , while                                                       model to penalize the candidate sub-tree pairs with
refers to that of the target sub-tree .                                                          quite different distance of path from the root of the
   Internal Word Alignment Features: The word                                                    parse tree to the root of the sub-tree.
alignment links account much for the co-
occurrence of the aligned terms. We define the                                                                       ,
internal word alignment features as follows:
                    ∑       ∑                    ,           ·         |   ·           |         5    Alignment Model
      ,
                                    |            |·|               |                             Given feature spaces defined in the last two sec-
where                                                                                            tions, we propose a 2-phase sub-tree alignment
                                    1       if           ,       is aligned                      model as follows:
                        ,
                                    0                            otherwise                          In the 1st phase, a kernel based classifier, SVM
   The binary function     , is employed to trig-                                                in our study, is employed to classify each candi-
ger the computation only when a word aligned                                                     date sub-tree pair as aligned or unaligned. The
link exists for the two words , within the sub-                                                  feature vector of the classifier is computed using a
tree span.                                                                                       composite kernel:
   Internal-External Word Alignment Features:
                                                                                                        · ,      ·
Similar to the lexical features, we also introduce
the internal-external word alignment features as                                                           · ,           ·           ∑K               · ,     ·
follows:
                                                                                                        ·,· is the normalized form of the polynomi-
                    ∑       ∑                        ,       ·         |   ·           |         al kennel       ·,· , which is a polynomial kernel
      ,
                                    |            |·|               |                             with the degree of 2, utilizing the plain features.
                    ∑           ∑                    ,       ·         |   ·           |
                                                                                                        ·,· is the normalized form of the BTK
      ,                                                                                                 ·,· , exploring the corresponding sub-
                                    |        |·|                   |                             structure space. The composite kernel can be con-
where                                                                                            structed using the polynomial kernel for plain fea-
                                    1       if           ,       is aligned
                        ,                                                                        tures and various BTKs for tree structure by linear
                                    0                            otherwise
                                                                                                 combination with coefficient , where ∑K          1.
4.2       Online Structural Features                                                                In the 2nd phase, we adopt a greedy search with
In addition to the lexical correspondence, we also                                               respect to the alignment probabilities. Since SVM
capture the structural divergence by introducing                                                 is a large margin based discriminative classifier
the following tree structural features.                                                          rather than a probabilistic model, we introduce a
   Span difference: Translational equivalent sub-                                                sigmoid function to convert the distance against
tree pairs tend to share similar length of spans.                                                the hyperplane to a posterior alignment probability
                                                                                                 as follows:


                                                                                               310


                                 1
                    | ,                                                                   Chinese English
                             1
                                                                 # of Sentence pair              5000
                                 1                               Avg. Sentence Length      12.93      12.92
                    | ,
                             1                                   Avg. # of sub-tree        21.40      23.58
   where     is the distance for the instances classi-           Avg. # of alignment            11.60
fied as aligned and       is that for the unaligned.              Table 1. Corpus Statistics for HIT corpus
We use         | ,    as the confidence to conduct
the sure links for those classified as aligned. On            Most linguistically motivated syntax based
this perspective, the alignment probability is suit-       SMT systems require an automatic parser to per-
able as a searching metric. The search space is            form the rule induction. Thus, it is important to
reduced to that of the candidates classified as            evaluate the sub-tree alignment on the automati-
aligned after the 1st phase.                               cally parsed corpus with parsing errors. In addition,
                                                           HIT corpus is not applicable for MT experiment
6     Experiments on Sub-Tree Alignments                   due to the problems of domain divergence, annota-
                                                           tion discrepancy (Chinese parse tree employs a
In order to evaluate the effectiveness of the align-
                                                           different grammar from Penn Treebank annota-
ment model and its capability in the applications
                                                           tions) and degree of tolerance for parsing errors.
requiring syntactic translational equivalences, we
                                                              Due to the above issues, we annotate a new data
employ two corpora to carry out the sub-tree
                                                           set to apply the sub-tree alignment in machine
alignment evaluation. The first is HIT gold stan-
                                                           translation. We randomly select 300 bilingual sen-
dard English Chinese parallel tree bank referred as
                                                           tence pairs from the Chinese-English FBIS corpus
HIT corpus1. The other is the automatically parsed
                                                           with the length 30 in both the source and target
bilingual tree pairs selected from FBIS corpus (al-
                                                           sides. The selected plain sentence pairs are further
lowing minor parsing errors) with human anno-
                                                           parsed by Stanford parser (Klein and Manning,
tated sub-tree alignment.
                                                           2003) on both the English and Chinese sides. We
6.1    Data preparation                                    manually annotate the sub-tree alignment for the
                                                           automatically parsed tree pairs according to the
HIT corpus, which is collected from English learn-         definition in Section 1. To be fully consistent with
ing text books in China as well as example sen-            the definition, we strictly reserve the semantic
tences in dictionaries, is used for the gold standard      equivalence for the aligned sub-trees to keep a
corpus evaluation. The word segmentation, toke-            high precision. In other words, we do not conduct
nization and parse-tree in the corpus are manually         any doubtful links. The corpus is further divided
constructed or checked. The corpus is constructed          into 200 aligned tree pairs for training and 100 for
with manually annotated sub-tree alignment. The            testing as shown in Table 2.
annotation strictly reserves the semantic equiva-
lence of the aligned sub-tree pair. Only sure links                                        Chinese English
are conducted in the internal node level, without                 # of Sentence pair              300
considering possible links adopted in word align-                 Avg. Sentence Length      16.94      20.81
ment. A different annotation criterion of the Chi-                Avg. # of sub-tree        28.97      34.39
nese parse tree, designed by the annotator, is em-                Avg. # of alignment            17.07
ployed. Compared with the widely used Penn
TreeBank annotation, the new criterion utilizes                    Table 2. Statistics of FBIS selected Corpus
some different grammar tags and is able to effec-
tively describe some rare language phenomena in            6.2     Baseline approach
Chinese. The annotator still uses Penn TreeBank            We implement the work in Tinsley et al. (2007) as
annotation on the English side. The statistics of          our baseline methodology.
HIT corpus used in our experiment is shown in                Given a tree pair          ,    , the baseline ap-
Table 1. We use 5000 sentences for experiment              proach first takes all the links between the sub-tree
and divide them into three parts, with 3k for train-       pairs as alignment hypotheses, i.e., the Cartesian
ing, 1k for testing and 1k for tuning the parameters       product of the two sub-tree sets:
of kernels and thresholds of pruning the negative                      ,…, ,…,              ,…, ,…,
instances.                                                    By using the lexical translation probabilities,
                                                           each hypothesis is assigned an alignment score.
1
 HIT corpus is designed and constructed by HIT-MITLAB.     All hypotheses with zero score are pruned out.
http://mitlab.hit.edu.cn/index.php/resources.html .


                                                         311


Feature Space                 P          R           F             Feature Space                 P         R         F
Lex                         61.62      58.33       59.93           Lex                         73.48     71.66     72.56
Lex +Online Str             70.08      69.02       69.54           Lex +Online Str             77.02     73.63     75.28
Plain +dBTK-STT             80.36      78.08       79.20           Plain +dBTK-STT             81.44     74.42     77.77
Plain +dBTK-RdSTT           87.52      74.13       80.27           Plain +dBTK-RdSTT           81.40     69.29     74.86
Plain +dBTK-RgSTT           88.54      70.18       78.30           Plain +dBTK-RgSTT           81.90     67.32     73.90
Plain +dBTK-Root            81.05      84.38       82.68           Plain +dBTK-Root            78.60     80.90     79.73
Plain +iBTK-STT             81.57      73.51       77.33           Plain +iBTK-STT             82.94     79.44     81.15
Plain +iBTK-RdSTT           82.27      77.85       80.00           Plain +iBTK-RdSTT           83.14      80       81.54
Plain +iBTK-RgSTT           82.92      78.77       80.80           Plain +iBTK-RgSTT           83.09     79.72     81.37
Plain +iBTK-Root            76.37      76.81       76.59           Plain +iBTK-Root            78.61     79.49     79.05
Plain +dBTK-Root            85.53      85.12       85.32           Plain +dBTK-Root            82.70     82.70     82.70
      +iBTK-RgSTT                                                        +iBTK-RdSTT
  Baseline                  64.14      66.99       65.53             Baseline                  70.48     78.70     74.36

Table 3. Structure feature contribution for HIT test set          Table 4. Structure feature contribution for FBIS test set
               *Plain= Lex +Online Str

Then the algorithm iteratively selects the link of                 6.3    Experimental settings
the sub-tree pairs with the maximum score as a                     We use SVM with binary classes as the classifier.
sure link, and blocks all hypotheses that contradict               In case of the implementation, we modify the Tree
with this link and itself, until no non-blocked hy-                Kernel tool (Moschitti, 2004) and SVMLight
potheses remain.                                                   (Joachims, 1999). The coefficient for the com-
   The baseline system uses many heuristics in                     posite kernel are tuned with respect to F-measure
searching the optimal solutions with alternative                   (F) on the development set of HIT corpus. We
score functions. Heuristic skip1 skips the tied hy-                empirically set C=2.4 for SVM and use        0.23,
potheses with the same score, until it finds the                   the default parameter      0.4 for BTKs.
highest-scoring hypothesis with no competitors of                     Since the negative training instances largely
the same score. Heuristic skip2 deals with the                     overwhelm the positive instances, we prune the
same problem. Initially, it skips over the tied hy-                negative instances using the thresholds according
potheses. When a hypothesis sub-tree pair ,                        to the lexical feature functions ( , , , ) and
without any competitor of the same score is found,                 online structural feature functions ( , , ).
where neither nor has been skipped over, the                       Those thresholds are also tuned on the develop-
hypothesis is chosen as a sure link. Heuristic                     ment set of HIT corpus with respect to F-measure.
span1 postpones the selection of the hypotheses                       To learn the lexical and word alignment fea-
on the POS level. Since the highest-scoring hypo-                  tures for both the proposed model and the baseline
theses tend to appear on the leaf nodes, it may in-                method, we train GIZA++ on the entire FBIS bi-
troduce ambiguity when conducting the alignment                    lingual corpus (240k). The evaluation is conducted
for a POS node whose child word appears twice in                   by means of Precision (P), Recall (R) and F-
a sentence.                                                        measure (F).
   The baseline method proposes two score func-
tions based on the lexical translation probability.                6.4    Experimental results
They also compute the score function by splitting                  In Tables 3 and 4, we incrementally enlarge the
the tree into the internal and external components.                feature spaces in certain order for both corpora
   Tinsley et al. (2007) adopt the lexical transla-                and examine the feature contribution to the align-
tion probabilities dumped by GIZA++ (Och and                       ment results. In detail, the iBTKs and dBTKs are
Ney, 2003) to compute the span based scores for                    firstly combined with the polynomial kernel for
each pair of sub-trees. Although all of their heuris-              plain features individually, then the best iBTK and
tics combinations are re-implemented in our study,                 dBTK are chosen to construct a more complex
we only present the best result among them with                    composite kernel along with the polynomial kernel
the highest Recall and F-value as our baseline,                    for both corpora. The experimental results show
denoted as skip2_s1_span12.                                        that:

2
                                                                   • All the settings with structural features of the
  s1 denotes score function 1 in Tinsley et al. (2007),              proposed approach achieve better performance
skip2_s1_span1 denotes the utilization of heuristics skip2 and
span1 while using score function 1                                   than the baseline method. This is because the

                                                                 312


  baseline only assesses semantic similarity using           than in FBIS corpus. Other than the factor of
  the lexical features. The improvement suggests             the amount of training data, this is also because
  that the proposed framework with syntactic                 the plain features in Table 3 are not as effective
  structural features is more effective in modeling          as those in Table 4, since they are trained on
  the bilingual syntactic correspondence.                    FBIS corpus which facilitates Table 4 more with
                                                             respect to the domains. On the other hand, the
• By introducing BTKs to construct a composite
                                                             grammatical tags and syntactic tree structures
  kernel, the performance in both corpora is sig-
                                                             are more accurate in HIT corpus, which facili-
  nificantly improved against only using the poly-
                                                             tates the performance of BTKs in Table 3.
  nomial kernel for plain features. This suggests
  that the structural features captured by BTKs are      • On the comparison across the different feature
  quite useful for the sub-tree alignment task. We         spaces of BTKs, we find that STT, RdSTT and
  also try to use BTKs alone without the poly-             TgSTT are rather selective, since Recalls of
  nomial kernel for plain features; however, the           those feature spaces are relatively low, exp. for
  performance is rather low. This suggests that the        HIT corpus. However, the Root sub-structure
  structure correspondence cannot be used to               obtains a satisfactory Recall for both corpora.
  measure the semantically equivalent tree struc-          That’s why we attempt to construct a more
  tures alone, since the same syntactic structure          complex composite kernel in adoption of the
  tends to be reused in the same parse tree and            kernel of dBTK-Root as below.
  lose the ability of disambiguation to some extent.
                                                         • To gain an extra performance boosting, we fur-
  In other words, to capture the semantic similari-
                                                           ther construct a composite kernel which includes
  ty, structure features requires lexical features to
                                                           the best iBTK and the best dBTK for each cor-
  cooperate.
                                                           pus along with the polynomial kernel for plain
• After comparing iBTKs with the corresponding             features. In the HIT corpus, we use dBTK in the
  dBTKs, we find that for FBIS corpus, iBTK                Root space and iBTK in the RgSST space; while
  greatly outperforms dBTK in any feature space            for FBIS corpus, we use dBTK in the Root
  except the Root space. However, when it comes            space and iBTK in the RdSST space. The expe-
  the HIT corpus, the gaps between the corres-             rimental results suggest that by combining iBTK
  ponding iBTKs and dBTKs are much closer,                 and dBTK together, we can achieve more im-
  while on the Root space, dBTK outperforms                provement.
  iBTK to a large amount. This finding can be ex-
  plained by the relationship between the amount         7     Experiments on Machine Translation
  of training data and the high dimensional feature
                                                         In addition to the intrinsic alignment evaluation,
  space. Since dBTKs are constructed in a joint
                                                         we further conduct the extrinsic MT evaluation.
  manner which obtains a much larger high di-
                                                         We explore the effectiveness of sub-tree alignment
  mensional feature space than those of iBTKs,
                                                         for both phrase based and linguistically motivated
  dBTKs require more training data to excel its
                                                         syntax based SMT systems.
  capability, otherwise it will suffer from the data
  sparseness problem. The reason that dBTK out-          7.1     Experimental configuration
  performs iBTK in the feature space of Root in
  FBIS corpus is that although it is a joint feature     In the experiments, we train the translation model
  space, the Root node pairs can be constructed          on FBIS corpus (7.2M (Chinese) + 9.2M (English)
  from a close set of grammar tags and to form a         words in 240,000 sentence pairs) and train a 4-
  relatively low dimensional space.                      gram language model on the Xinhua portion of the
     As a result, when applying to FBIS corpus,          English Gigaword corpus (181M words) using the
  which only contains limited amount of training         SRILM Toolkits (Stolcke, 2002). We use these
  data, dBTKs will suffer more from the data             sentences with less than 50 characters from the
  sparseness problem, and therefore, a relatively        NIST MT-2002 test set as the development set (to
  low performance. When enlarging the amount of          speed up tuning for syntax based system) and the
  training corpus to the HIT corpus, the ability of      NIST MT-2005 test set as our test set. We use the
  dBTKs excels and the benefit from data increas-        Stanford parser (Klein and Manning, 2003) to
  ing of dBTKs is more significant than iBTKs.           parse bilingual sentences on the training set and
                                                         Chinese sentences on the development and test set.
• We also find that the introduction of BTKs gains       The evaluation metric is case-sensitive BLEU-4.
  more improvement in HIT gold standard corpus


                                                       313


         System         Model          BLEU                    aware and syntactically meaningful. However,
                                                               utilizing syntactic translational equivalences alone
          Moses          BP*           23.86
                         DirC          23.98
                                                               for machine translation loses the capability of
                        EWoS           24.48                   modeling non-syntactic phrases (Koehn et al.,
          Syntax        STSG           24.71                   2003). Consequently, instead of using phrases
          STSG           DirC          25.16                   constraint by sub-tree alignment alone, we attempt
                        EWoS           25.38                   to combine word alignment and sub-tree align-
         Syntax         STSSG          25.92                   ment and deploy the capability of both with two
         STSSG           DirC          25.95                   methods.
                        EWoS           26.45
                                                               • Directly Concatenate (DirC) is operated by di-
      Table 5. MT evaluation on various systems                  rectly concatenating the rule set genereted from
            *BP denotes bilingual phrases                        sub-tree alignment and the original rule set gen-
                                                                 erated from word alignment (Tinsley et al.,
   For the phrase based system, we use Moses                     2009). As shown in Table 5, we gain minor im-
(Koehn et al., 2007) with its default settings. For              provement in the Bleu score for all configura-
the syntax based system, since sub-tree alignment                tions.
can directly benefit Tree-2-Tree based systems,                • Alternatively, we proposed a new approach to
we apply the sub-tree alignment in a syntax sys-                 generate the rule set from the scratch. We con-
tem based on Synchronous Tree Substitution                       strain the bilingual phrases to be consistent with
Grammar (STSG) (Zhang et al., 2007). The STSG                    Either Word alignment or Sub-tree alignment
based decoder uses a pair of elementary tree3 as a               (EWoS) instead of being originally consistent
basic translation unit. Recent research on tree                  with the word alignment only. The method helps
based systems shows that relaxing the restriction                tailoring the rule set decently without redundant
from tree structure to tree sequence structure                   counts for syntactic rules. The performance is
(Synchronous Tree Sequence Substitution Gram-                    further improved compared to DirC in all sys-
mar: STSSG) significantly improves the transla-                  tems.
tion performance (Zhang et al., 2008). We imple-
ment the STSG/STSSG based model in the Pisces                    The findings suggest that with the modeling of
decoder with the identical features and settings in            non-syntactic phrases maintained, more emphasis
Sun et al. (2009). In the Pisces decoder, the                  on syntactic phrases can benefit both the phrase
STSSG based decoder translates each span itera-                and syntax based SMT systems.
tively in a bottom up manner which guarantees
that when translating a source span, any of its sub-           8       Conclusion
spans is already translated. The STSG based de-                In this paper, we explore syntactic structure fea-
coding can be easily performed with the STSSG                  tures by means of Bilingual Tree Kernels and ap-
decoder by restricting the translation rule set to be          ply them to bilingual sub-tree alignment along
elementary tree pairs only.                                    with various lexical and plain structural features.
   As for the alignment setting, we use the word               We use both gold standard tree bank and the au-
alignment trained on the entire FBIS (240k) cor-               tomatically parsed corpus for the sub-tree align-
pus by GIZA++ with heuristic grow-diag-final for               ment evaluation. Experimental results show that
both Moses and the syntax system. For sub-tree-                our model significantly outperforms the baseline
alignment, we use the above word alignment to                  method and the proposed Bilingual Tree Kernels
learn lexical/word alignment feature, and train                are very effective in capturing the cross-lingual
with the FBIS training corpus (200) using the                  structural similarity. Further experiment shows
composite kernel of Plain+dBTK-Root+iBTK-                      that the obtained sub-tree alignment benefits both
RdSTT.                                                         phrase and syntax based MT systems by deliver-
7.2    Experimental results                                    ing more weight on syntactic phrases.

Compared with the adoption of word alignment,                  Acknowledgments
translational equivalences generated from struc-               We thank MITLAB4 in Harbin Institute of Tech-
tural alignment tend to be more grammatically                  nology for licensing us their sub-tree alignment
                                                               corpus for our research.
3
  An elementary tree is a fragment whose leaf nodes can be
                                                               4
either non-terminal symbols or terminal symbols.                   http://mitlab.hit.edu.cn/ .


                                                             314


References                                                 John Tinsley, Mary Hearne and Andy Way. 2009. Pa-
                                                             rallel treebanks in phrase-based statistical machine
David Burkett and Dan Klein. 2008. Two languages             translation. In Proceedings of CICLING-09.
  are better than one (for syntactic parsing). In Pro-
  ceedings of EMNLP-08. 877-886.                           Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
                                                             2006. A Composite Kernel to Extract Relations be-
Nello Cristianini and John Shawe-Taylor. 2000. An            tween Entities with both Flat and Structured Fea-
  introduction to support vector machines and other          tures. In Proceedings of ACL-COLING-06. 825-
  kernelbased learning methods. Cambridge: Cam-              832.
  bridge University Press.
                                                           Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng
Michael Collins and Nigel Duffy. 2001. Convolution           Li and Chew Lim Tan. 2007. A tree-to-tree align-
  Kernels for Natural Language. In Proceedings of            ment-based model for statistical machine transla-
  NIPS-01.                                                   tion. In Proceedings of MT Summit XI -07. 535-542.
Declan Groves, Mary Hearne and Andy Way. 2004.             Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li,
  Robust sub-sentential alignment of phrase-structure        Chew Lim Tan and Sheng Li. 2008. A tree sequence
  trees. In Proceedings of COLING-04, pages 1072-            alignment-based tree-to-tree translation model. In
  1078.                                                      Proceedings of ACL-08. 559-567.
Kenji Imamura. 2001. Hierarchical Phrase Alignment
  Harmonized with Parsing. In Proceedings of
  NLPRS-01, Tokyo. 377-384.
Thorsten Joachims. 1999. Making large-scale SVM
  learning practical. In B. SchÄolkopf, C. Burges, and
  A. Smola, editors, Advances in Kernel Methods -
  Support Vector Learning, MIT press.
Dan Klein and Christopher D. Manning. 2003. Accu-
  rate Unlexicalized Parsing. In Proceedings of ACL-
  03. 423-430.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
  2003. Statistical phrase-based translation. In Pro-
  ceedings of HLT-NAACL-03. 48-54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
  Callison-Burch, Marcello Federico, Nicola Bertoldi,
  Brooke Cowan, Wade Shen, Christine Moran, Ri-
  chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
  Constantin and Evan Herbst. 2007. Moses: Open
  Source Toolkit for Statistical Machine Translation.
  In Proceedings of ACL-07. 177-180.
Franz Josef Och and Hermann Ney. 2003. A systematic
   comparison of various statistical alignment models.
   Computational Linguistics, 29(1):19-51, March.
Alessandro Moschitti. 2004. A Study on Convolution
  Kernels for Shallow Semantic Parsing. In Proceed-
  ings of ACL-04.
Andreas Stolcke. 2002. SRILM - an extensible lan-
  guage modeling toolkit. In Proceedings of ICSLP-02.
  901-904.
Jun Sun, Min Zhang and Chew Lim Tan. 2009. A non-
  contiguous Tree Sequence Alignment-based Model
  for Statistical Machine Translation. In Proceedings
  of ACL-IJCNLP-09. 914-922.
John Tinsley, Ventsislav Zhechev, Mary Hearne and
  Andy Way. 2007. Robust language pair-independent
  sub-tree alignment. In Proceedings of MT Summit
  XI -07.



                                                         315

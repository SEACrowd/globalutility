                          Faster Parsing by Supertagger Adaptation
    Jonathan K. Kummerfeld a Jessika Roesner b Tim Dawborn a                                         James Haggerty a
                        James R. Curran a∗   Stephen Clark c∗
    School of Information Technologies a Department of Computer Science b Computer Laboratory c
           University of Sydney            University of Texas at Austin  University of Cambridge
           NSW 2006, Australia                  Austin, TX, USA           Cambridge CB3 0FD, UK
                  james@it.usyd.edu.au a∗          stephen.clark@cl.cam.ac.uk c∗


                        Abstract                                 Since the CCG lexical category set used by the su-
                                                                 pertagger is much larger than the Penn Treebank
      We propose a novel self-training method                    POS tag set, the accuracy of supertagging is much
      for a parser which uses a lexicalised gram-                lower than POS tagging; hence the CCG supertag-
      mar and supertagger, focusing on increas-                  ger assigns multiple supertags1 to a word, when
      ing the speed of the parser rather than                    the local context does not provide enough infor-
      its accuracy. The idea is to train the su-                 mation to decide on the correct supertag.
      pertagger on large amounts of parser out-                     The supertagger feeds lexical categories to the
      put, so that the supertagger can learn to                  parser, and the two interact, sometimes using mul-
      supply the supertags that the parser will                  tiple passes over a sentence. If a spanning analy-
      eventually choose as part of the highest-                  sis cannot be found by the parser, the number of
      scoring derivation. Since the supertag-                    lexical categories supplied by the supertagger is
      ger supplies fewer supertags overall, the                  increased. The supertagger-parser interaction in-
      parsing speed is increased. We demon-                      fluences speed in two ways: first, the larger the
      strate the effectiveness of the method us-                 lexical ambiguity, the more derivations the parser
      ing a CCG supertagger and parser, obtain-                  must consider; second, each further pass is as
      ing significant speed increases on newspa-                 costly as parsing a whole extra sentence.
      per text with no loss in accuracy. We also
                                                                    Our goal is to increase parsing speed without
      show that the method can be used to adapt
                                                                 loss of accuracy. The technique we use is a form
      the CCG parser to new domains, obtain-
                                                                 of self-training, in which the output of the parser is
      ing accuracy and speed improvements for
                                                                 used to train the supertagger component. The ex-
      Wikipedia and biomedical text.
                                                                 isting literature on self-training reports mixed re-
1     Introduction                                               sults. Clark et al. (2003) were unable to improve
                                                                 the accuracy of POS tagging using self-training.
In many NLP tasks and applications, e.g. distribu-               In contrast, McClosky et al. (2006a) report im-
tional similarity (Curran, 2004) and question an-                proved accuracy through self-training for a two-
swering (Dumais et al., 2002), large volumes of                  stage parser and re-ranker.
text and detailed syntactic information are both                    Here our goal is not to improve accuracy, only
critical for high performance. To avoid a trade-                 to maintain it, which we achieve through an adap-
off between these two, we need to increase parsing               tive supertagger. The adaptive supertagger pro-
speed, but without losing accuracy.                              duces lexical categories that the parser would have
   Parsing with lexicalised grammar formalisms,                  used in the final derivation when using the base-
such as Lexicalised Tree Adjoining Grammar and                   line model. However, it does so with much lower
Combinatory Categorial Grammar (CCG; Steed-                      ambiguity levels, and potentially during an ear-
man, 2000), can be made more efficient using a                   lier pass, which means sentences are parsed faster.
supertagger. Bangalore and Joshi (1999) call su-                 By increasing the ambiguity level of the adaptive
pertagging almost parsing because of the signifi-                models to match the baseline system, we can also
cant reduction in ambiguity which occurs once the                slightly increase supertagging accuracy, which can
supertags have been assigned.                                    lead to higher parsing accuracy.
   In this paper, we focus on the CCG parser and
                                                                    1
supertagger described in Clark and Curran (2007).                       We use supertag and lexical category interchangeably.


                                                           345
          Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355,
                   Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                                 I        ate         pizza              with          cutlery
   Using the parser to generate training data also              NP (S \NP)/NP NP ((S \NP)\(S \NP))/NP                    NP
has the advantage that it is not a domain specific                         S \NP
                                                                                        >
                                                                                                     (S \NP)\(S \NP)
                                                                                                                             >

                                                                                                                             <
process. Previous work has shown that parsers                                                    S \NP
                                                                                                                             <
typically perform poorly outside of their train-                                                 S

ing domain (Gildea, 2001). Using a newspaper-                         I         ate      pizza           with    anchovies
                                                                     NP (S \NP)/NP NP (NP\NP)/NP                    NP
trained parser, we constructed new training sets for                                                                     >
                                                                                                            NP\NP
Wikipedia and biomedical text. These were used                                                              NP
                                                                                                                         <

                                                                                                                         >
to create new supertagging models adapted to the                                                 S \NP
                                                                                                                         <
different domains.                                                                               S

   The self-training method of adapting the su-
pertagger to suit the parser increased parsing speed          Figure 1: Two CCG derivations with PP ambiguity.
by more than 50% across all three domains, with-
out loss of accuracy. Using an adapted supertagger            can be used to find the most probable supertag se-
with ambiguity levels tuned to match the baseline             quence. Alternatively the Forward-Backward al-
system, we were also able to increase F-score on              gorithm can be used to efficiently sum over all se-
labelled grammatical relations by 0.75%.                      quences, giving a probability distribution over su-
                                                              pertags for each word which is conditional only on
2     Background                                              the input sentence.
Many statistical parsers use two stages: a tag-                  Supertaggers can be made accurate enough for
ging stage that labels each word with its gram-               wide coverage parsing using multi-tagging (Chen
matical role, and a parsing stage that uses the tags          et al., 1999), in which more than one supertag
to form a parse tree. Lexicalised grammars typ-               can be assigned to a word; however, as more su-
ically contain a much smaller set of rules than               pertags are supplied by the supertagger, parsing
phrase-structure grammars, relying on tags (su-               efficiency decreases (Chen et al., 2002), demon-
pertags) that contain a more detailed description             strating the influence of lexical ambiguity on pars-
of each word’s role in the sentence. This leads to            ing complexity (Sarkar et al., 2000).
much larger tag sets, and shifts a large proportion              Clark and Curran (2004) applied supertagging
of the search for an optimal derivation to the tag-           to CCG, using a flexible multi-tagging approach.
ging component of the parser.                                 The supertagger assigns to a word all lexical cate-
   Figure 1 gives two sentences and their CCG                 gories whose probabilities are within some factor,
derivations, showing how some of the syntactic                β, of the most probable category for that word.
ambiguity is transferred to the supertagging com-             When the supertagger is integrated with the C&C
ponent in a lexicalised grammar. Note that the                parser, several progressively lower β values are
lexical category assigned to with is different in             considered. If a sentence is not parsed on one
each case, reflecting the fact that the prepositional         pass then the parser attempts to parse the sentence
phrase attaches differently. Either we need a tag-            again with a lower β value, using a larger set of
ging model that can resolve this ambiguity, or both           categories from the supertagger. Since most sen-
lexical categories must be supplied to the parser             tences are parsed at the first level (in which the av-
which can then attempt to resolve the ambiguity               erage number of supertags assigned to each word
by eventually selecting between them.                         is only slightly greater than one), this provides
                                                              some of the speed benefit of single tagging, but
2.1    Supertagging                                           without loss of coverage (Clark and Curran, 2004).
Supertaggers typically use standard linear-time                  Supertagging has since been effectively applied
tagging algorithms, and only consider words in the            to other formalisms, such as HPSG (Blunsom and
local context when assigning a supertag. The C&C              Baldwin, 2006; Zhang et al., 2009), and as an in-
supertagger is similar to the Ratnaparkhi (1996)              formation source for tasks such as Statistical Ma-
tagger, using features based on words and POS                 chine Translation (Hassan et al., 2007). The use
tags in a five-word window surrounding the target             of parser output for supertagger training has been
word, and defining a local probability distribution           explored for LTAG by Sarkar (2007). However, the
over supertags for each word in the sentence, given           focus of that work was on improving parser and
the previous two supertags. The Viterbi algorithm             supertagger accuracy rather than speed.


                                                        346


Previously , watch imports              were                     denied               such         duty-free treatment
      S/S      , N /N   N    (S [dcl ]\NP )/(S [pss]\NP ) (S [pss]\NP )/NP          NP/NP             N/N           N
      N           N               (S[dcl]\NP)/NP              S [pss]\NP        (N /N )/(N /N )
S [adj ]\NP                  (S [dcl ]\NP )/(S [adj ]\NP ) (S [pss]\NP )/NP           N /N
                                                         (S [pt]\NP )/NP
                                                         (S[dcl]\NP)/NP

Figure 2: An example sentence and the sets of categories assigned by the supertagger. The first category
in each column is correct and the categories used by the parser are marked in bold. The correct category
for watch is included here, for expository purposes, but in fact was not provided by the supertagger.

2.2       Semi-supervised training                           A perfect supertagger would assign the correct cat-
                                                             egory to every word. CCG supertaggers are about
Previous exploration of semi-supervised training
                                                             92% accurate when assigning a single lexical cate-
in NLP has focused on improving accuracy, often
                                                             gory to each word (Clark and Curran, 2004). This
for the case where only small amounts of manually
                                                             is not accurate enough for wide coverage parsing
labelled training data are available. One approach
                                                             and so a multi-tagging approach is used instead.
is co-training, in which two models with indepen-
                                                             In the final derivation, the parser uses one category
dent views of the data iteratively inform each other
                                                             from each set, and it is important to note that hav-
by labelling extra training data. Sarkar (2001) ap-
                                                             ing the correct category in the set does not guaran-
plied co-training to LTAG parsing, in which the su-
                                                             tee that the parser will use it.
pertagger and parser provide the two views. Steed-
                                                                Figure 2 gives an example sentence and the sets
man et al. (2003) extended the method to a variety
                                                             of lexical categories supplied by the supertagger,
of parser pairs.
                                                             for a particular value of β.2 The usual target of
   Another method is to use a re-ranker (Collins
                                                             the supertagging task is to produce the top row of
and Koo, 2002) on the output of a system to gener-
                                                             categories in Figure 2, the correct categories. We
ate new training data. Like co-training, this takes
                                                             propose a new task that instead aims for the cat-
advantage of a different view of the data, but the
                                                             egories the parser will use, which are marked in
two views are not independent as the re-ranker is
                                                             bold for this case. The purpose of this new task is
limited to the set of options produced by the sys-
                                                             to improve speed.
tem. This method has been used effectively to
                                                                The reason speed will be improved is that we
improve parsing performance on newspaper text
                                                             can construct models that will constrain the set of
(McClosky et al., 2006a), as well as adapting a
                                                             possible derivations more than the baseline model.
Penn Treebank parser to a new domain (McClosky
                                                             We can construct these models because we can
et al., 2006b).
                                                             obtain much more of our target output, parser-
   As well as using independent views of data to
                                                             annotated sentences, than we could for the gold-
generate extra training data, multiple views can be
                                                             standard supertagging task.
used to provide constraints at test time. Holling-
                                                                The new target data will contain tagging errors,
shead and Roark (2007) improved the accuracy
                                                             and so supertagging accuracy measured against
of a parsing pipeline by using the output of later
                                                             the correct categories may decrease. If we ob-
stages to constrain earlier stages.
                                                             tained perfect accuracy on our new task then we
   The only work we are aware of that uses self-             would be removing all of the categories not cho-
training to improve the efficiency of parsers is van         sen by the parser. However, parsing accuracy will
Noord (2009), who adopts a similar idea to the               not decrease since the parser will still receive the
one in this paper for improving the efficiency of            categories it would have used, and will therefore
a Dutch parser based on a manually constructed               be able to form the same highest-scoring deriva-
HPSG grammar.
                                                             tion (and hence will choose it).
                                                                To test this idea we parsed millions of sentences
3     Adaptive Supertagging
                                                                2
                                                                  Two of the categories for such have been left out for
The purpose of the supertagger is to cut down the            reasons of space, and the correct category for watch has been
                                                             included for expository reasons. The fact that the supertagger
search space for the parser by reducing the set of           does not supply this category is the reason that the parser does
categories that must be considered for each word.            not analyse the sentence correctly.


                                                       347


in three domains, producing new data annotated                      Source         Sentence Length     Corpus %
                                                                             Range Average Variance
with the categories that the parser used with the                              0-4      3.26      0.64       1.2
baseline model. We constructed new supertagging                               5-20     14.04     17.41     39.2
models that are adapted to suit the parser by train-                News     21-40     28.76     29.27     49.4
                                                                             41-250 49.73        86.73     10.2
ing on the combination of these sets and the stan-                             All     24.83    152.15    100.0
dard training corpora. We applied standard evalu-                              0-4      2.81      0.60     22.4
ation metrics for speed and accuracy, and explored                            5-20     11.64     21.56     48.9
                                                                     Wiki    21-40     28.02     28.48     24.3
the source of the changes in parsing performance.                            41-250 49.69        77.70       4.5
                                                                               All     15.33    154.57    100.0
4     Data                                                                     0-4      2.98      0.75       0.9
                                                                              5-20     14.54     15.14     41.3
In this work, we consider three domains: news-                       Bio      21-40    28.49     29.34     48.0
                                                                             41-250 49.17        68.34       9.8
wire, Wikipedia text and biomedical text.                                      All     24.53    139.35    100.0

4.1    Training and accuracy evaluation                       Table 1: Statistics for sentences in the supertagger
                                                              training data. Sentences containing more than 250
We have used Sections 02-21 of CCGbank (Hock-
                                                              tokens were not included in our data sets.
enmaier and Steedman, 2007), the CCG version of
the Penn Treebank (Marcus et al., 1993), as train-
ing data for the newspaper domain. Sections 00                same post-processing process as Rimell and Clark
and 23 were used for development and test eval-               (2009) to convert the C&C parser output to Stan-
uation. A further 113,346,430 tokens (4,566,241               ford format grammatical relations (de Marneffe
sentences) of raw data from the Wall Street Jour-             et al., 2006). For adaptive training we have
nal section of the North American News Corpus                 used 1,900,618,859 tokens (76,739,723 sentences)
(Graff, 1995) were parsed to produce the training             from the MEDLINE abstracts tokenised by McIn-
data for adaptation. This text was tokenised us-              tosh and Curran (2008). These sentences were
ing the C&C tools tokeniser and parsed using our              POS -tagged and parsed twice, once as for the
baseline models. For the smaller training sets, sen-          newswire and Wikipedia data, and then again, us-
tences from 1988 were used as they would be most              ing the bio-specific models developed by Rimell
similar in style to the evaluation corpus. In all ex-         and Clark (2009). Statistics for the sentences in
periments the sentences from 1989 were excluded               the training sets are given in Table 1.
to ensure no overlap occurred with CCGbank.
                                                              4.2    Speed evaluation data
   As Wikipedia text we have used 794,024,397
tokens (51,673,069 sentences) from Wikipedia ar-              For speed evaluation we held out three sets of sen-
ticles. This text was processed in the same way as            tences from each domain-specific corpus. Specif-
the NANC data to produce parser-annotated train-              ically, we used 30,000, 4,000 and 2,000 unique
ing data. For supertagger evaluation, one thousand            sentences of length 5-20, 21-40 and 41-250 tokens
sentences were manually annotated with CCG lex-               respectively. Speeds on these length controlled
ical categories and POS tags. For parser evalua-              sets were combined to calculate an overall pars-
tion, three hundred of these sentences were man-              ing speed for the text in each domain. Note that
ually annotated with DepBank grammatical rela-                more than 20% of the Wikipedia sentences were
tions (King et al., 2003) in the style of Briscoe             less than five words in length and the overall dis-
and Carroll (2006). Both sets of annotations were             tribution is skewed towards shorter sentences com-
produced by manually correcting the output of the             pared to the other corpora.
baseline system. The annotation was performed
                                                              5     Evaluation
by Stephen Clark and Laura Rimell.
   For the biomedical domain we have used sev-                We used the hybrid parsing model described in
eral different resources. As gold standard data for           Clark and Curran (2007), and the Viterbi decoder
supertagger evaluation we have used supertagged               to find the highest-scoring derivation. The multi-
GENIA data (Kim et al., 2003), annotated by                   pass supertagger-parser interaction was also used.
Rimell and Clark (2008). For parsing evalua-                     The test data was excluded from training data
tion, grammatical relations from the BioInfer cor-            for the supertagger for all of the newswire and
pus were used (Pyysalo et al., 2007), with the                Wikipedia models. For the biomedical models ten-


                                                        348


fold cross validation was used. The accuracy of               that the model is more confident. It should pro-
supertagging is measured by multi-tagging at the              duce similar accuracy results, but with lower am-
first β level and considering a word correct if the           biguity, which will lead to higher speed.
correct tag is amongst any of the assigned tags.                 For accuracy optimisation experiments we tune
   For the biomedical parser evaluation we have               the β levels to produce the same average tagging
used the parsing model and grammatical relation               ambiguity as the baseline model on Section 00 of
conversion script from Rimell and Clark (2009).               CCGbank. Accuracy depends heavily on the num-
   Our timing measurements are calculated in two              ber of categories supplied, so the new models are
ways. Overall times were measured using the C&C               at an accuracy disadvantage if they propose fewer
parser’s timers. Individual sentence measurements             categories. By matching the ambiguity of the de-
were made using the Intel timing registers, since             fault model, we can increase accuracy at the cost
standard methods are not accurate enough for the              of some of the speed improvements the new mod-
short time it takes to parse a single sentence.               els obtain.
   To check whether changes were statistically sig-
nificant we applied the test described by Chinchor            6     Results
(1995). This measures the probability that two sets           We have performed four primary sets of exper-
of responses are drawn from the same distribution,            iments to explore the ability of an adaptive su-
where a score below 0.05 is considered significant.           pertagger to improve parsing speed or accuracy. In
   Models were trained on an Intel Core2Duo                   the first two experiments, we explore performance
3GHz with 4GB of RAM. The evaluation was per-                 on the newswire domain, which is the source of
formed on a dual quad-core Intel Xeon 2.27GHz                 training data for the parsing model and the base-
with 16GB of RAM.                                             line supertagging model. In the second set of ex-
                                                              periments, we train on a mixture of gold standard
5.1   Tagging ambiguity optimisation                          newswire data and parser-annotated data from the
The number of lexical categories assigned to a                target domain.
word by the CCG supertagger depends on the prob-                 In both cases we perform two experiments. The
abilities calculated for each category and the β              first aimed to improve speed, keeping the β levels
level being used. Each lexical category with a                the same. This should lead to an increase in speed
probability within a factor of β of the most prob-            as the extra training data means the models are
able category is included. This means that the                more confident and so have lower ambiguity than
choice of β level determines the tagging ambigu-              the baseline model for a given β value. The second
ity, and so has great influence on parsing speed, ac-         experiment aimed to improve accuracy, tuning the
curacy and coverage. Also, the tagging ambiguity              β levels as described in the previous section.
produced by a β level will vary between models.
A more confident model will have a more peaked                6.1    Newswire speed improvement
distribution of category probabilities for a word,            In our first experiment, we trained supertagger
and therefore need a smaller β value to assign the            models using Generalised Iterative Scaling (GIS)
same number of categories.                                    (Darroch and Ratcliff, 1972), the limited mem-
   Additionally, the C&C parser uses multiple β               ory BFGS method (BFGS) (Nocedal and Wright,
levels. The first pass over a sentence is at a high β         1999), the averaged perceptron (Collins, 2002),
level, resulting in a low tagging ambiguity. If the           and the margin infused relaxed algorithm (MIRA)
categories assigned are too restrictive to enable a           (Crammer and Singer, 2003). Note that these
spanning analysis, the system makes another pass              are all alternative methods for estimating the lo-
with a lower β level, resulting in a higher tagging           cal log-linear probability distributions used by the
ambiguity. A maximum of five passes are made,                 Ratnaparkhi-style tagger. We do not use global
with the β levels varying from 0.075 to 0.001.                tagging models as in Lafferty et al. (2001) or
   We have taken two approaches to choosing β                 Collins (2002). The training data consisted of Sec-
levels. When the aim of an experiment is to im-               tions 02–21 of CCGbank and progressively larger
prove speed, we use the system’s default β levels.            quantities of parser-annotated NANC data – from
While this choice means a more confident model                zero to four million extra sentences. The results of
will assign fewer tags, this simply reflects the fact         these tests are presented in Table 2.


                                                        349


                 Ambiguity (%)      Tagging Accuracy (%)                    F-score             Speed (sents / sec)
    Data 0k       40k 400k 4m     0k     40k 400k 4m                0k    40k 400k 4m          0k 40k 400k 4m
 Baseline 1.27                   96.34                             85.46                     39.6
   BFGS 1.27      1.23 1.19 1.18 96.33 96.18 95.95 95.93           85.45 85.51 85.57 85.68   39.8 49.6 71.8 60.0
     GIS 1.28     1.24 1.21 1.20 96.44 96.27 96.09 96.11           85.44 85.46 85.58 85.62   37.4 44.1 51.3 54.1
   MIRA 1.30      1.24 1.17 1.13 96.44 96.14 95.56 95.18           85.44 85.40 85.38 85.42   34.1 44.8 60.2 73.3

  Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.
                                       Sentences              Av. Time Change (ms)   Total Time Change (s)
                  Sentence length 5-20 21-40 41-250           5-20 21-40 41-250        5-20 21-40 41-250
                  Lower tag amb.  1166    333     281        -7.54 -71.42 -183.23      -1.1      -29     -26
     Earlier pass Same tag amb.    248     38       8        -2.94 -27.08 -108.28   -0.095      -1.3   -0.44
                  Higher tag amb.  530     33      14        -5.84 -32.25 -44.10      -0.40     -1.3   -0.31
                  Lower tag amb. 19288 3120      1533        -1.13 -5.18 -38.05        -2.8      -20     -30
     Same pass Same tag amb.      7285    259      35        -0.29    0.94   24.57    -0.28    0.30     0.44
                  Higher tag amb. 1133    101      24        -0.25    2.70     8.09 -0.037     0.34   0.099
                  Lower tag amb.   334    114     104         0.90    7.60 -46.34    0.039       1.1    -2.5
     Later pass   Same tag amb.     14       1      0         1.06    4.26      n/a 0.0019 0.0053        0.0
                  Higher tag amb.    2       1      1        -0.13 26.43 308.03 -3.4e-05 0.033          0.16

Table 3: Breakdown of the source of changes in speed. The test sentences are divided into nine sets
based on the change in parsing behaviour between the baseline model and a model trained using MIRA,
Sections 02-21 of CCGbank and 4,000,000 NANC sentences.

   Using the default β levels we found that the              produce a particularly large improvement for the
perceptron-trained models lost accuracy, disqual-            sentences parsed at an earlier pass. In fact, despite
ifying them from this test. The BFGS, GIS and                making up only 7% of sentences in the set, those
MIRA models produced mixed results, but no                   parsed earlier with lower ambiguity provide 50%
statistically significant decrease in accuracy, and          of the speed improvement.
as the amount of parser-annotated data was in-                  It is also interesting to note the changes for sen-
creased, parsing speed increased by up to 85%.               tences parsed on the same pass, with the same
   To determine the source of the speed improve-             ambiguity. We may expect these sentences to be
ment we considered the times recorded by the tim-            parsed in approximately the same amount of time,
ing registers. In Table 3, we have aggregated these          and this is the case for the short set, but not for the
measurements based on the change in the pass at              two larger sets, where we see an increase in pars-
which the sentence is parsed, and how the tag-               ing time. This suggests that the categories being
ging ambiguity changes on that pass. For sen-                supplied are more productive, leading to a larger
tences parsed on two different passes the ambigu-            set of possible derivations.
ity comparison is at the earlier pass. The “Total
Time Change” section of the table is the change in           6.2     Newswire accuracy optimised
parsing time for sentences of that type when pars-           Any decrease in tagging ambiguity will generally
ing ten thousand sentences from the corpus. This             lead to a decrease in accuracy. The parser uses a
takes into consideration the actual distribution of          more sophisticated algorithm with global knowl-
sentence lengths in the corpus.                              edge of the sentence and so we would expect it
   Several effects can be observed in these re-              to be better at choosing categories than the su-
sults. 72% of sentences are parsed on the same               pertagger. Unlike the supertagger it will exclude
pass, but with lower tag ambiguity (5th row in Ta-           categories that cannot be used in a derivation. In
ble 3). This provides 44% of the speed improve-              the previous section, we saw that training the su-
ment. Three to six times as many sentences are               pertagger on parser output allowed us to develop
parsed on an earlier pass than are parsed on a later         models that produced the same categories, despite
pass. This means the sentences parsed later have             lower tagging ambiguity. Since they were trained
very little effect on the overall speed. At the same         on the categories the parser was able to use in
time, the average gain for sentences parsed earlier          derivations, these models should also now be pro-
is almost always larger than the average cost for            viding categories that are more likely to be useful.
sentences parsed later. These effects combine to                This leads us to our second experiment, opti-


                                                       350


                          Tagging Accuracy (%)                  F-score            Speed (sents / sec)
           NANC  sents 0k      40k 400k 4m         0k         40k 400k         4m 0k 40k 400k 4m
              Baseline 96.34                      85.46                         39.6
                 BFGS 96.33 96.42 96.42 96.66     85.45       85.55 85.64 85.98 39.5 43.7 43.9 42.7
                   GIS 96.34 96.43 96.53 96.62    85.36       85.47 85.84 85.87 39.1 41.4 41.7 42.6
            Perceptron 95.82 95.99 96.30      -   85.28       85.39 85.64   -   45.9 48.0 45.2         -
                 MIRA 96.23 96.29 96.46 96.63     85.47       85.45 85.55 85.84 37.7 41.4 41.4 42.9

 Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
          Train Corpus        Ambiguity           Tag. Acc.                  F-score        Speed (sents / sec)
                         News Wiki Bio       News Wiki Bio           News      Wiki    Bio News Wiki Bio
              Baseline   1.267 1.317 1.281   96.34 94.52 90.70       85.46     80.8    75.0 39.6 50.9 35.1
                News     1.126 1.151 1.130   95.18 93.56 90.07       85.42     81.2    75.2 73.3 83.9 60.3
                 Wiki    1.147 1.154 1.129   95.06 93.52 90.03       84.70     81.4    75.5 62.4 73.9 58.7
                  Bio    1.134 1.146 1.114   94.66 93.15 89.88       84.23     80.7    75.9 66.2 90.4 59.3

Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The
highlighted values are the top speed for each evaluation set and results that are statistically indistinguish-
able from it.

mising accuracy on newswire. We used the same                                Model Tag. Acc. F-score   Speed
                                                                                      (%)      (%)   (sents/sec)
models as in the previous experiment, but tuned                            Baseline  96.51    85.20     39.6
the β levels as described in Section 5.1.                        GIS , 4,000k NANC   96.83    85.95     42.6
                                                                BFGS , 4,000k NANC   96.91    85.90     42.7
   Comparing Tables 2 and 4 we can see the in-
                                                                MIRA , 4,000k NANC   96.84    85.79     42.9
fluence of β level choice, and therefore tagging
ambiguity. When the default β values were used                Table 6: Evaluation of top models on Section 23 of
ambiguity dropped consistently as more parser-                CCGbank. All changes in F-score are statistically
annotated data was used, and category accuracy                significant.
dropped in the same way. Tuning the β levels to
match ambiguity produces the opposite trend.
                                                              23 of CCGbank. All of the new models in the table
   Interestingly, while the decrease in supertag ac-          make a statistically significant improvement over
curacy in the previous experiment did not translate           the baseline.
into a decrease in F-score, the increase in tag accu-            It is also interesting to note that the results in
racy here does translate into an increase in F-score.         Tables 2, 4 and 6, are similar for all of the train-
This indicates that the supertagger is adapting to            ing algorithms. However, the training times differ
suit the parser. In the previous experiment, the              considerably. For all four algorithms the training
supertagger was still providing the categories the            time is proportional to the amount of data, but the
parser would have used with the baseline supertag-            GIS and BFGS models trained on only CCGbank
ging model, but it provided fewer other categories.           took 4,500 and 4,200 seconds to train, while the
Since the parser is not a perfect supertagger these           equivalent perceptron and MIRA models took 90
other categories may in fact have been incorrect,             and 95 seconds to train.
and so supertagger accuracy goes down, without
changing parsing results. Here we have allowed                6.3   Annotation method comparison
the supertagger to assign extra categories, which             To determine whether these improvements were
will only increase its accuracy.                              dependent on the annotations being produced
   The increase in F-score has two sources. First,            by the parser we performed a set of tests with
our supertagger is more accurate, and so the parser           supertagger, rather than parser, annotated data.
is more likely to receive category sets that can be           Three extra training sets were created by annotat-
combined into the correct derivation. Also, the su-           ing newswire sentences with supertags using the
pertagger has been trained on categories that the             baseline supertagging model. One set used the
parser is able to use in derivations, which means             one-best tagger, and two were produced using the
they are more productive.                                     most probable tag for each word out of the set sup-
   As Table 6 shows, this change translates into an           plied by the multi-tagger, with variations in the β
improvement of up to 0.75% in F-score on Section              value and dictionary cutoff for the two sets.


                                                        351


              Train Corpus    Ambiguity         Tag. Acc.                 F-score        Speed (sents / sec)
                             Wiki Bio      News Wiki Bio          News      Wiki    Bio News Wiki Bio
                  Baseline   1.317 1.281   96.34 94.52 90.70      85.46     80.8    75.0 39.6 50.9 35.1
                    News     1.331 1.322   96.53 94.86 91.32      85.84     80.1    75.2 41.8 32.6 31.4
                     Wiki    1.293 1.251   96.28 94.79 91.08      85.02     81.7    75.8 40.4 37.2 37.2
                      Bio    1.287 1.195   96.15 94.28 91.03      84.95     80.6    76.1 39.2 52.9 26.2

      Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.

         Annotation method Tag. Acc. F-score                annotated sentences from the target domain.
                   Baseline  96.34    85.46
                     Parser  96.46    85.55
                                                               As Table 5 shows, this training method pro-
            One-best super   95.94    85.24                 duces models adapted to the new domain. In par-
            Multi-tagger a   95.91    84.98                 ticular, note that models trained on Wikipedia or
             Multi-tagger b  96.00    84.99
                                                            the biomedical data produce lower F-scores3 than
Table 8: Comparison of annotation methods for               the baseline on newswire. Meanwhile, on the
extra data. The multi-taggers used β values 0.075           target domain they are adapted to, these models
and 0.001, and dictionary cutoffs 20 and 150, for           achieve a higher F-score and parse sentences at
taggers a and b respectively.                               least 45% faster than the baseline.
                                                               The changes in tagging ambiguity and accuracy
               Corpus Speed (sents / sec)                   also show that adaptation has occurred. In all
           Sent length 5-20 21-40 41-250
                                                            cases, the new models have lower tagging ambi-
                 News 242 44.8        8.24
                  Wiki 224 42.0       6.10                  guity, and lower supertag accuracy. However, on
                   Bio 268 41.5       6.48                  the corpus of the extra data, the performance of
                                                            the adapted models is comparable to the baseline
Table 9: Cross-corpus speed for the baseline
                                                            model, which means the parser is probably still be
model on data sets balanced on sentence length.
                                                            receiving the same categories that it used from the
                                                            sets provided by the baseline system.
   As Table 8 shows, in all cases the use of
supertagger-annotated data led to poorer perfor-            6.5     Cross-domain accuracy optimised
mance than the baseline system, while the use of            The ambiguity tuning method used to improve ac-
parser-annotated data led to an improvement in F-           curacy on the newspaper domain can also be ap-
score. The parser has access to a range of infor-           plied to the models trained on other domains. In
mation that the supertagger does not, producing a           Table 7, we have tested models trained using GIS
different view of the data that the supertagger can         and 400,000 sentences of parsed target-domain
productively learn from.                                    text, with β levels tuned to match ambiguity with
                                                            the baseline.
6.4   Cross-domain speed improvement
                                                               As for the newspaper domain, we observe in-
When applying parsers out of domain they are typ-           creased supertag accuracy and F-score. Also, in
ically slower and less accurate (Gildea, 2001). In          almost every case the new models perform worse
this experiment, we attempt to increase speed on            than the baseline on domains other than the one
out-of-domain data. Note that for some of the re-           they were trained on.
sults presented here it may appear that the C&C                In some cases the models in Table 7 are less ac-
parser does not lose speed when out of domain,              curate than those in Table 5. This is because as
since the Wikipedia and biomedical corpora con-             well as optimising the β levels we have changed
tain shorter sentences on average than the news             training methods. All of the training methods were
corpus. However, by testing on balanced sets it             tried, but only the method with the best results in
is clear that speed does decrease, particularly for         newswire is included here, which for F-score when
longer sentences, as shown in Table 9.                      trained on 400,000 sentences was GIS.
   For our domain adaptation development ex-                   The accuracy presented so far for the biomedi-
periments, we considered a collection of differ-
                                                                3
ent models; here we only present results for the                  Note that the F-scores for Wikipedia and biomedical text
                                                            are reported to only three significant figures as only 300 and
best set of models. For speed improvement these             500 sentences respectively were available for parser evalua-
were MIRA models trained on 4,000,000 parser-               tion.


                                                      352


           Train Corpus            F-score                  which controls the lexical category ambiguity at
           Rimell and Clark (2009) 81.5
           Baseline                 80.7
                                                            each level used by the parser.
           CCGbank + Genia          81.5                       The result is an accurate and efficient wide-
           + Newswire               81.9                    coverage CCG parser that can be easily adapted for
           + Wikipedia              82.2
                                                            NLP applications in new domains without manu-
           + Biomedical             81.7
           + R&C annotated Bio      82.3                    ally annotating data.

Table 10: Performance comparison for models us-             Acknowledgements
ing extra gold standard biomedical data. Models
were trained with GIS and 4,000,000 extra sen-              We thank the reviewers for their helpful feed-
tences, and are tested using a POS-tagger trained           back. This work was supported by Australian Re-
on biomedical data.                                         search Council Discovery grants DP0665973 and
                                                            DP1097291, the Capital Markets Cooperative Re-
                                                            search Centre, and a University of Sydney Merit
cal model is considerably lower than that reported          Scholarship. Part of the work was completed at the
by Rimell and Clark (2009). This is because no              Johns Hopkins University Summer Workshop and
gold standard biomedical training data was used             (partially) supported by National Science Founda-
in our experiments. Table 10 shows the results of           tion Grant Number IIS-0833652.
adding Rimell and Clark’s gold standard biomedi-
cal supertag data and using their biomedical POS-
tagger. The table also shows how accuracy can be            References
further improved by adding our parser-annotated
                                                            Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
data from the biomedical domain as well as the                 pertagging: An approach to almost parsing. Com-
additional gold standard data.                                 putational Linguistics, 25(2):237–265.

7   Conclusion                                              Phil Blunsom and Timothy Baldwin. 2006. Multi-
                                                              lingual deep lexical acquisition for HPSGs via su-
                                                              pertagging. In Proceedings of the 2006 Conference
This work has demonstrated that an adapted su-                on Empirical Methods in Natural Language Pro-
pertagger can improve parsing speed and accu-                 cessing, pages 164–171, Sydney, Australia.
racy. The purpose of the supertagger is to re-
duce the search space for the parser. By train-             Ted Briscoe and John Carroll. 2006. Evaluating the
                                                              accuracy of an unlexicalized statistical parser on the
ing the supertagger on parser output, we allow the            PARC DepBank. In Proceedings of the Poster Ses-
parser to reach the derivation it would have found,           sion of the 21st International Conference on Com-
sooner. This approach also enables domain adap-               putational Linguistics, Sydney, Australia.
tation, improving speed and accuracy outside the
                                                            John Chen, Srinivas Bangalore, and Vijay K. Shanker.
original domain of the parser.                                1999. New models for improving supertag disam-
   The perceptron-based algorithms used in this               biguation. In Proceedings of the Ninth Conference
work are also able to function online, modifying              of the European Chapter of the Association for Com-
the model weights after each sentence is parsed.              putational Linguistics, pages 188–195, Bergen, Nor-
                                                              way.
This could be used to construct a system that con-
tinuously adapts to the domain it is parsing.               John Chen, Srinivas Bangalore, Michael Collins, and
   By training on parser-annotated NANC data                  Owen Rambow. 2002. Reranking an n-gram su-
                                                              pertagger. In Proceedings of the 6th International
we constructed models that were adapted to the
                                                              Workshop on Tree Adjoining Grammars and Related
newspaper-trained parser. The fastest model                   Frameworks, pages 259–268, Venice, Italy.
parsed sentences 1.85 times as fast and was as
accurate as the baseline system. Adaptive train-            Nancy Chinchor.   1995.   Statistical significance
                                                              of MUC-6 results. In Proceedings of the Sixth
ing is also an effective method of improving per-
                                                              Message Understanding Conference, pages 39–43,
formance on other domains. Models trained on                  Columbia, MD, USA.
parser-annotated Wikipedia text and MEDLINE
text had improved performance on these target do-           Stephen Clark and James R. Curran. 2004. The impor-
                                                               tance of supertagging for wide-coverage CCG pars-
mains, in terms of both speed and accuracy. Op-                ing. In Proceedings of the 20th International Con-
timising for speed or accuracy can be achieved by              ference on Computational Linguistics, pages 282–
modifying the β levels used by the supertagger,                288, Geneva, Switzerland.


                                                      353


Stephen Clark and James R. Curran. 2007. Wide-                   Kristy Hollingshead and Brian Roark. 2007. Pipeline
   coverage efficient statistical parsing with CCG                 iteration. In Proceedings of the 45th Meeting of the
   and log-linear models. Computational Linguistics,               Association for Computational Linguistics, pages
   33(4):493–552.                                                  952–959, Prague, Czech Republic.

Stephen Clark, James R. Curran, and Miles Osborne.               Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
   2003. Bootstrapping POS-taggers using unlabelled                 Jun’ichi Tsujii. 2003. GENIA corpus - a seman-
   data. In Proceedings of the seventh Conference on                tically annotated corpus for bio-textmining. Bioin-
   Natural Language Learning, pages 49–55, Edmon-                   formatics, 19(1):180–182.
   ton, Canada.
                                                                 Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Michael Collins and Terry Koo. 2002. Discriminative                Dalrymple, and Ronald M. Kaplan. 2003. The
  reranking for natural language parsing. Computa-                 PARC 700 Dependency Bank. In Proceedings of
  tional Linguistics, 31(1):25–69.                                 the 4th International Workshop on Linguistically In-
                                                                   terpreted Corpora, Budapest, Hungary.
Michael Collins. 2002. Discriminative training meth-
  ods for Hidden Markov Models: Theory and experi-               John D. Lafferty, Andrew McCallum, and Fernando
  ments with perceptron algorithms. In Proceedings                 C. N. Pereira. 2001. Conditional random fields:
  of the 2002 Conference on Empirical Methods in                   Probabilistic models for segmenting and labeling se-
  Natural Language Processing, pages 1–8, Philadel-                quence data. In Proceedings of the Eighteenth In-
  phia, PA, USA.                                                   ternational Conference on Machine Learning, pages
                                                                   282–289, San Francisco, CA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
  servative online algorithms for multiclass problems.           Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
  Journal of Machine Learning Research, 3:951–991.                 Marcinkiewicz. 1993. Building a large annotated
                                                                   corpus of English: The Penn Treebank. Computa-
James R. Curran. 2004. From Distributional to Seman-               tional Linguistics, 19(2):313–330.
  tic Similarity. Ph.D. thesis, University of Edinburgh.
                                                                 David McClosky, Eugene Charniak, and Mark John-
John N. Darroch and David Ratcliff. 1972. General-                 son. 2006a. Effective self-training for parsing. In
  ized iterative scaling for log-linear models. The An-            Proceedings of the Human Language Technology
  nals of Mathematical Statistics, 43(5):1470–1480.                Conference of the North American Chapter of the
                                                                   Association for Computational Linguistics, Brook-
Marie-Catherine de Marneffe, Bill MacCartney, and                  lyn, NY, USA.
 Christopher D. Manning. 2006. Generating typed
 dependency parses from phrase structure parses. In              David McClosky, Eugene Charniak, and Mark John-
 Proceedings of the 5th International Conference on                son. 2006b. Reranking and self-training for parser
 Language Resources and Evaluation, pages 449–54,                  adaptation. In Proceedings of the 21st International
 Genoa, Italy.                                                     Conference on Computational Linguistics and the
                                                                   44th annual meeting of the Association for Compu-
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,                tational Linguistics, pages 337–344, Sydney, Aus-
  and Andrew Ng. 2002. Web question answering: Is                  tralia.
  more always better? In Proceedings of the 25th In-
  ternational ACMSIGIR Conference on Research and                Tara McIntosh and James R. Curran. 2008. Weighted
  Development, Tampere, Finland.                                   mutual exclusion bootstrapping for domain inde-
                                                                   pendent lexicon and template acquisition. In Pro-
Daniel Gildea. 2001. Corpus variation and parser per-              ceedings of the Australasian Language Technology
  formance. In Proceedings of the 2001 Conference                  Workshop, Hobart, Australia.
  on Empirical Methods in Natural Language Pro-
  cessing, Pittsburgh, PA, USA.                                  Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
                                                                    cal Optimization. Springer.
David Graff. 1995. North American News Text Cor-
  pus. LDC95T21. Linguistic Data Consortium.                     Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
  Philadelphia, PA, USA.                                           tri Haverinen, Juho Heimonen, and Tapio Salakoski.
                                                                   2007. On the unification of syntactic annotations
Hany Hassan, Khalil Sima’an, and Andy Way. 2007.                   under the Stanford dependency scheme: a case study
  Supertagged phrase-based statistical machine trans-              on bioinfer and GENIA. In Proceedings of the ACL
  lation. In Proceedings of the 45th Annual Meeting of             workshop on biological, translational, and clinical
  the Association of Computational Linguistics, pages              language processing, pages 25–32, Prague, Czech
  288–295, Prague, Czech Republic.                                 Republic.

Julia Hockenmaier and Mark Steedman. 2007. CCG-                  Adwait Ratnaparkhi. 1996. A maximum entropy part-
   bank: A corpus of CCG derivations and dependency                of-speech tagger. In Proceedings of the 1996 Con-
   structures extracted from the Penn Treebank. Com-               ference on Empirical Methods in Natural Language
   putational Linguistics, 33(3):355–396.                          Processing, pages 133–142, Philadelphia, PA, USA.


                                                           354


Laura Rimell and Stephen Clark. 2008. Adapting a
  lexicalized-grammar parser to contrasting domains.
  In Proceedings of the 2008 Conference on Empiri-
  cal Methods in Natural Language Processing, pages
  475–484, Honolulu, HI, USA.
Laura Rimell and Stephen Clark.      2009.   Port-
  ing a lexicalized-grammar parser to the biomedi-
  cal domain. Journal of Biomedical Informatics,
  42(5):852–865.
Anoop Sarkar, Fel Xia, and Aravind K. Joshi. 2000.
  Some experiments on indicators of parsing com-
  plexity for lexicalized grammars. In Proceedings of
  the COLING Workshop on Efficiency in Large-scale
  Parsing Systems, pages 37–42, Luxembourg.
Anoop Sarkar. 2001. Applying co-training methods
  to statistical parsing. In Proceedings of the Second
  Meeting of the North American Chapter of the As-
  sociation for Computational Linguistics, pages 1–8,
  Pittsburgh, PA, USA.
Anoop Sarkar. 2007. Combining supertagging and
  lexicalized tree-adjoining grammar parsing. In
  Srinivas Bangalore and Aravind Joshi, editors, Com-
  plexity of Lexical Descriptions and its Relevance to
  Natural Language Processing: A Supertagging Ap-
  proach. MIT Press, Boston, MA, USA.
Mark Steedman, Miles Osborne, Anoop Sarkar,
 Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
 Paul Ruhlen, Stephen Baker, and Jeremiah Crim.
 2003. Bootstrapping statistical parsers from small
 datasets. In Proceedings of the 10th Conference of
 the European Chapter of the Association for Compu-
 tational Linguistics, pages 331–338, Budapest, Hun-
 gary.
Geertjan van Noord. 2009. Learning efficient parsing.
  In Proceedings of the 12th Conference of the Euro-
  pean Chapter of the Association for Computational
  Linguistics, pages 817–825. Association for Com-
  putational Linguistics.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi
  Tsujii. 2009. HPSG supertagging: A sequence la-
  beling view. In Proceedings of the 11th Interna-
  tional Conference on Parsing Technologies, pages
  210–213, Paris, France.




                                                         355

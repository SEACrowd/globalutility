    Enhanced word decomposition by calibrating the decision threshold of
             probabilistic models and using a model ensemble

                   Sebastian Spiegler                                     Peter A. Flach
            Intelligent Systems Laboratory,                      Intelligent Systems Laboratory,
              University of Bristol, U.K.                          University of Bristol, U.K.
           spiegler@cs.bris.ac.uk                              peter.flach@bristol.ac.uk


                      Abstract                                 four tasks are assigned to morphological analy-
                                                               sis: word decomposition into morphemes, build-
    This paper demonstrates that the use of                    ing morpheme dictionaries, defining morphosyn-
    ensemble methods and carefully calibrat-                   tactical rules which state how morphemes can
    ing the decision threshold can signifi-                    be combined to valid words and defining mor-
    cantly improve the performance of ma-                      phophonological rules that specify phonological
    chine learning methods for morphologi-                     changes morphemes undergo when they are com-
    cal word decomposition. We employ two                      bined to words. Results of morphological analy-
    algorithms which come from a family of                     sis are applied in speech synthesis (Sproat, 1996)
    generative probabilistic models. The mod-                  and recognition (Hirsimaki et al., 2006), machine
    els consider segment boundaries as hidden                  translation (Amtrup, 2003) and information re-
    variables and include probabilities for let-               trieval (Kettunen, 2009).
    ter transitions within segments. The ad-
    vantage of this model family is that it can                1.1   Background
    learn from small datasets and easily gen-
                                                               In the past years, there has been a lot of inter-
    eralises to larger datasets. The first algo-
                                                               est and activity in the development of algorithms
    rithm P ROMODES, which participated in
                                                               for morphological analysis. All these approaches
    the Morpho Challenge 2009 (an interna-
                                                               have in common that they build a morphologi-
    tional competition for unsupervised mor-
                                                               cal model which is then applied to analyse words.
    phological analysis) employs a lower or-
                                                               Models are constructed using rule-based meth-
    der model whereas the second algorithm
                                                               ods (Mooney and Califf, 1996; Muggleton and
    P ROMODES -H is a novel development of
                                                               Bain, 1999), connectionist methods (Rumelhart
    the first using a higher order model. We
                                                               and McClelland, 1986; Gasser, 1994) or statisti-
    present the mathematical description for
                                                               cal or probabilistic methods (Harris, 1955; Hafer
    both algorithms, conduct experiments on
                                                               and Weiss, 1974). Another way of classifying ap-
    the morphologically rich language Zulu
                                                               proaches is based on the learning aspect during
    and compare characteristics of both algo-
                                                               the construction of the morphological model. If
    rithms based on the experimental results.
                                                               the data for training the model has the same struc-
                                                               ture as the desired output of the morphological
1    Introduction
                                                               analysis, in other words, if a morphological model
Words are often considered as the smallest unit                is learnt from labelled data, the algorithm is clas-
of a language when examining the grammatical                   sified under supervised learning. An example for
structure or the meaning of sentences, referred to             a supervised algorithm is given by Oflazer et al.
as syntax and semantics, however, words them-                  (2001). If the input data has no information to-
selves possess an internal structure denominated               wards the desired output of the analysis, the algo-
by the term word morphology. It is worthwhile                  rithm uses unsupervised learning. Unsupervised
studying this internal structure since a language              algorithms for morphological analysis are Lin-
description using its morphological formation is               guistica (Goldsmith, 2001), Morfessor (Creutz,
more compact and complete than listing all pos-                2006) and Paramor (Monson, 2008). Minimally or
sible words. This study is called morpholog-                   semi-supervised algorithms are provided with par-
ical analysis. According to Goldsmith (2009)                   tial information during the learning process. This


                                                         375
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 375–383,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


has been done, for instance, by Shalonova et al.                         abilistic generative process consisting of words as
(2009) who provided stems in addition to a word                          observed variables X and their hidden segmenta-
list in order to find multiple pre- and suffixes. A                      tion as latent variables Y . If a generative model is
comparison of different levels of supervision for                        fully parameterised it can be reversed to find the
morphology learning on Zulu has been carried out                         underlying word decomposition by forming the
by Spiegler et al. (2008).                                               conditional probability distribution Pr(Y |X).
   Our two algorithms, P ROMODES and                                        Let us first define the model-independent com-
P ROMODES -H, perform word decomposi-                                    ponents. A given word w j ∈ W with 1 ≤ j ≤ |W |
tion and are based on probabilistic methods                              consists of n letters and has m = n − 1 positions
by incorporating a probabilistic generative                              for inserting boundaries. A word’s segmentation is
model.1      Their parameters can be estimated                           depicted as a boundary vector b j = (b j1 , . . . , b jm )
from either labelled data, using maximum like-                           consisting of boundary values b ji ∈ {0, 1} with
lihood estimates, or from unlabelled data by                             1 ≤ i ≤ m which disclose whether or not a bound-
expectation maximization2 which makes them                               ary is placed in position i. A letter l j,i-1 precedes
either supervised or unsupervised algorithms.                            the position i in w j and a letter l ji follows it. Both
   The purpose of this paper is an analysis of the                       letters l j,i-1 and l ji are part of an alphabet. Fur-
underlying probabilistic models and the types of                         thermore, we introduce a letter transition t ji which
errors committed by each one. Furthermore, it is                         goes from l j,i-1 to l ji .
investigated how the decision threshold can be cal-
ibrated and a model ensemble is tested.                                  2.1   P ROMODES
   The remainder is structured as follows. In Sec-                       P ROMODES is based on a zero-order model for
tion 2 we introduce the probabilistic generative                         boundaries b ji and on a first-order model for letter
process and show in Sections 2.1 and 2.2 how                             transitions t ji . It describes a word’s segmentation
we incorporate this process in P ROMODES and                             by its morpheme boundaries and resulting letter
P ROMODES -H. We start our experiments with ex-                          transitions within morphemes. A boundary vector
amining the learning behaviour of the algorithms                         b j is found by evaluating each position i with
in 3.1. Subsequently, we perform a position-wise
                                                                                      arg max Pr(b ji |t ji ) =                (1)
comparison of predictions in 3.2, show how we                                            b ji
find a better decision threshold for placing mor-
                                                                                      arg max Pr(b ji )Pr(t ji |b ji ) .
pheme boundaries in 3.3 and combine both algo-                                           b ji
rithms using a model ensemble to leverage indi-
vidual strengths in 3.4. In 3.5 we examine how                                The first component of the equation above is
the single algorithms contribute to the result of the                    the probability distribution over non-/boundaries
ensemble. In Section 4 we will compare our ap-                           Pr(b ji ). We assume that a boundary in i is in-
proaches to related work and in Section 5 we will                        serted independently from other boundaries (zero-
draw our conclusions.                                                    order) and the graphemic representation of the
                                                                         word, however, is conditioned on the length of
2    Probabilistic generative model                                      the word m j which means that the probability
                                                                         distribution is in fact Pr(b ji |m j ). We guarantee
Intuitively, we could say that our models describe
                                                                         ∑1r=0 Pr(b ji =r|m j ) = 1. To simplify the notation
the process of word generation from the left to the
                                                                         in later explanations, we will refer to Pr(b ji |m j )
right by alternately using two dice, the first for de-
                                                                         as Pr(b ji ).
ciding whether to place a morpheme boundary in
                                                                              The second component is the letter transition
the current word position and the second to get a
                                                                         probability distribution Pr(t ji |b ji ). We suppose a
corresponding letter transition. We are trying to
                                                                         first-order Markov chain consisting of transitions
reverse this process in order to find the underlying
                                                                         t ji from letter l j,i-1 ∈ AB to letter l ji ∈ A where A
sequence of tosses which determine the morpheme
                                                                         is a regular letter alphabet and AB =A ∪ {B} in-
boundaries. We are applying the notion of a prob-
                                                                         cludes B as an abstract morpheme start symbol
    1 P ROMODES stands for P RO babilistic M O del for different
                                                                         which can occur in l j,i-1 . For instance, the suf-
DE grees of S upervision. The H of P ROMODES -H refers to
                                                                         fix ‘s’ of the verb form gets, marking 3rd person
H igher order.
    2 In (Spiegler et al., 2009; Spiegler et al., 2010a) we have         singular, would be modelled as B → s whereas a
presented an unsupervised version of P ROMODES.                          morpheme internal transition could be g → e. We


                                                                   376


guarantee ∑l ji ∈A Pr(t ji |b ji )=1 with t ji being a tran-                   again, we find the word’s best segmentation b∗j in
sition from a certain l j,i−1 ∈ AB to l ji . The ad-                           2m evaluations with
vantage of the model is that instead of evaluating
an exponential number of possible segmentations                                b∗ji = arg max Pr(b ji |t ji ,t j,i-1 , b j,i-1 ) =           (4)
                                                                                          b ji
(2m ), the best segmentation b∗j =(b∗j1 , . . . , b∗jm ) is                    
found with 2m position-wise evaluations using                                  1, if Pr(b ji =1|b j,i-1 )Pr(t ji |b ji =1,t j,i-1 , b j,i-1 )
                                                                               
                                                                                  > Pr(b ji =0|b j,i-1 )Pr(t ji |b ji =0,t j,i-1 , b j,i-1 )
         b∗ji = arg max Pr(b ji |t ji )                            (2)         
                                                                                0, otherwise .
                                                                               
                      b ji
                   
                                                                               We will show in the experimental results that in-
                   
                   
                   
                   
                   1, if Pr(b =1)Pr(t |b =1)
                              ji        ji ji                                  creasing the memory of the algorithm by looking
                 =
                   
                     > Pr(b ji =0)Pr(t ji |b ji =0)                           at b j,i−1 leads to a better performance.
                   
                   
                   0, otherwise .
                                                                               3     Experiments and Results
   The simplifying assumptions made, however,                                  In the Morpho Challenge 2009, P ROMODES
reduce the expressive power of the model by not                                achieved competitive results on Finnish, Turkish,
allowing any dependencies on preceding bound-                                  English and German – and scored highest on non-
aries or letters. This can lead to over-segmentation                           vowelized and vowelized Arabic compared to 9
and therefore influences the performance of P RO -                             other algorithms (Kurimo et al., 2009). For the
MODES . For this reason, we have extended the                                  experiments described below, we chose the South
model which led to P ROMODES -H, a higher-order                                African language Zulu since our research work
probabilistic model.                                                           mainly aims at creating morphological resources
                                                                               for under-resourced indigenous languages. Zulu
2.2      P ROMODES -H
                                                                               is an agglutinative language with a complex mor-
In contrast to the original P ROMODES model, we                                phology where multiple prefixes and suffixes con-
also consider the boundary value b j,i-1 and mod-                              tribute to a word’s meaning. Nevertheless, it
ify our transition assumptions for P ROMODES -                                 seems that segment boundaries are more likely in
H in such a way that the new algorithm applies                                 certain word positions. The P ROMODES family
a first-order boundary model and a second-order                                harnesses this characteristic in combination with
transition model. A transition t ji is now defined                             describing morphemes by letter transitions. From
as a transition from an abstract symbol in l j,i-1 ∈                           the Ukwabelana corpus (Spiegler et al., 2010b) we
{N , B} to a letter in l ji ∈ A. The abstract sym-                             sampled 2500 Zulu words with a single segmenta-
bol is N or B depending on whether b ji is 0 or 1.                             tion each.
This holds equivalently for letter transitions t j,i-1 .
The suffix of our previous example gets would be                               3.1    Learning with increasing experience
modelled N → t → B → s.                                                        In our first experiment we applied 10-fold cross-
   Our boundary vector b j is then constructed from                            validation on datasets ranging from 500 to 2500
                                                                               words with the goal of measuring how the learning
      arg max Pr(b ji |t ji ,t j,i-1 , b j,i-1 ) =                 (3)
          b ji
                                                                               improves with increasing experience in terms of
                                                                               training set size. We want to remind the reader that
      arg max Pr(b ji |b j,i-1 )Pr(t ji |b ji ,t j,i-1 , b j,i-1 ) .
          b ji                                                                 our two algorithms are aimed at small datasets.
                                                                                  We randomly split each dataset into 10 subsets
The first component, the probability distribution                              where each subset was a test set and the corre-
over non-/boundaries Pr(b ji |b j,i-1 ), satisfies                             sponding 9 remaining sets were merged to a train-
∑1r=0 Pr(b ji =r|b j,i-1 )=1 with b j,i-1 , b ji ∈ {0, 1}.                     ing set. We kept the labels of the training set
As for P ROMODES, Pr(b ji |b j,i-1 ) is short-                                 to determine model parameters through maximum
hand for Pr(b ji |b j,i-1 , m j ).               The second                    likelihood estimates and applied each model to
component,              the letter transition proba-                           the test set from which we had removed the an-
bility distribution Pr(t ji |b ji , b j,i-1 ),          fulfils                swer keys. We compared results on the test set
∑l ji ∈A Pr(t ji |b ji ,t j,i-1 , b j,i-1 )=1 with t ji being                  against the ground truth by counting true positive
a transition from a certain l j,i−1 ∈ AB to l ji . Once                        (TP), false positive (FP), true negative (TN) and


                                                                         377


false negative (FN) morpheme boundary predic-                       	                                                         	  FNPH	  =	  0.4606	  
                                                                                             TNPH 	  =	  0.8726	  
tions. Counts were summarised using precision3 ,                                             TNP	  	  	  =	  0.9472	      	  FNP	  	  	  	  =	  0.6955	  
                                                                                             	                                	  
recall4 and f-measure5 , as shown in Table 1.
                                                                                                                  0.8828	                                  0.5698	  
                                                                                                                  0.1172	                              0.4302	  
  Data      Precision             Recall        F-measure                  +	  0.0486	                                                                                 +	  0.0819	  
   500   0.7127±0.0418        0.3500±0.0272   0.4687±0.0284                     (net)	                                                                                       (net)	  
  1000   0.7435±0.0556        0.3350±0.0197   0.4614±0.0250
                                                                                                    0.6891	                            0.2111	  
  1500   0.7460±0.0529        0.3160±0.0150   0.4435±0.0206
  2000   0.7504±0.0235        0.3068±0.0141   0.4354±0.0168                                                    0.3109	                              0.7889	  
  2500   0.7557±0.0356        0.3045±0.0138   0.4337±0.0163
                         (a) P ROMODES                                                        FPPH =	  0.1274	                  TPPH =	  0.5394	  
                                                                                              FPP	  	  	  =	  0.0528	        TPP	  	  	  =	  0.3045	  
  Data      Precision             Recall        F-measure                                     	                                  	  
   500   0.6983±0.0511        0.4938±0.0404   0.5776±0.0395
  1000   0.6865±0.0298        0.5177±0.0177   0.5901±0.0205
  1500   0.6952±0.0308        0.5376±0.0197   0.6058±0.0173         Figure 1: Contingency table for P ROMODES [grey
  2000   0.7008±0.0140        0.5316±0.0146   0.6044±0.0110         with subscript P] and P ROMODES -H [black with
  2500   0.6941±0.0184        0.5396±0.0218   0.6068±0.0151         subscript PH] results including gross and net
                       (b) P ROMODES -H                             changes of P ROMODES -H.
    Table 1: 10-fold cross-validation on Zulu.
                                                                    3.2           Position-wise comparison of algorithmic
   For P ROMODES we can see in Table 1a that                                      predictions
the precision increases slightly from 0.7127 to                     In the second experiment, we investigated which
0.7557 whereas the recall decreases from 0.3500                     aspects of P ROMODES -H in comparison to P RO -
to 0.3045 going from dataset size 500 to 2500.                      MODES led to the above described differences in
This suggests that to some extent fewer morpheme                    performance. For this reason we broke down
boundaries are discovered but the ones which are                    the summary measures of precision and recall
found are more likely to be correct. We believe                     into their original components: true/false positive
that this effect is caused by the limited memory                    (TP/FP) and negative (TN/FN) counts presented in
of the model which uses order zero for the occur-                   the 2 × 2 contingency table of Figure 1. For gen-
rence of a boundary and order one for letter tran-                  eral evidence, we averaged across all experiments
sitions. It seems that the model gets quickly sat-                  using relative frequencies. Note that the relative
urated in terms of incorporating new information                    frequencies of positives (TP + FN) and negatives
and therefore precision and recall do not drasti-                   (TN + FP) each sum to one.
cally change for increasing dataset sizes. In Ta-
                                                                       The goal was to find out how predictions
ble 1b we show results for P ROMODES -H. Across
                                                                    in each word position changed when applying
the datasets precision stays comparatively con-
                                                                    P ROMODES -H instead of P ROMODES.             This
stant around a mean of 0.6949 whereas the recall
                                                                    would show where the algorithms agree and
increases from 0.4938 to 0.5396. Compared to
                                                                    where they disagree. P ROMODES classifies non-
P ROMODES we observe an increase in recall be-
                                                                    boundaries in 0.9472 of the times correctly as TN
tween 0.1438 and 0.2351 at a cost of a decrease in
                                                                    and in 0.0528 of the times falsely as boundaries
precision between 0.0144 and 0.0616.
                                                                    (FP). The algorithm correctly labels 0.3045 of the
   Since both algorithms show different behaviour                   positions as boundaries (TP) and 0.6955 falsely as
with increasing experience and P ROMODES -H                         non-boundaries (FN). We can see that P ROMODES
yields a higher f-measure across all datasets, we                   follows a rather conservative approach.
will investigate in the next experiments how these                     When applying P ROMODES -H, the majority of
differences manifest themselves at the boundary                     the FP’s are turned into non-boundaries, how-
level.                                                              ever, a slightly higher number of previously cor-
                                                                    rectly labelled non-boundaries are turned into
  3 precision =    TP                                               false boundaries. The net change is a 0.0486 in-
                T P+FP .
  4 recall     TP
          = T P+FN .                                                crease in FP’s which is the reason for the decrease
  5 f -measure = 2·precision·recall .                               in precision. On the other side, more false non-
                  precision+recall



                                                              378


boundaries (FN) are turned into boundaries than                          mance can be analysed more informatively using
in the opposite direction with a net increase of                         these kinds of curves. The PR curve is plotted with
0.0819 of correct boundaries which led to the in-                        recall on the x-axis and precision on the y-axis for
creased recall. Since the deduction of precision                         increasing thresholds h. The PR curves for P RO -
is less than the increase of recall, a better over-all                   MODES and P ROMODES -H are shown in Figure
performance of P ROMODES -H is achieved.                                 2 on the validation set from which we learnt our
   In summary, P ROMODES predicts more accu-                             optimal thresholds h∗ . Points were connected for
rately non-boundaries whereas P ROMODES -H is                            readability only – points on the PR curve cannot
better at finding morpheme boundaries. So far we                         be interpolated linearly.
have based our decision for placing a boundary in                           In addition to the PR curves, we plotted isomet-
a certain word position on Equation 2 and 4 as-                          rics for corresponding f-measure values which are
                                                                                                   f -measure·recall
suming that P(b ji =1| . . .) > P(b ji =0| . . .)6 gives the             defined as precision= 2recall  − f -measure and are hy-
best result. However, if the underlying distribu-                        perboles. For increasing f-measure values the iso-
tion for boundaries given the evidence is skewed,                        metrics are moving further to the top-right corner
it might be possible to improve results by introduc-                     of the plot. For a threshold of h = 0.50 (marked
ing a certain decision threshold for inserting mor-                      by ‘3’) P ROMODES -H has a better performance
pheme boundaries. We will put this idea to the test                      than P ROMODES. Nevertheless, across the entire
in the following section.                                                PR curve none of the algorithms dominates. One
                                                                         curve would dominate another if all data points
3.3    Calibration of the decision threshold                             of the dominated curve were beneath or equal
For the third experiment we slightly changed our                         to the dominating one. P ROMODES has its opti-
experimental setup. Instead of dividing datasets                         mal threshold at h∗ = 0.36 and P ROMODES -H at
during 10-fold cross-validation into training and                        h∗ = 0.37 where P ROMODES has a slightly higher
test subsets with the ratio of 9:1 we randomly split                     f-measure than P ROMODES -H. The points of op-
the data into training, validation and test sets with                    timal f-measure performance are marked with ‘4’
the ratio of 8:1:1. We then run our experiments                          on the PR curve.
and measured contingency table counts.
                                                                                                                Prec.   Recall   F-meas.
   Rather than placing a boundary if                                      P ROMODES validation (h=0.50)        0.7522   0.3087    0.4378
P(b ji =1| . . .) > P(b ji =0| . . .) which corresponds                   P ROMODES test (h=0.50)              0.7540   0.3084    0.4378
                                                                          P ROMODES validation (h∗ =0.36)      0.5857   0.7824    0.6699
to P(b ji =1| . . .) > 0.50 we introduced a decision                      P ROMODES test (h∗ =0.36)            0.5869   0.7803    0.6699
threshold P(b ji =1| . . .) > h with 0 ≤ h ≤ 1. This                      P ROMODES -H validation (h=0.50)     0.6983   0.5333    0.6047
is based on the assumption that the underlying                            P ROMODES -H test (h=0.50)           0.6960   0.5319    0.6030
                                                                          P ROMODES -H validation (h∗ =0.37)   0.5848   0.7491    0.6568
distribution P(b ji | . . .) might be skewed and an                       P ROMODES -H test (h∗ =0.37)         0.5857   0.7491    0.6574
optimal decision can be achieved at a different
threshold. The optimal threshold was sought on                           Table 2: P ROMODES and P ROMODES -H on vali-
the validation set and evaluated on the test set.                        dation and test set.
An overview over the validation and test results
is given in Table 2. We want to point out that the
                                                                            Summarizing, we have shown that both algo-
threshold which yields the best f-measure result
                                                                         rithms commit different errors at the word posi-
on the validation set returns almost the same
                                                                         tion level whereas P ROMODES is better in pre-
result on the separate test set for both algorithms
                                                                         dicting non-boundaries and P ROMODES -H gives
which suggests the existence of a general optimal
                                                                         better results for morpheme boundaries at the de-
threshold.
                                                                         fault threshold of h = 0.50. In this section, we
   Since this experiment provided us with a set of
                                                                         demonstrated that across different decision thresh-
data points where the recall varied monotonically
                                                                         olds h for P(b ji =1| . . .) > h none of algorithms
with the threshold and the precision changed ac-
                                                                         dominates the other one, and at the optimal thresh-
cordingly, we reverted to precision-recall curves
                                                                         old P ROMODES achieves a slightly higher perfor-
(PR curves) from machine learning. Following
                                                                         mance than P ROMODES -H. The question which
Davis and Goadrich (2006) the algorithmic perfor-
                                                                         arises is whether we can combine P ROMODES and
    6 Based on Equation 2 and 4 we use the notation P(b | . . .)
                                                       ji
                                                                         P ROMODES -H in an ensemble that leverages indi-
if we do not want to specify the algorithm.                              vidual strengths of both.


                                                                   379


                                   1
                                                                                       Promodes
                                                                                       Promodes−H
                                  0.9                                                  Promodes−E
                                                                                       F−measure isometrics
                                                                                       Default result
                                  0.8                                                  Optimal result (h*)


                                  0.7
                      Precision


                                  0.6


                                  0.5


                                  0.4



                                          0.4        0.5          0.6            0.7     0.8      0.9         1
                                                                        Recall

                             Figure 2: Precision-recall curves for algorithms on validation set.


3.4   A model ensemble to leverage individual                              boosted the original result by 0.1185 on the test
      strengths                                                            set. Compared to its components P ROMODES and
A model ensemble is a set of individually trained                          P ROMODES -H the f-measure increased by 0.0228
classifiers whose predictions are combined when                            and 0.0353 on the test set.
classifying new instances (Opitz and Maclin,                                  In short, we have shown that by combining
1999). The idea is that by combining P ROMODES                             P ROMODES and P ROMODES -H and finding the
and P ROMODES -H, we would be able to avoid cer-                           optimal threshold, the ensemble P ROMODES -E
tain errors each model commits by consulting the                           gives better results than the individual models
other model as well. We introduce P ROMODES -E                             themselves and therefore manages to leverage the
as the ensemble of P ROMODES and P ROMODES -                               individual strengths of both to a certain extend.
H. P ROMODES -E accesses the individual proba-                             However, can we pinpoint the exact contribution
bilities Pr(b ji =1| . . . ) and simply averages them:                     of each individual algorithm to the improved re-
                                                                           sult? We try to find an answer to this question in
  Pr(b ji =1|t ji ) + Pr(b ji =1|t ji , b j,i-1 ,t j,i-1 )                 the analysis of the subsequent section.
                                                           >h .
                         2
                                                                           3.5    Analysis of calibrated algorithms and
   As before, we used the default threshold                                       their model ensemble
h = 0.50 and found the calibrated threshold                                For the entire dataset of 2500 words, we have
h∗ = 0.38, marked with ‘3’ and ‘4’ in Figure 2                             examined boundary predictions dependent on the
and shown in Table 3. The calibrated threshold                             relative word position. In Figure 3 and 4 we have
improves the f-measure over both P ROMODES and                             plotted the absolute counts of correct boundaries
P ROMODES -H.                                                              (TP) and non-boundaries (TN) which P ROMODES
                                         Prec.   Recall    F-meas.         predicted but not P ROMODES -H, and vice versa,
 P ROMODES -E validation (h=0.50)       0.8445   0.4328    0.5723          as continuous lines. We furthermore provided the
 P ROMODES -E test (h=0.50)             0.8438   0.4352    0.5742
 P ROMODES -E validation (h∗ =0.38)     0.6354   0.7625    0.6931
                                                                           number of individual predictions which were ulti-
 P ROMODES -E test (h∗ =0.38)           0.6350   0.7620    0.6927          mately adopted by P ROMODES -E in the ensemble
                                                                           as dashed lines.
Table 3: P ROMODES -E on validation and test set.                             In Figure 3a we can see for the default thresh-
                                                                           old that P ROMODES performs better in predicting
 The optimal solution applying h∗ = 0.38 is                                non-boundaries in the middle and the end of the
more balanced between precision and recall and                             word in comparison to P ROMODES -H. Figure 3b


                                                                     380


shows the statistics for correctly predicted bound-          bination with a threshold, learnt on a different
aries. Here, P ROMODES -H outperforms P RO -                 dataset, were used to merge word analyses. In
MODES in predicting correct boundaries across the            contrast, our ensemble algorithm P ROMODES -E
entire word length. After the calibration, shown             directly accesses the probabilistic framework of
in Figure 4a, P ROMODES -H improves the correct              each algorithm and combines them based on an
prediction of non-boundaries at the beginning of             optimal threshold learnt on a validation set.
the word whereas P ROMODES performs better at
the end. For the boundary prediction in Figure 4b            5   Conclusions
the signal disappears after calibration.                     We have presented a method to learn a cali-
   Concluding, it appears that our test language             brated decision threshold from a validation set and
Zulu has certain features which are modelled best            demonstrated that ensemble methods in connec-
with either a lower or higher-order model. There-            tion with calibrated decision thresholds can give
fore, the ensemble leveraged strengths of both al-           better results than the individual models them-
gorithms which led to a better overall performance           selves. We introduced two algorithms for word de-
with a calibrated threshold.                                 composition which are based on generative prob-
                                                             abilistic models. The models consider segment
4   Related work                                             boundaries as hidden variables and include prob-
We have presented two probabilistic genera-                  abilities for letter transitions within segments.
tive models for word decomposition, P ROMODES                P ROMODES contains a lower order model whereas
and P ROMODES -H. Another generative model                   P ROMODES -H is a novel development of P RO -
                                                             MODES with a higher order model. For both
for morphological analysis has been described
by Snover and Brent (2001) and Snover et al.                 algorithms, we defined the mathematical model
(2002), however, they were interested in finding             and performed experiments on language data of
paradigms as sets of mutual exclusive operations             the morphologically complex language Zulu. We
on a word form whereas we are describing a gener-            compared the performance on increasing train-
ative process using morpheme boundaries and re-              ing set sizes and analysed for each word position
sulting letter transitions.                                  whether their boundary prediction agreed or dis-
                                                             agreed. We found out that P ROMODES was bet-
   Moreover, our probabilistic models seem to re-
                                                             ter in predicting non-boundaries and P ROMODES -
semble Hidden Markov Models (HMMs) by hav-
                                                             H gave better results for morpheme boundaries at
ing certain states and transitions. The main differ-
                                                             a default decision threshold. At an optimal de-
ence is that we have dependencies between states
                                                             cision threshold, however, both yielded a simi-
as well as between emissions whereas in HMMs
                                                             lar f-measure result. We then performed a fur-
emissions only depend on the underlying state.
                                                             ther analysis based on relative word positions and
   Combining different morphological analysers
                                                             found out that the calibrated P ROMODES -H pre-
has been performed, for example, by Atwell and
                                                             dicted non-boundaries better for initial word posi-
Roberts (2006) and Spiegler et al. (2009). Their
                                                             tions whereas the calibrated P ROMODES for mid-
approaches, though, used majority vote to decide
                                                             and final word positions. For boundaries, the cali-
whether a morpheme boundary is inserted in a cer-
                                                             brated algorithms had a similar behaviour. Subse-
tain word position or not. The algorithms them-
                                                             quently, we showed that a model ensemble of both
selves were treated as black-boxes.
                                                             algorithms in conjunction with finding an optimal
   Monson et al. (2009) described an indirect
                                                             threshold exceeded the performance of the single
approach to probabilistically combine ParaMor
                                                             algorithms at their individually optimal threshold.
(Monson, 2008) and Morfessor (Creutz, 2006).
They used a natural language tagger which was                Acknowledgements
trained on the output of ParaMor and Morfes-
sor. The goal was to mimic each algorithm since              We would like to thank Narayanan Edakunni and
ParaMor is rule-based and there is no access to              Bruno Golénia for discussions concerning this pa-
Morfessor’s internally used probabilities. The tag-          per as well as the anonymous reviewers for their
ger would then return a probability for starting a           comments. The research described was sponsored
new morpheme in a certain position based on the              by EPSRC grant EP/E010857/1 Learning the mor-
original algorithm. These probabilities in com-              phology of complex synthetic languages.


                                                       381


                                            Performance on non−boundaries, default threshold                                                       Performance on boundaries, default threshold
                                800                                                                                                   800
                                        Promodes (unique TN)                                                                                 Promodes (unique TP)
                                        Promodes−H (unique TN)                                                                               Promodes−H (unique TP)
                                700                                                                                                   700    Promodes and Promodes−E (unique TP)
                                        Promodes and Promodes−E (unique TN)
                                        Promodes−H and Promodes−E (unique TN)                                                                Promodes−H and Promodes−E (unique TP)
                                                                                                                                      600
Absolute true negatives (TN)




                                600




                                                                                                     Absolute true positives (TP)
                                500                                                                                                   500


                                400                                                                                                   400


                                300                                                                                                   300


                                200                                                                                                   200

                                100                                                                                                   100

                                 0                                                                                                     0
                                      0.1     0.2   0.3   0.4 0.5 0.6 0.7          0.8   0.9   1                                            0.1    0.2     0.3   0.4 0.5 0.6 0.7          0.8   0.9   1
                                                          Relative word position                                                                                 Relative word position

                                                 (a) True negatives, default                                                                             (b) True positives, default

                                                               Figure 3: Analysis of results using default threshold.
                                        Performance on non−boundaries, calibrated threshold                                                       Performance on boundaries, calibrated threshold
                                800                                                                                                   800
                                        Promodes (unique TN)                                                                                  Promodes (unique TP)
                                        Promodes−H (unique TN)                                                                                Promodes−H (unique TP)
                                700     Promodes and Promodes−E (unique TN)                                                           700     Promodes and Promodes−E (unique TP)
                                        Promodes−H and Promodes−E (unique TN)                                                                 Promodes−H and Promodes−E (unique TP)
 Absolute true negatives (TN)




                                600                                                                                                   600
                                                                                                       Absolute true positives (TP)




                                500                                                                                                   500

                                400                                                                                                   400

                                300                                                                                                   300

                                200                                                                                                   200

                                100                                                                                                   100

                                  0                                                                                                     0
                                      0.1     0.2   0.3   0.4 0.5 0.6 0.7          0.8   0.9   1                                            0.1    0.2     0.3   0.4 0.5 0.6 0.7          0.8   0.9   1
                                                          Relative word position                                                                                 Relative word position
                                               (a) True negatives, calibrated                                                                       (b) True positives, calibrated

                                                             Figure 4: Analysis of results using calibrated threshold.




                                                                                                   382


References                                                       C. Monson. 2008. ParaMor: From Paradigm
                                                                   Structure To Natural Language Morphology Induc-
J. W. Amtrup. 2003. Morphology in machine trans-                   tion. Ph.D. thesis, Language Technologies Institute,
   lation systems: Efficient integration of finite state           School of Computer Science, Carnegie Mellon Uni-
   transducers and feature structure descriptions. Ma-             versity, Pittsburgh, PA, USA.
   chine Translation, 18(3):217–238.
                                                                 R. J. Mooney and M. E. Califf. 1996. Learning the
E. Atwell and A. Roberts. 2006. Combinatory hy-                    past tense of English verbs using inductive logic pro-
   brid elementary analysis of text (CHEAT). Proceed-              gramming. Symbolic, Connectionist, and Statistical
   ings of the PASCAL Challenges Workshop on Un-                   Approaches to Learning for Natural Language Pro-
   supervised Segmentation of Words into Morphemes,                cessing, 370–384.
   Venice, Italy.
                                                                 S. Muggleton and M. Bain. 1999. Analogical predic-
                                                                    tion. Inductive Logic Programming: 9th Interna-
M. Creutz. 2006. Induction of the Morphology of Nat-
                                                                    tional Workshop, ILP-99, Bled, Slovenia, 234.
  ural Language: Unsupervised Morpheme Segmen-
  tation with Application to Automatic Speech Recog-             K. Oflazer, S. Nirenburg, and M. McShane. 2001.
  nition. Ph.D. thesis, Helsinki University of Technol-            Bootstrapping morphological analyzers by combin-
  ogy, Espoo, Finland.                                             ing human elicitation and machine learning. Com-
                                                                   putational. Linguistics, 27(1):59–85.
J. Davis and M. Goadrich. 2006. The relationship
   between precision-recall and ROC curves. Interna-             D. Opitz and R. Maclin. 1999. Popular ensemble
   tional Conference on Machine Learning, Pittsburgh,              methods: An empirical study. Journal of Artificial
   PA, 233–240.                                                    Intelligence Research, 11:169–198.
                                                                 D. E. Rumelhart and J. L. McClelland. 1986. On
M. Gasser. 1994. Modularity in a connectionist                     learning the past tenses of English verbs. MIT
  model of morphology acquisition. Proceedings of                  Press, Cambridge, MA, USA.
  the 15th conference on Computational linguistics,
  1:214–220.                                                     K. Shalonova, B. Golénia, and P. A. Flach. 2009. To-
                                                                   wards learning morphology for under-resourced fu-
J. Goldsmith. 2001. Unsupervised learning of the mor-              sional and agglutinating languages. IEEE Transac-
   phology of a natural language. Computational Lin-               tions on Audio, Speech, and Language Processing,
   guistics, 27:153–198.                                           17(5):956965.

J. Goldsmith. 2009. The Handbook of Computational                M. G. Snover and M. R. Brent. 2001. A Bayesian
   Linguistics, chapter Segmentation and morphology.               model for morpheme and paradigm identification.
   Blackwell.                                                      Proceedings of the 39th Annual Meeting on Asso-
                                                                   ciation for Computational Linguistics, 490 – 498.
M. A. Hafer and S. F. Weiss. 1974. Word segmenta-                M. G. Snover, G. E. Jarosz, and M. R. Brent. 2002.
  tion by letter successor varieties. Information Stor-            Unsupervised learning of morphology using a novel
  age and Retrieval, 10:371–385.                                   directed search algorithm: Taking the first step. Pro-
                                                                   ceedings of the ACL-02 workshop on Morphological
Z. S. Harris. 1955. From phoneme to morpheme. Lan-                 and phonological learning, 6:11–20.
   guage, 31(2):190–222.
                                                                 S. Spiegler, B. Golénia, K. Shalonova, P. A. Flach, and
T. Hirsimaki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-             R. Tucker. 2008. Learning the morphology of Zulu
   pioja, and J. Pylkkonen. 2006. Unlimited vocabu-                 with different degrees of supervision. IEEE Work-
   lary speech recognition with morph language mod-                 shop on Spoken Language Technology.
   els applied to Finnish. Computer Speech And Lan-
                                                                 S. Spiegler, B. Golénia, and P. A. Flach. 2009. Pro-
   guage, 20(4):515–541.
                                                                    modes: A probabilistic generative model for word
                                                                    decomposition. Working Notes for the CLEF 2009
K. Kettunen. 2009. Reductive and generative ap-
                                                                    Workshop, Corfu, Greece.
  proaches to management of morphological variation
  of keywords in monolingual information retrieval:              S. Spiegler, B. Golénia, and P. A. Flach. 2010a. Un-
  An overview. Journal of Documentation, 65:267 –                   supervised word decomposition with the Promodes
  290.                                                              algorithm. In Multilingual Information Access Eval-
                                                                    uation Vol. I, CLEF 2009, Corfu, Greece, Lecture
M. Kurimo, S. Virpioja, and V. T. Turunen. 2009.                    Notes in Computer Science, Springer.
  Overview and results of Morpho Challenge 2009.
  Working notes for the CLEF 2009 Workshop, Corfu,               S. Spiegler, A. v. d. Spuy, and P. A. Flach. 2010b. Uk-
  Greece.                                                           wabelana - An open-source morphological Zulu cor-
                                                                    pus. in review.
C. Monson, K. Hollingshead, and B. Roark. 2009.                  R. Sproat. 1996. Multilingual text analysis for text-to-
  Probabilistic ParaMor. Working notes for the CLEF                 speech synthesis. Nat. Lang. Eng., 2(4):369–380.
  2009 Workshop, Corfu, Greece.


                                                           383

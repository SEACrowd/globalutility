                    Latent variable models of selectional preference

                                           Diarmuid Ó Séaghdha
                                           University of Cambridge
                                            Computer Laboratory
                                              United Kingdom
                                          do242@cl.cam.ac.uk


                      Abstract                               to surface-level predicates such as verbs and mod-
                                                             ifiers, but also extends to semantic frames (Erk,
    This paper describes the application of                  2007) and inference rules (Pantel et al., 2007).
    so-called topic models to selectional pref-                 The fundamental problem that selectional prefer-
    erence induction. Three models related                   ence models must address is data sparsity: in many
    to Latent Dirichlet Allocation, a proven                 cases insufficient corpus data is available to reliably
    method for modelling document-word co-                   measure the plausibility of a predicate-argument
    occurrences, are presented and evaluated                 pair by counting its observed frequency. A rarely
    on datasets of human plausibility judge-                 seen pair may be fundamentally implausible (a
    ments. Compared to previously proposed                   carrot laughed) or plausible but rarely expressed
    techniques, these models perform very                    (a manservant laughed).1 In general, it is benefi-
    competitively, especially for infrequent                 cial to smooth plausibility estimates by integrating
    predicate-argument combinations where                    knowledge about the frequency of other, similar
    they exceed the quality of Web-scale pre-                predicate-argument pairs. The task thus share some
    dictions while using relatively little data.             of the nature of language modelling; however, it is
                                                             a task less amenable to approaches that require very
1   Introduction                                             large training corpora and one where the semantic
                                                             quality of a model is of greater importance.
Language researchers have long been aware that
                                                                This paper takes up tools (“topic models”)
many words place semantic restrictions on the
                                                             that have been proven successful in modelling
words with which they can co-occur in a syntactic
                                                             document-word co-occurrences and adapts them
relationship. Violations of these restrictions make
                                                             to the task of selectional preference learning. Ad-
the sense of a sentence odd or implausible:
                                                             vantages of these models include a well-defined
                                                             generative model that handles sparse data well,
 (1) Colourless green ideas sleep furiously.
                                                             the ability to jointly induce semantic classes and
 (2) The deer shot the hunter.                               predicate-specific distributions over those classes,
                                                             and the enhanced statistical strength achieved by
Recognising whether or not a selectional restriction         sharing knowledge across predicates. Section 2
is satisfied can be an important trigger for metaphor-       surveys prior work on selectional preference mod-
ical interpretations (Wilks, 1978) and also plays a          elling and on semantic applications of topic models.
role in the time course of human sentence process-           Section 3 describes the models used in our exper-
ing (Rayner et al., 2004). A more relaxed notion of          iments. Section 4 provides details of the experi-
selectional preference captures the idea that certain        mental design. Section 5 presents results for our
classes of entities are more likely than others to           models on the task of predicting human plausibility
fill a given argument slot of a predicate. In Natu-          judgements for predicate-argument combinations;
ral Language Processing, knowledge about proba-              we show that performance is generally competi-
ble, less probable and wholly infelicitous predicate-              1
                                                                     At time of writing, Google estimates 855 hits
argument pairs is of value for numerous applica-               for “a|the carrot|carrots laugh|laughs|laughed” and 0
tions, for example semantic role labelling (Gildea             hits for “a|the manservant|manservants|menservants
                                                               laugh|laughs|laughed”; many of the carrot hits are false
and Jurafsky, 2002; Zapirain et al., 2009). The                positives but a significant number are true subject-verb
notion of selectional preference is not restricted             observations.


                                                         435
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435–444,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


tive with or superior to a number of other models,       of interest.
including models using Web-scale resources, espe-            Keller and Lapata (2003) suggest a simple al-
cially for low-frequency examples. In Section 6 we       ternative to smoothing-based approaches. They
wrap up by summarising the paper’s conclusions           demonstrate that noisy counts from a Web search
and sketching directions for future research.            engine can yield estimates of plausibility for
                                                         predicate-argument pairs that are superior to mod-
2     Related work                                       els learned from a smaller parsed corpus. The as-
2.1    Selectional preference learning                   sumption inherent in this approach is that given suf-
                                                         ficient text, all plausible predicate-argument pairs
The representation (and latterly, learning) of selec-    will be observed with frequency roughly correlated
tional preferences for verbs and other predicates        with their degree of plausibility. While the model is
has long been considered a fundamental problem           undeniably straightforward and powerful, it has a
in computational semantics (Resnik, 1993). Many          number of drawbacks: it presupposes an extremely
approaches to the problem use lexical taxonomies         large corpus, the like of which will only be avail-
such as WordNet to identify the semantic classes         able for a small number of domains and languages,
that typically fill a particular argument slot for a     and it is only suitable for relations that are iden-
predicate (Resnik, 1993; Clark and Weir, 2002;           tifiable by searching raw text for specific lexical
Schulte im Walde et al., 2008). In this paper, how-      patterns.
ever, we focus on methods that do not assume
the availability of a comprehensive taxonomy but           2.2   Topic modelling
rather induce semantic classes automatically from
a corpus of text. Such methods are more generally        The task of inducing coherent semantic clusters is
applicable, for example in domains or languages          common to many research areas. In the field of
where handbuilt semantic lexicons have insufficient      document modelling, a class of methods known
coverage or are non-existent.                            as “topic models” have become a de facto stan-
   Rooth et al. (1999) introduced a model of se-         dard for identifying semantic structure in docu-
lectional preference induction that casts the prob-      ments. These include the Latent Dirichlet Al-
lem in a probabilistic latent-variable framework.        location (LDA) model of Blei et al. (2003) and
In Rooth et al.’s model each observed predicate-         the Hierarchical Dirichlet Process model of Teh
argument pair is probabilistically generated from a      et al. (2006). Formally seen, these are hierarchi-
latent variable, which is itself generated from an un-   cal Bayesian models which induce a set of latent
derlying distribution on variables. The use of latent    variables or topics that are shared across docu-
variables, which correspond to coherent clusters         ments. The combination of a well-defined prob-
of predicate-argument interactions, allow proba-         abilistic model and Gibbs sampling procedure for
bilities to be assigned to predicate-argument pairs      estimation guarantee (eventual) convergence and
which have not previously been observed by the           the avoidance of degenerate solutions. As a result
model. The discovery of these predicate-argument         of intensive research in recent years, the behaviour
clusters and the estimation of distributions on latent   of topic models is well-understood and computa-
and observed variables are performed simultane-          tionally efficient implementations have been de-
ously via an Expectation Maximisation procedure.         veloped. The tools provided by this research are
The work presented in this paper is inspired by          used in this paper as the building blocks of our
Rooth et al.’s latent variable approach, most di-        selectional preference models.
rectly in the model described in Section 3.3. Erk           Hierarchical Bayesian modelling has recently
(2007) and Padó et al. (2007) describe a corpus-        gained notable popularity in many core areas of
driven smoothing model which is not probabilistic        natural language processing, from morphological
in nature but relies on similarity estimates from        segmentation (Goldwater et al., 2009) to opinion
a “semantic space” model that identifies semantic        modelling (Lin et al., 2006). Yet so far there have
similarity with closeness in a vector space of co-       been relatively few applications to traditional lex-
occurrences. Bergsma et al. (2008) suggest learn-        ical semantic tasks. Boyd-Graber et al. (2007) in-
ing selectional preferences in a discriminative way,     tegrate a model of random walks on the WordNet
by training a collection of SVM classifiers to recog-    graph into an LDA topic model to build an unsuper-
nise likely and unlikely arguments for predicates        vised word sense disambiguation system. Brody


                                                     436


and Lapata (2009) adapt the basic LDA model for           ulary of N types and a relation vocabulary of
application to unsupervised word sense induction;         R types. Each predicate type is associated with
in this context, the topics learned by the model are      a singe relation; for example the predicate type
assumed to correspond to distinct senses of a partic-     eat:V:dobj (the direct object of the verb eat) is
ular lemma. Zhang et al. (2009) are also concerned        treated as distinct from eat:V:subj (the subject of
with inducing multiple senses for a particular term;      the verb eat). The training corpus consists of W
here the goal is to identify distinct entity types in     observations of argument-predicate pairs. Each
the output of a pattern-based entity set discovery        model has at least one vocabulary of Z arbitrar-
system. Reisinger and Paşca (2009) use LDA-like          ily labelled latent variables. fzn is the number of
models to map automatically acquired attribute            observations where the latent variable z has been
sets onto the WordNet hierarchy. Griffiths et al.         associated with the argument type n, fzv is the
(2007) demonstrate that topic models learned from         number of observations where z has been associ-
document-word co-occurrences are good predictors          ated with the predicate type v and fzr is the number
of semantic association judgements by humans.             of observations where z has been associated with
   Simultaneously to this work, Ritter et al. (2010)      the relation r. Finally, fz· is the total number of
have also investigated the use of topic models            observations associated with z and f·v is the total
for selectional preference learning. Their goal is        number of observations containing the predicate v.
slightly different to ours in that they wish to model
the probability of a binary predicate taking two          3.2   Latent Dirichlet Allocation
specified arguments, i.e., P (n1 , n2 |v), whereas we   As noted above, LDA was originally introduced to
model the joint and conditional probabilities of a      model sets of documents in terms of topics, or clus-
predicate taking a single specified argument. The       ters of terms, that they share in varying proportions.
model architecture they propose, LinkLDA, falls         For example, a research paper on bioinformatics
somewhere between our LDA and DUAL-LDA                  may use some vocabulary that is shared with gen-
models. Hence LinkLDA could be adapted to esti-         eral computer science papers and some vocabulary
mate P (n, v|r) as DUAL-LDA does, but a prelimi-        that is shared with biomedical papers. The analogi-
nary investigation indicates that it does not perform   cal move from modelling document-term cooccur-
well in this context. The most likely explanation       rences to modelling predicate-argument cooccur-
is that LinkLDA generates its two arguments in-         rences is intuitive: we assume that each predicate is
dependently, which may be suitable for distinct         associated with a distribution over semantic classes
argument positions of a given predicate but is un-      (“topics”) and that these classes are shared across
suitable when one of those “arguments” is in fact       predicates. The high-level “generative story” for
the predicate.                                          the LDA selectional preference model is as follows:
   The models developed in this paper, though in-
tended for semantic modelling, also bear some sim-        (1) For each predicate v, draw a multinomial dis-
ilarity to the internals of generative syntax models          tribution Θv over argument classes from a
such as the “infinite tree” (Finkel et al., 2007). In         Dirichlet distribution with parameters α.
some ways, our models are less ambitious than
comparable syntactic models as they focus on spe-         (2) For each argument class z, draw a multinomial
cific fragments of grammatical structure rather than          distribution Φz over argument types from a
learning a more general representation of sentence            Dirichlet with parameters β.
syntax. It would be interesting to evaluate whether
                                                          (3) To generate an argument for v, draw an ar-
this restricted focus improves the quality of the
                                                              gument class z from Θv and then draw an
learned model or whether general syntax models
                                                              argument type n from Φz
can also capture fine-grained knowledge about com-
binatorial semantics.                                   The resulting model can be written as:
3     Three selectional preference models                                  X
                                                            P (n|v, r) =        P (n|z)P (z|v, r)         (1)
3.1    Notation                                                             z
                                                                           X fzn + β      fzv + αz
In the model descriptions below we assume a predi-                     ∝                     P            (2)
                                                                             fz· + N β f·v + z 0 αz 0
cate vocabulary of V types, an argument vocab-                             z



                                                    437


Due to multinomial-Dirichlet conjugacy, the dis-          these hyperparameters.2
tributions Θv and Φz can be integrated out and do
not appear explicitly in the above formula. The           3.3    A Rooth et al.-inspired model
first term in (2) can be seen as a smoothed esti-         In Rooth et al.’s (1999) selectional preference
mate of the probability that class z produces the         model, a latent variable is responsible for generat-
argument n; the second is a smoothed estimate of          ing both the predicate and argument types of an ob-
the probability that predicate v takes an argument        servation. The basic LDA model can be extended to
belonging to class z. One important point is that         capture this kind of predicate-argument interaction;
the smoothing effects of the Dirichlet priors on Θv       the generative story for the resulting ROOTH-LDA
and Φz are greatest for predicates and arguments          model is as follows:
that are rarely seen, reflecting an intuitive lack of
certainty. We assume an asymmetric Dirichlet prior        (1) For each relation r, draw a multinomial dis-
on Θv (the α parameters can differ for each class)            tribution Θr over interaction classes from a
and a symmetric prior on Φz (all β parameters are             Dirichlet distribution with parameters α.
equal); this follows the recommendations of Wal-          (2) For each class z, draw a multinomial Φz over
lach et al. (2009) for LDA. This model estimates              argument types from a Dirichlet distribution
predicate-argument probabilities conditional on a             with parameters β and a multinomial Ψz over
given predicate v; it cannot by itself provide joint          predicate types from a Dirichlet distribution
probabilities P (n, v|r), which are needed for our            with parameters γ.
plausibility evaluation.
                                                          (3) To generate an observation for r, draw a class
   Given a dataset of predicate-argument combina-
                                                              z from Θr , then draw an argument type n
tions and values for the hyperparameters α and β,
                                                              from Φz and a predicate type v from Ψz .
the probability model is determined by the class
assignment counts fzn and fzv . Following Grif-         The resulting model can be written as:
fiths and Steyvers (2004), we estimate the model                     X
by Gibbs sampling. This involves resampling the         P (n, v|r) =    P (n|z)P (v|z)P (z|r)              (3)
topic assignment for each observation in turn using                       z
probabilities estimated from all other observations.                     X fzn + β fzv + γ        fzr + αz
                                                                     ∝                               P
One efficiency bottleneck in the basic sampler de-                         fz· + N β fz· + V γ f·r +    z 0 αz 0
                                                                         z
scribed by Griffiths and Steyvers is that the entire                                                       (4)
set of topics must be iterated over for each observa-
tion. Yao et al. (2009) propose a reformulation that    As suggested by the similarity between (4) and (2),
removes this bottleneck by separating the probabil-     the ROOTH-LDA model can be estimated by an
ity mass p(z|n, v) into a number of buckets, some       LDA-like Gibbs sampling procedure.
of which only require iterating over the topics cur-       Unlike LDA, ROOTH-LDA does model the joint
rently assigned to instances of type n, typically far   probability P (n, v|r) of a predicate and argument
fewer than the total number of topics. It is possible   co-occurring. Further differences are that infor-
to apply similar reformulations to the models pre-      mation about predicate-argument co-occurrence is
sented in Sections 3.3 and 3.4 below; depending on      only shared within a given interaction class rather
the model and parameterisation this can reduce the      than across the whole dataset and that the distribu-
running time dramatically.                              tion Φz is not specific to the predicate v but rather
   Unlike some topic models such as HDP (Teh et         to the relation r. This could potentially lead to a
al., 2006), LDA is parametric: the number of top-       loss of model quality, but in practice the ability to
ics Z must be set by the user in advance. However,      induce “tighter” clusters seems to counteract any
Wallach et al. (2009) demonstrate that LDA is rela-     deterioration this causes.
tively insensitive to larger-than-necessary choices       3.4    A “dual-topic” model
of Z when the Dirichlet parameters α are optimised
                                                          In our third model, we attempt to combine the ad-
as part of model estimation. In our implementation
                                                          vantages of LDA and ROOTH-LDA by cluster-
we use the optimisation routines provided as part
                                                          ing arguments and predicates according to separate
of the Mallet library, which use an iterative proce-
                                                            2
dure to compute a maximum likelihood estimate of                http://mallet.cs.umass.edu/


                                                    438


class vocabularies. Each observation is generated         their predictions and human judgements of plausi-
by two latent variables rather than one, which po-        bility on a dataset of predicate-argument pairs. This
tentially allows the model to learn more flexible         can be viewed as a more semantically relevant mea-
interactions between arguments and predicates.:           surement of model quality than likelihood-based
                                                          methods, and also permits comparison with non-
 (1) For each relation r, draw a multinomial distri-      probabilistic models. In Section 5, we use two
     bution Ξr over predicate classes from a Dirich-      plausibility datasets to evaluate our models and
     let with parameters κ.                               compare to other previously published results.
 (2) For each predicate class c, draw a multinomial          We trained our models on the 90-million word
     Ψc over predicate types and a multinomial Θc         written component of the British National Corpus
     over argument classes from Dirichlets with           (Burnard, 1995), parsed with the RASP toolkit
     parameters γ and α respectively.                     (Briscoe et al., 2006). Predicates occurring with
                                                          just one argument type were removed, as were all
 (3) For each argument class z, draw a multinomial        tokens containing non-alphabetic characters; no
     distribution Φz over argument types from a           other filtering was done. The resulting datasets con-
     Dirichlet with parameters β.                         sisted of 3,587,172 verb-object observations with
                                                          7,954 predicate types and 80,107 argument types,
 (4) To generate an observation for r, draw a predi-      3,732,470 noun-noun observations with 68,303
     cate class c from Ξr , a predicate type from Ψc ,    predicate types and 105,425 argument types, and
     an argument class z from Θc and an argument          3,843,346 adjective-noun observations with 29,975
     type from Φz .                                       predicate types and 62,595 argument types.
                                                             During development we used the verb-noun plau-
  The resulting model can be written as:
                                                          sibility dataset from Padó et al. (2007) to direct
P (n, v|r) =
             XX
                    P (n|z)P (z|c)P (v|c)P (c|r)          the design of the system. Unless stated other-
               c    z
                                                          wise, all results are based on runs of 1,000 iter-
                                          (5)             ations with 100 classes, with a 200-iteration burnin
             X X fzn + β      fzc + αz                    period after which hyperparameters were reesti-
           ∝                     P        ×               mated every 50 iterations.3 The probabilities es-
             c z
                 fz· + N β f·c + z 0 αz 0
                                                          timated by the models (P (n|v, r) for LDA and
                         fcv + γ     fcr + κc             P (n, v|r) for ROOTH- and DUAL-LDA) were
                                         P        (6)
                        fc· + V γ f·r + c0 κc0            sampled every 50 iterations post-burnin and av-
                                                          eraged over three runs to smooth out variance.
To estimate this model, we first resample the class
                                                          To compare plausibility scores for different pred-
assignments for all arguments in the data and
                                                          icates, we require the joint probability P (n, v|r);
then resample class assignments for all predicates.
                                                          as LDA does not provide this, we approximate
Other approaches are possible – resampling argu-
                                                          PLDA (n, v|r) = PBN C (v|r)PLDA (n|v, r), where
ment and then predicate class assignments for each
                                                          PBN C (v|r) is proportional to the frequency with
observation in turn, or sampling argument and pred-
                                                          which predicate v is observed as an instance of
icate assignments together by blocked sampling –
                                                          relation r in the BNC.
though from our experiments it does not seem that
                                                             For comparison, we reimplemented the methods
the choice of scheme makes a significant differ-
                                                          of Rooth et al. (1999) and Padó et al. (2007). As
ence.
                                                          mentioned above, Rooth et al. use a latent-variable
4   Experimental setup                                    model similar to (4) but without priors, trained
                                                          via EM. Our implementation (henceforth ROOTH-
In the document modelling literature, probabilistic       EM) chooses the number of classes from the range
topic models are often evaluated on the likelihood        (20, 25, . . . , 50) through 5-fold cross-validation on
they assign to unseen documents; however, it has          a held-out log-likelihood measure. Settings outside
been shown that higher log likelihood scores do           this range did not give good results. Again, we run
not necessarily correlate with more semantically          for 1,000 iterations and average predictions over
coherent induced topics (Chang et al., 2009). One                3
                                                                  These settings were based on the MALLET defaults; we
popular method for evaluating selectional prefer-             have not yet investigated whether modifying the simulation
ence models is by testing the correlation between             length or burnin period is beneficial.


                                                        439


          LDA             0     Nouns:   agreement, contract, permission, treaty, deal, . . .
                          1     Nouns    information, datum, detail, evidence, material, . . .
                          2     Nouns    skill, knowledge, country, technique, understanding, . . .
          ROOTH-LDA       0     Nouns    force, team, army, group, troops, . . .
                          0     Verbs    join, arm, lead, beat, send, . . .
                          1     Nouns    door, eye, mouth, window, gate, . . .
                          1     Verbs    open, close, shut, lock, slam, . . .
          DUAL-LDA        0N    Nouns    house, building, site, home, station, . . .
                          1N    Nouns    stone, foot, bit, breath, line, . . .
                          0V    Verbs    involve, join, lead, represent, concern, . . .
                          1V    Verbs    see, break, have, turn, round, . . .
          ROOTH-EM        0     Nouns    system, method, technique, skill, model, . . .
                          0     Verbs    use, develop, apply, design, introduce, . . .
                          1     Nouns    eye, door, page, face, chapter,. . .
                          1     Verbs    see, open, close, watch, keep,. . .

      Table 1: Most probable words for sample semantic classes induced from verb-object observations


three runs. Padó et al. (2007), a refinement of Erk     to clustering verbs: DUAL-LDA’s class 0V, like
(2007), is a non-probabilistic method that smooths       ROOTH-LDA’s class 0, has verbs that take groups
predicate-argument counts with counts for other ob-      as objects but its class 1V mixes sensible confla-
served arguments of the same predicate, weighted         tions (turn, round) with very common verbs such as
by the similarity between arguments. Following           see and have and the unrelated break. The general
their description, we use a 2,000-dimensional space      impression given by inspection of the DUAL-LDA
of syntactic co-occurrence features appropriate to       model is that it has problems with mixing and does
the relation being predicted, weight features with       not manage to learn a good model; we have tried
the G2 transformation and compute similarity with        a number of solutions (e.g., blocked sampling of
the cosine measure.                                      argument and predicate classes), without overcom-
                                                         ing this brittleness. Unsurprisingly, ROOTH-EM’s
5     Results                                            classes have a similar feel to ROOTH-LDA; our
                                                         general impression is that some of ROOTH-EM’s
5.1     Induced semantic classes                         classes look even more coherent than the LDA-
                                                         based models, presumably because it does not use
Table 1 shows sample semantic classes induced by
                                                         priors to smooth its per-class distributions.
models trained on the corpus of BNC verb-object
co-occurrences. LDA clusters nouns only, while
                                                         5.2   Comparison with Keller and Lapata
ROOTH-LDA and ROOTH-EM learn classes that
                                                               (2003)
generate both nouns and verbs and DUAL-LDA
clusters nouns and verbs separately. The LDA clus-     Keller and Lapata (2003) collected a dataset of
ters are generally sensible: class 0 is exemplified    human plausibility judgements for three classes
by agreement and contract and class 1 by informa-      of grammatical relation: verb-object, noun-noun
tion and datum. There are some unintuitive blips,      modification and adjective-noun modification. The
for example country appears between knowledge          items in this dataset were not chosen to balance
and understanding in class 2. The ROOTH-LDA            plausibility and implausibility (as in prior psy-
classes also feel right: class 0 deals with nouns      cholinguistic experiments) but according to their
such as force, team and army which one might join,     corpus frequency, leading to a more realistic task.
arm or lead and class 1 corresponds to “things that    30 predicates were selected for each relation;
can be opened or closed” such as a door, an eye or a   each predicate was matched with three arguments
mouth (though the model also makes the question-       from different co-occurrence bands in the BNC,
able prediction that all these items can plausibly     e.g., naughty-girl (high frequency), naughty-dog
be locked or slammed). The DUAL-LDA classes            (medium) and naughty-lunch (low). Each predicate
are notably less coherent, especially when it comes    was also matched with three random arguments


                                                   440


                               Verb-object                         Noun-noun                       Adjective-noun
                            Seen       Unseen                   Seen       Unseen                 Seen        Unseen
                          r      ρ     r    ρ                 r      ρ     r    ρ               r      ρ      r    ρ
  AltaVista (KL)        .641     –   .551   –               .700     –   .578   –             .650     –    .480   –
  Google (KL)           .624     –   .520   –               .692     –   .595   –             .641     –    .473   –
  BNC (RASP)            .620 .614 .196 .222                 .544 .604 .114 .125               .543 .622 .135 .102
  ROOTH-EM              .455 .487 .479 .520                 .503 .491 .586 .625               .514 .463 .395 .355
  Padó et al.          .484 .490 .398 .430                 .431 .503 .558 .533               .479 .570 .120 .138
  LDA                   .504 .541 .558 .603                 .615 .641 .636 .666               .594 .558 .468 .459
  ROOTH-LDA             .520 .548 .564 .605                 .607 .622 .691 .722               .575 .599 .501 .469
  DUAL-LDA              .453 .494 .446 .516                 .496 .494 .553 .573               .460 .400 .334 .278

Table 2: Results (Pearson r and Spearman ρ correlations) on Keller and Lapata’s (2003) plausibility data


with which it does not co-occur in the BNC (e.g.,                 seen dataset. LDA also performs consistently well,
naughty-regime, naughty-rival, naughty-protocol).                 surpassing ROOTH-EM and Padó et al. on all but
In this way two datasets (Seen and Unseen) of 90                  one occasion. For frequent predicate-argument
items each were assembled for each predicate.                     pairs (Seen datasets), Web counts are clearly better;
   Table 2 presents results for a variety of predictive           however, the BNC counts are unambiguously supe-
models – the Web frequencies reported by Keller                   rior to LDA and ROOTH-LDA (whose predictions
and Lapata (2003) for two search engines, frequen-                are based entirely on the generative model even for
cies from the RASP-parsed BNC,4 the reimple-                      observed items) for the Seen verb-object data only.
mented methods of Rooth et al. (1999) and Padó et                As might be suspected from the mixing problems
al. (2007), and the LDA, ROOTH-LDA and DUAL-                      observed with DUAL-LDA, this model does not
LDA topic models. Following Keller and Lapata,                    perform as well as LDA and ROOTH-LDA, though
we report Pearson correlation coefficients between                it does hold its own against the other selectional
log-transformed predicted frequencies and the gold-               preference methods.
standard plausibility scores (which are already log-                 To identify significant differences between mod-
transformed). We also report Spearman rank cor-                   els, we use the statistical test for correlated corre-
relations except where we do not have the origi-                  lation coefficients proposed by Meng et al. (1992),
nal predictions (the Web count models), for com-                  which is appropriate for correlations that share
pleteness and because the predictions of preference               the same gold standard.5 For the seen data there
models are may not be log-normally distributed as                 are few significant differences: ROOTH-LDA and
corpus counts are. Zero values (found only in the                 LDA are significantly better (p < 0.01) than Padó
BNC frequency predictions) were smoothed by 0.1                   et al.’s model for Pearson’s r on seen noun-noun
to facilitate the log transformation; it seems natural            data, and ROOTH-LDA is also significantly better
to take a zero prediction as a non-specific predic-               (p < 0.01) using Spearman’s ρ. For the unseen
tion of very low plausibility rather than a “missing              datasets, the BNC frequency predictions are unsur-
value” as is done in other work (e.g., Padó et al.,              prisingly significantly worse at the p < 0.01 level
2007).                                                            than all smoothing models. LDA and ROOTH-
   Despite their structural differences, LDA and                  LDA are significantly better (p < 0.01) than Padó
ROOTH-LDA perform similarly - indeed, their                       et al. on every unseen dataset; ROOTH-EM is sig-
predictions are highly correlated. ROOTH-LDA                      nificantly better (p < 0.01) than Padó et al. on
scores best overall, outperforming Padó et al.’s                 Unseen adjectives for both correlations. Meng et
(2007) method and ROOTH-EM on every dataset                       al.’s test does not find significant differences be-
and evaluation measure, and outperforming Keller                  tween ROOTH-EM and the LDA models despite
and Lapata’s (2003) Web predictions on every Un-                  the latter’s clear advantages (a number of condi-
                                                                  tions do come close). This is because their pre-
    4
      The correlations presented here for BNC counts are no-      dictions are highly correlated, which is perhaps
tably better than those reported by Keller and Lapata (2003),
                                                                       5
presumably reflecting our use of full parsing rather than shal-          We cannot compare our data to Keller and Lapata’s Web
low parsing.                                                        counts as we do not possess their per-item scores.


                                                              441


       1                                          1                                              1

      0.9                                        0.9                                            0.9

      0.8                                        0.8                                            0.8

      0.7                                        0.7                                            0.7

      0.6                                        0.6                                            0.6

      0.5                                        0.5                                            0.5
  ρ




                                             ρ




                                                                                            ρ
      0.4                                        0.4                                            0.4

      0.3                                        0.3                                            0.3

      0.2                                        0.2                                            0.2

      0.1                                        0.1                                            0.1

       0                                          0                                              0
            50       100         150   200             50       100        150     200                50     100         150   200
                      No. of classes                            No. of classes                                No. of classes

                 (a) Verb-object                            (b) Noun-noun                             (c) Adjective-noun

Figure 1: Effect of number of argument classes on Spearman rank correlation with LDA: the solid and
dotted lines show the Seen and Unseen datasets respectively; bars show locations of individual samples


unsurprising given that they are structurally similar                  servations” of class distributions are available when
models trained on the same data. We hypothesise                        reestimating them.
that the main reason for the superior numerical per-
formance of the LDA models over EM is the prin-                        5.3        Comparison with Bergsma et al. (2008)
cipled smoothing provided by the use of Dirichlet                      As mentioned in Section 2.1, Bergsma et al. (2008)
priors, which has a small but discriminative effect                    propose a discriminative approach to preference
on model predictions. Collating the significance                       learning. As part of their evaluation, they compare
scores, we find that ROOTH-LDA achieves the                            their approach to a number of others, including
most positive outcomes, followed by LDA and then                       that of Erk (2007), on a plausibility dataset col-
by ROOTH-EM. DUAL-LDA is found significantly                           lected by Holmes et al. (1989). This dataset con-
better than Padó et al.’s model on unseen adjective-                  sists of 16 verbs, each paired with one plausible
noun combinations, and significantly worse than                        object (e.g., write-letter) and one implausible ob-
the same model on seen adjective-noun data.                            ject (write-market). Bergsma et al.’s model, trained
   Latent variable models that use EM for infer-                       on the 3GB AQUAINT corpus, is the only model
ence can be very sensitive to the number of latent                     reported to achieve perfect accuracy on distinguish-
variables chosen. For example, the performance                         ing plausible from implausible arguments. It would
of ROOTH-EM worsens quickly if the number of                           be interesting to do a full comparison that controls
clusters is overestimated; for the Keller and Lap-                     for size and type of corpus data; in the meantime,
ata datasets, settings above 50 classes lead to clear                  we can report that the LDA and ROOTH-LDA
overfitting and a precipitous drop in Pearson cor-                     models trained on verb-object observations in the
relation scores. On the other hand, Wallach et al.                     BNC (about 4 times smaller than AQUAINT) also
(2009) demonstrate that LDA is relatively insensi-                     achieve a perfect score on the Holmes et al. data.6
tive to the choice of topic vocabulary size Z when
                                                                       6         Conclusions and future work
the α and β hyperparameters are optimised appro-
priately during estimation. Figure 1 plots the effect                  This paper has demonstrated how Bayesian tech-
of Z on Spearman correlation for the LDA model.                        niques originally developed for modelling the top-
In general, Wallach et al.’s finding for document                      ical structure of documents can be adapted to
modelling transfers to selectional preference mod-                     learn probabilistic models of selectional preference.
els; within the range Z = 50–200 performance                           These models are especially effective for estimat-
remains at a roughly similar level. In fact, we do                     ing plausibility of low-frequency items, thus distin-
not find that performance becomes significantly                        guishing rarity from clear implausibility.
less robust when hyperparameter reestimation is                           The models presented here derive their predic-
deactiviated; correlation scores simply drop by a                      tions by modelling predicate-argument plausibility
small amount (1–2 points), irrespective of the Z                       through the intermediary of latent variables. As
chosen. ROOTH-LDA (not graphed) seems slightly                         observed in Section 5.2 this may be a suboptimal
more sensitive to Z; this may be because the α pa-                           6
                                                                            Bergsma et al. report that all plausible pairs were seen in
rameters in this model operate on the relation level                   their corpus; three were unseen in ours, as well as 12 of the
rather than the document level and thus fewer “ob-                     implausible pairs.


                                                                442


strategy for frequent combinations, where corpus          Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
counts are probably reliable and plausibility judge-        The second release of the RASP system. In Pro-
                                                            ceedings of the ACL-06 Interactive Presentation Ses-
ments may be affected by lexical collocation ef-
                                                            sions, Sydney, Australia.
fects. One principled method for folding corpus
counts into LDA-like models would be to use hi-           Samuel Brody and Mirella Lapata. 2009. Bayesian
erarchical priors, as in the n-gram topic model of          word sense induction. In Proceedings of EACL-09,
                                                            Athens, Greece.
Wallach (2006). Another potential direction for
system improvement would be an integration of             Lou Burnard, 1995. Users’ Guide for the British Na-
our generative model with Bergsma et al.’s (2008)           tional Corpus. British National Corpus Consortium,
discriminative model – this could be done in a num-         Oxford University Computing Service, Oxford, UK.
ber of ways, including using the induced classes          Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
of a topic model as features for a discriminative           Chong Wang, and David M. Blei. 2009. Reading
classifier or using the discriminative classifier to        tea leaves: How humans interpret topic models. In
produce additional high-quality training data from          Proceedings of NIPS-09, Vancouver, BC.
noisy unparsed text.                                      Stephen Clark and David Weir. 2002. Class-based
   Comparison to plausibility judgements gives an            probability estimation using a semantic hierarchy.
intrinsic measure of model quality. As mentioned             Computational Linguistics, 28(2):187–206.
in the Introduction, selectional preferences have
                                                          Katrin Erk. 2007. A simple, similarity-based model
many uses in NLP applications, and it will be inter-        for selectional preferences. In Proceedings of ACL-
esting to evaluate the utility of Bayesian preference       07, Prague, Czech Republic.
models in contexts such as semantic role labelling
                                                          Jenny Rose Finkel, Trond Grenager, and Christopher D.
or human sentence processing modelling. The prob-            Manning. 2007. The infinite tree. In Proceedings of
abilistic nature of topic models, coupled with an           ACL-07, Prague, Czech Republic.
appropriate probabilistic task model, may facilitate
the integration of class induction and task learning      Daniel Gildea and Daniel Jurafsky. 2002. Automatic
                                                            labeling of semantic roles. Computational Linguis-
in a tight and principled way. We also anticipate           tics, 28(3):245–288.
that latent variable models will prove effective for
learning selectional preferences of semantic predi-       Sharon Goldwater, Thomas L. Griffiths, and Mark
cates (e.g., FrameNet roles) where direct estimation        Johnson. 2009. A Bayesian framework for word
                                                            segmentation: Exploring the effects of context. Cog-
from a large corpus is not a viable option.                 nition, 112(1):21–54.

Acknowledgements                                          Thomas L. Griffiths and Mark Steyvers. 2004. Find-
                                                            ing scientific topics. Proceedings of the National
This work was supported by EPSRC grant                      Academy of Sciences, 101(suppl. 1):5228–5235.
EP/G051070/1. I am grateful to Frank Keller and
                                                          Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Mirella Lapata for sharing their plausibility data,         Tenenbaum. 2007. Topics in semantic representa-
and to Andreas Vlachos and the anonymous ACL                tion. Psychological Review, 114(2):211–244.
and CoNLL reviewers for their helpful comments.
                                                          Virginia M. Holmes, Laurie Stowe, and Linda Cupples.
                                                            1989. Lexical expectations in parsing complement-
                                                            verb sentences. Journal of Memory and Language,
References                                                  28(6):668–689.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.        Frank Keller and Mirella Lapata. 2003. Using the Web
  Discriminative learning of selectional preferences        to obtain frequencies for unseen bigrams. Computa-
  from unlabeled text. In Proceedings of EMNLP-08,          tional Linguistics, 29(3):459–484.
  Honolulu, HI.
                                                          Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.        Alexander Hauptmann. 2006. Which side are you
  2003. Latent Dirichlet allocation. Journal of Ma-         on? Identifying perspectives at the document and
  chine Learning Research, 3:993–1022.                      sentence levels. In Proceedings of CoNLL-06, New
                                                           York, NY.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
   2007. A topic model for word sense disambigua-         Xiao-Li Meng, Robert Rosenthal, and Donald B. Rubin.
   tion. In Proceedings of EMNLP-CoNLL-07, Prague,          1992. Comparing correlated correlation coefficients.
   Czech Republic.                                          Psychological Bulletin, 111(1):172–175.


                                                    443


Sebastian Padó, Ulrike Padó, and Katrin Erk. 2007.          Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong
  Flexible, corpus-based modelling of human plau-               Wen. 2009. Employing topic models for pattern-
  sibility judgements. In Proceedings of EMNLP-                 based semantic class discovery. In Proceedings of
  CoNLL-07, Prague, Czech Republic.                             ACL-IJCNLP-09, Singapore.

Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
  Timothy Chklovski, and Eduard Hovy. 2007. ISP:
  Learning inferential selectional preferences. In Pro-
  ceedings of NAACL-HLT-07, Rochester, NY.

Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-
  mon P. Liversedge. 2004. The effect of plausibility
  on eye movements in reading. Journal of Experi-
  mental Psychology: Learning Memory and Cogni-
  tion, 30(6):1290–1301.

Joseph Reisinger and Marius Paşca. 2009. Latent vari-
   able models of concept-attribute attachment. In Pro-
   ceedings of ACL-IJCNLP-09, Singapore.

Philip S. Resnik. 1993. Selection and Information:
  A Class-Based Approach to Lexical Relationships.
  Ph.D. thesis, University of Pennsylvania.

Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
  tent Dirichlet Allocation method for selectional pref-
  erences. In Proceedings of ACL-10, Uppsala, Swe-
  den.

Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
 roll, and Franz Beil. 1999. Inducing a semantically
 annotated lexicon via EM-based clustering. In Pro-
 ceedings of ACL-99, College Park, MD.

Sabine Schulte im Walde, Christian Hying, Christian
  Scheible, and Helmut Schmid. 2008. Combining
  EM training and the MDL principle for an automatic
  verb classification incorporating selectional prefer-
  ences. In Proceedings of ACL-08:HLT, Columbus,
  OH.

Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
  David M. Blei. 2006. Hierarchical Dirichlet pro-
  cesses. Journal of the American Statistical Associa-
  tion, 101(476):1566–1581.

Hanna Wallach, David Mimno, and Andrew McCallum.
  2009. Rethinking LDA: Why priors matter. In Pro-
  ceedings of NIPS-09, Vancouver, BC.

Hanna Wallach. 2006. Topic modeling: Beyond bag-
  of-words. In Proceedings of ICML-06, Pittsburgh,
  PA.

Yorick Wilks. 1978. Making preferences more active.
  Artificial Intelligence, 11:197–225.

Limin Yao, David Mimno, and Andrew McCallum.
  2009. Efficient methods for topic model inference
  on streaming document collections. In Proceedings
  of KDD-09, Paris, France.

Beñat Zapirain, Eneko Agirre, and Lluı́s Màrquez.
  2009. Generalizing over lexical features: Selec-
  tional preferences for semantic role classification. In
  Proceedings of ACL-IJCNLP-09, Singapore.


                                                        444

                    Efficient Staggered Decoding for Sequence Labeling

        Nobuhiro Kaji  Yasuhiro Fujiwara Naoki Yoshinaga Masaru Kitsuregawa
                               Institute of Industrial Science,
                                 The University of Tokyo,
                     4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan
             {kaji,fujiwara,ynaga,kisture}@tkl.iis.u-tokyo.ac.jp


                         Abstract                                 it becomes prohibitively slow when dealing with
                                                                  a large number of labels, since its computational
       The Viterbi algorithm is the conventional
                                                                  cost is quadratic in L (Dietterich et al., 2008).
       decoding algorithm most widely adopted
                                                                      Unfortunately, several sequence-labeling prob-
       for sequence labeling. Viterbi decoding
                                                                  lems in NLP involve a large number of labels. For
       is, however, prohibitively slow when the
                                                                  example, there are more than 40 and 2000 labels
       label set is large, because its time com-
                                                                  in POS tagging and supertagging, respectively
       plexity is quadratic in the number of la-
                                                                  (Brants, 2000; Matsuzaki et al., 2007). These
       bels. This paper proposes an exact decod-
                                                                  tasks incur much higher computational costs than
       ing algorithm that overcomes this prob-
                                                                  simpler tasks like NP chunking. What is worse,
       lem. A novel property of our algorithm is
                                                                  the number of labels grows drastically if we jointly
       that it efficiently reduces the labels to be
                                                                  perform multiple tasks. As we shall see later,
       decoded, while still allowing us to check
                                                                  we need over 300 labels to reduce joint POS tag-
       the optimality of the solution. Experi-
                                                                  ging and chunking into the single sequence label-
       ments on three tasks (POS tagging, joint
                                                                  ing problem. Although joint learning has attracted
       POS tagging and chunking, and supertag-
                                                                  much attention in recent years, how to perform de-
       ging) show that the new algorithm is sev-
                                                                  coding efficiently still remains an open problem.
       eral orders of magnitude faster than the
                                                                      In this paper, we present a new decoding algo-
       basic Viterbi and a state-of-the-art algo-
                                                                  rithm that overcomes this problem. The proposed
       rithm, C ARPE D IEM (Esposito and Radi-
                                                                  algorithm has three distinguishing properties: (1)
       cioni, 2009).
                                                                  It is much more efficient than the Viterbi algorithm
1 Introduction                                                    when dealing with a large number of labels. (2) It
                                                                  is an exact algorithm, that is, the optimality of the
In the past decade, sequence labeling algorithms
                                                                  solution is always guaranteed unlike approximate
such as HMMs, CRFs, and Collins’ perceptrons
                                                                  algorithms. (3) It is automatic, requiring no task-
have been extensively studied in the field of NLP                 dependent hyperparameters that have to be manu-
(Rabiner, 1989; Lafferty et al., 2001; Collins,                   ally adjusted.
2002). Now they are indispensable in a wide range                     Experiments evaluate our algorithm on three
of NLP tasks including chunking, POS tagging,                     tasks: POS tagging, joint POS tagging and chunk-
NER and so on (Sha and Pereira, 2003; Tsuruoka                    ing, and supertagging2 . The results demonstrate
and Tsujii, 2005; Lin and Wu, 2009).                              that our algorithm is up to several orders of mag-
   One important task in sequence labeling is how                 nitude faster than the basic Viterbi algorithm and a
to find the most probable label sequence from
                                                                  state-of-the-art algorithm (Esposito and Radicioni,
among all possible ones. This task, referred to as
                                                                  2009); it makes exact decoding practical even in
decoding, is usually carried out using the Viterbi
                                                                  labeling problems with a large label set.
algorithm (Viterbi, 1967). The Viterbi algorithm
has O(N L2 ) time complexity,1 where N is the                     2 Preliminaries
input size and L is the number of labels. Al-
though the Viterbi algorithm is generally efficient,              We first provide a brief overview of sequence la-
   1                                                              beling and introduce related work.
     The first-order Markov assumption is made throughout
                                                                     2
this paper, although our algorithm is applicable to higher-            Our implementation is available at http://www.tkl.iis.u-
order Markov models as well.                                      tokyo.ac.jp/˜kaji/staggered


                                                            485
           Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485–494,
                    Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


2.1    Models                                                        corresponding to it. Parameter w can be estimated
Sequence labeling is the problem of predicting la-                   in the same way as in the conventional perceptron
bel sequence y = {yn }N   n=1 for given token se-
                                                                     algorithm. See (Collins, 2002) for details.
quence x = {xn }N  n=1 . This is typically done by                   2.2    Viterbi decoding
defining a score function f (x, y) and locating the
best label sequence: ymax = argmax f (x, y).                         Given the score function f (x, y), we have to lo-
                                             y                       cate the best label sequence. This is usually per-
The form of f (x, y) is dependent on the learn-                      formed by applying the Viterbi algorithm. Let
ing model used. Here, we introduce two models                        ω(yn ) be the best score of the partial label se-
widely used in the literature.                                       quence ending with yn . The idea of the Viterbi
Generative models HMM is the most famous                             algorithm is to use dynamic programming to com-
generative model for labeling token sequences                        pute ω(yn ). In HMMs, ω(yn ) can be can be de-
(Rabiner, 1989). In HMMs, the score function                         fined as
f (x, y) is the joint probability distribution over
                                                                     max{ω(yn−1 ) + log p(yn |yn−1 )} + log p(xn |yn ).
(x, y). If we assume a one-to-one correspondence                     yn−1
between the hidden states and the labels, the score
function can be written as:                                          Using this recursive definition, we can evaluate
                                                                     ω(yn ) for all yn . This results in the identification
f (x, y) = log p(x, y)                                               of the best label sequence.
         = log p(x|y) + log p(y)                                        Although the Viterbi algorithm is commonly
                                                                     adopted in past studies, it is not always efficient.
             N
                                    N
                                                                    The computational cost of the Viterbi algorithm is
         =         log p(xn |yn )+         log p(yn |yn−1 ).         O(N L2 ), where N is the input length and L is
             n=1                     n=1
                                                                     the number of labels; it is efficient enough if L
The parameters log p(xn |yn ) and log p(yn |yn−1 )                   is small. However, if there are many labels, the
are usually estimated using maximum likelihood                       Viterbi algorithm becomes prohibitively slow be-
or the EM algorithm. Since parameter estimation                      cause of its quadratic dependence on L.
lies outside the scope of this paper, a detailed de-
scription is omitted.                                                2.3    Related work
                                                                     To the best of our knowledge, the Viterbi algo-
Discriminative models Recent years have seen
                                                                     rithm is the only algorithm widely adopted in the
the emergence of discriminative training methods
                                                                     NLP field that offers exact decoding. In other
for sequence labeling (Lafferty et al., 2001; Tasker
                                                                     communities, several exact algorithms have al-
et al., 2003; Collins, 2002; Tsochantaridis et al.,
                                                                     ready been proposed for handling large label sets.
2005). Among them, we focus on the perceptron
                                                                     While they are successful to some extent, they de-
algorithm (Collins, 2002). Although we do not
                                                                     mand strong assumptions that are unusual in NLP.
discuss the other discriminative models, our algo-
                                                                     Moreover, none were challenged with standard
rithm is equivalently applicable to them. The ma-
                                                                     NLP tasks.
jor difference between those models lies in param-
                                                                        Felzenszwalb et al. (2003) presented a fast
eter estimation; the decoding process is virtually
                                                                     inference algorithm for HMMs based on the as-
the same.
                                                                     sumption that the hidden states can be embed-
   In the perceptron, the score function f (x, y) is
                                                                     ded in a grid space, and the transition probabil-
given as f (x, y) = w · φ(x, y) where w is the
                                                                     ity corresponds to the distance on that space. This
weight vector, and φ(x, y) is the feature vector
                                                                     type of probability distribution is not common in
representation of the pair (x, y). By making the
                                                                     NLP tasks. Lifshits et al. (2007) proposed a
first-order Markov assumption, we have
                                                                     compression-based approach to speed up HMM
      f (x, y) = w · φ(x, y)                                         decoding. It assumes that the input sequence is
                        K
                      N 
                                                                    highly repetitive. Amongst others, C ARPE D IEM
                =               wk φk (x, yn−1 , yn ),               (Esposito and Radicioni, 2009) is the algorithm
                      n=1 k=1                                        closest to our work. It accelerates decoding by
where K = |φ(x, y)| is the number of features, φk                    assuming that the adjacent labels are not strongly
is the k-th feature function, and wk is the weight                   correlated. This assumption is appropriate for


                                                               486


                                                                    A   A         A   A                 A        A         A   A
some NLP tasks. For example, as suggested in                        B   B         B   B                  B                 B   B
(Liang et al., 2008), adjacent labels do not provide                C   C         C   C                  C                     C
                                                                    D   D         D   D                 D                      D
strong information in POS tagging. However, the                     E   E         E   E

applicability of this idea to other NLP tasks is still              F   F         F   F
                                                                    G   G         G   G
unclear.                                                            H   H         H   H

   Approximate algorithms, such as beam search                              (a)                                      (b)
or island-driven search, have been proposed for
speeding up decoding. Tsuruoka and Tsujii (2005)           Figure 1: (a) An example of a lattice, where the
proposed easiest-first deterministic decoding. Sid-        letters {A, B, C, D, E, F, G, H} represent labels
diqi and Moore (2005) presented the parameter ty-          associated with nodes. (b) The degenerate lattice.
ing approach for fast inference in HMMs. A simi-
lar idea was applied to CRFs as well (Cohn, 2006;
                                                           the perceptron algorithm is presented in Section 4.
Jeong et al., 2009).
   In general, approximate algorithms have the ad-         3.1    Degenerate lattice
vantage of speed over exact algorithms. However,           We begin by introducing the degenerate lattice,
both types of algorithms are still widely adopted          which plays a central role in our algorithm. Con-
by practitioners, since exact algorithms have mer-         sider the lattice in Figure 1(a). Following conven-
its other than speed. First, the optimality of the so-     tion, we regard each path on the lattice as a label
lution is always guaranteed. It is hard for most of        sequence. Note that the label set is {A, B, C, D,
the approximate algorithms to even bound the er-           E, F, G, H}. By aggregating several nodes in the
ror rate. Second, approximate algorithms usually           same column of the lattice, we can transform the
require hyperparameters, which control the trade-          original lattice into a simpler form, which we call
off between accuracy and efficiency (e.g., beam            the degenerate lattice (Figure 1(b)).
width), and these have to be manually adjusted.               Let us examine the intuition behind the degen-
On the other hand, most of the exact algorithms,           erate lattice. Aggregating nodes can be viewed as
including ours, do not require such a manual ef-           grouping several labels into a new one. Here, a
fort.                                                      label is referred to as an active label if it is not ag-
   Despite these advantages, exact algorithms are          gregated (e.g., A, B, C, and D in the first column
rarely used when dealing with a large number of            of Figure 1(b)), and otherwise as an inactive label
labels. This is because exact algorithms become            (i.e., dotted nodes). The new label, which is made
considerably slower than approximate algorithms            by grouping the inactive labels, is referred to as
in such situations. The paper presents an exact al-        a degenerate label (i.e., large nodes covering the
gorithm that avoids this problem; it provides the          dotted ones). Two degenerate labels can be seen
research community with another option for han-            as equivalent if their corresponding inactive label
dling a lot of labels.                                     sets are the same (e.g., degenerate labels in the first
                                                           and the last column). In this approach, each path
3 Algorithm                                                of the degenerate lattice can also be interpreted as
This section presents the new decoding algorithm.          a label sequence. In this case, however, the label to
The key is to reduce the number of labels ex-              be assigned is either an active label or a degenerate
amined. Our algorithm locates the best label se-           label.
quence by iteratively solving labeling problems               We then define the parameters associated with
with a reduced label set. This results in signifi-         degenerate label z. For reasons that will become
cant time savings in practice, because each itera-         clear later, they are set to the maxima among the
tion becomes much more efficient than solving the          parameters of the inactive labels:
original labeling problem. More importantly, our                 log p(x|z) = max log p(x|y  ),                                   (1)
algorithm always obtains the exact solution. This                                     y  ∈I(z)
is because the algorithm allows us to check the op-              log p(z|y) = max log p(y  |y),                                   (2)
timality of the solution achieved by using only the                                   y  ∈I(z)

reduced label set.                                               log p(y|z) = max log p(y|y  ),                                   (3)
                                                                                      y  ∈I(z)
   In the following discussions, we restrict our fo-
cus to HMMs for presentation clarity. Extension to               log p(z|z  ) =             max                log p(y  |y  ), (4)
                                                                                      y  ∈I(z),y  ∈I(z  )


                                                     487


       A     A         A   A       A    A         A   A               A     A         A   A   A   A         A   A   A   A         A   A
       B     B         B   B       B              B   B                                       B   B         B   B   B   B         B   B
       C     C         C   C       C                  C                                                             C   C         C   C
       D     D         D   D       D                  D                                                             D   D         D   D
       E     E         E   E
       F     F         F   F
       G     G         G   G
       H     H         H   H

                 (a)                        (b)                                 (a)                   (b)                   (c)
Figure 2: (a) The path y = {A, E, G, C} of the                        Figure 3: (a) The best path of the initial degenerate
original lattice. (b) The path z of the degenerate                    lattice, which is denoted by the line, is located. (b)
lattice that corresponds to y.                                        The active labels are expanded and the best path is
                                                                      searched again. (c) The best path without degen-
                                                                      erate labels is obtained.
where y is an active label, z and z are degenerate
labels, and I(z) denotes one-to-one mapping from
z to its corresponding inactive label set.                            3.2       Staggered decoding
   The degenerate lattice has an important prop-                      Now we can describe our algorithm, which we call
erty which is the key to our algorithm:                               staggered decoding. The algorithm successively
                                                                      constructs degenerate lattices and checks whether
Lemma 1. If the best path of the degenerate lat-                      the best path includes degenerate labels. In build-
tice does not include any degenerate label, it is                     ing each degenerate lattice, labels with high prob-
equivalent to the best path of the original lattice.                  ability p(y), estimated from training data, are pref-
                                                                      erentially selected as the active label; the expecta-
Proof. Let zmax be the best path of the degenerate                    tion is that such labels are likely to belong to the
lattice. Our goal is to prove that if zmax does not                   best path. The algorithm is detailed as follows:
include any degenerate label, then
                                                                      Initialization step The algorithm starts by build-
                                                                           ing a degenerate lattice in which there is only
  ∀y ∈ Y,         log p(x, y) ≤ log p(x, zmax )           (5)              one active label in each column. We select la-
                                                                           bel y with the highest p(y) as the active label.
where Y is the set of all paths on the original lat-
tice. We prove this by partitioning Y into two dis-                   Search step The best path of the degenerate lat-
joint sets: Y0 and Y1 , where Y0 is the subset of                         tice is located (Figure 3(a)). This is done
Y appearing in the degenerate lattice. Notice that                        by using the Viterbi algorithm (and pruning
zmax ∈ Y0 . Since zmax is the best path of the                            technique, as we describe in Section 3.3). If
degenerate lattice, we have                                               the best path does not include any degenerate
                                                                          label, we can terminate the algorithm since it
 ∀y ∈ Y0 ,         log p(x, y) ≤ log p(x, zmax ).                         is identical with the best path of the original
                                                                          lattice according to Lemma 1. Otherwise, we
The equation holds when y = zmax . We next ex-                            proceed to the next step.
amine the label sequence y such that y ∈ Y1 . For                     Expansion step We double the number of the ac-
each path y ∈ Y1 , there exists a unique path z on                        tive labels in the degenerate lattice. The new
the degenerate lattice that corresponds to y (Fig-                        active labels are selected from the current in-
ure 2). Therefore, we have                                                active label set in descending order of p(y).
                                                                          If the inactive label set becomes empty, we
∀y ∈ Y1 , ∃z ∈ Z, log p(x, y) ≤ log p(x, z)                               simply reconstructed the original lattice. Af-
                                       < log p(x, zmax )                  ter expanding the active labels, we go back to
                                                                          the previous step (Figure 3(b)). This proce-
where Z is the set of all paths of the degenerate                         dure is repeated until the termination condi-
lattice. The inequality log p(x, y) ≤ log p(x, z)                         tion in the search step is satisfied, i.e., the best
can be proved by using Equations (1)-(4). Using                           path has no degenerate label (Figure 3(c)).
these results, we can complete (5).                                     Compared to the Viterbi algorithm, staggered
                                                                      decoding requires two additional computations for


                                                                488


training. First, we have to estimate p(y) so as to           Presuming that we traverse the lattice from left to
select active labels in the initialization and expan-        right, ω(yn ) can be defined as
sion step. Second, we have to compute the pa-
rameters regarding degenerate labels according to            max{ω(yn−1 ) + log p(yn |yn−1 )} + log p(xn |yn ).
                                                             yn−1
Equations (1)-(4). Both impose trivial computa-
tion costs.                                                  If we traverse the lattice from right to left, an anal-
                                                             ogous score ω̄(yn ) can be defined as
3.3   Pruning
To achieve speed-up, it is crucial that staggered            log p(xn |yn ) + max{ω̄(yn+1 ) + log p(yn |yn+1 )}.
                                                                              yn+1
decoding efficiently performs the search step. For
this purpose, we can basically use the Viterbi algo-         Using these two scores, we have
rithm. In earlier iterations, the Viterbi algorithm is
                                                             M AX (yn ) = ω(yn ) + ω̄(yn ) − log p(xn |yn ).
indeed efficient because the label set to be han-
dled is much smaller than the original one. In later         Notice that updating ω(yn ) or ω̄(yn ) is equivalent
iterations, however, our algorithm drastically in-           to the forward or backward Viterbi algorithm, re-
creases the number of labels, making Viterbi de-             spectively.
coding quite expensive.                                         Although it is expensive to compute ω(yn ) and
   To handle this problem, we propose a method of            ω̄(yn ), we can efficiently estimate their upper
pruning the lattice nodes. This technique is moti-           bounds. Let λ(yn ) and λ̄(yn ) be scores analogous
vated by the observation that the degenerate lattice         to ω(yn ) and ω̄(yn ) that are computed using the
shares many active labels with the previous itera-           degenerate lattice. We have ω(yn ) ≤ λ(yn ) and
tion. In the remainder of Section3.3, we explain             ω̄(yn ) ≤ λ̄(yn ), by following similar discussions
the technique by taking the following steps:                 as raised in the proof of Lemma 1. Therefore, we
  • Section 3.3.1 examines a lower bound l such              can still check whether M AX(yn ) is smaller than l
    that l ≤ maxy log p(x, y).                               by using λ(yn ) and λ̄(yn ):

  • Section 3.3.2 examines the maximum score                   M AX (yn ) = ω(yn ) + ω̄(yn ) − log p(xn |yn )
    M AX(yn ) in case token xn takes label yn :                            ≤ λ(yn ) + λ̄(yn ) − log p(xn |yn )
                                                                           < l.
         M AX(yn ) = max
                     
                         log p(x, y  ).
                       yn =yn
                                                             For the sake of simplicity, we assume that yn is an
  • Section 3.3.3 presents our pruning procedure.            active label. Although we do not discuss the other
    The idea is that if M AX(yn ) < l, then the              cases, our pruning technique is also applicable to
    node corresponding to yn can be removed                  them. We just point out that, if yn is an inactive
    from consideration.                                      label, then there exists a degenerate label zn in the
                                                             n-th column such that yn ∈ I(zn ), and we can use
3.3.1 Lower bound                                            λ(zn ) and λ̄(zn ) instead of λ(yn ) and λ̄(yn ).
Lower bound l can be trivially calculated in the                We compute λ(yn ) and λ̄(yn ) by using the
search step. This can be done by retaining the               forward and backward Viterbi algorithm, respec-
best path among those consisting of only active              tively. In the search step immediately following
labels. The score of that path is obviously the              initialization, we perform the forward Viterbi al-
lower bound. Since the search step is repeated un-           gorithm to find the best path, that is, λ(yn ) is
til the termination criteria is met, we can update           updated for all yn . In the next search step, the
the lower bound at every search step. As the it-             backward Viterbi algorithm is carried out, and
eration proceeds, the degenerate lattice becomes             λ̄(yn ) is updated. In the succeeding search steps,
closer to the original one, so the lower bound be-           these updates are alternated. As the algorithm pro-
comes tighter.                                               gresses, λ(yn ) and λ̄(yn ) become closer to ω(yn )
                                                             and ω̄(yn ).
3.3.2 Maximum score
The maximum score M AX(yn ) can be computed                  3.3.3 Pruning procedure
from the original lattice. Let ω(yn ) be the best            We make use of the bounds in pruning the lattice
score of the partial label sequence ending with yn .         nodes. To do this, we keep the values of l, λ(yn )


                                                       489


and λ̄(yn ). They are set as l = −∞ and λ(yn ) =          A     A         A   A   A   A
                                                                                      B
                                                                                                A
                                                                                                B
                                                                                                    A
                                                                                                    B
                                                                                                        A   A
                                                                                                            B
                                                                                                                      A
                                                                                                                      B
                                                                                                                          A
                                                                                                                          B
λ̄(yn ) = ∞ in the initialization step, and are up-                                                         C             C

dated in the search step. The lower bound l is up-                                                          D             D


dated at the end of the search step, while λ(yn )
and λ̄(yn ) can be updated during the running of
the Viterbi algorithm. When λ(yn ) or λ̄(yn ) is                    (a)                   (b)                   (c)
changed, we check whether M AX(yn ) < l holds
and the node is pruned if the condition is met.           Figure 4: Staggered decoding with column-wise
                                                          expansion: (a) The best path of the initial degen-
3.4   Analysis                                            erate lattice, which does not pass through the de-
We provide here a theoretical analysis of staggered       generate label in the first column. (b) Column-
decoding. In the following proofs, L, V , and N           wise expansion is performed and the best path is
represent the number of original labels, the num-         searched again. Notice that the active label in the
ber of distinct tokens, and the length of input token     first column is not expanded. (c) The final result.
sequence, respectively. To simplify the discussion,
we assume that log2 L is an integer (e.g., L = 64).
  We first introduce three lemmas:                        memory space to perform Viterbi decoding in the
                                                          search step.
Lemma 2. Staggered decoding requires at most
(log2 L + 1) iterations to terminate.                     Theorem 2. Staggered decoding has O(N ) best
                                                          case time complexity and O(N L2 ) worst case time
Proof. We have 2m−1 active labels in the m-th
                                                          complexity.
search step (m = 1, 2 . . . ), which means we have
L active labels and no degenerate labels in the           Proof. To perform the m-th search step, staggered
(log2 L + 1)-th search step. Therefore, the algo-         decoding requires the order of O(N 4m−1 ) time
rithm always terminates within (log2 L + 1) itera-        becausewe have 2m−1 active labels. Therefore, it
tions.                                                    has O( M  m=1 N 4
                                                                            m−1 ) time complexity if it termi-

                                                          nates after the M -th search step. In the best case,
Lemma 3. The number of degenerate labels is
                                                          M = 1, the time complexity is O(N ). In the worst
log2 L.
                                                          case, M = log2 L + 1, the time complexity is the
                                                                                       log2 L+1
Proof. Since we create one new degenerate label           order of O(N L2 ) because m=1           N 4m−1 <
                                                          4     2
in all but the last expansion step, we have log2 L        3NL .
degenerate labels.
                                                             Theorem 1 shows that staggered decoding
Lemma 4. The Viterbi algorithm requires O(L2 +            asymptotically requires the same order of mem-
LV ) memory space and has O(N L2 ) time com-              ory space as the Viterbi algorithm. Theorem 2 re-
plexity.                                                  veals that staggered decoding has the same order
                                                          of time complexity as the Viterbi algorithm even
Proof. Since we need O(L2 ) and O(LV ) space to
                                                          in the worst case.
keep the transition and emission probability ma-
trices, we need O(L2 + LV ) space to perform              3.5       Heuristic techniques
the Viterbi algorithm. The time complexity of the
                                                          We present two heuristic techniques for further
Viterbi algorithm is O(N L2 ) since there are NL
                                                          speeding up our algorithm.
nodes in the lattice and it takes O(L) time to eval-
                                                             First, we can initialize the value of lower bound
uate the score of each node.
                                                          l by selecting a path from the original lattice in
  The above statements allow us to establish our          some way, and then computing the score of that
main results:                                             path. In our experiments, we use the path lo-
Theorem 1. Staggered decoding requires O(L2 +             cated by the left-to-right deterministic decoding
LV ) memory space.                                        (i.e., beam search with a beam width of 1). Al-
                                                          though this method requires an additional cost to
Proof. Since we have L original labels and log2 L         locate the path, it is very effective in practice. If
degenerate labels, staggered decoding requires            l is initialized in this manner, the best case time
O((L+log2 L)2 +(L+log2 L)V ) = O(L2 +LV )                 complexity of our algorithm becomes O(N L).


                                                    490


  The second technique is for the expansion step.                 the initialization step (cf. Equation (1)). This pre-
Instead of the expansion technique described in                   process requires O(N L) time, which is negligible
Section 3.2, we can expand the active labels in a                 compared with the cost required by the Viterbi al-
heuristic manner to keep the number of active la-                 gorithm.
bels small:                                                          Unfortunately, we cannot use       the2 same  tech-
                                                                  nique for computing maxy,y k wk φk (x, y, y  )
                                                                                                               2
Column-wise expansion step We double the                          because a similar computation would take
    number of the active labels in the column                     O(N L2 ) time (cf. Equation (4)). For bigram fea-
    only if the best path of the degenerate lattice               tures, we compute its upper bound offline. For ex-
    passes through the degenerate label of that                   ample, the following bound was proposed by Es-
    column (Figure 4).                                            posito and Radicioni (2009):
                                                                                                      
                                                                               2 2           
   A drawback of this strategy is that the algorithm              max       w  k φk (x, y, y   ) ≤ max     wk2 δ(0 < wk2 )
requires N (log2 L+1) iterations in the worst case.               y,y
                                                                            k
                                                                                                     
                                                                                                     y,y
                                                                                                           k
As the result, we can no longer derive a reasonable
upper bound for the time complexity. Neverthe-                    where δ(·) is the delta function and the summa-
less, column-wise expansion is highly effective in                tions are taken over all feature functions associated
practice as we will demonstrate in the experiment.                with both y and y . Intuitively, the upper bound
Note that Theorem 1 still holds true even if we use               corresponds to an ideal case in which all features
column-wise expansion.                                            with positive weight are activated.3 It can be com-
                                                                  puted without any task-specific knowledge.
4 Extension to the Perceptron                                        In practice, however, we can compute better
                                                                  bounds based on task-specific knowledge. The
The discussion we have made so far can be applied                 simplest case is that the bigram features are inde-
to perceptrons. This can be clarified by comparing                pendent of the token sequence x. In such a situ-
the score functions f (x, y). In HMMs, the score                  ation, we can trivially compute the exact maxima
function can be written as                                        offline, as we did in the case of HMMs. Fortu-
      N 
                                                                nately, such a feature set is quite common in NLP
         log(xn |yn ) + log(yn |yn−1 ) .                          problems and we could use this technique in our
      n=1                                                         experiments. Even if bigram features are depen-
                                                                  dent on x, it is still possible to compute better
In perceptrons, on the other hand, it is given as
                                                                  bounds if several features are mutually exclusive,
N 
                                                               as discussed in (Esposito and Radicioni, 2009).
            wk1 φ1k (x, yn ) +       wk2 φ2k (x, yn−1 , yn )         Finally, it is worth noting that we can use stag-
n=1    k                         k                                gered decoding in training perceptrons as well, al-
                                                                  though such application lies outside the scope of
where we explicitly distinguish the unigram fea-                  this paper. The algorithm does not support train-
ture function φ1k and bigram feature function φ2k .               ing acceleration for other discriminative models.
Comparing the form of the two functions, we can
see that our discussion on HMMs     can be extended              5 Experiments and Discussion
                                           1 1
to perceptrons
      2 2 by substituting            k wk φk (x, yn )
                                                                  5.1      Setting
and k wk φk (x, yn−1 , yn ) for log p(xn |yn ) and
log p(yn |yn−1 ).                                                 The proposed algorithm was evaluated with three
   However, implementing the perceptron algo-                     tasks: POS tagging, joint POS tagging and chunk-
rithm is not straightforward. The problem is                      ing (called joint tagging for short), and supertag-
that                                                              ging. To reduce joint tagging into a single se-
 it1 is1 difficult, ifnot impossible,
                             2 2         
                                           to compute
   k wk φk (x, y) and    k wk φk (x, y, y ) offline be-
                                                                  quence labeling problem, we produced the labels
cause they are dependent on the entire token se-                  by concatenating the POS tag and the chunk tag
quence x, unlike log p(x|y) and log p(y|y ). Con-                (BIO format), e.g., NN/B-NP. In the two tasks
sequently, we cannot evaluate the maxima analo-                   other than supertagging, the input token is the
gous to Equations (1)-(4) offline either.                         word. In supertagging, the token is the pair of the
   For unigram                                                    word and its oracle POS tag.
               features, we compute the maxi-
mum, maxy k wk1 φ1k (x, y), as a preprocess in                       3
                                                                         We assume binary feature functions.


                                                            491


       Table 1: Decoding speed (sent./sec).                            Table 2: The average number of iterations.
              POS tagging   Joint tagging   Supertagging                         POS tagging   Joint tagging   Supertagging
 V ITERBI            4000              77            1.1          SD                    6.02            8.15           10.0
 C ARPE D IEM        8600              51           0.26          SD+ C - EXP.          6.12            8.62           10.6
 SD                  8800             850            121
 SD+ C - EXP.      14,000            1600            300                          Table 3: Training time.
                                                                               POS tagging     Joint tagging   Supertagging
                                                                  V ITERBI        100 sec.           20 min.      100 hour
                                                                  SD+ C - EXP.      37 sec.         1.5 min.       5.3 hour
   The data sets we used for the three experiments
are the Penn TreeBank (PTB) corpus, CoNLL
2000 corpus, and an HPSG treebank built from the
                                                                 demonstrated that C ARPE D IEM worked poorly in
PTB corpus (Matsuzaki et al., 2007). We used sec-
                                                                 two of the three tasks. We consider this is because
tions 02-21 of PTB for training, and section 23 for
                                                                 the transition information is crucial for the two
testing. The number of labels in the three tasks is
                                                                 tasks, and the assumption behind C ARPE D IEM is
45, 319 and 2602, respectively.
                                                                 violated. In contrast, the proposed algorithms per-
   We used the perceptron algorithm for train-
                                                                 formed reasonably well for all three tasks, demon-
ing. The models were averaged over 10 itera-
                                                                 strating the wide applicability of our algorithm.
tions (Collins, 2002). For features, we basically
                                                                    Table 2 presents the average iteration num-
followed previous studies (Tsuruoka and Tsujii,
                                                                 bers of SD and SD+ C - EXP. We can observe
2005; Sha and Pereira, 2003; Ninomiya et al.,
                                                                 that the two algorithms required almost the same
2006). In POS tagging, we used unigrams of the
                                                                 number of iterations on average, although the
current and its neighboring words, word bigrams,
                                                                 iteration number is not tightly bounded if we
prefixes and suffixes of the current word, capital-
                                                                 use column-wise expansion. This indicates that
ization, and tag bigrams. In joint tagging, we also
                                                                 SD+ C - EXP. virtually avoided performing extra it-
used the same features. In supertagging, we used
                                                                 erations, while heuristically restricting active label
POS unigrams and bigrams in addition to the same
                                                                 expansion.
features other than capitalization.
                                                                    Table 3 compares the training time spent by
   As the evaluation measure, we used the average                V ITERBI and SD+ C - EXP. Although speeding up
decoding speed (sentences/sec) to two significant                perceptron training is a by-product, it is interest-
digits over five trials. To strictly measure the time            ing to see that our algorithm is in fact effective at
spent for decoding, we ignored the preprocessing                 reducing the training time as well. The result also
time, that is, the time for loading the model file               indicates that the speed-up is more significant at
and converting the features (i.e., strings) into inte-           test time. This is probably because the model is
gers. We note that the accuracy was comparable to                not predictive enough at the beginning of training,
the state-of-the-art in the three tasks: 97.08, 93.21,           and the pruning is not that effective.
and 91.20% respectively.
                                                                 5.3    Comparison with approximate algorithm
5.2   Results and discussions
                                                                 Table 4 compares two exact algorithms (V ITERBI
Table 1 presents the performance of our algo-                    and SD+ E - XP.) with beam search, which is the ap-
rithm. SD represents the proposed algorithm with-                proximate algorithm widely adopted for sequence
out column-wise expansion, while SD+ C - EXP.                    labeling in NLP. For this experiment, the beam
uses column-wise expansion. For comparison, we                   width, B, was exhaustively calibrated: we tried B
present the results of two baseline algorithms as                = {1, 2, 4, 8, ...} until the beam search achieved
well: V ITERBI and C ARPE D IEM (Esposito and                    comparable accuracy to the exact algorithms, i.e.,
Radicioni, 2009). In almost all settings, we see                 the difference fell below 0.1 in our case.
that both of our algorithms outperformed the other                  We see that there is a substantial difference in
two. We also find that SD+ C - EXP. performed con-               the performance between V ITERBI and B EAM.
sistently better than SD. This indicates the effec-              On the other hand, SD+ C - EXP. reached speeds
tiveness of column-wise expansion.                               very close to those of B EAM. In fact, they
   Following V ITERBI, C ARPE D IEM is the most                  achieved comparable performance in our exper-
relevant algorithm, for sequence labeling in NLP,                iment. These results demonstrate that we could
as discussed in Section 2.3. However, our results                successfully bridge the gap in the performance be-


                                                           492


                                                                   thank Yusuke Miyao for providing us with the
Table 4: Comparison with beam search (sent./sec).
                POS tagging   Joint tagging   Supertagging         HPSG Treebank data.
 V ITERBI              4000              77            1.1
 SD+ C - EXP.        14,000            1600            300
 B EAM               18,000            2400            180         References
                                                                   Thorsten Brants. 2000. TnT - a statistical part-of-
                                                                     speech tagger. In Proceedings of ANLP, pages 224–
tween exact and approximate algorithms, while re-                    231.
taining the advantages of exact algorithms.
                                                                   Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
                                                                     Austerweil, David Ellis, Isaac Haxton, Catherine
6 Relation to coarse-to-fine approach                                Hill, R. Shrivaths, Jeremy Moore, Michael Pozar,
                                                                     and Theresa Vu. 2006. Multi-level coarse-to-fine
Before concluding remarks, we briefly examine                        PCFG parsing. In Proceedings of NAACL, pages
the relationship between staggered decoding and                      168–175.
coarse-to-fine PCFG parsing (2006). In coarse-to-                  Trevor Cohn. 2006. Efficient inference in large con-
fine parsing, the candidate parse trees are pruned                   ditional random fields. In Proceedings of ECML,
by using the parse forest produced by a coarse-                      pages 606–613.
grained PCFG. Since the degenerate label can be
                                                                   Michael Collins. 2002. Discriminative training meth-
interpreted as a coarse-level label, one may con-                    ods for hidden Markov models: Theory and exper-
sider that staggered decoding is an instance of                      iments with perceptron algorithms. In Proceedings
coarse-to-fine approach. While there is some re-                     of EMNLP, pages 1–8.
semblance, there are at least two essential differ-
                                                                   Thomas G. Dietterich, Pedro Domingos, Lise Getoor,
ences. First, coarse-to-fine approach is a heuristic                 Stephen Muggleton, and Prasad Tadepalli. 2008.
pruning, that is, it is not an exact algorithm. Sec-                 Structured machine learning: the next ten years.
ond, our algorithm does not always perform de-                       Machine Learning, 73(1):3–23.
coding at the fine-grained level. It is designed to                Roberto Esposito and Daniele P. Radicioni. 2009.
be able to stop decoding at the coarse-level.                        C ARPE D IEM: Optimizing the Viterbi algorithm
                                                                     and applications to supervised sequential learning.
7 Conclusions                                                        Jorunal of Machine Learning Research, 10:1851–
                                                                     1880.
The sequence labeling algorithm is indispensable
                                                                   Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and
to modern statistical NLP. However, the Viterbi                      Jon M. Kleinberg. 2003. Fast algorithms for large-
algorithm, which is the standard decoding algo-                      state-space HMMs with applications to Web usage
rithm in NLP, is not efficient when we have to                       analysis. In Proceedings of NIPS, pages 409–416.
deal with a large number of labels. In this paper                  Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
we presented staggered decoding, which provides                      2009. Efficient inference of CRFs for large-scale
a principled way of resolving this problem. We                       natural language data. In Proceedings of ACL-
consider that it is a real alternative to the Viterbi                IJCNLP Short Papers, pages 281–284.
algorithm in various NLP tasks.                                    John Lafferty, Andrew McCallum, and Fernand
   An interesting future direction is to extend the                  Pereira. 2001. Conditional random fields: Prob-
proposed technique to handle more complex struc-                     abilistic models for segmenting and labeling se-
tures than the Markov chains, including semi-                        quence data. In Proceedings of ICML, pages 282–
                                                                     289.
Markov models and factorial HMMs (Sarawagi
and Cohen, 2004; Sutton et al., 2004). We hope                     Percy Liang, Hal Daumé III, and Dan Klein. 2008.
this work opens a new perspective on decoding al-                    Structure compilation: Trading structure for fea-
                                                                     tures. In Proceedings of ICML, pages 592–599.
gorithms for a wide range of NLP problems, not
just sequence labeling.                                            Yury Lifshits, Shay Mozes, Oren Weimann, and Michal
                                                                     Ziv-Ukelson. 2007. Speeding up HMM decod-
Acknowledgement                                                      ing and training by exploiting sequence repetitions.
                                                                     Computational Pattern Matching, pages 4–15.
We wish to thank the anonymous reviewers for                       Dekang Lin and Xiaoyun Wu. 2009. Phrae clustering
their helpful comments, especially on the com-                       for discriminative training. In Proceedings of ACL-
putational complexity of our algorithm. We also                      IJCNLP, pages 1030–1038.


                                                             493


Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsu-
  jii. 2007. Efficient HPSG parsing with supertagging
  and CFG-filtering. In Proceedings of IJCAI, pages
  1671–1676.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
  ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
  Extremely lexicalized models for accurate and fast
  HPSG parsing. In Proceedings of EMNLP, pages
  155–163.
Lawrence R. Rabiner. 1989. A tutorial on hidden
  Markov models and selected applications in speech
  recognition. In Proceedings of The IEEE, pages
  257–286.
Sunita Sarawagi and Willian W. Cohen. 2004. Semi-
  Markov conditional random fields for information
  extraction. In Proceedings of NIPS, pages 1185–
  1192.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
  with conditional random fields. In Proceedings of
  HLT-NAACL, pages 134–141.
Sajid M. Siddiqi and Andrew W. Moore. 2005. Fast
  inference and learning in large-state-space HMMs.
  In Proceedings of ICML, pages 800–807.

Charles Sutton, Khashayar Rohanimanesh, and An-
  drew McCallum. 2004. Dynamic conditional ran-
  dom fields: Factorized probabilistic models for la-
  beling and segmenting sequence data. In Proceed-
  ings of ICML.
Ben Tasker, Carlos Guestrin, and Daphe Koller. 2003.
  Max-margin Markov networks. In Proceedings of
  NIPS, pages 25–32.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas
   Hofmann, and Yasemin Altun. 2005. Large margin
   methods for structured and interdependent output
   variables. Journal of Machine Learning Research,
   6:1453–1484.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
  rectional inference with the easiest-first strategy
  for tagging sequence data. In Proceedings of
  HLT/EMNLP, pages 467–474.
Andrew J. Viterbi. 1967. Error bounds for convo-
  lutional codes and an asymeptotically optimum de-
  coding algorithm. IEEE Transactios on Information
  Theory, 13(2):260–267.




                                                        494

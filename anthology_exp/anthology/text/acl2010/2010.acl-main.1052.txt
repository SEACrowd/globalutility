                                  Practical very large scale CRFs

      Thomas Lavergne                             Olivier Cappé                              François Yvon
       LIMSI – CNRS                             Télécom ParisTech                        Université Paris-Sud 11
    lavergne@limsi.fr                              LTCI – CNRS                                LIMSI – CNRS
                                                cappe@enst.fr                               yvon@limsi.fr



                                                                   grows quadratically with respect to the number of
                       Abstract                                    output labels and so does the number of structural
                                                                   features, ie. features testing adjacent pairs of la-
    Conditional Random Fields (CRFs) are                           bels. Most empirical studies on CRFs thus ei-
    a widely-used approach for supervised                          ther consider tasks with a restricted output space
    sequence labelling, notably due to their                       (typically in the order of few dozens of output la-
    ability to handle large description spaces                     bels), heuristically reduce the use of features, es-
    and to integrate structural dependency be-                     pecially of features that test pairs of adjacent la-
    tween labels. Even for the simple linear-                      bels1 , and/or propose heuristics to simulate con-
    chain model, taking structure into account                     textual dependencies, via extended tests on the ob-
    implies a number of parameters and a                           servations (see discussions in, eg., (Punyakanok
    computational effort that grows quadrati-                      et al., 2005; Liang et al., 2008)). Limitating the
    cally with the cardinality of the label set.                   feature set or the number of output labels is how-
    In this paper, we address the issue of train-                  ever frustrating for many NLP tasks, where the
    ing very large CRFs, containing up to hun-                     type and number of potentially relevant features
    dreds output labels and several billion fea-                   are very large. A number of studies have tried to
    tures. Efficiency stems here from the spar-                    alleviate this problem. Pal et al. (2006) propose
    sity induced by the use of a `1 penalty                        to use a “sparse” version of the forward-backward
    term. Based on our own implementa-                             algorithm during training, where sparsity is en-
    tion, we compare three recent proposals                        forced through beam pruning. Related ideas are
    for implementing this regularization strat-                    discussed by Dietterich et al. (2004); by Cohn
    egy. Our experiments demonstrate that                          (2006), who considers “generalized” feature func-
    very large CRFs can be trained efficiently                     tions; and by Jeong et al. (2009), who use approx-
    and that very large models are able to im-                     imations to simplify the forward-backward recur-
    prove the accuracy, while delivering com-                      sions. In this paper, we show that the sparsity that
    pact parameter sets.                                           is induced by `1 -penalized estimation of CRFs can
                                                                   be used to reduce the total training time, while
1   Introduction                                                   yielding extremely compact models. The benefits
Conditional Random Fields (CRFs) (Lafferty et                      of sparsity are even greater during inference: less
al., 2001; Sutton and McCallum, 2006) constitute                   features need to be extracted and included in the
a widely-used and effective approach for super-                    potential functions, speeding up decoding with a
vised structure learning tasks involving the map-                  lesser memory footprint. We study and compare
ping between complex objects such as strings and                   three different ways to implement `1 penalty for
trees. An important property of CRFs is their abil-                CRFs that have been introduced recently: orthant-
ity to handle large and redundant feature sets and                 wise Quasi Newton (Andrew and Gao, 2007),
to integrate structural dependency between out-                    stochastic gradient descent (Tsuruoka et al., 2009)
put labels. However, even for simple linear chain                  and coordinate descent (Sokolovska et al., 2010),
CRFs, the complexity of learning and inference                     concluding that these methods have complemen-
                                                                      1
     This work was partly supported by ANR projects CroTaL              In CRFsuite (Okazaki, 2007), it is even impossible to
(ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-                       jointly test a pair of labels and a test on the observation, bi-
02).                                                               grams feature are only of the form f (yt−1 , yt ).


                                                             504
         Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


tary strengths and weaknesses. Based on an effi-                         functions and {θk }1≤k≤K are the associated pa-
cient implementation of these algorithms, we were                        rameter values. We denote by Y and X, respec-
able to train very large CRFs containing more than                       tively, the sets in which yt and xt take their values.
a hundred of output labels and up to several billion                     The normalization factor in (1) is defined by
features, yielding results that are as good or better                                             ( K                )
than the best reported results for two NLP bench-
                                                                                         X           X
                                                                             Zθ (x) =         exp        θk Fk (x, y) . (2)
marks, text phonetization and part-of-speech tag-                                          y∈Y T           k=1
ging.
   Our contribution is therefore twofold: firstly a                      The most common choice of feature functions is to
detailed analysis of these three algorithms, dis-                        use binary tests. In the sequel, we distinguish be-
cussing implementation, convergence and com-                             tween two types of feature functions: unigram fea-
paring the effect of various speed-ups. This                             tures fy,x , associated with parameters µy,x , and bi-
comparison is made fair and reliable thanks to                           gram features fy0 ,y,x , associated with parameters
the reimplementation of these techniques in the                          λy0 ,y,x . These are defined as
same software package. Second, the experimen-
                                                                           fy,x (yt−1 , yt , xt ) = 1(yt = y, xt = x)
tal demonstration that using large output label sets
is doable and that very large feature sets actually                      fy0 ,y,x (yt−1 , yt , xt ) = 1(yt−1 = y 0 , yt = y, xt = x)
help improve prediction accuracy. In addition, we
                                                                         where 1(cond.) is equal to 1 when the condition
show how sparsity in structured feature sets can
                                                                         is verified and to 0 otherwise. In this setting, the
be used in incremental training regimes, where
                                                                         number of parameters K is equal to |Y |2 ×|X|train ,
long-range features are progressively incorporated
                                                                         where |·| denotes the cardinal and |X|train refers to
in the model insofar as the shorter range features
                                                                         the number of configurations of xt observed dur-
have proven useful.
                                                                         ing training. Thus, even in moderate size applica-
   The rest of the paper is organized as follows: we                     tions, the number of parameters can be very large,
first recall the basics of CRFs in Section 2, and dis-                   mostly due to the introduction of sequential de-
cuss three ways to train CRFs with a `1 penalty in                       pendencies in the model. This also explains why it
Section 3. We then detail several implementation                         is hard to train CRFs with dependencies spanning
issues that need to be addressed when dealing with                       more than two adjacent labels. Using only uni-
massive feature sets in Section 4. Our experiments                       gram features {fy,x }(y,x)∈Y ×X results in a model
are reported in Section 5. The main conclusions of                       equivalent to a simple bag-of-tokens position-
this study are drawn in Section 6.                                       by-position logistic regression model. On the
                                                                         other hand, bigram features {fy0 ,y,x }(y,x)∈Y 2 ×X
2     Conditional Random Fields                                          are helpful in modelling dependencies between
In this section, we recall the basics of Conditional                     successive labels. The motivations for using si-
Random Fields (CRFs) (Lafferty et al., 2001; Sut-                        multaneously both types of feature functions are
ton and McCallum, 2006) and introduce the nota-                          evaluated experimentally in Section 5.
tions that will be used throughout.                                      2.2   Parameter Estimation
2.1    Basics                                                            Given N independent sequences {x(i) , y(i) }N  i=1 ,
                                                                         where x(i) and y(i) contain T (i) symbols, condi-
CRFs are based on the following model                                    tional maximum likelihood estimation is based on
                                (   K
                                                         )               the minimization, with respect to θ, of the negated
                 1                  X
                                                                         conditional log-likelihood of the observations
    pθ (y|x) =        exp                 θk Fk (x, y)       (1)
               Zθ (x)
                                    k=1                                              N
                                                                                     X
                                                                         l(θ) = −            log pθ (y(i) |x(i) )                      (3)
where x = (x1 , . . . , xT ) and y = (y1 , . . . , yT )                              i=1
are, respectively, the input and                       2
                              PToutput sequences ,                                 N                           K
                                                                                         (                                                  )
                                                                                   X                           X
and Fk (x, y) is equal to       t=1 fk (yt−1 , yt , xt ),                      =           log Zθ (x(i) ) −          θk Fk (x(i) , y(i) )
where {fk }1≤k≤K is an arbitrary set of feature                                    i=1                         k=1

    2
      Our implementation also includes a special label y0 , that         This term is usually complemented with an addi-
is always observed and marks the beginning of a sequence.                tional regularization term so as to avoid overfitting


                                                                   505


(see Section 3.1 below). The gradient of l(θ) is                     parameter space. Hence, any numerical optimiza-
                                                                     tion strategy may be used and practical solutions
               N T (i)
    ∂l(θ) X X                                (i)                     include limited memory BFGS (L-BFGS) (Liu
         =    Epθ (y|x(i) ) fk (yt−1 , yt , xt )                     and Nocedal, 1989), which is used in the popu-
     ∂θk
               i=1 t=1
                                                                     lar CRF++ (Kudo, 2005) and CRFsuite (Okazaki,
                   N X
                     T   (i)
                   X                   (i)   (i)   (i)               2007) packages; conjugate gradient (Nocedal and
               −               fk (yt−1 , yt , xt )      (4)         Wright, 2006) and Stochastic Gradient Descent
                   i=1 t=1
                                                                     (SGD) (Bottou, 2004; Vishwanathan et al., 2006),
where Epθ (y|x) denotes the conditional expecta-                     used in CRFsgd (Bottou, 2007). The only caveat
tion given the observation sequence, i.e.                            is to avoid numerical optimizers that require the
                                                                     full Hessian matrix (e.g., Newton’s algorithm) due
                                 (i)                                 to the size of the parameter vector in usual appli-
    Epθ (y|x) fk (yt−1 , yt , xt ) =
    X                                                                cations of CRFs.
          fk (y, y 0 , xt ) Pθ (yt−1 = y 0 , yt = y|x) (5)
                                                                        The most significant alternative to `2 regulariza-
(y 0 ,y)∈Y 2
                                                                     tion is to use a `1 penalty term ρ1 kθk1 : such regu-
Although l(θ) is a smooth convex function, its op-                   larizers are able to yield sparse parameter vectors
timum cannot be computed in closed form, and                         in which many component have been zeroed (Tib-
l(θ) has to be optimized numerically. The com-                       shirani, 1996). Using a `1 penalty term thus im-
putation of its gradient implies to repeatedly com-                  plicitly performs feature selection, where ρ1 con-
pute the conditional expectation in (5) for all in-                  trols the amount of regularization and the number
put sequences x(i) and all positions t. The stan-                    of extracted features. In the following, we will
dard approach for computing these expectations                       jointly use both penalty terms, yielding the so-
is inspired by the forward-backward algorithm for                    called elastic net penalty (Zhou and Hastie, 2005)
hidden Markov models: using the notations intro-                     which corresponds to the objective function
duced above, the algorithm implies the computa-                                                      ρ2
tion of the forward                                                               l(θ) + ρ1 kθk1 +      kθk22         (6)
                                                                                                     2
(
   α1 (y) = exp(µy,x1 + λy0 ,y,x1 )
                                                                     The use of both penalty terms makes it possible
   αt+1 (y) = y0 αt (y 0 ) exp(µy,xt+1 + λy0 ,y,xt+1 )
               P
                                                                     to control the number of non zero coefficients and
                                                                     to avoid the numerical problems that might occur
and backward recursions
                                                                     in large dimensional parameter settings (see also
                                                                     (Chen, 2009)). However, the introduction of a `1
(
  βTi (y) = 1
                                                                     penalty term makes the optimization of (6) more
  βt (y 0 ) = y βt+1 (y) exp(µy,xt+1 + λy0 ,y,xt+1 ),
             P
                                                                     problematic, as the objective function is no longer
for all indices 1P≤ t ≤ T and all labels y ∈ Y .                     differentiable in 0. Various strategies have been
Then, Zθ (x) = y αT (y) and the pairwise prob-                       proposed to handle this difficulty. We will only
abilities Pθ (yt = y 0 , yt+1 = y|x) are given by                    consider here exact approaches and will not dis-
                                                                     cuss heuristic strategies such as grafting (Perkins
 αt (y 0 ) exp(µy,xt+1 + λy0 ,y,xt+1 )βt+1 (y)/Zθ (x)                et al., 2003; Riezler and Vasserman, 2004).

These recursions require a number of operations                      3.2   Quasi Newton Methods
that grows quadratically with |Y |.
                                                                     To deal with `1 penalties, a simple idea is that of
3     `1 Regularization in CRFs                                      (Kazama and Tsujii, 2003), originally introduced
                                                                     for maxent models. It amounts to reparameteriz-
3.1    Regularization                                                ing θk as θk = θk+ − θk− , where θk+ and θk− are pos-
The standard approach for parameter estimation in                    itive. The `1 penalty thus becomes ρ1 (θ+ − θ− ).
CRFs consists in minimizing the logarithmic loss                     In this formulation, the objective function recovers
l(θ) defined by (3) with an additional `2 penalty                    its smoothness and can be optimized with conven-
term ρ22 kθk22 , where ρ2 is a regularization parame-                tional algorithms, subject to domain constraints.
ter. The objective function is then a smooth convex                  Optimization is straightforward, but the number
function to be minimized over an unconstrained                       of parameters is doubled and convergence is slow


                                                               506


(Andrew and Gao, 2007): the procedure lacks a                3.4   Block Coordinate Descent
mechanism for zeroing out useless parameters.
                                                             The coordinate descent approach of Dudı́k et
   A more efficient strategy is the orthant-wise
                                                             al. (2004) and Friedman et al. (2008) uses the
quasi-Newton (OWL-QN) algorithm introduced in
                                                             fact that optimizing a mono-dimensional quadratic
(Andrew and Gao, 2007). The method is based on
                                                             function augmented with a `1 penalty can be per-
the observation that the `1 norm is differentiable
                                                             formed analytically. For arbitrary functions, this
when restricted to a set of points in which each
                                                             idea can be adapted by considering quadratic ap-
coordinate never changes its sign (an “orthant”),
                                                             proximations of the objective around the current
and that its second derivative is then zero, mean-
                                                             value θ̄
ing that the `1 penalty does not change the Hessian
of the objective on each orthant. An OWL-QN                                  ∂l(θ̄)                1 ∂ 2 l(θ̄)
update then simply consists in (i) computing the             lk,θ̄ (θk ) =          (θk − θ̄k ) +              (θk − θ̄k )2
                                                                              ∂θk                  2 ∂θk2
Newton update in a well-chosen orthant; (ii) per-                                               ρ2
forming the update, which might cause some com-                                   + ρ1 |θk | + θk2 + C st               (9)
                                                                                                2
ponent of the parameter vector to change sign; and
(iii) projecting back the parameter value onto the           The minimizer of the approximation (9) is simply
initial orthant, thereby zeroing out those compo-
                                                                                     n                                 o
nents. In (Gao et al., 2007), the authors show that                                      ∂ 2 l(θ̄)          ∂l(θ̄)
                                                                                 s        ∂θk2 k
                                                                                                   θ̄   −   ∂θk , ρ1
OWL-QN is faster than the algorithm proposed by                         θk =                                               (10)
                                                                                              ∂ 2 l(θ̄)
Kazama and Tsujii (2003) and can perform model                                                 ∂θk2
                                                                                                          + ρ2
selection even in very high-dimensional problems,
with no loss of performance compared to the use              where s is the soft-thresholding function
of `2 penalty terms.
                                                                                 
3.3   Stochastic Gradient Descent                                                z − ρ
                                                                                                           if z > ρ
Stochastic gradient (SGD) approaches update the                         s(z, ρ) = z + ρ                     if z < −ρ      (11)
                                                                                 
                                                                                  0                         otherwise
                                                                                 
parameter vector based on an crude approximation
of the gradient (4), where the computation of ex-
pectations only includes a small batch of observa-           Coordinate descent is ported to CRFs in
tions. SGD updates have the following form                   (Sokolovska et al., 2010). Making this scheme
                                                             practical requires a number of adaptations,
                              ∂l(θ)                          including (i) approximating the second order
               θ k ← θk + η         ,           (7)
                               ∂θk                           term in (10), (ii) performing updates in block,
where η is the learning rate. In (Tsuruoka et al.,           where a block contains the |Y | × |Y + 1| fea-
2009), various ways of adapting this update to `1 -          tures νy0 ,y,x and λy,x for a fixed test x on the
penalized likelihood functions are discussed. Two            observation sequence and (iii) approximating the
effective ideas are proposed: (i) only update pa-            Hessian for a block by its diagonal terms. (ii)
rameters that correspond to active features in the           is specially critical, as repeatedly cycling over
current observation, (ii) keep track of the cumu-            individual features to perform the update (10)
lated penalty zk that θk should have received, had           is only possible with restricted sets of features.
the gradient been computed exactly, and use this             The block update schemes uses the fact that
value to “clip” the parameter value. This is imple-          all features within a block appear in the same
mented by patching the update (7) as follows                 set of sequences, which means that most of the
  (                                                          computations needed to perform theses updates
    if (θk > 0) θk ← max(0, θk − zk )                        can be shared within the block. One advantage
                                                (8)          of the resulting algorithm, termed BCD in the
    else if (θk < 0) θk ← min(0, θk − zk )
                                                             following, is that the update of θk only involves
Based on a study of three NLP benchmarks, the                carrying out the forward-backward recursions for
authors of (Tsuruoka et al., 2009) claim this ap-            the set of sequences that contain symbols x such
proach to be much faster than the orthant-wise ap-           that at least one {fk (y 0 , y, x)}(y,y0 )∈Y 2 is non
proach and yet to yield very comparable perfor-              null, which can be much smaller than the whole
mance, while selecting slightly larger feature sets.         training set.


                                                       507


4     Implementation Issues                                          dealing with very large datasets). In our imple-
                                                                     mentation, it is distributed on all available cores,
Efficiently processing very-large feature and ob-
                                                                     resulting in significant speed-ups for OWL-QN
servation sets requires to pay attention to many
                                                                     and L-BFGS; for BCD the gain is less acute, as
implementation details. In this section, we present
                                                                     parallelization only helps when updating the pa-
several optimizations devised to speed up training.
                                                                     rameters for a block of features that are occur in
4.1    Sparse Forward-Backward Recursions                            many sequences; for SGD, with batches of size
                                                                     one, this parallelization policy is useless.
For all algorithms, the computation time is domi-
nated by the evaluations of the gradient: our im-                    4.2   Scaling
plementation takes advantage of the sparsity to ac-
                                                                     Most existing implementations of CRFs, eg.
celerate these computations. Assume the set of bi-
                                                                     CRF++ and CRFsgd perform the forward-
gram features {λy0 ,y,xt+1 }(y0 ,y)∈Y 2 is sparse with
                                                                     backward recursions in the log-domain, which
only r(xt+1 )  |Y |2 non null values and define
                                                                     guarantees that numerical over/underflows are
the |Y | × |Y | sparse matrix
                                                                     avoided no matter the length T (i) of the sequence.
           Mt (y 0 , y) = exp(λy0 ,y,xt ) − 1.                       It is however very inefficient from an implementa-
                                                                     tion point of view, due to the repeated calls to the
Using M , the forward-backward recursions are                        exp() and log() functions. As an alternative way
          X                 X                                        of avoiding numerical problems, our implementa-
 αt (y) =     ut−1 (y 0 ) +   ut−1 (y 0 )Mt (y 0 , y)                tion, like crfSuite’s, resorts to “scaling”, a solution
               y0                y0                                  commonly used for HMMs. Scaling amounts to
                                                                     normalizing the values of αt and βt to one, making
               X                 X
 βt (y 0 ) =        vt+1 (y) +        Mt+1 (y 0 , y)vt+1 (y)
               y                 y                                   sure to keep track of the cumulated normalization
                                                                     factors so as to compute Zθ (x) and the conditional
with ut−1 (y)         =      exp(µy,xt )αt−1 (y) and                 expectations Epθ (y|x) . Also note that in our imple-
vt+1 (y) = exp(µy,xt+1 )βt+1 (y). (Sokolovska et                     mentation, all the computations of exp(x) are vec-
al., 2010) explains how computational savings can                    torized, which provides an additional speed up of
be obtained using the fact that the vector/matrix                    about 20%.
products in the recursions above only involve
the sparse matrix Mt+1 (y 0 , y). They can thus be                   4.3   Optimization in Large Parameter Spaces
computed with exactly r(xt+1 ) multiplications                       Processing very large feature vectors, up to bil-
instead of |Y |2 . The same idea can be used                         lions of components, is problematic in many ways.
when the set {µy,xt+1 }y∈Y of unigram features is                    Sparsity has been used here to speed up forward-
sparse. Using this implementation, the complexity                    backward, but we have made no attempt to accel-
of the forward-backward procedure for x(i) can be                    erate the computation of the OWL-QN updates,
made proportional to the average number of active                    which are linear in the size of the parameter vector.
features per position, which can be much smaller                     Of the three algorithms, BCD is the most affected
than the number of potentially active features.                      by increases in the number of features, or more
   For BCD, forward-backward can even be made                        precisely, in the number of features blocks, where
slightly faster. When computing the gradient wrt.                    one block correspond to a specific test of the ob-
features λy,x and µy0 ,y,x (for all the values of y                  servation. In the worst case scenario, each block
and y 0 ) for sequence x(i) , assuming that x only                   may require to visit all the training instances,
occurs once in x(i) at position t, all that is needed                yielding terrible computational wastes. In prac-
is αt0 (y), ∀t0 ≤Pt and βt0 (y), ∀t0 ≥ t. Zθ (x) is then             tice though, most blocks only require to process
recovered as y αt (y)βt (y). Forward-backward                        a small fraction of the training set, and the ac-
recursions can thus be truncated: in our experi-                     tual complexity depends on the average number of
ments, this divided the computational cost by 1,8                    blocks per observations. Various strategies have
on average.                                                          been tried to further accelerate BCD, such as pro-
   Note finally that forward-backward is per-                        cessing blocks that only visit one observation in
formed on a per-observation basis and is easily                      parallel and updating simultaneously all the blocks
parallelized (see also (Mann et al., 2009) for more                  that visit all the training instances, leading to a
powerful ways to distribute the computation when                     small speed-up on the POS-tagging task.


                                                               508


   Working with billions of features finally re-              5.1     Tasks and Settings
quires to worry also about memory usage. In this              5.1.1    Nettalk
respect, BCD is the most efficient, as it only re-
quires to store one K-dimensional vector for the              Our first benchmark is the word phonetization
parameter itself. SGD requires two such vectors,              task, using the Nettalk dictionary (Sejnowski and
one for the parameter and one for storing the zk              Rosenberg, 1987). This dataset contains approxi-
(see Eq. (8)). In comparison, OWL-QN requires                 mately 20,000 English word forms, their pronun-
much more memory, due to the internals of the                 ciation, plus some prosodic information (stress
update routines, which require several histories of           markers for vowels, syllabic parsing for con-
the parameter vector and of its gradient. Typi-               sonants). Grapheme and phoneme strings are
cally, our implementation necessitates in the order           aligned at the character level, thanks to the use of
of a dozen K-dimensional vectors. Parallelization             a “null sound” in the latter string when it is shorter
only makes things worse, as each core will also               than the former; likewise, each prosodic mark is
need to maintain its own copy of the gradient.                aligned with the corresponding letter. We have de-
                                                              rived two test conditions from this database. The
                                                              first one is standard and aims at predicting the pro-
5   Experiments                                               nunciation information only. In this setting, the set
                                                              of observations (X) contains 26 graphemes, and
                                                              the output label set contains |Y | = 51 phonemes.
Our experiments use two standard NLP tasks,
                                                                 The second condition aims at jointly predict-
phonetization and part-of-speech tagging, chosen
                                                              ing phonemic and prosodic information3 . The rea-
here to illustrate two very different situations, and
                                                              sons for designing this new condition are twofold:
to allow for comparison with results reported else-
                                                              firstly, it yields a large set of composite labels
where in the literature. Unless otherwise men-
                                                              (|Y | = 114) and makes the problem computation-
tioned, the experiments use the same protocol: 10
                                                              ally challenging. Second, it allows to quantify how
fold cross validation, where eight folds are used
                                                              much the information provided by the prosodic
for training, one for development, and one for test-
                                                              marks help predict the phonemic labels. Both in-
ing. Results are reported in terms of phoneme er-
                                                              formation are quite correlated, as the stress mark
ror rates or tag error rates on the test set.
                                                              and the syllable openness, for instance, greatly in-
   Comparing run-times can be a tricky matter, es-            fluence the realization of some archi-phonemes.
pecially when different software packages are in-                The features used in Nettalk experiments take
volved. As discussed above, the observed run-                 the form fy,w (unigram) and fy0 ,y,w (bigram),
times depend on many small implementation de-                 where w is a n-gram of letters. The n-grm feature
tails. As the three algorithms share as much code             sets (n = {1, 3, 5, 7}) includes all features testing
as possible, we believe the comparison reported               embedded windows of k letters, for all 0 ≤ k ≤ n;
hereafter to be fair and reliable. All experiments            the n-grm- setting is similar, but only includes
were performed on a server with 64G of memory                 the window of length n; in the n-grm+ setting,
and two Xeon processors with 4 cores at 2.27 Ghz.             we add features for odd-size windows; in the n-
For comparison, all measures of run-times include             grm++ setting, we add all sequences of letters up
the cumulated activity of all cores and give very             to size n occurring in current window. For in-
pessimistic estimates of the wall time, which can             stance, the active bigram features at position t = 2
be up to 7 times smaller. For OWL-QN, we use 5                in the sequence x=’lemma’ are as follows: the 3-
past values of the gradient to approximate the in-            grm feature set contains fy,y0 , fy,y0 ,e and fy0 ,y,lem ;
verse of the Hessian matrix: increasing this value            only the latter appears in the 3-grm- setting. In
had no effect on accuracy or convergence and was              the 3-grm+ feature set, we also have fy0 ,y,le and
detrimental to speed; for SGD, the learning rate              fy0 ,y,em . The 3-grm++ feature set additionally in-
parameter was tuned manually.                                 cludes fy0 ,y,l and fy0 ,y,m . The number of features
  Note that we have not spent much time optimiz-              ranges from 360 thousands (1-grm setting) to 1.6
ing the values of ρ1 and ρ2 . Based on a pilot study          billion (7-grm).
on Nettalk, we found that taking ρ1 = .5 and ρ2 in               3
                                                                   Given the design of the Nettalk dictionary, this experi-
the order of 10−5 to yield nearly optimal perfor-             ment required to modify the original database so as to reas-
mance, and have used these values throughout.                 sign prosodic marks to phonemes, rather than to letters.


                                                        509


 Features      With         Without                                        `2     `1   Elastic-net
 Nettalk                                                           1-grm 17.81% 17.86% 17.79%
 3-grm    10.74% 14.3M 14.59% 0.3M                                 3-grm 10.62% 10.74% 10.70%
 5-grm     8.48% 132.5M 11.54% 2.5M                                5-grm 8.50% 8.45%     8.48%
 POS tagging
 base      2.91% 436.7M 3.47% 70.2M                          Table 3: Error rates of the three regularizers on the
                                                             Nettalk task.
Table 1: Features jointly testing label pairs and
the observation are useful (error rates and features         5.3   Speed, Sparsity, Convergence
counts.)
                                                             The training speed depends of two main factors:
                                                             the number of iterations needed to achieve conver-
               `2  `1 -sparse `1 % zero                      gence and the computational cost of one iteration.
      1-grm 84min 41min 57min 44.6%                          In this section, we analyze and compare the run-
      3-grm- 65min 16min 44min 99.6%                         time efficiency of the three optimizers.
      3-grm 72min 48min 58min 19.9%
                                                             5.3.1 Convergence
Table 2: Sparse vs standard forward-backward                 As far as convergence is concerned, the two forms
(training times and percentages of sparsity of M )           of regularization (`2 and `1 ) yield the same per-
                                                             formance (see Table 3), and the three algorithms
                                                             exhibit more or less the same behavior. They
5.1.2    Part-of-Speech Tagging
                                                             quickly reach an acceptable set of active param-
Our second benchmark is a part-of-speech (POS)               eters, which is often several orders of magnitude
tagging task using the PennTreeBank corpus                   smaller than the whole parameter set (see results
(Marcus et al., 1993), which provides us with a              below in Table 4 and 5). Full convergence, re-
quite different condition. For this task, the number         flected by a stabilization of the objective function,
of labels is smaller (|Y | = 45) than for Nettalk,           is however not so easily achieved. We have of-
and the set of observations is much larger (|X| =            ten observed a slow, yet steady, decrease of the
43207). This benchmark, which has been used in               log-loss, accompanied with a diminution of the
many studies, allows for direct comparisons with             number of active features as the number of iter-
other published work. We thus use a standard ex-             ations increases. Based on this observation, we
perimental set-up, where sections 0-18 of the Wall           have chosen to stop all algorithms based on their
Street Journal are used for training, sections 19-21         performance on an independent development set,
for development, and sections 22-24 for testing.             allowing a fair comparison of the overall training
                                                             time; for OWL-QN, it allowed to divide the total
   Features are also standard and follow the design
                                                             training time by almost 2.
of (Suzuki and Isozaki, 2008) and test the current
                                                                It has finally often been found useful to fine
words (as written and lowercased), prefixes and
                                                             tune the non-zero parameters by running a final
suffixes up to length 4, and typographical charac-
                                                             handful of L-BFGS iterations using only a small
teristics (case, etc.) of the words. Our baseline
                                                             `2 penalty; at this stage, all the other features are
feature set also contains tests on individual and
                                                             removed from the model. This had a small impact
pairs of words in a window of 5 words.
                                                             BCD and SGD’s performance and allowed them to
                                                             catch up with OWL-QN’s performance.
5.2     Using Large Feature Sets
                                                             5.3.2 Sparsity and the Forward-Backward
The first important issue is to assess the benefits          As explained in section 4.1, the forward-backward
of using large feature sets, notably including fea-          algorithm can be written so as to use the sparsity
tures testing both a bigram of labels and an obser-          of the matrix My,y0 ,x . To evaluate the resulting
vation. Table 1 compares the results obtained with           speed-up, we ran a series of experiments using
and without these features for various setting (us-          Nettalk (see Table 2). In this table, the 3-grm- set-
ing OWL-QN to perform the optimization), sug-                ting corresponds to maximum sparsity for M , and
gesting that for the tasks at hand, these features           training with the sparse algorithm is three times
are actually helping.                                        faster than with the non-sparse version. Throwing


                                                       510


               Method Iter. # Feat. Error Time                           Method         Error       Time
SGD OWL - QN    1-grm 63.4 4684 17.79% 11min                               5-grm   14.71% / 8.11%   55min




                                                                   SGD
                7-grm 140.2 38214 8.12% 1h02min                           5-grm+   13.91% / 7.51% 2h45min
               5-grm+ 141.0 43429 7.89% 1h37min                            5-grm   14.57% / 8.06% 2h46min
                1-grm 21.4 3540 18.21%




                                                                   BCD
                                          9min                             7-grm   14.12% / 7.86% 3h02min
               5-grm+ 28.5 34319 8.01%    45min                           5-grm+   13.85% / 7.47% 7h14min
                1-grm 28.2 5017 18.27% 27min                             5-grm++   13.69% / 7.36% 16h03min
  BCD




                7-grm  9.2   3692 8.21% 1h22min
               5-grm+ 8.7 47675 7.91% 2h18min                Table 5: Performance on Nettalk+prosody. Error
                                                             is given for both joint labels and phonemic labels.
                 Table 4: Performance on Nettalk
                                                             neously predicting the phoneme and its prosodic
in more features has the effect of making M much             markers allows to improve the accuracy on the pre-
more dense, mitigating the benefits of the sparse            diction of phonemes, which improves of almost a
recursions. Nevertheless, even for very large fea-           half point as compared to the best Nettalk system.
ture sets, the percentage of zeros in M averages                For the POS tagging task, BCD appears to be
20% to 30%, and the sparse version remains 10 to             unpractically slower to train than the others ap-
20% faster than the non-sparse one. Note that the            proaches (SGD takes about 40min to train, OWL-
non-sparse version is faster with a `1 penalty term          QN about 1 hour) due the simultaneous increase
than with only the `2 term: this is because exp(0)           in the sequence length and in the number of ob-
is faster to evaluate than exp(x) when x 6= 0.               servations. As a result, one iteration of BCD typi-
                                                             cally requires to repeatedly process over and over
5.3.3            Training Speed and Test Accuracy            the same sequences: on average, each sequence is
Table 4 displays the results achieved on the Nettalk         visited 380 times when we use the baseline fea-
task. The three algorithms yield very compara-               ture set. This technique should reserved for tasks
ble accuracy results, and deliver compact models:            where the number of blocks is small, or, as below,
for the 5-gram+ setting, only 50,000 out of 250              when memory usage is an issue.
million features are selected. SGD is the fastest
of the three, up to twice as fast as OWL-QN and              5.4     Structured Feature Sets
BCD depending on the feature set. The perfor-                In many tasks, the ambiguity of tokens can be re-
mance it achieves are consistently slightly worst            duced by looking up increasingly large windows
than the other optimizers, and only catch up when            of local context. This strategy however quickly
the parameters are fine-tuned (see above). There             runs into a combinatorial increase of the number
are not so many comparisons for Nettalk with                 of features. A side note of the Nettalk experiments
CRFs, due to the size of the label set. Our results          is that when using embedded features, the active
compare favorably with those reported in (Pal et             feature set tends to reflect this hierarchical organi-
al., 2006), where the accuracy attains 91.7% us-             zation. This means that when a feature testing a
ing 19075 examples for training and 934 for test-            n-gram is active, in most cases, the features for all
ing, and with those in (Jeong et al., 2009) (88.4%           embedded k-grams are also selected.
accuracy with 18,000 (2,000) training (test) in-                Based on this observation, we have designed
stances). Table 5 gives the results obtained for             an incremental training strategy for the POS tag-
the larger Nettalk+prosody task. Here, we only               ging task, where more specific features are pro-
report the results obtained with SGD and BCD.                gressively incorporated into the model if the cor-
For OWL-QN, the largest model we could han-                  responding less specific feature is active. This ex-
dle was the 3-grm model, which contained 69 mil-             periment used BCD, which is the most memory ef-
lion features, and took 48min to train. Here again,          ficient algorithm. The first iteration only includes
performance steadily increase with the number of             tests on the current word. During the second it-
features, showing the benefits of large-scale mod-           eration, we add tests on bigram of words, on suf-
els. We lack comparisons for this task, which                fixes and prefixes up to length 4. After four itera-
seems considerably harder than the sole phone-               tions, we throw in features testing word trigrams,
tization task, and all systems seem to plateau               subject to the corresponding unigram block being
around 13.5% accuracy. Interestingly, simulta-               active. After 6 iterations, we finally augment the


                                                       511


model with windows of length 5, subject to the                   vanced Lectures on Machine Learning, Lecture
corresponding trigram being active. After 10 iter-               Notes in Artificial Intelligence, LNAI 3176, pages
                                                                 146–168. Springer Verlag, Berlin.
ations, the model contains about 4 billion features,
out of which 400,000 are active. It achieves an                Léon Bottou. 2007. Stochastic gradient descent (sgd)
error rate of 2.63% (resp. 2.78%) on the develop-                 implementation. http://leon.bottou.org/projects/sgd.
ment (resp. test) data, which compares favorably               Stanley Chen. 2009. Performance prediction for ex-
with some of the best results for this task (for in-              ponential language models. In Proceedings of the
stance (Toutanova et al., 2003; Shen et al., 2007;                Annual Conference of the North American Chap-
Suzuki and Isozaki, 2008)).                                       ter of the Association for Computational Linguistics,
                                                                  pages 450–458, Boulder, Colorado, June.
6   Conclusion and Perspectives                                Trevor Cohn. 2006. Efficient inference in large con-
                                                                 ditional random fields. In Proceedings of the 17th
In this paper, we have discussed various ways to
                                                                 European Conference on Machine Learning, pages
train extremely large CRFs with a `1 penalty term                606–613, Berlin, September.
and compared experimentally the results obtained,
both in terms of training speed and of accuracy.               Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav
                                                                 Bulatov. 2004. Training conditional random fields
The algorithms studied in this paper have com-                   via gradient tree boosting.     In Proceedings of
plementary strength and weaknesses: OWL-QN is                    the International Conference on Machine Learning,
probably the method of choice in small or moder-                 Banff, Canada.
ate size applications while BCD is most efficient              Miroslav Dudı́k, Steven J. Phillips, and Robert E.
when using very large feature sets combined with                 Schapire. 2004. Performance guarantees for reg-
limited-size observation alphabets; SGD comple-                  ularized maximum entropy density estimation. In
mented with fine tuning appears to be the preferred              John Shawe-Taylor and Yoram Singer, editors, Pro-
                                                                 ceedings of the 17th annual Conference on Learning
choice in most large-scale applications. Our anal-
                                                                 Theory, volume 3120 of Lecture Notes in Computer
ysis demonstrate that training large-scale sparse                Science, pages 472–486. Springer.
models can be done efficiently and allows to im-
prove over the performance of smaller models.                  Jenny Rose Finkel, Alex Kleeman, and Christopher D.
                                                                  Manning. 2008. Efficient, feature-based, condi-
The CRF package developed in the course of this                   tional random field parsing. In Proceedings of the
study implements many algorithmic optimizations                   Annual Meeting of the Association for Computa-
and allows to design innovative training strategies,              tional Linguistics, pages 959–967, Columbus, Ohio.
such as the one presented in section 5.4. This                 Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
package is released as open-source software and                   2008. Regularization paths for generalized linear
is available at http://wapiti.limsi.fr.                           models via coordinate descent. Technical report,
   In the future, we intend to study how spar-                    Department of Statistics, Stanford University.
sity can be used to speed-up training in the face              Jianfeng Gao, Galen Andrew, Mark Johnson, and
of more complex dependency patterns (such as                      Kristina Toutanova. 2007. A comparative study of
higher-order CRFs or hierarchical dependency                      parameter estimation methods for statistical natural
structures (Rozenknop, 2002; Finkel et al., 2008).                language processing. In Proceedings of the 45th An-
                                                                  nual Meeting of the Association of Computational
From a performance point of view, it might also                   Linguistics, pages 824–831, Prague, Czech republic.
be interesting to combine the use of large-scale
feature sets with other recent improvements such               Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
                                                                 2009. Efficient inference of crfs for large-scale nat-
as the use of semi-supervised learning techniques
                                                                 ural language data. In Proceedings of the Joint Con-
(Suzuki and Isozaki, 2008) or variable-length de-                ference of the Annual Meeting of the Association
pendencies (Qian et al., 2009).                                  for Computational Linguistics and the International
                                                                 Joint Conference on Natural Language Processing,
                                                                 pages 281–284, Suntec, Singapore.
References                                                     Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evalua-
Galen Andrew and Jianfeng Gao. 2007. Scalable train-             tion and extension of maximum entropy models with
  ing of l1-regularized log-linear models. In Proceed-           inequality constraints. In Proceedings of the 2003
  ings of the International Conference on Machine                Conference on Empirical Methods in Natural Lan-
  Learning, pages 33–40, Corvalis, Oregon.                       guage Processing, pages 137–144.
Léon Bottou. 2004. Stochastic learning. In Olivier            Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
   Bousquet and Ulrike von Luxburg, editors, Ad-                 http://crfpp.sourceforge.net/.


                                                         512


John Lafferty, Andrew McCallum, and Fernando                      Antoine Rozenknop. 2002. Modèles syntaxiques
  Pereira. 2001. Conditional random fields: prob-                   probabilistes non-génératifs. Ph.D. thesis, Dpt.
  abilistic models for segmenting and labeling se-                  d’informatique, École Polytechnique Fédérale de
  quence data. In Proceedings of the International                  Lausanne.
  Conference on Machine Learning, pages 282–289.
  Morgan Kaufmann, San Francisco, CA.                             Terrence J. Sejnowski and Charles R. Rosenberg.
                                                                    1987. Parallel networks that learn to pronounce en-
Percy Liang, Hal Daumé, III, and Dan Klein. 2008.                  glish text. Complex Systems, 1.
  Structure compilation: trading structure for features.
  In Proceedings of the 25th international conference             Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
  on Machine learning, pages 592–599.                               Guided learning for bidirectional sequence classi-
                                                                    fication. In Proceedings of the 45th Annual Meet-
Dong C. Liu and Jorge Nocedal. 1989. On the limited                 ing of the Association of Computational Linguistics,
  memory BFGS method for large scale optimization.                  pages 760–767, Prague, Czech Republic.
  Mathematical Programming, 45:503–528.
                                                                  Nataliya Sokolovska, Thomas Lavergne, Olivier
Gideon Mann, Ryan McDonald, Mehryar Mohri,                          Cappé, and François Yvon. 2010. Efficient learning
  Nathan Silberman, and Dan Walker. 2009. Efficient                 of sparse conditional random fields for supervised
  large-scale distributed training of conditional maxi-             sequence labelling. IEEE Selected Topics in Signal
  mum entropy models. In Y. Bengio, D. Schuurmans,                  Processing.
  J. Lafferty, C. K. I. Williams, and A.Culotta, editors,
  Advances in Neural Information Processing Systems               Charles Sutton and Andrew McCallum. 2006. An in-
  22, pages 1231–1239.                                              troduction to conditional random fields for relational
                                                                    learning. In Lise Getoor and Ben Taskar, editors, In-
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and                     troduction to Statistical Relational Learning, Cam-
  Beatrice Santorini. 1993. Building a large anno-                  bridge, MA. The MIT Press.
  tated corpus of English: The Penn treebank. Com-
  putational Linguistics, 19(2):313–330.                          Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
                                                                    sequential labeling and segmentation using giga-
Jorge Nocedal and Stephen Wright. 2006. Numerical                   word scale unlabeled data. In Proceedings of the
   Optimization. Springer.                                          Conference of the Association for Computational
                                                                    Linguistics on Human Language Technology, pages
Naoaki Okazaki. 2007. CRFsuite: A fast im-                          665–673, Columbus, Ohio.
  plementation of conditional random fields (CRFs).
  http://www.chokkan.org/software/crfsuite/.                      Robert Tibshirani. 1996. Regression shrinkage and
                                                                    selection via the lasso. J.R.Statist.Soc.B, 58(1):267–
Chris Pal, Charles Sutton, and Andrew McCallum.                     288.
  2006. Sparse forward-backward using minimum di-
  vergence beams for fast training of conditional ran-            Kristina Toutanova, Dan Klein, Christopher D. Man-
  dom fields. In Proceedings of the International Con-              ning, and Yoram Singer. 2003. Feature-rich part-of-
  ference on Acoustics, Speech, and Signal Process-                 speech tagging with a cyclic dependency network.
  ing, Toulouse, France.                                            In Proceedings of the Conference of the North Amer-
                                                                    ican Chapter of the Association for Computational
Simon Perkins, Kevin Lacker, and James Theiler.                     Linguistics on Human Language Technology, pages
  2003. Grafting: Fast, incremental feature selection               173–180.
  by gradient descent in function space. Journal of
  Machine Learning Research, 3:1333–1356.                         Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
                                                                    niadou. 2009. Stochastic gradient descent training
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav                    for l1-regularized log-linear models with cumula-
  Zimak. 2005. Learning and inference over con-                     tive penalty. In Proceedings of the Joint Conference
  strained output. In Proceedings of the International              of the Annual Meeting of the Association for Com-
  Joint Conference on Artificial Intelligence, pages                putational Linguistics and the International Joint
  1124–1129.                                                        Conference on Natural Language Processing, pages
                                                                    477–485, Suntec, Singapore.
Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing
  Huang, and Lide Wu. 2009. Sparse higher order                   S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark
  conditional random fields for improved sequence la-                Schmidt, and Kevin Murphy. 2006. Accelerated
  beling. In Proceedings of the Annual International                 training of conditional random fields with stochas-
  Conference on Machine Learning, pages 849–856.                     tic gradient methods. In Proceedings of the 23th In-
                                                                     ternational Conference on Machine Learning, pages
Stefan Riezler and Alexander Vasserman. 2004. Incre-                 969–976. ACM Press, New York, NY, USA.
   mental feature selection and l1 regularization for re-
   laxed maximum-entropy modeling. In Dekang Lin                  Hui Zhou and Trevor Hastie. 2005. Regularization and
   and Dekai Wu, editors, Proceedings of the confer-                variable selection via the elastic net. J. Royal. Stat.
   ence on Empirical Methods in Natural Language                    Soc. B., 67(2):301–320.
   Processing, pages 174–181, Barcelona, Spain, July.


                                                            513

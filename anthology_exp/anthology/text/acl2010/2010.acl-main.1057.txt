          Identifying Non-explicit Citing Sentences for Citation-based
                                Summarization
                   Vahed Qazvinian                                       Dragomir R. Radev
                  Department of EECS                                   Department of EECS and
                 University of Michigan                                 School of Information
                    Ann Arbor, MI                                       University of Michigan
                 vahed@umich.edu                                           Ann Arbor, MI
                                                                        radev@umich.edu

                      Abstract                                  However, the citation to a paper may not always
                                                                include explicit information about the cited paper:
    Identifying background (context) informa-
    tion in scientific articles can help schol-                  “This approach is one of those described in Eis-
    ars understand major contributions in their                     ner (1996)”
    research area more easily. In this paper,
    we propose a general framework based                        Although this sentence alone does not provide any
    on probabilistic inference to extract such                  information about the cited paper, it suggests that
    context information from scientific papers.                 its surrounding sentences describe the proposed
    We model the sentences in an article and                    approach in Eisner’s paper:
    their lexical similarities as a Markov Ran-
    dom Field tuned to detect the patterns that                  “... In an all pairs approach, every possible
    context data create, and employ a Belief                     pair of two tokens in a sentence is considered
    Propagation mechanism to detect likely                       and some score is assigned to the possibility of
    context sentences. We also address the                       this pair having a (directed) dependency rela-
    problem of generating surveys of scien-                      tion. Using that information as building blocks,
    tific papers. Our experiments show greater                   the parser then searches for the best parse for
    pyramid scores for surveys generated us-                     the sentence. This approach is one of those de-
    ing such context information rather than                     scribed in Eisner (1996).”
    citation sentences alone.
                                                                   We refer to such implicit citations that contain
1   Introduction                                                information about a specific secondary source but
                                                                do not explicitly cite it, as sentences with con-
In scientific literature, scholars use citations to re-
                                                                text information or context sentences for short.
fer to external sources. These secondary sources
                                                                We look at the patterns that such sentences cre-
are essential in comprehending the new research.
                                                                ate and observe that context sentences occur with-
Previous work has shown the importance of cita-
                                                                ing a small neighborhood of explicit citations. We
tions in scientific domains and indicated that ci-
                                                                also discuss the problem of extracting context sen-
tations include survey-worthy information (Sid-
                                                                tences for a source-reference article pair. We pro-
dharthan and Teufel, 2007; Elkiss et al., 2008;
                                                                pose a general framework that looks at each sen-
Qazvinian and Radev, 2008; Mohammad et al.,
                                                                tence as a random variable whose value deter-
2009; Mei and Zhai, 2008).
                                                                mines its state about the target paper. In summary,
   A citation to a paper in a scientific article may
                                                                our proposed model is based on the probabilistic
contain explicit information about the cited re-
                                                                inference of these random variables using graphi-
search. The following example is an excerpt from
                                                                cal models. Finally we give evidence on how such
a CoNLL paper1 that contains information about
                                                                sentences can help us produce better surveys of re-
Eisner’s work on bottom-up parsers and the notion
                                                                search areas. The rest of this paper is organized as
of span in parsing:
                                                                follows. Preceded by a review of prior work in
 “Another use of bottom-up is due to Eisner                     Section 2, we explain the data collection and our
    (1996), who introduced the notion of a span.”               annotation process in Section 3. Section 4 explains
    1
      Buchholz and Marsi “CoNLL-X Shared Task On Multi-         our methodology and is followed by experimental
lingual Dependency Parsing”, CoNLL 2006                         setup in Section 5.


                                                          555
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                                                                        #Refs
 ACL-ID       Author                Title                                                 Year    all     AAN    # Sents
 P08-2026     McClosky & Charniak   Self-Training for Biomedical Parsing                  2008    12         8     102
 N07-1025∗    Mihalcea              Using Wikipedia for Automatic ...                     2007    21        12     153
 N07-3002     Wang                  Learning Structured Classifiers ...                   2007    22        14      74
 P06-1101     Snow et, al.          Semantic Taxonomy Induction ...                       2006    19         9     138
 P06-1116     Abdalla & Teufel      A Bootstrapping Approach To ...                       2006    24        10     231
 W06-2933     Nivre et, al.         Labeled Pseudo-Projective Dependency ...              2006    27         5      84
 P05-1044     Smith & Eisner        Contrastive Estimation: Training Log-Linear ...       2005    30        13     262
 P05-1073     Toutanova et, al.     Joint Learning Improves Semantic Role Labeling        2005    14        10     185
 N03-1003     Barzilay & Lee        Learning To Paraphrase: An Unsupervised ...           2003    26        13     203
 N03-2016∗    Kondrak et, al.       Cognates Can Improve Statistical Translation ...      2003     8         5      92

Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publi-
cation year, number of references (in AAN) and number of sentences. Papers marked with ∗ are used to
calculate inter-judge agreement.


2   Prior Work                                                search papers and evaluates the result using 4 an-
                                                              notated articles.
Analyzing the structure of scientific articles and
                                                                 In our work we use graphical models to ex-
their relations has received a lot of attention re-
                                                              tract context sentences. Graphical models have
cently. The structure of citation and collaboration
                                                              a number of properties and corresponding tech-
networks has been studied in (Teufel et al., 2006;
                                                              niques and have been used before on Information
Newman, 2001), and summarization of scientific
                                                              Retrieval tasks. Romanello et al, (Romanello et
documents is discussed in (Teufel and Moens,
                                                              al., 2009) use Conditional Random Fields (CRF)
2002). In addition, there is some previous work
                                                              to extract references from unstructured text in dig-
on the importance of citation sentences. Elkiss et
                                                              ital libraries of classic texts. Similar work include
al, (Elkiss et al., 2008) perform a large-scale study
                                                              term dependency extraction (Metzler and Croft,
on citations in the free PubMed Central (PMC)
                                                              2005), query expansion (Metzler and Croft, 2007),
and show that they contain information that may
                                                              and automatic feature selection (Metzler, 2007).
not be present in abstracts. In other work, Nanba
et al, (Nanba and Okumura, 1999; Nanba et al.,
                                                              3 Data
2004b; Nanba et al., 2004a) analyze citation sen-
tences and automatically categorize them in order             The ACL Anthology Network (AAN)2 is a col-
to build a tool for survey generation.                        lection of papers from the ACL Anthology3 pub-
   The text of scientific citations has been used in          lished in the Computational Linguistics journal
previous research. Bradshaw (Bradshaw, 2002;                  and proceedings from ACL conferences and work-
Bradshaw, 2003) uses citations to determine the               shops and includes more than 14, 000 papers over
content of articles. Similarly, the text of cita-             a period of four decades (Radev et al., 2009).
tion sentences has been directly used to produce              AAN includes the citation network of the papers
summaries of scientific papers in (Qazvinian and              in the ACL Anthology. The papers in AAN are
Radev, 2008; Mei and Zhai, 2008; Mohammad                     publicly available in text format retrieved by an
et al., 2009). Determining the scientific attribu-            OCR process from the original pdf files, and are
tion of an article has also been studied before.              segmented into sentences.
Siddharthan and Teufel (Siddharthan and Teufel,                  To build a corpus for our experiments we picked
2007; Teufel, 2005) categorize sentences accord-              10 recently published papers from various areas
ing to their role in the author’s argument into pre-          in NLP4 , each of which had references for a to-
defined classes: Own, Other, Background, Tex-                 tal of 203 candidate paper-reference pairs. Table 1
tual, Aim, Basis, Contrast.                                   lists these papers together with their authors, titles,
   Little work has been done on automatic cita-               publication year, number of references, number of
tion extraction from research papers. Kaplan et               references within AAN, and the number of sen-
al, (Kaplan et al., 2009) introduces “citation-site”             2
                                                                   http://clair.si.umich.edu/clair/anthology/
as a block of text in which the cited text is dis-               3
                                                                   http://www.aclweb.org/anthology-new/
cussed. The mentioned work uses a machine                        4
                                                                   Regardless of data selection, the methodology in this
learning method for extracting citations from re-             work is applicable to any of the papers in AAN.


                                                        556


L&PS&al Sentence                                                                   ACL-ID      vector size   # Annotations        κ
        ···
 C C    Jacquemin (1999) and Barzilay and McKeown (2001) identify
                                                                                   N07-1025∗      153             21         0.889 ± 0.30
        phrase level paraphrases, while Lin and Pantel (2001) and                  N03-2016∗       92              8         0.853 ± 0.35
        Shinyama et al. (2002) acquire structural paraphrases encoded
        as templates.                                                             Table 3: Average κ coefficient as inter-judge
 1  1   These latter are the most closely related to the sentence-level para-
        phrases we desire, and so we focus in this section on template-           agreement for annotations of two sets
        induction approaches.
 C 0    Lin and Pantel (2001) extract inference rules, which are related
        to paraphrases (for example, X wrote Y implies X is the author of
        Y), to improve question answering.
 1  0   They assume that paths in dependency trees that take similar argu-        tions were already marked with Cs. The annota-
        ments (leaves) are close in meaning.
 1  0   However, only two-argument templates are considered.                      tion guidelines instructed the annotator to look at
 0 C    Shinyama et al. (2002) also use dependency-tree information to
        extract templates of a limited form (in their case, determined by
                                                                                  each explicit citation sentence, and read up to 15
        the underlying information extraction application).                       sentences before and after, then mark context sen-
 1  1   Like us (and unlike Lin and Pantel, who employ a single large
        corpus), they use articles written about the same event in different      tences around that sentence with 1s. Next, the 29
        newspapers as data.                                                       annotation instances done by the external annota-
 1  1   Our approach shares two characteristics with the two methods just
        described: pattern comparison by analysis of the patterns respec-         tor were compared with the corresponding anno-
        tive arguments, and use of nonparallel corpora as a data source.
 0  0   However, extraction methods are not easily extended to generation         tations that we did, and the Kappa coefficient (κ)
        methods.
 1  1   One problem is that their templates often only match small frag-
                                                                                  was calculated. The κ statistic is formulated as
        ments of a sentence.
 1  1   While this is appropriate for other applications, deciding whether                             Pr(a) − Pr(e)
        to use a given template to generate a paraphrase requires informa-                       κ=
        tion about the surrounding context provided by the entire sentence.                              1 − Pr(e)
        ···

                                                                                  where P r(a) is the relative observed agreement
Table 2: Part of the annotation for N03-1003 with
                                                                                  among raters, and P r(e) is the probability that an-
respect to two of its references “Lin and Pan-
                                                                                  notators agree by chance if each annotator is ran-
tel (2001)” (the first column) “Shinyama et al.
                                                                                  domly assigning categories. To calculate κ, we ig-
(2002)” (the second column). Cs indicate explicit
                                                                                  nored all explicit citations (since they were pro-
citations, 1s indicate implicit citations and 0s are
                                                                                  vided to the external annotator) and used the bi-
none.
                                                                                  nary categories (i.e., 1 for context sentences, and
                                                                                  0 otherwise) for all other sentences. Table 3 shows
tences.                                                                           the annotation vector size (i.e., number of sen-
                                                                                  tences), number of annotation instances (i.e., num-
3.1 Annotation Process
                                                                                  ber of references), and average κ for each set. The
We annotated the sentences in each paper from Ta-                                 average κ is above 0.85 in both cases, suggest-
ble 1. Each annotation instance in our setting cor-                               ing that the annotation process has a low degree
responds to a paper-reference pair, and is a vec-                                 of subjectivity and can be considered reliable.
tor in which each dimension corresponds to a sen-
tence and is marked with a C if it explicitly cites                               3.2 Analysis
the reference, and with a 1 if it implicitly talks                                In this section we describe our analysis. First,
about it. All other sentences are marked with 0s.                                 we look at the number of explicit citations each
Table 2 shows a portion of two separate annota-                                   reference has received in a paper. Figure 1 (a)
tion instances of N03-1003 corresponding to two                                   shows the histogram corresponding to this distri-
of its references. Our annotation has resulted in                                 bution. It indicates that the majority of references
203 annotation instances each corresponding to                                    get cited in only 1 sentence in a scientific arti-
one paper-reference pair. The goal of this work                                   cle, while the maximum being 9 in our collected
is to automatically identify all context sentences,                               dataset with only 1 instance (i.e., there is only 1
which are marked as “1”.                                                          reference that gets cited 9 times in a paper). More-
                                                                                  over, the data exhibits a highly positive-skewed
3.1.1 Inter-judge Agreement
                                                                                  distribution. This is illustrated on a log-log scale
We also asked a neutral annotator5 to annotate                                    in Figure 1 (b). This highly skewed distribution
two of our datasets that are marked with ∗ in Ta-                                 indicates that the majority of references get cited
ble 1. For each paper-reference pair, the annotator                               only once in a citing paper. The very small number
was provided with a vector in which explicit cita-                                of citing sentences can not make a full inventory of
  5
    Someone not involved with the paper but an expert in                          the contributions of the cited paper, and therefore,
NLP.                                                                              extracting explicit citations alone without context


                                                                            557


     gap size              0            1        2   4         9   10          15    16                  tences. In summary, each sentence is represented
     instance             273           14       2   1         2   1           1      1
                                                                                                         with a node and is given two scores (context, non-
Table 4: The distribution of gaps in the annotated                                                       context), and we update these scores to be in har-
data                                                                                                     mony with the neighbors’ scores.
                                                                                                            A particular class of graphical models known
                                                                                                         as Markov Random Fields (MRFs) are suited for
sentences may result in information loss about the
                                                                                                         solving inference problems with uncertainty in ob-
contributions of the cited paper.
                                                                                                         served data. The data is modeled as an undirected
    140                                               0
                                                                        p(cit)                           graph with two types of nodes: hidden and ob-
                                                     10

    120
                                                                        alpha = 3.13; D=0.02
                                                                                                         served. Observed nodes represent values that are
    100
                                                      −1
                                                     10
                                                                                                         known from the data. Each hidden node xu , cor-
     80
                                                                                                         responding to an observed node yu , represents the
     60
                                                      −2
                                                     10                                                  true state underlying the observed value. The state
     40


     20
                                                                                                         of a hidden node is related to the value of its cor-
      0
          1   2   3   4   5     6   7   8    9
                                                      −3
                                                     10    0
                                                          10                               10
                                                                                               1
                                                                                                         responding observed node as well as the states of
                          cit
                                                                         cit
                                                                                                         its neighboring hidden nodes.
                          a                                             b                                   The local Markov property of an MRF indi-
                                                                                                         cates that a variable is conditionally independent
Figure 1: (a) Histogram of the number of differ-                                                         on all other variables given its neighbors: xv ⊥
ent citations to each reference in a paper. (b) The                                                      ⊥ xV \cl(v) |xne(v) , where ne(v) is the set of neigh-
distribution observed for the number of different                                                        bors of v, and cl(v) = {v} ∪ ne(v) is the closed
citations on a log-log scale.                                                                            neighborhood of v. Thus, the state of a node is as-
                                                                                                         sumed to statistically depend only upon its hidden
   Next, we investigate the distance between con-                                                        node and each of its neighbors, and independent
text sentences and the closest citations. For each                                                       of any other node in the graph given its neighbors.
context sentence, we find its distance to the clos-                                                         Dependencies in an MRF are represented using
ets context sentence or explicit citation. Formally,                                                     two functions: Compatibility function (ψ) and Po-
we define the gap to be the number of sentences                                                          tential function (φ). ψuv (xc , xd ) shows the edge
between a context sentence (marked with 1) and                                                           potential of an edge between two nodes u, v of
the closest context sentence or explicit citation                                                        classes xc and xd . Large values of ψuv would
(marked with either C or 1) to it. For example,                                                          indicate a strong association between xc and xd
the second column of Table 2 shows that there is a                                                       at nodes u, v. The Potential function, φi (xc , yc ),
gap of size 1 in the 9th sentence in the set of con-                                                     shows the statistical dependency between xc and
text and citation sentences about Shinyama et al.                                                        yc at each node i assumed by the MRF model.
(2002). Table 4 shows the distribution of gap sizes                                                         In order to find the marginal probabilities of
in the annotated data. This observation suggests                                                         xi s in a MRF we can use Belief Propagation
that the majority of context sentences directly oc-                                                      (BP) (Yedidia et al., 2003). If we assume the yi s
cur after or before a citation or another context                                                        are fixed and show φi (xi , yi ) by φi (xi ), we can
sentence. However, it shows that gaps between                                                            find the joint probability distribution for unknown
sentences describing a cited paper actually exist,                                                       variables xi as
and a proposed method should have the capability
                                                                                                                            1 Y                  Y
to capture them.                                                                                                p({x}) =          ψij (xi , xj )   φi (xi )
                                                                                                                           Z ij                  i
4     Proposed Method
                                                                                                            In the BP algorithm a set of new variables m is
In this section we propose our methodology that                                                          introduced where mij (xj ) is the message passed
enables us to identify the context information of a                                                      from i to j about what state xj should be in. Each
cited paper. Particularly, the task is to assign a bi-                                                   message, mij (xj ), is a vector with the same di-
nary label XC to each sentence Si from a paper S,                                                        mensionality of xj in which each dimension shows
where XC = 1 shows a context sentence related                                                            i’s opinion about j being in the corresponding
to a given cited paper, C. To solve this problem                                                         class. Therefore each message could be consid-
we propose a systematic way to model the net-                                                            ered as a probability distribution and its compo-
work level relationship between consecutive sen-                                                         nents should sum up to 1. The final belief at a


                                                                                                   558


                                                                                 (a)                          (b)
Figure 2: The illustration of the message updating
rule. Elements that make up the message from a
                                                                         Figure 3: The structure of the MRF constructed
node i to another node j: messages from i’s neigh-
                                                                         based on the independence of non-adjacent sen-
bors, local evidence at i, and propagation function
                                                                         tences; (a) left, each sentence is independent on
between i, j summed over all possible states of
                                                                         all other sentences given its immediate neighbors.
node i.
                                                                         (b) right, sentences have dependency relationship
                                                                         with each other regardless of their position.
node i, in the BP algorithm, is also a vector with
the same dimensionality of messages, and is pro-
                                                                      depends on its surrounding sentences. Said dif-
portional to the local evidence as well as all mes-
                                                                      ferently, each sentence is written independently of
sages from the node’s neighbors:
                                                                      all other sentences given a number of its neigh-
                               Y
      bi (xi ) ← kφi (xi )             mji (xi )             (1)      bors. This local dependence assumption can result
                             j∈ne(i)                                  in a number of different MRFs, each built assum-
                                                                      ing a dependency between a sentence and all sen-
   where k is the normalization factor of the be-                     tences within a particular distance. Figure 3 shows
liefs about different classes. The message passed                     the structure of the two MRFs at either extreme of
from i to j is proportional to the propagation func-                  the local dependence assumption. In Figure 3 a,
tion between i, j, the local evidence at i, and all                   each sentence only depends on one following and
messages sent to i from its neighbors except j:                       one preceding sentence, while Figure 3 b shows
                                                                      an MRF in which sentences are dependent on each
              X                               Y                       other regardless of their position. We refer to the
mij (xj ) ←        φi (xi )ψij (xi , xj )               mki (xi )     former by BP1 , and to the latter by BPn . Gen-
                                                                    (2)
              xi                            k∈ne(i)\j                 erally, we use BPi to denote an MRF in which
                                                                      each sentence is connected to i sentences before
   Figure 2 illustrates the message update rule.                      and after.
   Convergence can be determined based on a va-
riety of criteria. It can occur when the maximum                                   ψij (xc , xd )   xd = 0      xd = 1
change of any message between iteration steps is                                   xc = 0             0.5         0.5
less than some threshold. Convergence is guaran-                                   xc = 1           1 − Sij       Sij
teed for trees but not for general graphs. However,
it typically occurs in practice (McGlohon et al.,                        Table 5: The compatibility function ψ between
2009). Upon convergence, belief scores are deter-                        any two nodes in the MRFs from the sentences in
mined by Equation 1.                                                     scientific papers

4.1 MRF construction
To find the sentences from a paper that form the                         4.2 Compatibility Function
context information of a given cited paper, we                           The compatibility function of an MRF represents
build an MRF in which a hidden node xi and                               the association between the hidden node classes.
an observed node yi correspond to each sentence.                         A node’s belief to be in class 1 is its probability to
The structure of the graph associated with the                           be included in the context. The belief of a node i,
MRF is dependent upon the validity of a basic as-                        about its neighbor j to be in either classes is as-
sumption. This assumption indicates that the gen-                        sumed to be 0.5 if i is in class 0. In other words, if
eration of a sentence (in form of its words) only                        a node is not part of the context itself, we assume


                                                                   559


it has no effect on its neighbors’ classes. In con-              normalized fi as the unweighted linear combina-
trast, if i is in class 1 its belief about its neighbor          tion of individual features. Based on fi s, we com-
j is determined by their mutual lexical similarity.              pute the potential function, φ, as shown in Table 6.
If this similarity is close to 1 it indicates a stronger
tie between i, j. However, if i, j are not similar,                         φi (xc , yc )   xc = 0    xc = 1
i’s probability of being in class 1, should not af-                                         1 − fi      fi
fect that of j’s. To formalize this assumption we
use the sigmoid of the cosine similarity of two sen-             Table 6: The node potential function φ for each
tences to build ψ. More formally, we define S to                 node in the MRFs from the sentences in scientific
be                                                               papers is built using the sentences’ flags computed
                                1                                using sentence level features.
                 Sij =        −cosine(i,j)
                        1+e
   The sigmoid function obtains a value of 0.5 for               5 Experiments
a cosine of 0 indicating that there is no bias in the
association of the two sentences. The matrix in Ta-              The intrinsic evaluation of our methodology
ble 5 shows the compatibility function built based               means to directly compare the output of our
on the above arguments.                                          method with the gold standards obtained from the
                                                                 annotated data. Our methodology finds the sen-
4.3 Potential Function                                           tences that cite a reference implicitly. Therefore
                                                                 the output of the inference method is a vector, υ,
The node potential function of an MRF can incor-                 of 1’s and 0’s, whereby a 1 at element i means
porate some other features observable from data.                 that sentence i in the source document is a con-
Here, the goal is to find all sentences that are about           text sentence about the reference while a 0 means
a specific cited paper, without having explicit cita-            an explicit citation or neither. The gold standard
tions. To build the node potential function of the               for each paper-reference pair, ω (obtained from the
observed nodes, we use some sentence level fea-                  annotated vectors in Section 3.1 by changing all
tures. First, we use the explicit citation as an im-             Cs to 0s), is also a vector of the same format and
portant feature of a sentence. This feature can af-              dimensionality.
fect the belief of the corresponding hidden node,                   Precision, recall, and Fβ for this task can be de-
which can in turn affect its neighbors’ beliefs. For             fined as
a given paper-reference pair, we flag (with a 1)
each sentence that has an explicit citation to the
                                                                        υ·ω           υ·ω            (1 + β 2 )p · r
reference.                                                         p=       ;    r=       ;   Fβ =                     (3)
                                                                        υ·1           ω·1               β2p + r
   The second set of features that we are inter-
ested in are discourse-based features. In particu-                where 1 is a vector of 1’s with the same dimen-
lar we match each sentence with specific patterns                sionality and β is a non-negative real number.
and flag those that match. The first pattern is a bi-
gram in which the first term matches any of “this;               5.1 Baseline Methods
that; those; these; his; her; their; such; previ-                The first baseline that we use is an IR-based
ous”, and the second term matches any of “work;                  method. This baseline, B1 , takes explicit citations
approach; system; method; technique; result; ex-                 as an input but use them to find context sentences.
ample”. The second pattern includes all sentences                Given a paper-reference pair, for each explicit ci-
that start with “this; such”.                                    tation sentence, marked with C, B1 picks its pre-
   Finally, the similarity of each sentence to the               ceding and following sentences if their similarities
reference is observable from the data and can be                 to that sentence is greater than a cutoff (the median
used as a sentence-level feature. Intuitively, if a              of all such similarities), and repeats this for neigh-
sentence has higher similarity with the reference                boring sentences of newly marked sentences. In-
paper, it should have a higher potential of being                tuitively, B1 tries to find the best chain (window)
in class 1 or C. The flag of each sentence here is               around citing sentences.
a value between 0 and 1 and is determined by its                    As the second baseline, we use the hand-crafted
cosine similarity to the reference. Once the flags               discourse based features used in MRF’s potential
for each sentence, Si are determined, we calculate               function. Particularly, this baseline, B2 , marks


                                                           560


                     paper          B1       B2         SVM         BP1       BP4       BPn
                     P08-2026      0.441    0.237       0.249       0.470     0.613     0.285
                     N07-1025      0.388    0.102       0.124       0.313     0.466     0.138
                     N07-3002      0.521    0.339       0.232       0.742     0.627     0.315
                     P06-1101      0.125    0.388       0.127       0.649     0.889     0.193
                     P06-1116      0.283    0.104       0.100       0.307     0.341     0.130
                     W06-2933      0.313    0.100       0.176       0.338     0.413     0.160
                     P05-1044      0.225    0.100       0.060       0.172     0.586     0.094
                     P05-1073      0.144    0.100       0.144       0.433     0.518     0.171
                     N03-1003      0.245    0.249       0.126       0.523     0.466     0.125
                     N03-2016      0.100    0.181       0.224       0.439     0.482     0.185

Table 7: Average Fβ=3 for similarity based baseline (B1 ), discourse-based baseline (B2 ), a supervised
method (SVM) and three MRF-based methods.


each sentence that is within a particular distance               Table 7 shows Fβ=3 for our experiments and
(4 in our experiments) of an explicit citation and            shows how BP4 outperforms the other methods
matches one of the two patterns mentioned in Sec-             on average. The value 4 may suggest the fact that
tion 4.3. After marking all such sentences, B2                although sentences might be independent of dis-
also marks all sentences between them and the                 tant sentences, they depend on more than one sen-
closest explicit citation, which is no farther than           tence on each side.
4 sentences away. This baseline helps us under-                  The final experiment we do to intrinsically eval-
stand how effectively this sentence level feature             uate the MRF-base method is to compare differ-
can work in the absence of other features and the             ent sentence-level features. The first feature used
network structure.                                            to build the potential function is explicit citations.
   Finally, we use a supervised method, SVM,                  This feature does not directly affect context sen-
to classify sentences as context/non-context. We              tences (i.e., it affects the marginal probability of
use 4 features to train the SVM model. These                  context sentences through the MRF network con-
4 features comprise the 3 sentence level features             nections). Therefore, we do not alter this fea-
used in MRF’s potential function (i.e., similar-              ture in comparing different features. However, we
ity to reference, explicit citation, matching certain         look at the effect of the second and the third fea-
regular-expressions) and a network level feature:             tures: hand-crafted regular expression-based fea-
distance to the closes explicit citation. For each            tures and similarity to the reference. For each pa-
source paper, P , we use all other source papers              per, we use BP4 to perform 3 experiments: two in
and their source-reference annotation instances to            absence of each feature and one including all fea-
train a model. We then use this model to clas-                tures. Figure 4 shows the average Fβ=3 for each
sify all instances in P . Although the number of              experiment. This plot shows that the features lead
references and thus source-reference pairs are dif-           to better results when used together.
ferent for different papers, this can be considered
similar to a 10-fold cross validation scheme, since           6 Impact on Survey Generation
for each source paper the model is built using all            We also performed an extrinsic evaluation of
source-reference pairs of all other 9 papers.                 our context extraction methodology. Here we
   We compare these baselines with 3 MRF-based                show how context sentences add important survey-
systems each with a different assumption about in-            worthy information to explicit citations. Previous
dependence of sentences. BP1 denotes an MRF                   work that generate surveys of scientific topics use
in which each sentence is only connected to 1 sen-            the text of citation sentences alone (Mohammad
tence before and after. In BP4 locality is more               et al., 2009; Qazvinian and Radev, 2008). Here,
relaxed and each sentence is connected to 4 sen-              we show how the surveys generated using citations
tences on each sides. BPn denotes an MRF in                   and their context sentences are better than those
which all sentences are connected to each other               generated using citation sentences alone.
regardless of their position in the paper.                       We use the data from (Mohammad et al., 2009)


                                                        561


   ... Naturally, our current work on question answering for the reading comprehension task is most related to those of
   (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000 ; Wang et al. , 2000). In fact, all of this
   body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the
   same development set of stories. The work of (Hirschman et al. , 1999) initiated this series of work, and it reported
   an accuracy of 36.3% on answering the questions in the test stories. Subsequently, the work of (Riloffand Thelen ,
   2000) and (Chaxniak et al. , 2000) improved the accuracy further to 39.7% and 41%, respectively. However, all
   of these three systems used handcrafted, deterministic rules and algorithms...
   ...The cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM
   > S-SVM > Retrieval-M. Compared with retrieval-based [Yang et al. 2003], pattern-based [Ravichandran et al. 2002
   and Soubbotin et al. 2002], and deep NLP-based [Moldovan et al. 2002, Hovy et al. 2001; and Pasca et al. 2001]
   answer selection, machine learning techniques are more effective in constructing QA components from scratch. These
   techniques suffer, however, from the problem of requiring an adequate number of handtagged question-answer
   training pairs. It is too expensive and labor intensive to collect such training pairs for supervised machine
   learning techniques ...
   ... As expected, the definition and person-bio answer types are covered well by these resources. The web has
   been employed for pattern acquisition (Ravichandran et al. , 2003), document retrieval (Dumais et al. , 2002), query
   expansion (Yang et al. , 2003), structured information extraction, and answer validation (Magnini et al. , 2002). Some
   of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a
   less complex approach to find correct answers ...

       Table 8: A portion of the QA survey generated by LexRank using the context information.


                                                                                   citation survey context survey
                                                                                                  QA
                                                                  CT nuggets            0.416          0.634
                                                                  AB nuggets            0.397          0.594
                                                                                                  DP
                                                                  CT nuggets            0.324          0.379

                                                                 Table 9: Pyramid Fβ=3 scores of automatic
                                                                 surveys of QA and DP data. The QA surveys
                                                                 are evaluated using nuggets drawn from citation
                                                                 texts (CT), or abstracts (AB), and DP surveys are
                                                                 evaluated using nuggets from citation texts (CT).


Figure 4: Average Fβ=3 for BP4 employing dif-
ferent features.                                                 BP4 marks them as context sentences. Therefore,
                                                                 we build a new corpus in which each explicit ci-
                                                                 tation sentence is replaced with the same sentence
that contains two sets of cited papers and corre-                attached to at most 4 sentence on each side.
sponding citing sentences, one on Question An-                      After building the context corpus, we use
swering (QA) with 10 papers and the other on De-                 LexRank (Erkan and Radev, 2004) to generate 2
pendency Parsing (DP) with 16 papers. The QA                     QA and 2 DP surveys using the citation sentences
set contains two different sets of nuggets extracted             only, and the new context corpus explained above.
by experts respectively from paper abstracts and                 LexRank is a multidocument summarization sys-
citation sentences. The DP set includes nuggets                  tem, which first builds a cosine similarity graph of
extracted only from citation sentences. We use                   all the candidate sentences. Once the network is
these nugget sets, which are provided in form of                 built, the system finds the most central sentences
regular expressions, to evaluate automatically gen-              by performing a random walk on the graph. We
erated summaries. To perform this experiment we                  limit these surveys to be of a maximum length of
needed to build a new corpus that includes con-                  1000 words. Table 8 shows a portion of the sur-
text sentences. For each citation sentence, BP4 is               vey generated from the QA context corpus. This
used on the citing paper to extract the proper con-              example shows how context sentences add mean-
text. Here, we limit the context size to be 4 on                 ingful and survey-worthy information along with
each side. That is, we attach to a citing sentence               citation sentences. Table 9 shows the Pyramid
any of its 4 preceding and following sentences if                Fβ=3 score of automatic surveys of QA and DP


                                                           562


data. The QA surveys are evaluated using nuggets             References
drawn from citation texts (CT), or abstracts (AB),           Shannon Bradshaw. 2002. Reference Directed Index-
and DP surveys are evaluated using nuggets from                ing: Indexing Scientific Literature in the Context of
citation texts (CT). In all evaluation instances the           Its Use. Ph.D. thesis, Northwestern University.
surveys generated with the context corpora excel
                                                             Shannon Bradshaw. 2003. Reference directed index-
at covering nuggets drawn from abstracts or cita-              ing: Redeeming relevance for subject search in ci-
tion sentences.                                                tation indexes. In Proceedings of the 7th European
                                                               Conference on Research and Advanced Technology
                                                               for Digital Libraries.
7   Conclusion
                                                             Aaron Elkiss, Siwei Shen, Anthony Fader, Güneş
                                                               Erkan, David States, and Dragomir R. Radev. 2008.
In this paper we proposed a framework based on                 Blind men and elephants: What do citation sum-
probabilistic inference to extract sentences that              maries tell us about a research article? Journal of
appear in the scientific literature, and which are             the American Society for Information Science and
                                                               Technology, 59(1):51–62.
about a secondary source, but which do not con-
tain explicit citations to that secondary source.            Güneş Erkan and Dragomir R. Radev. 2004. Lexrank:
Our methodology is based on inference in an MRF                Graph-based centrality as salience in text summa-
                                                               rization. Journal of Artificial Intelligence Research
built using the similarity of sentences and their              (JAIR).
lexical features. We show, by numerical exper-
iments, that an MRF in which each sentence is                Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009.
connected to only a few adjacent sentences prop-               Automatic extraction of citation contexts for re-
                                                               search paper summarization: A coreference-chain
erly fits this problem. We also investigate the use-           based approach. In Proceedings of the 2009 Work-
fulness of such sentences in generating surveys of             shop on Text and Citation Analysis for Scholarly
scientific literature. Our experiments on generat-             Digital Libraries, pages 88–95, Suntec City, Sin-
ing surveys for Question Answering and Depen-                  gapore, August. Association for Computational Lin-
                                                               guistics.
dency Parsing show how surveys generated using
such context information along with citation sen-            Mary McGlohon, Stephen Bay, Markus G. Anderle,
tences have higher quality than those built using             David M. Steier, and Christos Faloutsos. 2009.
                                                              Snare: a link analytic system for graph labeling and
citations alone.
                                                              risk detection. In KDD ’09: Proceedings of the 15th
   Generating fluent scientific surveys is difficult          ACM SIGKDD international conference on Knowl-
in absence of sufficient background information.              edge discovery and data mining, pages 1265–1274.
Our future goal is to combine summarization                  Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
and bibliometric techniques towards building au-               impact-based summaries for scientific literature. In
tomatic surveys that employ context information                Proceedings of ACL ’08, pages 816–824.
as an important part of the generated surveys.               Donald Metzler and W. Bruce Croft. 2005. A markov
                                                               random field model for term dependencies. In SI-
                                                               GIR ’05: Proceedings of the 28th annual interna-
8   Acknowledgments                                            tional ACM SIGIR conference on Research and de-
                                                               velopment in information retrieval, pages 472–479.
The authors would like to thank Arzucan Özgür              Donald Metzler and W. Bruce Croft. 2007. Latent con-
from University of Michigan for annotations.                   cept expansion using markov random fields. In SI-
   This paper is based upon work supported by the              GIR ’07: Proceedings of the 30th annual interna-
                                                               tional ACM SIGIR conference on Research and de-
National Science Foundation grant ”iOPENER: A                  velopment in information retrieval, pages 311–318.
Flexible Framework to Support Rapid Learning in
Unfamiliar Research Domains”, jointly awarded                Donald A. Metzler. 2007. Automatic feature selection
to University of Michigan and University of Mary-              in the markov random field model for information
                                                               retrieval. In CIKM ’07: Proceedings of the sixteenth
land as IIS 0705832. Any opinions, findings, and               ACM conference on Conference on information and
conclusions or recommendations expressed in this               knowledge management, pages 253–262.
paper are those of the authors and do not necessar-
                                                             Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
ily reflect the views of the National Science Foun-            Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
dation.                                                        Dragomir Radev, and David Zajic. 2009. Using ci-
                                                               tations to generate surveys of scientific paradigms.


                                                       563


  In Proceedings of Human Language Technologies:
  The 2009 Annual Conference of the North American
  Chapter of the Association for Computational Lin-
  guistics, pages 584–592, Boulder, Colorado, June.
  Association for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
  wards multi-paper summarization using reference
  information. In IJCAI1999, pages 926–931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-
  mura, and Suguru Saito. 2004a. Bilingual presri:
  Integration of multiple research paper databases. In
  Proceedings of RIAO 2004, pages 195–211, Avi-
  gnon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Oku-
  mura. 2004b. Classification of research papers us-
  ing citation links and citation types: Towards au-
  tomatic review article generation. In Proceedings
  of the 11th SIG Classification Research Workshop,
  pages 117–134, Chicago, USA.
Mark E. J. Newman. 2001. The structure of scientific
 collaboration networks. PNAS, 98(2):404–409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
  entific paper summarization using citation summary
  networks. In COLING 2008, Manchester, UK.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
  hed Qazvinian. 2009. The ACL anthology network
  corpus. In ACL workshop on Natural Language
  Processing and Information Retrieval for Digital Li-
  braries.
Matteo Romanello, Federico Boschetti, and Gregory
 Crane. 2009. Citations in the digital library of clas-
 sics: Extracting canonical references by using con-
 ditional random fields. In Proceedings of the 2009
 Workshop on Text and Citation Analysis for Schol-
 arly Digital Libraries, pages 80–87, Suntec City,
 Singapore, August. Association for Computational
 Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
  idea was this, and why does it matter? attribut-
  ing scientific work to citations. In Proceedings of
  NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summarizing
  scientific articles: experiments with relevance and
  rhetorical status. Comput. Linguist., 28(4):409–445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
  2006. Automatic classification of citation function.
  In Proceedings of the EMNLP, Sydney, Australia,
  July.
Simone Teufel. 2005. Argumentative Zoning for Im-
  proved Citation Indexing. Computing Attitude and
  Affect in Text: Theory and Applications, pages 159–
  170.
Jonathan S. Yedidia, William T. Freeman, and Yair
  Weiss. 2003. Understanding belief propagation and
  its generalizations. pages 239–269.



                                                          564

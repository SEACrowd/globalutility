Sentence and Expression Level Annotation of Opinions in User-Generated
                              Discourse
                Cigdem Toprak and Niklas Jakob and Iryna Gurevych
                         Ubiquitous Knowledge Processing Lab
    Computer Science Department, Technische Universität Darmstadt, Hochschulstraße 10
                            D-64289 Darmstadt, Germany
                           www.ukp.tu-darmstadt.de


                      Abstract                                    Compared to the newspaper and meeting dialog
    In this paper, we introduce a corpus of                    genres, little corpus-based work has been carried
    consumer reviews from the rateitall and                    out for interpreting the opinions and evaluations in
    the eopinions websites annotated with                      user-generated discourse. Due to the high popular-
    opinion-related information. We present                    ity of Web 2.0 communities1 , the amount of user-
    a two-level annotation scheme. In the                      generated discourse and the interest in the analysis
    first stage, the reviews are analyzed at                   of such discourse has increased over the last years.
    the sentence level for (i) relevancy to a                  To the best of our knowledge, there are two cor-
    given topic, and (ii) expressing an eval-                  pora of user-generated discourse which are anno-
    uation about the topic. In the second                      tated for opinion related information at the expres-
    stage, on-topic sentences containing eval-                 sion level: The corpus of Hu & Liu (2004) consists
    uations about the topic are further investi-               of customer reviews about consumer electronics,
    gated at the expression level for pinpoint-                and the corpus of Zhuang et al. (2006) consists of
    ing the properties (semantic orientation,                  movie reviews. Both corpora are tailored for ap-
    intensity), and the functional components                  plication specific needs, therefore, do not contain
    of the evaluations (opinion terms, targets                 certain related information explicitly annotated in
    and holders). We discuss the annotation                    the discourse, which we consider important (see
    scheme, the inter-annotator agreement for                  Section 2). Furthermore, none of these works pro-
    different subtasks and our observations.                   vide inter-annotator agreement studies.
                                                                  Our goal is to create sentence and expression
1   Introduction                                               level annotated corpus of customer reviews which
There has been a huge interest in the automatic                fulfills the following requirements: (1) It filters
identification and extraction of opinions from free            individual sentences regarding their topic rele-
text in recent years. Opinion mining spans a va-               vancy and the existence of an opinion or factual
riety of subtasks including: creating opinion word             information which implies an evaluation. (2) It
lexicons (Esuli and Sebastiani, 2006; Ding et al.,             identifies opinion expressions including the re-
2008), identifying opinion expressions (Riloff and             spective opinion target, opinion holder, modi-
Wiebe, 2003; Fahrni and Klenner, 2008), identi-                fiers, and anaphoric expressions if applicable. (3)
fying polarities of opinions in context (Breck et              The semantic orientation of the opinion expres-
al., 2007; Wilson et al., 2005), extracting opinion            sion is identified while considering negation, and
targets (Hu and Liu, 2004; Zhuang et al., 2006;                the opinion expression is linked to the respective
Cheng and Xu, 2008) and opinion holders (Kim                   holder and target in the discourse. Such a re-
and Hovy, 2006; Choi et al., 2005).                            source would (i) enable novel applications of opin-
   Data-driven approaches for extracting opinion               ion mining such as a fine-grained identification of
expressions, their holders and targets require re-             opinion properties, e.g. opinion modification de-
liably annotated data at the expression level. In              tection including negation, and (ii) enhance opin-
previous research, expression level annotation of              ion target extraction and the polarity assignment
opinions was extensively investigated on newspa-               by linking the opinion expression with its target
per articles (Wiebe et al., 2005; Wilson and Wiebe,               1
                                                                  http://blog.nielsen.com/nielsenwire/
2005; Wilson, 2008b) and on meeting dialogs (So-               wp-content/uploads/2008/10/press_
masundaran et al., 2008; Wilson, 2008a).                       release24.pdf


                                                         575
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


and providing anaphoric resolutions in discourse.             opinions in discourse and discover implicit evalu-
   We present an annotation scheme which fulfills             ations through link transitivity.
the mentioned requirements, an inter-annotator                    Similar to Somasundaran et al. (2008), Asher
agreement study, and discuss our observations.                et al. (2008) performs discourse level analysis of
The rest of this paper is structured as follows:              opinions. They propose a scheme which first iden-
Section 2 presents the related work. In Sections              tifies and assigns categories to the opinion seg-
3, we describe the annotation scheme. Section 4               ments as reporting, judgment, advice, or senti-
presents the data and the annotation study, while             ment; and then links the opinion segments with
Section 5 summarizes the main conclusions.                    each other via rhetorical relations including con-
                                                              trast, correction, support, result, or continuation.
2       Previous Opinion Annotated Corpora                    However, in contrast to our scheme and other
2.1      Newspaper Articles and Meeting Dialogs               schemes, instead of marking expression bound-
                                                              aries without any restriction they annotate an opin-
Most prominent work concerning the expres-
                                                              ion segment only if it contains an opinion word
sion level annotation of opinions is the Multi-
                                                              from their lexicon, or if it has a rhetorical relation
Perspective Question Answering (MPQA) corpus2
                                                              to another opinion segment.
(Wiebe et al., 2005). It was extended several times
over the last years, either by adding new docu-               2.2    User-generated Discourse
ments or annotating new types of opinion related              The two annotated corpora of user-generated con-
information (Wilson and Wiebe, 2005; Stoyanov                 tent and their corresponding annotation schemes
and Cardie, 2008; Wilson, 2008b). The MPQA                    are far less complex. Hu & Liu (2004) present
annotation scheme builds upon the private state               a dataset of customer reviews for consumer elec-
notion (Quirk et al., 1985) which describes men-              tronics crawled from amazon.com. The follow-
tal states including opinions, emotions, specula-             ing example shows two annotations taken from the
tions and beliefs among others. The annotation                corpus of Hu & Liu (2004):
scheme strives to represent the private states in             camera[+2]##This is my first digital camera and what a toy
terms of their functional components (i.e. expe-              it is...
riencer holding an attitude towards a target). It             size[+2][u]##it is small enough to fit easily in a coat pocket
consists of frames (direct subjective, expressive             or purse.
subjective element, objective speech event, agent,            The corpus provides only target and polarity anno-
attitude, and target frames) with slots represent-            tations, and do not contain opinion expression or
ing various attributes and properties (e.g.intensity,         opinion modifier annotations which lead to these
nested source) of the private states.                         polarity scores. The annotation scheme allows the
   Wilson (2008a) adapts and extends the concepts             annotation of implicit features (indicated with the
from the MPQA scheme to annotate subjective                   the attribute [u] ). Implicit features are not re-
content in meetings (AMI corpus), and creates the             solved to any actual product feature instances in
AMIDA scheme. Besides subjective utterances,                  discourse. In fact, the actual positions of the prod-
the AMIDA scheme contains objective polar ut-                 uct features (or any anaphoric references to them)
terances which annotates evaluations without ex-              are not explicitly marked in the discourse, i.e, it is
pressing explicit opinion expressions.                        unclear to which mention of the feature the opin-
   Somasundaran et al. (2008) proposes opinion                ion refers to.
frames for representing discourse level associa-                 In their paper on movie review mining and sum-
tions in meeting dialogs. The annotation scheme               marization, Zhuang et al. (2006) introduce an an-
focuses on two types of opinions, sentiment and               notated corpus of movie reviews from the Internet
arguing. It annotates the opinion expression and              Movie Database. The corpus is annotated regard-
target spans. The link and link type attributes asso-         ing movie features and corresponding opinions.
ciate the target with other targets in the discourse          The following example shows an annotated sen-
through same or alternative relations. The opinion            tence:
frames are built based on the links between tar-              hSentenceiI have never encountered a movie whose
gets. Somasundaran et al. (2008) show that opin-              supporting cast was so perfectly realized.hFO
ion frames enable a coherent interpretation of the            Fword=“supporting cast” Ftype=“PAC” Oword=“perfect”
    2
        http://www.cs.pitt.edu/mpqa/                          Otype=“PRO”/ih/Sentencei


                                                        576


The movie features (Fword) are attributed to one                     (2) In a 6-week class, I counted 3 comments from the
of 20 predefined categories (Ftype). The opin-                       professors directly to me and two directed to my team.
                                                                     (3) I found that I spent most of my time learning from my
ion words (Oword) and their semantic orientations                    fellow students.
(Otype) are identified. Possible negations are di-                   (4) A standard response from my professors would be that of
rectly reflected by the semantic orientation, but not                a sentence fragment.
explicitly labeled in the sentence. (PD) in the fol-                 The example above provides an evaluation about
lowing example indicates that the movie feature is                   the professors without stating any explicit expres-
referenced by anaphora:                                              sions of opinions. We call such objectively verifi-
hSentenceiIt is utter nonsense and insulting to my
                                                                     able, but evaluative sentences polar facts. Explicit
intelligence and sense of history. hFO Fword=“film(PD)”
                                                                     expressions of opinions typically contain specific
Ftype=“OA” Oword=“nonsense, insulting”
                                                                     cues, i.e. opinion words, loaded with a positive or
Otype=“CON”/ih/Sentencei
                                                                     negative connotation (e.g., nightmare). Even when
                                                                     they are taken out of the context in which they ap-
However, similar to the corpus of Hu & Liu (2004)                    pear, they evoke an evaluation. However, evalu-
the referring pronouns are not explicitly marked in                  ations in polar facts can only be inferred within
discourse. It is therefore neither possible to au-                   the context of the review. For instance, the targets
tomatically determine which pronoun creates the                      of the implied evalution in the polar facts (2), (3)
link if there are more than one in a sentence, nor it                and (4) are the professors. However, (3) may have
is denoted which antecedent, i.e. the actual men-                    been perceived as a positive statement if the re-
tion of the feature in the discourse it relates to.                  view was explaining how good the fellow students
                                                                     were or how the course enforced team work etc.
3     Annotation Scheme
                                                                        The annotation scheme consists of two levels.
3.1    Opinion versus Polar Facts                                    First, the sentence level scheme analyses each sen-
The goal of the annotation scheme is to capture the                  tence in terms of (i) its relevancy to the overall
evaluations regarding the topics being discussed in                  topic of the review, and (ii) whether it contains
the consumer reviews. The evaluations in con-                        an evaluation (an opinion or a polar fact) about
sumer reviews are either explicit expressions of                     the topic. Once the on-topic sentences contain-
opinions, or facts which imply evaluations as dis-                   ing evaluations are identified, the expression level
cussed below.                                                        scheme first focuses either on marking the text
                                                                     spans of the opinion expressions (if the sentence
Explicit expressions of opinions: Opinions are                       contains an explicit expression of an opinion) or
private states (Wiebe et al., 2005; Quirk et al.,                    marking the targets of the polar facts (if the sen-
1985) which are not open to objective observation                    tence is a polar fact). Upon marking an opin-
or verification. In this study, we focus on the opin-                ion expression span, the target and holder of the
ions stating the quality or value of an entity, ex-                  opinion is marked and linked to the marked opin-
perience or a proposition from one’s perspective.                    ion expression. Furthermore, the expression level
(1) illustrates an example of an explicit expression                 scheme allows assigning polarities to the marked
of an opinion. Similar to Wiebe et al. (2005), we                    opinion expression spans and targets of the polar
view opinions in terms of their functional compo-                    facts.
nents, as opinion holders, e.g., the author in (1),                     The following subsections introduce the sen-
holding attitudes (polarity), e.g., negative attitude                tence and the expression level annotation schemes
indicated with the word nightmare, towards possi-                    in detail with examples.
ble targets, e.g., Capella University.
                                                                     3.2   Sentence Level Annotation
(1) I had a nightmare with Capella University.3
                                                                     The sentence annotation strives to identify the sen-
Facts implying evaluations: Besides opinions,                        tences containing evaluations about the topic. In
there are facts which can be objectively verified,                   consumer reviews people occasionally drift off the
but still imply an evaluation of the quality or value                actual topic being reviewed. For instance, as in
of an entity or a proposition. For instance, con-                    (5) taken from a review about an online university,
sider the snippet below:                                             they tend to provide information about their back-
    3
      We use authentic examples from the corpus without cor-         ground or other experiences.
recting grammatical or spelling errors.                              (5) I am very fortunate and almost right out of high school


                                                               577


                                                                  (6) Many people are knocking Devry but I have seen them to
                                                                  be a very great school. [Topic: Devry University]
                                                                  (7) University of Phoenix was a surprising disappointment.
                                                                  [Topic: University of Phoenix]
                                                                  (8) Assignments were passed down, but when asked to
                                                                  clarify the assignment because the syllabus had
                                                                  contradicting, poorly worded, information, my professors
                                                                  regularly responded....”refer to the syllabus”....but wait, the
                                                                  syllabus IS the question. [Topic: University of Phoenix]
                                                                     polar fact attribute is labeled as yes if the sen-
                                                                  tence is a polar fact. This attribute is presented
                                                                  if the opinionated attribute has been labeled as
                                                                  no. Examples (2)-(4) demonstrate sentences la-
 Figure 1: The sentence level annotation scheme                   beled as topic relevant=yes, opinionated=no and
                                                                  polar fact=yes.
                                                                     polar fact polarity attribute represents the po-
with a very average GPA and only 20; I already make above         larity of the evaluation in a polar fact sentence.
$45,000 a year as a programmer with a large health care
company for over a year and have had 3 promotions up in           The possible values for this attribute include posi-
the first year and a half.                                        tive, negative, both. The value both is intended for
                                                                  the polar fact sentences containing more than one
Such sentences do not provide information about
                                                                  evaluation with contradicting polarities. At the
the actual topic, but typically serve for justifying
                                                                  expression level analysis, the targets of the con-
the user’s point of view or provide a better under-
                                                                  tradicting polar fact evaluations are identified dis-
standing about her circumstances. However, they
                                                                  tinctly and assigned polarities of positive or neg-
are not valuable for an application aiming to ex-
                                                                  ative later on. Examples (9)-(11) demonstrate ex-
tract opinions about a specific topic.
                                                                  amples of polar fact sentences with different val-
   Reviews given to the annotators contain meta
                                                                  ues of the attribute polar fact polarity.
information stating the topic, for instance, the
                                                                  (9) There are students in the first programming class and
name of the university or the service being re-                   after taking this class twice they cannot write a single line of
viewed. A markable (i.e. an annotation unit) is                   code. [polar fact polarity=negative]
                                                                  (10) The same class (i.e. computer class) being teach at Ivy
created for each sentence prior to the annotation                 League schools are being offered at Devry.
process. At this level, the annotation process is                 [polar fact polarity=positive]
therefore a sentence labeling task. The annotators                (11) The lectures are interactive and recorded, but you need
                                                                  a consent from the instructor each time.
are able to see the whole review, and instructed to               [polar fact polarity=both]
label sentences in the context of the whole review.
Figure 1 presents the sentence level scheme. At-                  3.3    Expression Level Annotation
tribute names are marked with oval circles and the                At the expression level, we focus on the topic
possible values are given in parenthesis. The fol-                relevant sentences containing evaluations, i.e.,
lowing attributes are used:                                       sentences labeled as topic relevant=yes, opinion-
   topic relevant attribute is labeled as yes if the              ated=yes or topic relevant=yes, opinionated=no,
sentence discusses the given topic itself or its as-              polar fact=yes. If the sentence is a polar fact, then
pects, properties or features as in examples (1)-                 the aim is to mark the target and label the polarity
(4). Other possible values for this attribute include             of the evaluation. If the sentence is opinionated,
none given which can be chosen in the absence of                  then, the aim is to mark the opinion expression
meta data, or no if the sentence drifted off the topic            span, and label its polarity and strength (i.e. in-
as in example (5).                                                tensity), and to link it to the target and the holder.
   opinionated attribute is labeled as yes if the                    Figure 2 presents the expression level scheme.
sentence contains any explicit expressions of opin-               At this stage, annotators mark text spans, and are
ions about the given topic. This attribute is pre-                allowed to assign one of the five labels to the
sented if the topic relevant attribute has been la-               marked span:
beled as none given or yes. In other words, only                     The polar target is used to label the targets of
the on-topic sentences are considered in this step.               the evaluations implied by polar facts. The is-
Examples (6)-(8) illustrate examples labeled as                   Reference attribute labels polar targets which are
topic relevant=yes and opinionated=yes.                           anaphoric references. The polar target polarity


                                                            578


                                                                        authors of the reviews. To ease the annotation pro-
                                                                        cess, the holder is not labeled when this is the au-
                                                                        thor.
                                                                           The modifier annotation labels the lexical items,
                                                                        such as not, very, hardly etc., which affect the
                                                                        strength of an opinion or shift its polarity. Upon
                                                                        creation of a modifier markable, annotators are
                                                                        asked to choose between negation, increase, de-
                                                                        crease for identifying the influence of the modifier
                                                                        on the opinion. For instance, the marked span in
                                                                        (15) is labeled as modifier=increase as it gives the
                                                                        impression that the author is really offended by the
                                                                        negative comments about her university.
                                                                        (15) I am quite honestly appauled by some of the negative
                                                                        comments given for Capella University on this website.

                                                                           The opinionexpression annotation is used to la-
                                                                        bel the opinion terms in the sentence. This mark-
Figure 2: The expression level annotation scheme                        able type has five attributes, three of which, i.e.,
                                                                        modifier, holder, and target are pointer attributes
attribute is used to label the polarity as positive                     to the previously defined markable types. The po-
or negative. If the isReference attribute is labeled                    larity attribute assesses the semantic orientation of
as true, then the referent attribute appears which                      the attitude, where the strength attribute marks the
enables the annotator to resolve the reference to                       intensity of this attitude. The polarity and strength
its antecedent. Consider the example sentences                          attributes focus solely on the marked opinionex-
(12) and (13) below. The polar target in (13),                          pression span, not the whole evaluation implied
written bold, is labeled as isReference=true, po-                       in the sentence. For instance, the opinionexpres-
lar target polarity=negative. To resolve the ref-                       sion span in (16) is labeled as polarity=negative,
erence, annotator first creates another polar target                    strength=average. We infer the polarity of the
markable for the antecedent, namely the bold text                       evaluation only after considering the modifier, po-
span in (12), then, links the antecedent to the ref-                    larity and the strength attributes together. In (16),
erent attribute of the polar target in (13).                            the evaluation about the target is strongly negative
(12) Since classes already started, CTU told me they would
                                                                        after considering all three attributes of the opinion-
extend me so that I could complete the classes and get credit           expression annotation. In (17), the polarity of the
once I got back.                                                        opinionexpression1 itself (complaints) is labeled
(13) What they didn’t tell me is in order to extend, I also had
to be enrolled in the next semester.                                    as negative. It is linked to the modifier1 which
                                                                        is labeled as negation. Target1 (PhD journey) is
   The target annotation represents what the opin-
                                                                        linked to the opinionexpression1. The overall eval-
ion is about. Both polar targets and targets can be
                                                                        uation regarding the target1 is positive after ap-
the topic of the review or different aspects, i.e. fea-
                                                                        plying the affect of the modifier1 to the polarity
tures of the topic. Similar to the polar targets, the
                                                                        of the opinionexpression1, i.e., after negating the
isReference attribute allows the identification of
                                                                        negative polarity.
the targets which are anaphoric references and the
                                                                        (16) I am quite honestly[ modif ier] appauled
referent attribute links them to their antecedents in                   by[ opinionexpression] some of the negative comments
the discourse. Bold span in (14) shows an example                       given for Capella University on this website[ target] .
of a target in an opinionated sentence.                                 (17) I have no[ modif ier1]
(14) Capella U has incredible faculty in the Harold Abel                complaints[ opinionexpression1] about the entire PhD
School of Psychology.                                                   journey[ target1] and highly[ modif ier2]
                                                                        recommend[ opinionexpression2] this school[ target2] .
  The holder type represents the holder of an
opinion in the discourse and is labeled in the same                        Finally, Figure 3 demonstrates all expression
manner as the targets and polar targets. In con-                        level markables created for an opinionated sen-
sumer reviews, holders are most of the time the                         tence and how they relate to each other.


                                                                  579


                                                                                         University    Service       All
                                                               Reviews                         240         234       474
                                                               Sentences                     2786         6091      8877
                                                               Words                        49624      102676    152300
                                                               Avg sent./rev.                 11.6          26      18.7
                                                               Std. dev. sent./rev.             8.2         16      14.6
                                                               Avg. words/rev.               206.7       438.7     321.3
                                                               Std. dev. words/rev.          159.2       232.1     229.8

                                                              Table 1: Descriptive statistics about the corpus

    Figure 3: Expression level annotation example            4.2   Sentence Level Agreement
                                                             Sentence level markables were already created au-
4        Annotation Study                                    tomatically prior to the annotation, i.e., the set of
                                                             annotation units were the same for both annota-
Each review has been annotated by two annotators             tors. We use Cohen’s kappa (κ) (Cohen, 1960)
independently according to the annotation scheme             for measuring the IAA. The sentence level anno-
introduced above. We used the freely available               tation scheme has a hierarchical structure. A new
MMAX24 annotation tool capable of stand-off                  attribute is presented based on the decision made
multi-level annotations. Annotators were native              for the previous attribute, for instance, opinionated
speaker linguistic students. They were trained on            attribute is only presented if the topic relevant at-
15 reviews after reading the annotation manual.5             tribute is labeled as yes or none given; polar fact
In the training stage, the annotators discussed with         attribute is only presented if the opinionated at-
each other if different decisions have been made             tribute is labeled as no etc. We calculate κ for each
and were allowed to ask questions to clarify their           attribute considering only the markables which
understanding of the scheme. Annotators had ac-              were labeled the same by both annotators in the
cess to the review text as a whole while making              previously required step. Table 2 shows the κ val-
their decisions.                                             ues for each attribute, the size of the markable set
                                                             on which the value was calculated, and the per-
4.1       Data                                               centage agreement.

The corpus consists of consumer reviews col-                       Attribute               Markables     Agr.    κ
                                                                   topic relevant          1151          0.89    0.73
lected from the review portals rateitall6 and eopin-               opinionated             682           0.80    0.61
ions7 . It contains reviews from two domains in-                   polar fact              258           0.77    0.56
cluding online universities, e.g., Capella Univer-                 polar fact polarity     103           0.96    0.92
sity, Pheonix, University of Maryland University             Table 2: Sentence level inter-annotator agreement
College etc. and online services, e.g., PayPal,
egroups, eTrade, eCircles etc. These two domains                The agreement for topic relevancy shows that
were selected with the project-relevant, domain-             it is possible to label this attribute reliably. The
specific research goals in mind. We selected a spe-          sentences labeled as topic relevant by both anno-
cific topic, e.g. Pheonix, if there were more than 3         tators correspond to 59% of all sentences, suggest-
reviews written about it. Table 1 shows descriptive          ing that people often drift off the topic in consumer
statistics regarding the data.                               reviews. This is usually the case when they pro-
   We used 118 reviews containing 1151 sentences             vide information about their backgrounds or alter-
from the university domain for measuring the sen-            natives to the given topic.
tence and expression level agreements. In the fol-              On the other hand, we obtain moderate agree-
lowing subsections, we report the inter-annotator            ment levels for the opinionated and polar fact at-
agreement (IAA) at each level.                               tributes. 62% of the topic relevant sentences were
                                                             labeled as opinionated by at least one annotator,
     4
    http://mmax2.sourceforge.net/                            and the rest 38% constitute the topic relevant sen-
     5
    http://www.ukp.tu-darmstadt.de/
research/data/sentiment-analysis
                                                             tences labeled as not opinionated by both anno-
  6
    http://www.rateitall.com                                 tators. Nonetheless, they still contain evaluations
  7
    http://www.epinions.com                                  (polar facts), as 15% of the topic relevant sen-


                                                       580


tences were labeled as polar facts by both anno-             matching in which the text spans should perfectly
tators. When we merge the attributes opinionated             match; (ii) lenient (relaxed) matching in which the
and polar fact into a single category, we obtain κ           overlap between spans is considered as a match,
of 0.75 and a percentage agreement of 87%. Thus,             and (iii) subset matching in which a span has to
we conclude that opinion-relevant sentences, ei-             be contained in another span in order to be consid-
ther in the form of an explicit expression of opin-          ered as a match.8 Agreement naturally increases
ion or a polar fact, can be labeled reliably in con-         as we relax the matching constraints. However,
sumer reviews. However, there is a thin border be-           there were no differences between the lenient and
tween polar facts and explicit expressions of opin-          the subset agreement values. Therefore, we report
ions.                                                        only the exact and lenient matching agreement re-
   To the best of our knowledge, similar annotation          sults for each annotation type in Table 3. The
efforts on consumer or movie reviews do not pro-             same agreement results for the lenient and subset
vide any agreement figures for direct comparison.            matching indicates that inexact matches are still
However, Wiebe et al. (2005) present an annota-              very similar to each other, i.e., at least one span is
tion study where they mark textual spans for sub-            totally contained in the other.
jective expressions in a newspaper corpus. They                 Somasundaran et al. (2008) do not report any
report pairwise κ values for three annotators rang-          F-measure. However, they report span agreement
ing between 0.72 - 0.84 for the sentence level sub-          results in terms of precision and recall ranging
jective/objective judgments. Wiebe et al. (2005)             between 0.44 - 0.87 for opinion spans and be-
mark subjective spans, and do not explicitly per-            tween 0.74 - 0.90 for the target spans. Wiebe et
form the sentence level labeling task. They calcu-           al. (2005) use the lenient matching approach for
late the sentence level κ values based on the ex-            reporting text span agreements ranging between
istence of a subjective expression span in the sen-          0.59 - 0.81 for subjective expressions. We ob-
tence. Although the task definitions, approaches             tain higher agreement values for both opinion ex-
and the corpora have quite disparate characteris-            pression and target spans. We attribute this to the
tics in both studies, we obtain comparable results           fact that the annotators look for opinion expression
when we merge opinionated and polar fact cate-               and target spans within the opinionated sentences
gories.                                                      which they agreed upon. Sentence level analysis
                                                             indeed increases the reliability at the expression
4.3   Expression Level Agreement                             level. Compared to the high agreement on mark-
                                                             ing target spans, we obtain lower agreement val-
At the expression level, annotators focus only on            ues on marking polar target spans. We observe
the sentences which were labeled as opinionated              that it is easier to attribute explicit expressions of
or polar fact by both annotators. Annotators were            evaluations to topic relevant entities compared to
instructed to mark text spans, and then, assign              attributing evaluations implied by experiences to
them the annotation types such as polar target,              specific topic relevant entities in the reviews.
opinionexpression etc. (see Figure 2). For calcu-
                                                                We calculated the agreement on identifying
lating the text span agreement, we use the agree-
                                                             anaphoric references using the method introduced
ment metric presented by Wiebe et al. (2005) and
                                                             in (Passonneau, 2004) which utilizes Krippen-
Somasundaran et al. (2008). This metric corre-
                                                             dorf’s α (Krippendorff, 2004) for computing reli-
sponds to the precision (P) and recall (R) metrics
                                                             ability for coreference annotation. We considered
in information retrieval where the decisions of one
                                                             the overlapping target and polar target spans to-
annotator are treated as the system; the decisions
                                                             gether in this calculation, and obtained an α value
of the other annotator are treated as the gold stan-
                                                             of 0.29. Compared to Passonneau (α values from
dard; and the overlapping spans correspond to the
                                                             0.46 to 0.74), we obtain a much lower agreement
correctly retrieved documents.
                                                             value. This may be due to the different definitions
   Somasundaran et al. (2008) present a discourse            and organizations of the annotation tasks. Passon-
level annotation study in which opinion and tar-             neau requires prior marking of all noun phrases (or
get spans are marked and linked with each other              instances which needs to be processed by the an-
in a meeting transcript corpus. Following Soma-
sundaran et al. (2008), we compute three differ-                8
                                                                  An example of subset matching: waste of time vs. total
ent measures for the text span agreement: (i) exact          waste of time


                                                       581


                                                         Exact                     Lenient
                             Span
                                                     P      R         F        P       R        F
                             opinionexpression    0.70   0.80      0.75     0.82    0.93     0.87
                             modifier             0.80   0.82      0.81     0.86    0.86     0.86
                             target               0.80   0.81      0.80     0.91    0.90     0.91
                             holder               0.75   0.72      0.73     0.93    0.88     0.91
                             polar target         0.67   0.42      0.51     0.75    0.49     0.59

                   Table 3: Inter-annotator agreement on text spans at the expression level


notator). Annotator’s task is to identify whether                The majority of the modifiers were annotated as
an instance refers to another marked entity in the               intensifiers (70%), while 20% of the modifiers
discourse, and then, to identify corefering entity               were labeled as negation.
chains. However, in our annotation process anno-
tators were tasked to identify only one entity as the            4.4      Discussion
referent, and was free to choose it from anywhere                We analyzed the discrepancies in the annotations
in the discourse. In other words, our chains con-                to gain insights about the challenges involved in
tain only one entity. It is possible that both annota-           various opinion related labeling tasks. At the sen-
tors performed correct resolutions, but still did not            tence level, there were several trivial cases of dis-
overlap with each other, as they resolve to differ-              agreement, for instance, failing to recognize topic
ent instances of the same entity in the discourse.               relevancy when the topic was not mentioned or
We plan to further investigate reference resolution              referenced explicitly in the sentence, as in (18).
annotation discrepancies and perform corrections                 Occasionally, annotators disagreed about whether
in the future.                                                   a sentence that was written as a reaction to the
   Some annotation types require additional at-                  other reviewers, as in (19), should be considered
tributes to be labeled after marking the span.                   as topic relevant or not. Another source of dis-
For instance, upon marking a text span as a po-                  agreement included sentences similar to (20) and
lar target or an opinionexpression, one has to la-               (21). One annotator interpreted them as univer-
bel the polarity and strength. We consider the                   sally true statements regardless of the topic, while
overlapping spans for each annotation type and                   the other attributed them to the discussed topic.
use κ for reporting the agreement on these at-                   (18) Go to a state university if you know whats good for you!
tributes. Table 4 shows the κ values.                            (19) Those with sour grapes couldnt cut it, have an ax to
                                                                 grind, and are devoting their time to smearing the school.
    Attribute               Markables   Agr.     κ               (20) As far as learning, you really have to WANT to learn
    polarity                329         0.97     0.94            the material.
    strength                329         0.74     0.55            (21) On an aside, this type of education is not for the
    modifier                136         0.88     0.77            undisciplined learner.
    polar target polarity   63          0.80     0.67
                                                                    Annotators easily distinguished the evaluations
Table 4: Inter-annotator agreement at the expres-                at the sentence level. However, they had diffi-
sion level                                                       culties distinguishing between a polar fact and an
                                                                 opinion. For instance, both annotators agreed that
   We observe that the strength of the opinionex-                the sentences (22) and (23) contain evaluations re-
pression and the polar target polarity cannot be                 garding the topic of the review. However, one an-
labeled as reliably as the polarity of the opinion-              notator interpreted both sentences as objectively
expression. 61% of the agreed upon polar targets                 verifiable facts giving a positive impression about
were labeled as negative by both annotators. On                  the school, while the other one treated them as
the other hand, only 35% of the agreed upon opin-                opinions.
ionexpressions were labeled as negative by both                  (22) All this work in the first 2 Years!
annotators. There were no neutral instances. This                (23) The school has a reputation for making students work
indicates that reviewers tend to report negative ex-             really hard.
periences using polar facts, probably objectively                   Sentence level annotation increases the relia-
describing what has happened, but report posi-                   bility of the expression level annotation in terms
tive experiences with explicit opinion expressions.              of marking text spans. However, annotators of-
Distribution of the strength attribute was as fol-               ten had disagreements on labeling the strength at-
lows: weak 6%, average 54%, and strong 40%.                      tribute. For instance, one annotator labeled the


                                                          582


opinion expression in (24) as strong, while the                         position of functional components and it is eas-
other one labeled it as average. We observe that                        ily extendable. Therefore, we hypothesize that the
it is not easy to identify trivial causes of disagree-                  scheme can also be applied to other genres with
ments regarding strength as its perception by each                      minor extensions or as it is. Finally, the corpus
individual is highly subjective. However, most of                       and the annotation manual will be made available
the disagreements occurred between weak and av-                         at http://www.ukp.tu-darmstadt.de/
erage cases.                                                            research/data/sentiment-analysis.
(24) the experience that i have when i visit student finance is
                                                                        Acknowledgements
much like going to the dentist, except when i leave, nothing
is ever fixed.                                                          This research was funded partially by the German Fed-
   We did not apply any consolidation steps during                      eral Ministry of Economy and Technology under grant
our agreement studies. However, a final version of                      01MQ07012 and partially by the German Research Founda-
the corpus will be produced by the third judge (one                     tion (DFG) as part of the Research Training Group on Feed-
of the co-authors) by consolidating the judgements                      back Based Quality Management in eLearning under grant
of the two annotators.                                                  1223. We are very grateful to Sandra Kübler for her help in
                                                                        organizing the annotators, and to Lizhen Qu for his program-
5    Conclusions                                                        ming support in harvesting the data.

We presented a corpus of consumer reviews from
the rateitall and eopinions websites annotated                          References
with opinion related information. Existing opin-
                                                                        Nicholas Asher, Farah Benamara, and Yvette Yannick
ion annotated user-generated corpora suffer from                          Mathieu. 2008. Distilling opinion in discourse: A
several limitations which result in difficulties for                      preliminary study. In Coling 2008: Companion vol-
interpreting the experimental results and for per-                        ume: Posters, pages 7–10, Manchester, UK.
forming error analysis. To name a few, they do                          Eric Breck, Yejin Choi, and Claire Cardie. 2007.
not explicitly link the functional components of                           Identifying expressions of opinion in context. In
the opinions like targets, holders, or modifiers with                      Proceedings of the Twentieth International Joint
the opinion expression; some of them do not mark                           Conference on Artificial Intelligence (IJCAI-2007),
                                                                           pages 2683–2688, Hyderabad, India.
opinion expression spans, none of them resolves
anaphoric references in discourse. Therefore, we                        Xiwen Cheng and Feiyu Xu. 2008. Fine-grained opin-
introduced a two level annotation scheme consist-                         ion topic and polarity identification. In Proceedings
                                                                          of the 6th International Conference on Language
ing of the sentence and expression levels, which
                                                                          Resources and Evaluation, pages 2710–2714, Mar-
overcomes the limitations of the existing review                          rekech, Morocco.
corpora. The sentence level annotation labels sen-
tences for (i) relevancy to a given topic, and (ii)                     Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
                                                                          Patwardhan. 2005. Identifying sources of opin-
expressing an evaluation about the topic. Similar                         ions with conditional random fields and extraction
to (Wilson, 2008a), our annotation scheme allows                          patterns. In HLT ’05: Proceedings of the confer-
capturing evaluations made with factual (objec-                           ence on Human Language Technology and Empiri-
tive) sentences. The expression level annotation                          cal Methods in Natural Language Processing, pages
                                                                          355–362, Morristown, NJ, USA.
further investigates on-topic sentences containing
evaluations for pinpointing the properties (polar-                      Jacob Cohen. 1960. A coefficient of agreement
ity, strength), and marking the functional com-                            for nominal scales. Educational and Psychological
                                                                           Measurement, 20(1):37–46.
ponents of the evaluations (opinion terms, modi-
fiers, targets and holders), and linking them within                    Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
a discourse. We applied the annotation scheme                             holistic lexicon-based approach to opinion mining.
to the consumer review genre and presented an                             In Proceedings of the International Conference on
                                                                          Web Search and Web Data Mining, WSDM 2008,
extensive inter-annotator study providing insights                        pages 231–240, Palo Alto, California, USA.
to the challenges involved in various opinion re-
lated labeling tasks in consumer reviews. Simi-                         Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
                                                                          WordNet: A publicly available lexical resource for
lar to the MPQA scheme, which is successfully                             opinion mining. In Proceedings of the 5th Interna-
applied to the newspaper genre, the annotation                            tional Conference on Language Resources and Eval-
scheme treats opinions and evaluations as a com-                          uation, pages 417–422, Genova, Italy.


                                                                  583


Angela Fahrni and Manfred Klenner. 2008. Old wine                Theresa Wilson. 2008a. Annotating subjective con-
  or warm beer: Target-specific sentiment analysis of              tent in meetings. In Proceedings of the Sixth
  adjectives. In Proceedings of the Symposium on                   International Language Resources and Evaluation
  Affective Language in Human and Machine, AISB                    (LREC’08), Marrakech, Morocco.
  2008 Convention, pages 60 – 63, Aberdeen, Scot-
  land.                                                          Theresa Ann Wilson. 2008b. Fine-grained Subjectiv-
                                                                   ity and Sentiment Analysis: Recognizing the Inten-
Minqing Hu and Bing Liu. 2004. Mining and sum-                     sity, Polarity, and Attitudes of Private States. Ph.D.
  marizing customer reviews. In KDD’04: Proceed-                   thesis, University of Pittsburgh.
  ings of the Tenth ACM SIGKDD International Con-
  ference on Knowledge Discovery and Data Mining,                Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
  pages 168–177, Seattle, Washington.                              Movie review mining and summarization. In CIKM
                                                                   ’06: Proceedings of the 15th ACM international
Soo-Min Kim and Eduard Hovy. 2006. Extracting                      conference on Information and knowledge manage-
  opinions, opinion holders, and topics expressed in               ment, pages 43–50, Arlington, Virginia, USA.
  online news media text. In Proceedings of the Work-
  shop on Sentiment and Subjectivity in Text at the
  joint COLING-ACL Conference, pages 1–8, Sydney,
  Australia.
Klaus Krippendorff. 2004. Content Analysis: An
  Introduction to Its Methology. Sage Publications,
  Thousand Oaks, California.
Rebecca J. Passonneau. 2004. Computing reliability
  for coreference. In Proceedings of LREC, volume 4,
  pages 1503–1506, Lisbon.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
  and Jan Svartvik. 1985. A Comprehensive Gram-
  mar of the English Language. Longman, New York.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
   tion patterns for subjective expressions. In EMNLP-
   03: Proceedings of the Conference on Empirical
   Methods in Natural Language Processing, pages
   105–112.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
  Wiebe. 2008. Discourse level opinion relations:
  An annotation study. In In Proceedings of SIGdial
  Workshop on Discourse and Dialogue, pages 129–
  137, Columbus, Ohio.
Veselin Stoyanov and Claire Cardie. 2008. Topic
  identification for fine-grained opinion analysis. In
  Proceedings of the 22nd International Conference
  on Computational Linguistics (Coling 2008), pages
  817–824, Manchester, UK.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
   2005. Annotating expressions of opinions and emo-
   tions in language. Language Resources and Evalu-
   ation, 39:165–210.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
  ing attributions and private states. In Proceedings of
  the Workshop on Frontiers in Corpus Annotations II:
  Pie in the Sky, pages 53–60, Ann Arbor, Michigan.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
  2005. Recognizing contextual polarity in phrase-
  level sentiment analysis. In HLT ’05: Proceed-
  ings of the conference on Human Language Tech-
  nology and Empirical Methods in Natural Language
  Processing, pages 347–354, Vancouver, British
  Columbia, Canada.


                                                           584

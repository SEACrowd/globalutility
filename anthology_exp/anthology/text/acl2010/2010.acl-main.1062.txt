                           Error Detection for Statistical Machine
                           Translation Using Linguistic Features
                              Deyi Xiong, Min Zhang, Haizhou Li
                                  Human Language Technology
                                 Institute for Infocomm Research
                     1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
                     {dyxiong, mzhang, hli}@i2r.a-star.edu.sg


                      Abstract                                 tems (Akibay et al., 2004; Jayaraman and Lavie,
                                                               2005).
    Automatic error detection is desired in                       In this paper we restrict the “parts” to words.
    the post-processing to improve machine                     That is, we detect errors at the word level for SMT.
    translation quality. The previous work is                  A common approach to SMT error detection at the
    largely based on confidence estimation us-                 word level is calculating the confidence at which a
    ing system-based features, such as word                    word is correct. The majority of word confidence
    posterior probabilities calculated from N -                estimation methods follows three steps:
    best lists or word lattices. We propose to
    incorporate two groups of linguistic fea-                   1) Calculate features that express the correct-
    tures, which convey information from out-                      ness of words either based on SMT model
    side machine translation systems, into er-                     (e.g. translation/language model) or based on
    ror detection: lexical and syntactic fea-                      SMT system output (e.g. N -best lists, word
    tures. We use a maximum entropy clas-                          lattices) (Blatz et al., 2003; Ueffing and Ney,
    sifier to predict translation errors by inte-                  2007).
    grating word posterior probability feature                  2) Combine these features together with a clas-
    and linguistic features. The experimen-                        sification model such as multi-layer percep-
    tal results show that 1) linguistic features                   tron (Blatz et al., 2003), Naive Bayes (Blatz
    alone outperform word posterior probabil-                      et al., 2003; Sanchis et al., 2007), or log-
    ity based confidence estimation in error                       linear model (Ueffing and Ney, 2007).
    detection; and 2) linguistic features can
    further provide complementary informa-                      3) Divide words into two groups (correct trans-
    tion when combined with word confidence                        lations and errors) by using a classification
    scores, which collectively reduce the clas-                    threshold optimized on a development set.
    sification error rate by 18.52% and im-
                                                               Sometimes the step 2) is not necessary if only one
    prove the F measure by 16.37%.
                                                               effective feature is used (Ueffing and Ney, 2007);
                                                               and sometimes the step 2) and 3) can be merged
1   Introduction
                                                               into a single step if we directly output predicting
Translation hypotheses generated by a statistical              results from binary classifiers instead of making
machine translation (SMT) system always contain                thresholding decision.
both correct parts (e.g. words, n-grams, phrases                   Various features from different SMT models
matched with reference translations) and incor-                and system outputs are investigated (Blatz et al.,
rect parts. Automatically distinguishing incorrect             2003; Ueffing and Ney, 2007; Sanchis et al., 2007;
parts from correct parts is therefore very desir-              Raybaud et al., 2009). Experimental results show
able not only for post-editing and interactive ma-             that they are useful for error detection. However,
chine translation (Ueffing and Ney, 2007) but also             it is not adequate to just use these features as dis-
for SMT itself: either by rescoring hypotheses in              cussed in (Shi and Zhou, 2005) because the infor-
the N -best list using the probability of correct-             mation that they carry is either from the inner com-
ness calculated for each hypothesis (Zens and Ney,             ponents of SMT systems or from system outputs.
2006) or by generating new hypotheses using N -                To some extent, it has already been considered by
best lists from one SMT system or multiple sys-                SMT systems. Hence finding external information


                                                         604
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604–611,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


sources from outside SMT systems is desired for               confidence estimation at the word level as well as
error detection.                                              at the sentence level. The features they use for
   Linguistic knowledge is exactly such a good                word level CE include word posterior probabil-
choice as an external information source. It has al-          ities estimated from N -best lists, features based
ready been proven effective in error detection for            on SMT models, semantic features extracted from
speech recognition (Shi and Zhou, 2005). How-                 WordNet as well as simple syntactic features, i.e.
ever, it is not widely used in SMT error detection.           parentheses and quotation mark check. Among all
The reason is probably that people have yet to find           these features, the word posterior probability is the
effective linguistic features that outperform non-            most effective feature, which is much better than
linguistic features such as word posterior proba-             linguistic features such as semantic features, ac-
bility features (Blatz et al., 2003; Raybaud et al.,          cording to their final results.
2009). In this paper, we would like to show an                   Ueffing and Ney (2007) exhaustively explore
effective use of linguistic features in SMT error             various word-level confidence measures to label
detection.                                                    each word in a generated translation hypothe-
   We integrate two sets of linguistic features into          sis as correct or incorrect. All their measures
a maximum entropy (MaxEnt) model and develop                  are based on word posterior probabilities, which
a MaxEnt-based binary classifier to predict the cat-          are estimated from 1) system output, such as
egory (correct or incorrect) for each word in a               word lattices or N -best lists and 2) word or
generated target sentence. Our experimental re-               phrase translation table. Their experimental re-
sults show that linguistic features substantially im-         sults show that word posterior probabilities di-
prove error detection and even outperform word                rectly estimated from phrase translation table are
posterior probability features. Further, they can             better than those from system output except for the
produce additional improvements when combined                 Chinese-English language pair.
with word posterior probability features.                        Sanchis et al. (2007) adopt a smoothed naive
   The rest of the paper is organized as follows. In          Bayes model to combine different word posterior
Section 2, we review the previous work on word-               probability based confidence features which are
level confidence estimation which is used for error           estimated from N -best lists, similar to (Ueffing
detection. In Section 3, we introduce our linguistic          and Ney, 2007).
features as well as the word posterior probability               Raybaud et al. (2009) study several confi-
feature. In Section 4, we elaborate our MaxEnt-               dence features based on mutual information be-
based error detection model which combine lin-                tween words and n-gram and backward n-gram
guistic features and word posterior probability fea-          language model for word-level and sentence-level
ture together. In Section 5, we describe the SMT              CE. They also explore linguistic features using in-
system which we use to generate translation hy-               formation from syntactic category, tense, gender
potheses. We report our experimental results in               and so on. Unfortunately, such linguistic features
Section 6 and conclude in Section 7.                          neither improve performance at the word level nor
                                                              at the sentence level.
2   Related Work                                                 Our work departs from the previous work in two
                                                              major respects.
In this section, we present an overview of confi-
dence estimation (CE) for machine translation at                • We exploit various linguistic features and
the word level. As we are only interested in error                show that they are able to produce larger im-
detection, we focus on work that uses confidence                  provements than widely used system-related
estimation approaches to detect translation errors.               features such as word posterior probabilities.
Of course, confidence estimation is not limited to                This is in contrast to some previous work. Yet
the application of error detection, it can also be                another advantage of using linguistic features
used in other scenarios, such as translation predic-              is that they are system-independent, which
tion in an interactive environment (Grandrabur and                therefore can be used across different sys-
Foster, 2003) .                                                   tems.
   In a JHU workshop, Blatz et al. (2003) investi-
gate using neural networks and a naive Bayes clas-              • We treat error detection as a complete bi-
sifier to combine various confidence features for                 nary classification problem. Hence we di-


                                                        605


        rectly output prediction results from our dis-              ungrammatical part of a target sentence are prone
        criminatively trained classifier without opti-              to be incorrect. The challenge of using syntac-
        mizing a classification threshold on a distinct             tic knowledge for error detection is that machine-
        development set beforehand.1 Most previous                  generated hypotheses are rarely fully grammati-
        approaches make decisions based on a pre-                   cal. They are mixed with grammatical and un-
        tuned classification threshold τ as follows                 grammatical parts, which hence are not friendly
                  {                                                 to traditional parsers trained on grammatical sen-
                      correct,   Φ(correct, θ) > τ                  tences because ungrammatical parts of a machine-
        class =
                      incorrect, otherwise                          generated sentence could lead to a parsing failure.
                                                                       To overcome this challenge, we select the Link
        where Φ is a classifier or a confidence mea-
                                                                    Grammar (LG) parser 3 as our syntactic parser to
        sure and θ is the parameter set of Φ. The per-
                                                                    generate syntactic features. The LG parser pro-
        formance of these approaches is strongly de-
                                                                    duces a set of labeled links which connect pairs of
        pendent on the classification threshold.
                                                                    words with a link grammar (Sleator and Temper-
3       Features                                                    ley, 1993).
                                                                       The main reason why we choose the LG parser
We explore two sets of linguistic features for each                 is that it provides a robustness feature: null-link
word in a machine generated translation hypoth-                     scheme. The null-link scheme allows the parser to
esis. The first set of linguistic features are sim-                 parse a sentence even when the parser can not fully
ple lexical features. The second set of linguistic                  interpret the entire sentence (e.g. including un-
features are syntactic features which are extracted                 grammatical parts). When the parser fail to parse
from link grammar parse. To compare with the                        the entire sentence, it ignores one word each time
previously widely used features, we also investi-                   until it finds linkages for remaining words. After
gate features based on word posterior probabili-                    parsing, those ignored words are not connected to
ties.                                                               any other words. We call them null-linked words.
3.1 Lexical Features                                                   Our hypothesis is that null-linked words are
                                                                    prone to be syntactically incorrect. We hence
We use the following lexical features.                              straightforwardly define a syntactic feature for a
    • wd: word itself                                               word w according to its links as follows
                                                                                             {
    • pos: part-of-speech tag from a tagger trained                            link(w) =
                                                                                                 yes, w has links
      on WSJ corpus. 2                                                                           no, otherwise

   For each word, we look at previous n                                In Figure 1 we show an example of a generated
words/tags and next n words/tags. They together                     translation hypothesis with its link parse. Here
form a word/tag sequence pattern. The basic idea                    links are denoted with dotted lines which are an-
of using these features is that words in rare pat-                  notated with link types (e.g., Jp, Op). Bracketed
terns are more likely to be incorrect than words                    words, namely “,” and “including”, are null-linked
in frequently occurring patterns. To some extent,                   words.
these two features have similar function to a tar-
get language model or pos-based target language                     3.3 Word Posterior Probability Features
model.                                                              Our word posterior probability is calculated on N -
                                                                    best list, which is first proposed by (Ueffing et al.,
3.2 Syntactic Features                                              2003) and widely used in (Blatz et al., 2003; Ueff-
High-level linguistic knowledge such as syntac-                     ing and Ney, 2007; Sanchis et al., 2007).
tic information about a word is a very natural and                    Given a source sentence f , let {en }N1 be the N -
promising indicator to decide whether this word is                  best list generated by an SMT system, and let ein is
syntactically correct or not. Words occurring in an                 the i-th word in en . The major work of calculating
    1
      This does not mean we do not need a development set.
                                                                    word posterior probabilities is to find the Leven-
We do validate our feature selection and other experimental         shtein alignment (Levenshtein, 1966) between the
settings on the development set.                                    best hypothesis e1 and its competing hypothesis
    2
      Available via http://www-tsujii.is.s.u-tokyo.ac.jp/
∼tsuruoka/postagger/                                                   3
                                                                           Available at http://www.link.cs.cmu.edu/link/


                                                              606


                           Figure 1: An example of Link Grammar parsing results.


en in the N -best list {en }N1 . We denote the align-            For classification, we employ the maximum
ment between them as ℓ(e1 , en ). The word in the             entropy model (Berger et al., 1996) to predict
hypothesis en which ei1 is Levenshtein aligned to             whether a word w is correct or incorrect given its
is denoted as ℓi (e1 , en ).                                  feature vector ψ.
   The word posterior probability of ei1 is then cal-                                    ∑
culated by summing up the probabilities over all                                 exp( i θi fi (c, ψ))
                                                                      p(c|ψ) = ∑       ∑          ′
hypotheses containing ei1 in a position which is                                c′ exp( i θi fi (c , ψ))
Levenshtein aligned to ei1 .
                       ∑                                      where fi is a binary model feature defined on c
                       en : ℓi (e1 ,en )=ei1   p(en )         and the feature vector ψ. θi is the weight of fi .
       pwpp (ei1 ) =         ∑N                               Table 1 shows some examples of our binary model
                                1   p(en )
                                                              features.
   To use the word posterior probability in our er-              In order to learn the model feature weights θ for
ror detection model, we need to make it discrete.             probability estimation, we need a training set of
We introduce a feature for a word w based on its              m samples {ψ i , ci }m1 . The challenge of collect-
word posterior probability as follows                         ing training instances is that the correctness of a
                                                              word in a generated translation hypothesis is not
        dwpp(w) = ⌊−log(pwpp (w))/df ⌋                        intuitively clear (Ueffing and Ney, 2007). We will
                                                              describe the method to determine the correctness
where df is the discrete factor which can be set to
                                                              of a word in Section 6.1, which is broadly adopted
1, 0.1, 0.01 and so on. “⌊ ⌋” is a rounding oper-
                                                              in previous work.
ator which takes the largest integer that does not
                                                                 We tune our model feature weights using an
exceed −log(pwpp (w))/df . We optimize the dis-
                                                              off-the-shelf MaxEnt toolkit (Zhang, 2004). To
crete factor on our development set and find the
                                                              avoid overfitting, we optimize the Gaussian prior
optimal value is 1. Therefore a feature “dwpp =
                                                              on the development set. During test, if the proba-
2” represents that the logarithm of the word poste-
                                                              bility p(correct|ψ) is larger than p(incorrect|ψ)
rior probability is between -3 and -2;
                                                              according the trained MaxEnt model, the word is
4   Error Detection with a Maximum                            labeled as correct otherwise incorrect.
    Entropy Model
                                                              5 SMT System
As mentioned before, we consider error detec-
tion as a binary classification task. To formal-              To obtain machine-generated translation hypothe-
ize this task, we use a feature vector ψ to rep-              ses for our error detection, we use a state-of-the-art
resent a word w in question, and a binary vari-               phrase-based machine translation system MOSES
able c to indicate whether this word is correct or            (Koehn et al., 2003; Koehn et al., 2007). The
not. In the feature vector, we look at 2 words                translation task is on the official NIST Chinese-
before and 2 words after the current word posi-               to-English evaluation data. The training data con-
tion (w−2 , w−1 , w, w1 , w2 ). We collect features           sists of more than 4 million pairs of sentences (in-
{wd, pos, link, dwpp} for each word among these               cluding 101.93M Chinese words and 112.78M En-
words and combine them into the feature vector                glish words) from LDC distributed corpora. Table
ψ for w. As such, we want the feature vector to               2 shows the corpora that we use for the translation
capture the contextual environment, e.g., pos se-             task.
quence pattern, syntactic pattern, where the word                We build a four-gram language model using the
w occurs.                                                     SRILM toolkit (Stolcke, 2002), which is trained


                                                        607


                     Feature     Example {
                                              1,   ψ.w.wd = ”.”, c = correct
                     wd          f (c, ψ) =
                                            { 0,           otherwise
                                              1,   ψ.w2 .pos = ”N N ”, c = incorrect
                     pos         f (c, ψ) =
                                            { 0,               otherwise
                                              1,   ψ.w.link = no, c = incorrect
                     link        f (c, ψ) =
                                            { 0,             otherwise
                                              1,   ψ.w−2 .dwpp = 2, c = correct
                     dwpp        f (c, ψ) =
                                              0,             otherwise

                                   Table 1: Examples of model features.

    LDC ID          Description                                                  Corpus     Sentences     Words
    LDC2004E12      United Nations                              Training          MT-02           878     24,225
    LDC2004T08      Hong Kong News                              Development       MT-05          1082     31,321
    LDC2005T10      Sinorama Magazine                           Test              MT-03           919     25,619
    LDC2003E14      FBIS
    LDC2002E18      Xinhua News V1 beta                        Table 3: Corpus statistics (number of sentences
    LDC2005T06      Chinese News Translation                   and words) for the error detection task.
    LDC2003E07      Chinese Treebank
    LDC2004T07      Multiple Translation Chinese
                                                                  To obtain the linkage information, we run the
Table 2: Training corpora for the translation task.            LG parser on all translation hypotheses. We find
                                                               that the LG parser can not fully parse 560 sen-
                                                               tences (63.8%) in the training set (MT-02), 731
on Xinhua section of the English Gigaword cor-                 sentences (67.6%) in the development set (MT-05)
pus (181.1M words). For minimum error rate tun-                and 660 sentences (71.8%) in the test set (MT-03).
ing (Och, 2003), we use NIST MT-02 as the de-                  For these sentences, the LG parser will use the the
velopment set for the translation task. In order               null-link scheme to generate null-linked words.
to calculate word posterior probabilities, we gen-
                                                                  To determine the true class of a word in a gen-
erate 10,000 best lists for NIST MT-02/03/05 re-
                                                               erated translation hypothesis, we follow (Blatz et
spectively. The performance, in terms of BLEU
                                                               al., 2003) to use the word error rate (WER). We
(Papineni et al., 2002) score, is shown in Table 4.
                                                               tag a word as correct if it is aligned to itself in
                                                               the Levenshtein alignment between the hypothesis
6    Experiments
                                                               and the nearest reference translation that has min-
We conducted our experiments at several levels.                imum edit distance to the hypothesis among four
Starting with MaxEnt models with single linguis-               reference translations. Figure 2 shows the Lev-
tic feature or word posterior probability based fea-           enshtein alignment between a machine-generated
ture, we incorporated additional features incre-               hypothesis and its nearest reference translation.
mentally by combining features together. In do-                The “Class” row shows the label of each word ac-
ing so, we would like the experimental results not             cording to the alignment, where “c” and “i” repre-
only to display the effectiveness of linguistic fea-           sent correct and incorrect respectively.
tures for error detection but also to identify the ad-            There are several other metrics to tag single
ditional contribution of each feature to the task.             words in a translation hypothesis as correct or in-
                                                               correct, such as PER where a word is tagged as
6.1 Data Corpus                                                correct if it occurs in one of reference translations
For the error detection task, we use the best trans-           with the same number of occurrences, Set which is
lation hypotheses of NIST MT-02/05/03 generated                a less strict variant of PER, ignoring the number of
by MOSES as our training, development, and test                occurrences per word. In Figure 2, the two words
corpus respectively. The statistics about these cor-           “last year” in the hypothesis will be tagged as cor-
pora is shown in Table 3. Each translation hypoth-             rect if we use the PER or Set metric since they do
esis has four reference translations.                          not consider the occurring positions of words. Our


                                                         608


    Hypothesis        China   Unicom        alst   year net      profit    rose    up       38%

    Reference         China   Unicom                     net     profit    rose    up       38%   alst   year

    Class             China/c Unicom/c      alst/i year/inet/c profit/c rose/c up/c 38%/c


            Figure 2: Tagging a word as correct/incorrect according to the Levenshtein alignment.


         Corpus      BLEU (%)            RCW (%)                          percentage of words correctly tagged as transla-
         MT-02         33.24              47.76                           tion errors.
                                                                                                      nm
         MT-05         32.03              47.85                                                P re =
         MT-03         32.86              47.57                                                        nt
                                                                          The recall Rec is the proportion of actual transla-
Table 4: Case-insensitive BLEU score and ratio                            tion errors that are found by classifiers.
of correct words (RCW) on the training, develop-                                                      nm
                                                                                               Rec =
ment and test corpus.                                                                                  ng
                                                                          F measure, the trade-off between precision and re-
metric corresponds to the m-WER used in (Ueff-                            call, is also used.
ing and Ney, 2007), which is stricter than PER and                                            2 × P re × Rec
                                                                                          F =
Set. It is also stricter than normal WER metric                                                 P re + Rec
which compares each hypothesis to all references,                         6.3 Experimental Results
rather than the nearest reference.
                                                                          Table 5 shows the performance of our experiments
   Table 4 shows the case-insensitive BLEU score
                                                                          on the error detection task. To compare with pre-
and the percentage of words that are labeled as cor-
                                                                          vious work using word posterior probabilities for
rect according to the method described above on
                                                                          confidence estimation, we carried out experiments
the training, development and test corpus.
                                                                          using wpp estimated from N -best lists with the
6.2 Evaluation Metrics                                                    classification threshold τ , which was optimized on
                                                                          our development set to minimize CER. A relative
To evaluate the overall performance of the error
                                                                          improvement of 9.27% is achieved over the base-
detection, we use the commonly used metric, clas-
                                                                          line CER, which reconfirms the effectiveness of
sification error rate (CER) to evaluate our classi-
                                                                          word posterior probabilities for error detection.
fiers. CER is defined as the percentage of words
                                                                             We conducted three groups of experiments us-
that are wrongly tagged as follows
                                                                          ing the MaxEnt based error detection model with
                   # of wrongly tagged words                              various feature combinations.
        CER =
                        Total # of words                                     • The first group of experiments uses single
   The baseline CER is determined by assuming                                  feature, such as dwpp, pos. We find the
the most frequent class for all words. Since the ra-                           most effective feature is pos, which achieves
tio of correct words in both the development and                               a 16.12% relative improvement over the base-
test set is lower than 50%, the most frequent class                            line CER and 7.55% relative improvement
is “incorrect”. Hence the baseline CER in our ex-                              over the CER of word posterior probabil-
periments is equal to the ratio of correct words as                            ity thresholding. Using discrete word pos-
these words are wrongly tagged as incorrect.                                   terior probabilities as features in the Max-
   We also use precision and recall on errors to                               Ent based error detection model is marginally
evaluate the performance of error detection. Let                               better than word posterior probability thresh-
ng be the number of words of which the true class                              olding in terms of CER, but obtains a 13.79%
is incorrect, nt be the number of words which are                              relative improvement in F measure. The syn-
tagged as incorrect by classifiers, and nm be the                              tactic feature link also improves the error de-
number of words tagged as incorrect that are in-                               tection in terms of CER and particularly re-
deed translation errors. The precision P re is the                             call.


                                                                 609


     Combination                               Features         CER (%)               Pre (%)          Rec (%)        F (%)
     Baseline                                         -            47.57                    -                -            -
     Thresholding wpp                                 -            43.16                58.98            58.07        58.52
     MaxEnt (dwpp)                                   44            43.07                56.12            81.86        66.59
     MaxEnt (wd)                                 19,164            41.57                58.25            73.11        64.84
     MaxEnt (pos)                                   199            39.90                58.88            79.23        67.55
     MaxEnt (link)                                   19            44.31                54.72            89.72        67.98
     MaxEnt (wd + pos)                           19,363            39.43                59.36            78.60        67.64
     MaxEnt (wd + pos + link)                    19,382            39.79                58.74            80.97        68.08
     MaxEnt (dwpp + wd)                          19,208            41.04                57.18            83.75        67.96
     MaxEnt (dwpp + wd + pos)                    19,407            38.88                59.87            78.38        67.88
     MaxEnt (dwpp + wd + pos + link)             19,426            38.76                59.89            78.94        68.10

                             Table 5: Performance of the error detection task.

  • The second group of experiments concerns
    with the combination of linguistic features
                                                                           40.6
    without word posterior probability feature.
                                                                           40.4
    The combination of lexical features improves
                                                                           40.2
    both CER and precision over single lexical
                                                                           40.0
    feature (wd, pos). The addition of syntactic
                                                                           39.8
    feature link marginally undermines CER but
                                                                 CER (%)




                                                                           39.6
    improves recall by a lot.
                                                                           39.4

  • The last group of experiments concerns about                           39.2

    the additional contribution of linguistic fea-                         39.0

    tures to error detection with word posterior                           38.8

    probability. We added linguistic features in-                          38.6
                                                                                  0   200        400       600        800     1000
    crementally into the feature pool. The best                                             Number of Training Sentences
    performance was achieved by using all fea-
    tures, which has a relative of improvement of
    18.52% over the baseline CER.                             Figure 3: CER vs. the number of training sen-
                                                              tences.
   The first two groups of experiments show that
linguistic features, individually (except for link)
or by combination, are able to produce much better            the pos has the largest number of features, 199,
performance than word posterior probability fea-              which is a small set of features. This suggests that
tures in both CER and F measure. The best com-                our error detection model can be learned from a
bination of linguistic features achieves a relative           rather small training set.
improvement of 8.64% and 15.58% in CER and                       Figure 3 shows CERs for the feature combina-
F measure respectively over word posterior prob-              tion MaxEnt (dwpp + wd + pos + link) when
ability thresholding.                                         the number of training sentences is enlarged incre-
   The Table 5 also reveals how linguistic fea-               mentally. CERs drop significantly when the num-
tures improve error detection. The lexical features           ber of training sentences is increased from 100 to
(pos, wd) improve precision when they are used.               500. After 500 sentences are used, CERs change
This suggests that lexical features can help the sys-         marginally and tend to converge.
tem find errors more accurately. Syntactic features
(link), on the other hand, improve recall whenever            7 Conclusions and Future Work
they are used, which indicates that they can help
the system find more errors.                                  In this paper, we have presented a maximum en-
   We also show the number of features in each                tropy based approach to automatically detect er-
combination in Table 5. Except for the wd feature,            rors in translation hypotheses generated by SMT


                                                        610


systems. We incorporate two sets of linguistic                   Brooke Cowan, Wade Shen, Christine Moran,
features together with word posterior probability                Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
                                                                 Constrantin, and Evan Herbst. 2007. Moses: Open
based features into error detection.
                                                                 source toolkit for statistical machine translation. In
   Our experiments validate that linguistic features             Proceedings of ACL, Demonstration Session.
are very useful for error detection: 1) they by
themselves achieve a higher improvement in terms               V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
                                                                  recting Deletions, Insertions and Reversals. Soviet
of both CER and F measure than word posterior                     Physics Doklady, Feb.
probability features; 2) the performance is further
improved when they are combined with word pos-                 Franz Josef Och. 2003. Minimum Error Rate Training
                                                                 in Statistical Machine Translation. In Proceedings
terior probability features.                                     of ACL 2003.
   The extracted linguistic features are quite com-
pact, which can be learned from a small train-                 Kishore Papineni, Salim Roukos, Todd Ward and Wei-
ing set. Furthermore, The learned linguistic fea-                Jing Zhu. 2002. BLEU: a Method for Automatically
                                                                 Evaluation of Machine Translation. In Proceedings
tures are system-independent. Therefore our ap-                  of ACL 2002.
proach can be used for other machine translation
systems, such as rule-based or example-based sys-              Sylvain Raybaud, Caroline Lavecchia, David Langlois,
                                                                 Kamel Smaı̈li. 2009. Word- and Sentence-level
tem, which generally do not produce N -best lists.               Confidence Measures for Machine Translation. In
   Future work in this direction involve detect-                 Proceedings of EAMT 2009.
ing particular error types such as incorrect po-
sitions, inappropriate/unnecessary words (Elliott,             Alberto Sanchis, Alfons Juan and Enrique Vidal. 2007.
                                                                 Estimation of Confidence Measures for Machine
2006) and automatically correcting errors.                       Translation. In Procedings of Machine Translation
                                                                 Summit XI.

References                                                     Daniel Sleator and Davy Temperley. 1993. Parsing En-
                                                                 glish with a Link Grammar. In Proceedings of Third
Yasuhiro Akibay, Eiichiro Sumitay, Hiromi Nakaiway,              International Workshop on Parsing Technologies.
  Seiichi Yamamotoy, and Hiroshi G. Okunoz. 2004.
  Using a Mixture of N-best Lists from Multiple MT             Yongmei Shi and Lina Zhou. 2005. Error Detec-
  Systems in Rank-sum-based Confidence Measure                   tion Using Linguistic Features. In Proceedings of
  for MT Outputs. In Proceedings of COLING.                      HLT/EMNLP 2005.

Adam L. Berger, Stephen A. Della Pietra andVincent             Andreas Stolcke. 2002. SRILM - an Extensible Lan-
  J. Della Pietra. 1996. A Maximum Entropy Ap-                   guage Modeling Toolkit. In Proceedings of Interna-
  proach to Natural Language Processing. Computa-                tional Conference on Spoken Language Processing,
  tional Linguistics, 22(1): 39-71.                              volume 2, pages 901-904.

John Blatz, Erin Fitzgerald, George Foster, Simona             Nicola Ueffing, Klaus Macherey, and Hermann Ney.
  Gandrabur, Cyril Goutte, Alex Kulesza, Alberto                 2003. Confidence Measures for Statistical Machine
  Sanchis, Nicola Ueffing. 2003. Confidence estima-              Translation. In Proceedings. of MT Summit IX.
  tion for machine translation. final report, jhu/clsp
  summer workshop.                                             Nicola Ueffing and Hermann Ney. 2007. Word-
                                                                 Level Confidence Estimation for Machine Transla-
Debra Elliott. 2006 Corpus-based Machine Transla-                tion. Computational Linguistics, 33(1):9-40.
  tion Evaluation via Automated Error Detection in
  Output Texts. Phd Thesis, University of Leeds.               Richard Zens and Hermann Ney. 2006. N-gram Pos-
                                                                 terior Probabilities for Statistical Machine Transla-
Simona Gandrabur and George Foster. 2003. Confi-                 tion. In HLT/NAACL: Proceedings of the Workshop
  dence Estimation for Translation Prediction. In Pro-           on Statistical Machine Translation.
  ceedings of HLT-NAACL.
                                                               Le Zhang.     2004.     Maximum Entropy Model-
S. Jayaraman and A. Lavie. 2005. Multi-engine Ma-                ing Tooklkit for Python and C++. Available at
   chine Translation Guided by Explicit Word Match-              http://homepages.inf.ed.ac.uk/s0450736
   ing. In Proceedings of EAMT.                                  /maxent toolkit.html.

Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
  2003. Statistical Phrase-based Translation. In Pro-
  ceedings of HLT-NAACL.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
  Callison-Burch, Marcello Federico, Nicola Bertoldi,


                                                         611

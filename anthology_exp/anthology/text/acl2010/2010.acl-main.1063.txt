     TrustRank: Inducing Trust in Automatic Translations via Ranking

                 Radu Soricut             Abdessamad Echihabi
              Language Weaver, Inc.        Language Weaver, Inc.
           6060 Center Drive, Suite 150 6060 Center Drive, Suite 150
             Los Angeles, CA 90045        Los Angeles, CA 90045
       rsoricut@languageweaver.com echihabi@languageweaver.com


                      Abstract                                 solution. However, travel reviews present specific
                                                               challenges: the reviews tend to have poor spelling,
    The adoption of Machine Translation tech-                  loose grammar, and broad topics of discussion.
    nology for commercial applications is                      The result is unpredictable levels of MT quality.
    hampered by the lack of trust associated                   This is undesirable for the commercial enterprise,
    with machine-translated output. In this pa-                who is not content to simply reach a broad audi-
    per, we describe TrustRank, an MT sys-                     ence, but also wants to deliver a high-quality prod-
    tem enhanced with a capability to rank the                 uct to that audience.
    quality of translation outputs from good to                   We propose the following solution. We develop
    bad. This enables the user to set a quality                TrustRank, an MT system enhanced with a ca-
    threshold, granting the user control over                  pability to rank the quality of translation outputs
    the quality of the translations.                           from good to bad. This enables the user to set a
    We quantify the gains we obtain in trans-                  quality threshold, granting the user control over
    lation quality, and show that our solution                 the quality of the translations that it employs in
    works on a wide variety of domains and                     its product. With this enhancement, MT adop-
    language pairs.                                            tion stops being a binary should-we-or-shouldn’t-
                                                               we question. Rather, each user can make a per-
1   Introduction                                               sonal trade-off between the scope and the quality
The accuracy of machine translation (MT) soft-                 of their product.
ware has steadily increased over the last 20 years
                                                               2   Related Work
to achieve levels at which large-scale commercial
applications of the technology have become feasi-              Work on automatic MT evaluation started with the
ble. However, widespread adoption of MT tech-                  idea of comparing automatic translations against
nology remains hampered by the lack of trust as-               human-produced references. Such comparisons
sociated with machine-translated output. This lack             are done either at lexical level (Papineni et al.,
of trust is a normal reaction to the erratic trans-            2002; Doddington, 2002), or at linguistically-
lation quality delivered by current state-of-the-              richer levels using paraphrases (Zhou et al., 2006;
art MT systems. Unfortunately, the lack of pre-                Kauchak and Barzilay, 2006), WordNet (Lavie and
dictable quality discourages the adoption of large-            Agarwal, 2007), or syntax (Liu and Gildea, 2005;
scale automatic translation solutions.                         Owczarzak et al., 2007; Yang et al., 2008; Amigó
   Consider the case of a commercial enterprise                et al., 2009). In contrast, we are interested in per-
that hosts reviews written by travellers on its web            forming MT quality assessments on documents for
site. These reviews contain useful information                 which reference translations are not available.
about hotels, restaurants, attractions, etc. There                Reference-free approaches to automatic MT
is a large and continuous stream of reviews posted             quality assessment, based on Machine Learning
on this site, and the large majority is written in En-         techniques such as classification (Kulesza and
glish. In addition, there is a large set of potential          Shieber, 2004), regression (Albrecht and Hwa,
customers who would prefer to have these reviews               2007), and ranking (Ye et al., 2007; Duh, 2008),
available in their (non-English) native languages.             have a different focus compared to ours. Their ap-
As such, this enterprise presents the perfect oppor-           proach, which uses a test set that is held constant
tunity for the deployment of a large-volume MT                 and against which various MT systems are mea-


                                                         612
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 612–621,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


sured, focuses on evaluating system performance.              sess translation quality (Doddington, 2002; Lavie
Similar proposals exist outside the MT field, for             and Agarwal, 2007). In a similar manner, the
instance in syntactic parsing (Ravi et al., 2008). In         work of (Specia et al., 2009) uses NIST scores,
this case, the authors focus on estimating perfor-            and the work of (Ravi et al., 2008) uses PARSE-
mance over entire test sets, which in turn is used            VAL scores. The main advantage of this approach
for evaluating system performance. In contrast,               is that we can generate quickly and cheaply as
we focus on evaluating the quality of the trans-              many learning examples as needed. Additionally,
lations themselves, while the MT system is kept               we can customize the prediction models on a large
constant.                                                     variety of genres and domains, and quickly scale
   A considerable amount of work has been done                to multiple language pairs. In contrast, solutions
in the related area of confidence estimation for              that require training labels produced manually by
MT, for which Blatz et al. (2004) provide a good              humans (Gamon et al., 2005; Albrecht and Hwa,
overview. The goal of this work is to identify small          2007) have difficulties producing prediction mod-
units of translated material (words and phrases)              els fast enough, trained on enough data, and cus-
for which one can be confident in the quality of              tomized for specific domains.
the translation. Related to this goal, and closest to            Third, the main metric we use to assess the per-
our proposal, is the work of Gamon et al. (2005)              formance of our solution is targeted directly at
and Specia et al. (2009). They describe Ma-                   measuring translation quality gains. We are inter-
chine Learning approaches (classification and re-             ested in the extrinsic evaluation of the quantitative
gression, respectively) aimed at predicting which             impact of the TrustRank solution, rather than in
sentences are likely to be well/poorly translated.            the intrinsic evaluation of prediction errors (Ravi
Our work, however, departs from all these works               et al., 2008; Specia et al., 2009).
in several important aspects.
                                                              3     Experimental Framework
   First, we want to make the quality predic-
tions at document-level, as opposed to sentence-              3.1    Domains
level (Gamon et al., 2005; Specia et al., 2009), or           We are interested in measuring the impact of
word/phrase-level (Blatz et al., 2004; Ueffing and            TrustRank on a variety of genres, domains, and
Ney, 2005). Document-level granularity is a re-               language pairs. Therefore, we set up the exper-
quirement for large-scale commercial applications             imental framework accordingly. We use three
that use fully-automated translation solutions. For           proprietary data sets, taken from the domains of
these applications, the need to make the distinction          Travel (consumer reviews), Consumer Electron-
between “good translation” and “poor translation”             ics (customer support for computers, data storage,
must be done at document level. Otherwise, it is              printers, etc.), and HighTech (customer support for
not actionable. In contrast, quality-prediction or            high-tech components). All these data sets come
confidence estimation at sentence- or word-level              in a variety of European and Asian language pairs.
fits best a scenario in which automated translation           We also use the publicly available data set used
is only a part of a larger pipeline. Such pipelines           in the WMT09 task (Koehn and Haddow, 2009)
usually involve human post-editing, and are useful            (a combination of European parliament and news
for translation productivity (Lagarda et al., 2009).          data). Information regarding the sizes of these data
Such solutions, however, suffer from the inherent             sets is provided in Table 2.
volume bottleneck associated with human involve-
ment. Our fully-automated solution targets large              3.2    Metrics
volume translation needs, on the order of 10,000              We first present the experimental framework de-
documents/day or more.                                        signed to answer the main question we want to
   Second, we use automatically generated train-              address: can we automatically produce a ranking
ing labels for the supervised Machine Learning                for document translations (for which no human-
approach. In the experiments presented in this pa-            produced references are available), such that the
per, we use BLEU scores (Papineni et al., 2002)               translation quality of the documents at the top of
as training labels. However, they can be substi-              this ranking is higher than the average translation
tuted with any of the proposed MT metrics that use            quality? To this end, we use several metrics that
human-produced references to automatically as-                can gauge how well we answer this question.


                                                        613


   The first metric is Ranking Accuracy (rAcc),             solution to stay unchanged while the acceptance
see (Gunawardana and Shani, 2009). We are inter-            quality threshold can vary, we cannot treat this as
ested in ranking N documents and assigning them             a classification problem. Instead, we need to pro-
into n quantiles. The formula is:                           vide a complete ranking over an input set of doc-
                                                            uments. As already mentioned, TrustRank uses a
                           TPi       1
    rAcc[n] = Avgni=1      N
                                 =     × Σni=1 TPi          regression method that is trained on BLEU scores
                           n
                                     N                      as training labels. The regression functions are
                                                            then used to predict a BLEU-like number for each
where TPi (True-Positivei ) is the number of
                                                            document in the input set. The rankings are de-
correctly-assigned documents in quantile i. Intu-
                                                            rived trivially from the predicted BLEU numbers,
itively, this formula is an average of the ratio of
                                                            by simply sorting from highest to lowest. Ref-
documents correctly assigned in each quantile.
                                                            erence ranking is obtained similarly, using actual
   The rAcc metric provides easy to understand
                                                            BLEU scores.
lowerbounds and upperbounds. For example, with
                                                               Although we are mainly interested in the rank-
a method that assigns random ranks, when using 4
                                                            ing problem here, it helps to look at the error pro-
quantiles, the accuracy is 25% in any of the quan-
                                                            duced by the regression models to arrive at a more
tiles, hence an rAcc of 25%. With an oracle-based
                                                            complete picture. Besides the two metrics for
ranking, the accuracy is 100% in any of the quan-
                                                            ranking described above, we use the well-known
tiles, hence an rAcc of 100%. Therefore, the per-
                                                            regression metrics MAE (mean absolute error) and
formance of any decent ranking method, when us-
                                                            TE (test-level error):
ing 4 quantiles, can be expected to fall somewhere
between these bounds.                                                        1
   The second and main metric is the volume-                       MAE =       × ΣN
                                                                                  k=1 |predBLEUk − BLEUk |
                                                                             N
weighted BLEU gain (vBLEU∆) metric. It mea-
                                                                             TE = predBLEU − BLEU
sures the average BLEU gain when trading-off
volume for accuracy on a predefined scale. The              where BLEUk is the BLEU score for document
general formula, for n quantiles, is                        k, predBLEUk is the predicted BLEU value, and
                                                            predBLEU is a weighted average of the predicted
vBLEU∆[n] = Σn−1
             i=1 wi × (BLEU1...i − BLEU)                    document-level BLEU numbers over the entire set
                i
                                                            of N documents.
                             i             2i
  with wi =     n
                   j   =   Σn−1
                                     =
              Σn−1
               j=1 n        j=1 j
                                         n(n−1)
                                                            3.3      Experimental conditions
where BLEU1...i is the BLEU score of the first
                                                            The MT system used by TrustRank (TrustRank-
i quantiles, and BLEU is the score over all the
                                                            MT) is a statistical phrase-based MT system sim-
quantiles. Intuitively, this formula provides a
                                                            ilar to (Och and Ney, 2004). As a reference point
volume-weighted average of the BLEU gain ob-
                                                            regarding the performance of this system, we use
tained while varying the threshold of acceptance
                                                            the official WMT09 parallel data, monolingual
from 1 to n-1. (A threshold of acceptance set to
                                                            data, and development tuning set (news-dev2009a)
the n-th quantile means accepting all the transla-
                                                            to train baseline TrustRank-MT systems for each
tions and therefore ignore the rankings, so we do
                                                            of the ten WMT09 language pairs. Our system
not include it in the average.) Without rankings
                                                            produces translations that are competitive with
(or with random ranks), the expected vBLEU∆[n]
                                                            state-of-the-art systems. We show our baseline-
is zero, as the value BLEU1...i is expected to be
                                                            system BLEU scores on the official development
the same as the overall BLEU for any i. With ora-
                                                            test set (news-dev2009b) for the WMT09 task in
cle ranking, the expected vBLEU∆[n] is a positive
                                                            Table 1, along with the BLEU scores reported for
number representative of the upperbound on the
                                                            the baseline Moses system (Koehn and Haddow,
quality of the translations that pass an acceptance
                                                            2009).
threshold. We report the vBLEU∆[n] values as
                                                                For each of the domains we consider, we par-
signed numbers, both within a domain and when
                                                            tition the data sets as follows. We first set aside
computed as an average across domains.
                                                            3000 documents, which we call the Regression
   The choice regarding the number of quantiles
                                                            set 1 . The remaining data is called the training MT
is closely related to the choice of setting an ac-
                                                               1
ceptance quality threshold. Because we want the                    For parallel data for which we do not have document


                                                      614


    From Eng       Fra     Spa     Ger     Cze    Hun             LP              MT set               Regression set
    Moses          17.8   22.4    13.5    11.4     6.5                             Train       Train         Test       BLEU

    TrustRank-MT   21.3   22.8    14.3     9.1     8.5            WMT09
    Into Eng       Fra     Spa     Ger     Cze    Hun             Eng-Spa     41Mw           277Kw         281Kw        41.0
    Moses          21.2   22.5    16.6    16.9     8.8            Eng-Fra     41Mw           282Kw         283Kw        37.1
    TrustRank-MT   22.4   23.8    19.8    13.3    10.4            Eng-Ger     41Mw           282Kw         280Kw        23.7
                                                                  Eng-Cze     1.2Mw          241Kw         242Kw        10.3
Table 1: BLEU scores (uncased) for the                            Eng-Hun 30Mw               209Kw         206Kw        14.5
TrustRank-MT system compared to Moses                             Spa-Eng     42Mw           287Kw         293Kw        40.1
(WMT09 data).                                                     Fra-Eng     44Mw           305Kw         308Kw        37.9
                                                                  Ger-Eng     39Mw           269Kw         267Kw        29.4
                                                                  Cze-Eng     1.0Mw          218Kw         219Kw        19.7
set, on which the MT system is trained. From the                  Hun-Eng 26Mw               177Kw         176Kw        24.0
Regression set, we set aside 1000 parallel docu-                  Travel
ments to be used as a blind test set (called Regres-              Eng-Spa     4.3Mw          123Kw         121Kw        31.2
sion Test) for our experiments. An additional set                 Eng-Fra     3.5Mw          132Kw         126Kw        27.8
of 1000 parallel documents is used as a develop-                  Eng-Ita     3.4Mw          179Kw         183Kw        22.5
ment set, and the rest of 1000 parallel documents                 Eng-Por 13.1Mw              83Kw         83Kw         41.9
is used as the regression-model training set.                     Eng-Ger     7.0Mw           69Kw         69Kw         27.6
   We have also performed learning-curve exper-                   Eng-Dut     0.7Mw           89Kw         84Kw         41.9
iments using between 100 and 2000 documents                       Electronics
for regression-model training. We do not go into                  Eng-Spa     7.0Mw          150Kw         149Kw        65.2
the details of these experiments here for lack of                 Eng-Fra     6.5Mw          129Kw         129Kw        55.8
space. The conclusion derived from these exper-                   Eng-Ger     5.9Mw          139Kw         140Kw        42.1
iments is that 1000 documents is the point where                  Eng-Chi     7.1Mw          135Kw         136Kw        63.9
the learning-curves level off.                                    Eng-Por     2.0Mw          124Kw         115Kw        47.9
   In Table 2, we provide a few data points with                  HiTech
respect to the data size of these sets (tokenized                 Eng-Spa     2.8Mw          143Kw         148Kw        59.0
word-count on the source side). We also report the                Eng-Ger     5.1Mw          162Kw         155Kw        36.6
BLEU performance of the TrustRank-MT system                       Eng-Chi     5.6Mw          131Kw         129Kw        60.6
on the Regression Test set.                                       Eng-Rus     2.8Mw          122Kw         117Kw        39.2
   Note that the differences between the BLEU                     Eng-Kor     4.2Mw          129Kw         140Kw        49.4
scores reported in Table 1 and the BLEU scores
under the WMT09 label in Table 2 reflect dif-                    Table 2: Data sizes and BLEU on Regression Test.
ferences in the genres of these sets. The offi-
cial development test set (news-dev2009b) for the
WMT09 task is news only. The regression Test                     4.1   The learning method
sets have the same distribution between Europarl                 The results we report here are obtained using
data and news as the corresponding training data                 the freely-available Weka engine 2 . We have
set for each language pair.                                      compared and contrasted results using all the
                                                                 regression packages offered by Weka, includ-
4      The ranking algorithm                                     ing regression functions based on simple and
                                                                 multiple-feature Linear regression, Pace regres-
As mentioned before, TrustRank takes a super-                    sion, RBF networks, Isotonic regression, Gaussian
vised Machine Learning approach. We automat-                     Processes, Support Vector Machines (with SMO
ically generate the training labels by computing                 optimization) with polynomial and RBF kernels,
BLEU scores for every document in the Regres-                    and regression trees such as REP trees and M5P
sion training set.                                               trees. Due to lack of space and the tangential im-
                                                                 pact on the message of this paper, we do not report
                                                                    2
boundaries, we simply simulate document boundaries after              Weka software at http://www.cs.waikato.ac.nz/ml/weka/,
every 10 consecutive sentences.                                  version 3.6.1, June 2009.


                                                           615


these contrastive experiments here.                           between good and bad translations (Albrecht and
   The learning technique that consistently                   Hwa, 2008). When computed on the target side,
yields the best results is M5P regression trees               this type of features requires one or more sec-
(weka.classifiers.trees.M5P). Therefore, we report            ondary MT systems, used to generate transla-
all the results in this paper using this learning             tions starting from the same input. These pseudo-
method. As an additional advantage, the decision              references are useful in gauging translation con-
trees and the regression models produced in train-            vergence, using BLEU scores as feature values.
ing are easy to read, understand, and interpret.              In intuitive terms, their usefulness can be summa-
One can get a good insight into what the impact               rized as follows: “if system X produced a trans-
of a certain feature on a final predicted value is by         lation A and system Y produced a translation B
simply inspecting these trees.                                starting from the same input, and A and B are sim-
                                                              ilar, then A is probably a good translation”.
4.2   The features                                               An important property here is that systems X
In contrast to most of the work on confidence es-             and Y need to be as different as possible from each
timation (Blatz et al., 2004), the features we use            other. This property ensures that a convergence on
are not internal features of the MT system. There-            similar translations is not just an artifact, but a true
fore, TrustRank can be applied for a large variety            indication that the translations are correct. The
of MT approaches, from statistical-based to rule-             secondary systems we use here are still phrase-
based approaches.                                             based, but equipped with linguistically-oriented
   The features we use can be divided into text-              modules similar with the ones proposed in (Collins
based, language-model–based, pseudo-reference–                et al., 2005; Xu et al., 2009).
based, example-based, and training-data–based                    The source-side pseudo-reference–based fea-
feature types. These feature types can be com-                ture type is of a slightly different nature. It still re-
puted either on the source-side (input documents)             quires one or more secondary MT systems, but op-
or on the target-side (translated documents).                 erating in the reverse direction. A translated doc-
                                                              ument produced by the main MT system is fed to
Text-based features                                           the secondary MT system(s), translated back into
These features simply look at the length of the in-           the original source language, and used as pseudo-
put in terms of (tokenized) number of words. They             reference(s) when computing a BLEU score for
can be applied on the input, where they induce a              the original input. In intuitive terms: “if system
correlation between the number of words in the in-            X takes document A and produces B, and system
put document and the expected BLEU score for                  X −1 takes B and produces C, and A and C are
that document size. They can also be applied on               similar, then B is probably a good translation”.
the produced output, and learn a similar correla-
                                                              Example-based features
tion for the produced translation.
                                                              For example-based features, we use a develop-
Language-model–based features                                 ment set of 1000 parallel documents, for which we
These features are among the ones that were first             produce translations and compute document-level
proposed as possible differentiators between good             BLEU scores. We set aside the top-100 BLEU
and bad translations (Gamon et al., 2005). They               scoring documents and bottom-100 BLEU scoring
are a measure of how likely a collection of strings           documents. They are used as positive examples
is under a language model trained on monolingual              (with better-than-average BLEU) and negative ex-
data (either on the source or target side).                   amples (with worse-than-average BLEU), respec-
   The language-model–based feature values we                 tively. We define a positive-example–based fea-
use here are computed as document-level per-                  ture function as a geometric mean of 1-to-4–gram
plexity numbers using a 5-gram language model                 precision scores (i.e., BLEU score without length
trained on the MT training set.                               penalty) between a document (on either source
                                                              or target side) and the positive examples used as
Pseudo-reference–based features                               references (similarly for negative-example–based
Previous work has shown that, in the absence                  features).
of human-produced references, automatically-                     The intuition behind these features can be sum-
produced ones are still helpful in differentiating            marized as follows: “if system X translated docu-


                                                        616


ment A well/poorly, and A and B are similar, then             Domain         rAcc    vBLEU∆[4]     MAE     TE
system X probably translates B well/poorly”.                  Baseline
                                                              WMT09          25%          0         9.9   +0.4
Training-data–based features
                                                              Travel         25%          0         8.3   +2.0
If the main MT system is trained on a parallel cor-           Electr.        25%          0        12.2   +2.6
pus, the data in this corpus can be exploited to-
                                                              HiTech         25%          0        16.9   +2.4
wards assessing translation quality (Specia et al.,
                                                              Dom. avg.      25%          0        11.8   1.9
2009). In our context, the documents that make up
this corpus can be used in a fashion similar with             Oracle
the positive examples. One type of training-data–             WMT09         100%       +8.2         0       0
based features operates by computing the number               Travel        100%       +6.4         0       0
of out-of-vocabulary (OOV) tokens with respect to             Electr.       100%       +9.2         0       0
the training data (on either source or target side).          HiTech        100%       +13.5        0       0
   A more powerful type of training-data–based                Dom. avg.     100%       +9.3         0       0
features operates by computing a BLEU score be-
tween a document (source or target side) and the             Table 4: Lower- and upper-bounds for ranking and
training-data documents used as references. Intu-            regression accuracy (English-Spanish).
itively, we assess the coverage with respect to the
training data and correlate it with a BLEU score:
“if the n-grams of input document A are well cov-            for English-Spanish, using all the features de-
ered by the source-side of the training data, the            scribed, is presented in Table 3. The ranking ac-
translation of A is probably good” (on the source            curacy numbers on a per-quantile basis reveals
side); “if the n-grams in the output translation B           an important property for the approach we ad-
are well covered by the target-side of the parallel          vocate. The ranking accuracy on the first quan-
training data, then B is probably a good transla-            tile Q1 (identifying the best 25% of the transla-
tion” (on the target side).                                  tions) is 52% on average across the domains. For
                                                             the last quantile Q4 (identifying the worst 25% of
4.3   Results                                                the translations), it is 56%. This is much better
We are interested in the best performance for                than the ranking accuracy for the median-quality
TrustRank using the features described above. In             translations (35-37% accuracy for the two middle
this section, we focus on reporting the results ob-          quantiles). This property fits well our scenario, in
tain for the English-Spanish language pair. In the           which we are interested in associating trust in the
next section, we report results obtained on all the          quality of the translations in the top quantile.
language pairs we considered.                                   The quality of the top quantile translations is
   Before we discuss the results of TrustRank, let           quantifiable in terms of BLEU gain. The 250 doc-
us anchor the numerical values using some lower-             ument translations in Q1 for Travel have a BLEU
and upper-bounds. As a baseline, we use a re-                score of 38.0, a +6.8 BLEU gain compared to the
gression function that outputs a constant number             overall BLEU of 31.2 (Q1−4 ). The Q1 HiTech
for each document, equal to the BLEU score of                translations, with a BLEU of 77.9, have a +18.9
the Regression Training set. As an upperbound,               BLEU gain compared to the overall BLEU of
we use an oracle regression function that outputs a          59.0. The TrustRank algorithm allows us to trade-
number for each document that is equal to the ac-            off quantity versus quality on any scale. The re-
tual BLEU score of that document. In Table 4, we             sults under the BLEU heading in Table 3 repre-
present the performance of these regression func-            sent an instantiation of this ability to a 3-point
tions across all the domains considered.                     scale (Q1 ,Q1−2 ,Q1−3 ). The vBLEU∆ numbers
   As already mentioned, the rAcc values are                 reflect an average of the BLEU gains for this in-
bounded by the 25% lowerbound and the 100%                   stantiation (e.g., a +11.6 volume-weighted average
upperbound. The vBLEU∆ values are bounded by                 BLEU gain for the HiTech domain).
0 as lowerbound, and some positive BLEU gain                    We are also interested in the best performance
value that varies among the domains we consid-               under more restricted conditions, such as time
ered from +6.4 (Travel) to +13.5 (HiTech).                   constraints. The assumption we make here is that
   The best performance obtained by TrustRank                the translation time dwarfs the time needed for fea-


                                                       617


 Domain                 Ranking Accuracy                            Translation Accuracy                 MAE     TE
                                                                       BLEU                  vBLEU∆[4]
                Q1      Q2      Q3      Q4     rAcc          Q1    Q1−2        Q1−3   Q1−4
 WMT09         34%     26%     29%     40%     32%          44.8    43.6       42.4   41.1    +2.1       9.6    -0.1
 Travel        50%     26%     29%     41%     36%          38.0    35.1       33.0   31.2    +3.4       7.4    -1.9
 Electronics   57%     38%     39%     68%     51%          76.1    72.7       69.6   65.2    +6.5       8.4    -2.6
 HiTech        65%     48%     49%     75%     59%          77.9    72.7       66.7   59.0    +11.6      8.6    -2.1
 Dom. avg.     52%     35%     37%     56%     45%                         -                  +5.9       8.5     1.7

                  Table 3: Detailed performance using all features (English-Spanish).


ture and regression value computation. Therefore,           only 26% of the oracle performance). In contrast,
the most time-expensive feature is the source-side          the BLEU gain for the HiTech domain is +11.6
pseudo-reference–based feature, which effectively           vBLEU∆ (compared to the +13.5 vBLEU∆ up-
doubles the translation time required. Under the            perbound, it is 86% of the oracle performance).
“time-constrained” condition, we exclude this fea-
ture and use all of the remaining features. Table 5         Positive feature synergy and overlap
presents the results obtained for English-Spanish.
                                                            The features we described capture different infor-
                                                            mation, and their combination achieves the best
 Domain       rAcc vBLEU∆[4]           MAE      TE
                                                            performance. For instance, in the Electronics do-
 “Time-constrained” condition                               main, the best single feature is the target-side n-
 WMT09        32%        +2.1           9.6    -0.1         gram coverage feature, with +5.3 vBLEU∆. The
 Travel       35%        +3.2           7.4    -1.8         combination of all features gives a +6.5 vBLEU∆.
 Electronics 50%         +6.3           8.4    -2.2            The numbers in Table 3 also show that elimi-
 HiTech       59%       +11.6           8.9    -2.1         nating some of the features results in lower perfor-
 Dom. avg. 44%           +5.8           8.6    1.6          mance. The rAcc drops from 45% to 44% in under
                                                            the “time-constraint” condition (Table 5). The dif-
Table 5:      “Time-constrained”      performance           ference in the rankings is statistically significant at
(English-Spanish).                                          p < 0.01 using the Wilcoxon test (Demšar, 2006).
                                                               However, this drop is quantitatively small (1%
   The results presented above allow us to draw a
                                                            rAcc drop, -0.1 in vBLEU∆, averaged across do-
series of conclusions.
                                                            mains). This suggests that, even when eliminating
Benefits vary by domain                                     features that by themselves have a good discrim-
                                                            inatory power (the source-side pseudo-reference–
Even with oracle rankings (Table 4), the benefits
                                                            based feature achieves a +5.0 vBLEU∆ as a sin-
vary from one domain to the next. For Travel, with
                                                            gle feature in the Electronics domain), the other
an overall BLEU score in the low 30s (31.2), we
                                                            features compensate to a large degree.
stand to gain at most +6.4 BLEU points on average
(+6.4 vBLEU∆ upperbound). For a domain such
                                                            Poor regression performance
as HiTech, even with a high overall BLEU score
close to 60 (59.0), we stand to gain twice as much          By looking at the results of the regression metrics,
(+13.5 vBLEU∆ upperbound).                                  we conclude that the predicted BLEU numbers are
                                                            not accurate in absolute value. The aggregated
Performance varies by domain                                Mean Absolute Error (MAE) is 8.5 when using all
As the results in Table 3 show, the best perfor-            the features. This is less than the baseline MAE of
mance we obtain also varies from one domain to              11.8, but it is too high to allow us to confidently
the next. For instance, the ranking accuracy for            use the document-level BLEU numbers as reliable
the WMT09 domain is only 32%, while for the                 indicators of translation accuracy. The Test Error
HiTech domain is 59%. Also, the BLEU gain for               (TE) numbers are not encouraging either, as the
the WMT09 domain is only +2.1 vBLEU∆ (com-                  1.7 TE of TrustRank is close to the baseline TE of
pared to the upperbound vBLEU∆ of +8.2, it is               1.9 (see Table 4 for baseline numbers).


                                                      618


5   Large-scale experimental results                          Domain      BLEU    rAcc   vBLEU∆[4]   MAE     TE

In this section, we present the performance of                WMT09
TrustRank on a variety of language pairs (Table 6).           Eng-Spa 41.0       35%       +2.4       9.2   -0.3
We report the BLEU score obtained on our 1000-                Eng-Fra 37.1       37%       +3.3       8.3   -0.5
document regression Test, as well as ranking and              Eng-Ger 23.7       32%       +1.9       5.8   -0.7
regression performance using the rAcc, vBLEU∆,                Eng-Cze 10.3       38%       +1.3       3.1   -0.6
MAE, and TE metrics.                                          Eng-Hun 14.5       55%       +4.3       3.7   -1.1
   As the numbers for the ranking and regres-                 Spa-Eng 40.1       37%       +3.3       8.1   -0.2
sion metrics show, the same trends we observed                Fra-Eng 37.9       39%       +3.8      10.1   -0.6
for English-Spanish hold for many other language              Ger-Eng 29.4       36%       +2.7       5.9   -0.9
pairs as well. Some domains, such as HiTech, are              Cze-Eng 19.7       40%       +2.4       4.3   -0.6
easier to rank regardless of the language pair, and           Hun-Eng 24.0       61%       +7.1       4.9   -1.8
the quality gains are consistently high (+9.9 av-             Travel
erage vBLEU∆ for the 5 language pairs consid-                 Eng-Spa 31.2       36%      +3.4        7.4   -1.9
ered). Other domains, such as WMT09 and Travel,               Eng-Fra 27.8       39%      +2.7        6.2   -0.9
are more difficult to rank. However, the WMT09                Eng-Ita     22.5   39%      +2.4        5.1   +0.0
English-Hungarian data set appears to be better               Eng-Por 41.9       51%      +5.6        8.6   +1.1
suited for ranking, as the vBLEU∆ numbers are                 Eng-Ger 27.6       37%      +5.7       11.8   -0.4
higher compared to the rest of the language pairs             Eng-Dut 41.9       52%      +12.9      12.9   -0.7
from this domain (+4.3 vBLEU∆ for Eng-Hun,
                                                              Electronics
+7.1 vBLEU∆ for Hun-Eng). For Travel, English-
                                                              Eng-Spa 65.2       51%       +6.5      8.4    -2.6
Dutch is also an outlier in terms of quality gains
                                                              Eng-Fra 55.8       49%       +7.7      8.4    -2.3
(+12.9 vBLEU∆).
                                                              Eng-Ger 42.1       57%       +8.9      7.4    -1.6
   Overall, the results indicate that TrustRank ob-
                                                              Eng-Chi 63.9       48%       +6.4      8.6    -0.8
tains consistent performance across a large vari-
                                                              Eng-Por 47.9       49%       +6.9      9.0    -1.8
ety of language pairs. Similar with the conclusion
for English-Spanish, the regression performance               HiTech
is currently too poor to allow us to confidently              Eng-Spa 59.0       59%      +11.6      8.6    -2.1
use the absolute document-level predicted BLEU                Eng-Ger 36.6       62%      +9.2       7.1    -1.0
numbers as indicators of translation accuracy.                Eng-Chi 60.3       54%      +7.5       8.4    -1.0
                                                              Eng-Rus 39.2       62%      +10.7      8.7    -2.1
6   Examples and Illustrations                                Eng-Kor 49.4       61%      +10.5      9.7    -3.2
As the experimental results in Table 6 show, the
regression performance varies considerably across            Table 6: Performance of TrustRank on a variety of
domains. Even within the same domain, the nature             domains and language pairs.
of the material used to perform the experiments
can influence considerably the results we obtain.
In Figure 1, we plot hBLEU,predBLEUi points for              Travel Eng-Fra case, the predicted BLEU numbers
three of our language pairs presented in Table 6:            are spread across a narrower band (95% of the val-
Travel Eng-Fra, Travel Eng-Dut, and HiTech Eng-              ues are in the [19-35] interval), compared to the
Rus. These plots illustrate the tendency of the pre-         actual BLEU scores (95% of the values are in the
dicted BLEU values to correlate with the actual              [11-47] interval).
BLEU scores. The amount of correlation visible in               These intervals are also useful for gauging the
these plots matches the performance numbers pro-             level of difficulty stemming from the nature of the
vided in Table 6, with Travel Eng-Fra at a lower             material used to perform the experiments. In the
level of correlation compared to Travel Eng-Dut              case of Travel Eng-Fra, the actual BLEU scores
and HiTech Eng-Rus. The hBLEU,predBLEUi points               are clustered in a narrower band (interval [11-47]
tend to align along a line at an angle smaller than          covers 95% of the values), compared to the actual
45◦ , an indication of the fact that the BLEU pre-           BLEU scores for Travel Eng-Dut (interval [11-92]
dictions tend to be more conservative compared               covers 95% of the values) and HiTech Eng-Rus
to the actual BLEU scores. For example, in the               (interval [3-80] covers 95% of the values). This


                                                       619


                                                                             la chambre, le personnel, même d’autres clients dans
                                                                             d’autres pays, c’est très agréable de voir que tout le
                                                                             monde vous aurais savoir au cours de ces dernières
                                                                             années, même si, ou bien ils vous, ne parlent pas
                                                                             chaque d’autres langues. Nous adorons l’ı̂le des que,
                                                                             hopefuly, c’est l’endroit où nous avons retiring, nous
                                                                             ne pour chercher un endroit abordable.

                                                                       B-Eng Stayed at the Intercontinental for 4 nights. It is in an
                                                                           excellent location, not far from the French Quarter. The
                                                                           rooms are large, clean, and comfortable. The staff is
                                                                           friendly and helpful. Parking is very expensive, around
                                                                           $29. 00 a day. There is a garage next door which is a
                                                                           little more reasonable. I certainly suggest this hotel to
                                                                           others.

                                                                       B-Fra J’ai séjourné à l’Intercontinental pour 4 nuits. Il est
                                                                            très bien situé, pas loin du Quartier Français. Les
                                                                            chambres sont grandes, propres et confortables. Le per-
                                                                            sonnel est sympa et serviable. Le parking est très cher,
                                                                            autour de 29 $ par jour. Il y a un garage à côté, ce
                                                                            qui est un peu plus raisonnable. Je conseille cet hôtel à
                                                                            d’autres.

                                                                       Document A-Fra is a poor translation, and is
                                                                       ranked in the bottom 10%, while document B-Fra
                                                                       is a nearly-perfect translation ranked in the top
                                                                       10%, out of a total of 1000 documents.

                                                                       7    Conclusions and Future Work
                                                                       Commercial adoption of MT technology requires
                                                                       trust in the translation quality. Rather than delay
  Figure 1: Examples of BLEU versus predBLEU.
                                                                       this adoption until MT attains a near-human level
                                                                       of sophistication, we propose an interim approach.
means that the documents in the latter cases are                       We present a mechanism that allows MT users
easier to distinguish, compared to the documents                       to trade quantity for quality, using automatically-
in Travel Eng-Fra.                                                     determined translation quality rankings.
   To provide an intuitive feel for the difference                        The results we present in this paper show that
between the level of translation performance be-                       document-level translation quality rankings pro-
tween documents ranked close to the bottom and                         vide quantitatively strong gains in translation qual-
documents ranked close to the top, we present                          ity, as measured by BLEU. A difference of +18.9
here two example translations. They are docu-                          BLEU, like the one we obtain for the English-
ments that we randomly picked from the bottom                          Spanish HiTech domain (Table 3), is persuasive
10% and top 10% of the Travel Eng-Fra document                         evidence for inspiring trust in the quality of se-
set, and they correspond to points A and B in the                      lected translations. This approach enables us to
first plot of Figure 1, respectively. The A-Fra and                    develop TrustRank, a complete MT solution that
B-Fra entries below are produced by our Eng-Fra                        enhances automatic translation with the ability to
TrustRank-MT system, starting from A-Eng and                           identify document subsets containing translations
B-Eng3 , respectively.                                                 that pass an acceptable quality threshold.
A-Eng This will be our 18th year,still love it. Same hotel,               When measuring the performance of our solu-
    room, staff, even other guests from other countries, its           tion across several domains, it becomes clear that
    lovely to see everyone that you have gotten to know                some domains allow for more accurate quality pre-
    over the years, even if ,you or they ,do not speak each
    others language. We love the Island some much that,                diction than others. Given the immediate benefit
    hopefuly, that is where we are retiring to, we do keep             that can be derived from increasing the ranking
    looking for that affordable place.
                                                                       accuracy for translation quality, we plan to open
A-Fra Ce sera notre 18ème année, adore. Même hôtel,                up publicly available benchmark data that can be
    3
      We preserved the original writing style of the documents         used to stimulate and rigorously monitor progress
in the source language.                                                in this direction.


                                                                 620


References                                                        A. Lavie and A. Agarwal. 2007. METEOR: An au-
                                                                    toamtic metric for mt evaluation with high levels of
Joshua Albrecht and Rebecca Hwa. 2007. Regression                   correlation with human judgments. In Proceedings
   for sentence-level MT evaluation with pseudo refer-              of ACL Workshop on Statistical Machine Transla-
   ences. In Proceedings of ACL.                                    tion.
Joshua Albrecht and Rebecca Hwa. 2008. The role of                Ding Liu and Daniel Gildea. 2005. Syntactic fea-
   pseudo references in MT evaluation. In Proceedings               tures for evaluation of machine translations. In Pro-
   of ACL.                                                          ceedings of ACL Workshop on Intrinsic and Extrin-
Enrique Amigó, Jesús Giménez, Julio Gonzalo, and Fe-             sic Evaluation Measures for Machine Translation
  lisa Verdejo. 2009. The contribution of linguistic                and/or Summarization.
  features to automatic machine translation evaluation.           Franz Joseph Och and Hermann Ney. 2004. The align-
  In Proceedings of ACL.                                            ment template approach to statistical machine trans-
John Blatz, Erin Fitzgerald, GEorge Foster, Simona                  lation. Computational Linguistics, 30(4):417–449.
  Gandrabur, Cyril Gouette, Alex Kulesza, Alberto
                                                                  Karolina Owczarzak, Josef Genabith, and Andy Way.
  Sanchis, and Nicola Ueffing. 2004. Confidence es-
                                                                    2007. Evaluating machine translation with LFG
  timation for machine translation. In Proceedings of
                                                                    dependencies. Machine Translation, 21(2):95–119,
  COLING.
                                                                    June.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
                                                                  Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
  2005. Clause restructuring for statistical machine
                                                                    Jing Zhu. 2002. BLEU: a method for automatic
  translation. In Proceedings of ACL.
                                                                    evaluation of machine translation. In Proceedings
J. Demšar. 2006. Statistical comparisons of classifiers            of ACL.
   over multiple data sets. Journal of Machine Learn-
                                                                  Sujith Ravi, Kevin Knight, and Radu Soricut. 2008.
   ing Research, 7.
                                                                    Automatic prediction of parsing accuracy. In Pro-
George Doddington. 2002. Automatic evaluation of                    ceedings of EMNLP.
  machine translation quality using n-gram coocur-
  rence statistics. In Proceedings of HLT.                        Lucia Specia, Nicola Cancedda, Marc Dymetman,
                                                                    Marcho Turchi, and Nello Cristianini. 2009. Esti-
Kevin Duh. 2008. Ranking vs. regression in machine                  mating the sentence-level quality of machine trans-
  translation evaluation. In Proceedings of the ACL                 lation. In Proceedings of EAMT.
  Third Workshop on Statistical Machine Translation.
                                                                  Nicola Ueffing and Hermann Ney. 2005. Applica-
Michael Gamon, Anthony Aue, and Martine Smets.                      tion of word-level confidence measures in interac-
  2005. Sentence-level MT evaluation without refer-                 tive statistical machine translation. In Proceedings
  ence translations: Beyond language modeling. In                   of EAMT.
  Proceedings of EAMT.
                                                                  Peng Xu, Jaeho Kang, Michael Ringaard, and Franz
Asela Gunawardana and Guy Shani. 2009. A sur-                       Och. 2009. Using a dependency parser to improve
  vey of accuracy evaluation metrics of recommenda-                 SMT for Subject-Object-Verb languages. In Pro-
  tion tasks. Journal of Machine Learning Research,                 ceedings of ACL.
  10:2935–2962.
                                                                  Muyun Yang, Shuqi Sun, Jufeng Li, Sheng Li, and
David Kauchak and Regina Barzilay. 2006. Para-                     Zhao Tiejun. 2008. A linguistically motivated MT
  phrasing for automatic evaluation. In Proceedings                evaluation system based on SVM regression. In
  of HLT/NAACL.                                                    Proceedings of AMTA.

Philipp Koehn and Barry Haddow. 2009. Edinburgh’s                 Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
  submission to all tracks of the WMT2009 shared                    tence level machine translation evaluation as a rank-
  task with reordering and speed improvements to                    ing. In Proceedings of the ACL Second Workshop on
  Moses. In Proceedings of EACL Workshop on Sta-                    Statistical Machine Translation.
  tistical Machine Translation.
                                                                  Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-                  Re-evaluating machine translation results with para-
  ing approach to improving sentence-level MT evalu-                phrase support. In Proceedings of EMNLP.
  ation. In Proceedings of the 10th International Con-
  ference on Theoretical and Methodological Issues in
  Machine Translation.
A.-L. Lagarda, V. Alabau, F. Casacuberta, R. Silva, and
  E. Dı́az de Liaño. 2009. Statistical post-editing of a
  rule-based machine translation system. In Proceed-
  ings of HLT/NAACL.


                                                            621

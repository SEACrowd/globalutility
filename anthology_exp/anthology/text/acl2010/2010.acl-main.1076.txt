      Boosting-based System Combination for Machine Translation

                       Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang

                         Natural Language Processing Lab.
                           Northeastern University, China
             {xiaotong,zhujingbo,wanghuizhen}@mail.neu.edu.cn
                            zhumuhua@gmail.com


                                                             how the translation is combined and what voting
                     Abstract                                strategy is adopted, several methods can be used
                                                             for system combination, e.g. sentence-level com-
    In this paper, we present a simple and effective         bination (Hildebrand and Vogel, 2008) simply
    method to address the issue of how to generate           selects one from original translations, while
    diversified translation systems from a single            some more sophisticated methods, such as word-
    Statistical Machine Translation (SMT) engine             level and phrase-level combination (Matusov et
    for system combination. Our method is based
                                                             al., 2006; Rosti et al., 2007), can generate new
    on the framework of boosting. First, a se-
    quence of weak translation systems is gener-             translations differing from any of the original
    ated from a baseline system in an iterative              translations.
    manner. Then, a strong translation system is                One of the key factors in SMT system combi-
    built from the ensemble of these weak transla-           nation is the diversity in the ensemble of transla-
    tion systems. To adapt boosting to SMT sys-              tion outputs (Macherey and Och, 2007). To ob-
    tem combination, several key components of               tain diversified translation outputs, most of the
    the original boosting algorithms are redes-              current system combination methods require
    igned in this work. We evaluate our method on            multiple translation engines based on different
    Chinese-to-English Machine Translation (MT)              models. However, this requirement cannot be
    tasks in three baseline systems, including a
                                                             met in many cases, since we do not always have
    phrase-based system, a hierarchical phrase-
    based system and a syntax-based system. The              the access to multiple SMT engines due to the
    experimental results on three NIST evaluation            high cost of developing and tuning SMT systems.
    test sets show that our method leads to signifi-         To reduce the burden of system development, it
    cant improvements in translation accuracy                might be a nice way to combine a set of transla-
    over the baseline systems.                               tion systems built from a single translation en-
                                                             gine. A key issue here is how to generate an en-
1    Introduction                                            semble of diversified translation systems from a
                                                             single translation engine in a principled way.
Recent research on Statistical Machine Transla-                 Addressing this issue, we propose a boosting-
tion (SMT) has achieved substantial progress.                based system combination method to learn a
Many SMT frameworks have been developed,                     combined translation system from a single SMT
including phrase-based SMT (Koehn et al., 2003),             engine. In this method, a sequence of weak trans-
hierarchical phrase-based SMT (Chiang, 2005),                lation systems is generated from a baseline sys-
syntax-based SMT (Eisner, 2003; Ding and                     tem in an iterative manner. In each iteration, a
Palmer, 2005; Liu et al., 2006; Galley et al., 2006;         new weak translation system is learned, focusing
Cowan et al., 2006), etc. With the emergence of              more on the sentences that are relatively poorly
various structurally different SMT systems, more             translated by the previous weak translation sys-
and more studies are focused on combining mul-               tem. Finally, a strong translation system is built
tiple SMT systems for achieving higher transla-              from the ensemble of the weak translation sys-
tion accuracy rather than using a single transla-            tems.
tion system.                                                    Our experiments are conducted on Chinese-to-
   The basic idea of system combination is to ex-            English translation in three state-of-the-art SMT
tract or generate a translation by voting from an            systems, including a phrase-based system, a hier-
ensemble of translation outputs. Depending on                archical phrase-based system and a syntax-based


                                                         739
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 739–748,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


         Input: a model u, a sequence of (training) samples {(f1, r1), ..., (fm, rm)} where fi is the
         i-th source sentence, and ri is the set of reference translations for fi.
         Output: a new translation system
         Initialize: D1(i) = 1 / m for all i = 1, ..., m
         For t = 1, ..., T
               1. Train a translation system u(λ*t) on {(fi, ri)} using distribution Dt
               2. Calculate the error rate ε t of u(λ*t) on {(fi, ri)}
               3. Set
                                                       1 1+ εt
                                                α t = ln(          )                                 (3)
                                                       2      εt
               4. Update weights
                                                            D (i )eα t ⋅li
                                                Dt +1 (i ) = t                                       (4)
                                                                 Zt
                  where li is the loss on the i-th training sample, and Zt is the normalization factor.
         Output the final system:
                                                v(u(λ*1), ..., u (λ*T))
                                         Figure 1: Boosting-based System Combination

system. All the systems are evaluated on three                    data set1 to obtain an optimized weight vector λ*
NIST MT evaluation test sets. Experimental re-                    and consequently an optimized system u(λ*).
sults show that our method leads to significant
improvements in translation accuracy over the                     3     Boosting-based System Combination
baseline systems.                                                       for Single Translation Engine

2    Background                                                   Suppose that there are T available SMT systems
                                                                  {u1(λ*1), ..., uT(λ*T)}, the task of system combina-
Given a source string f, the goal of SMT is to                    tion is to build a new translation system
find a target string e* by the following equation.                v(u1(λ*1), ..., uT(λ*T)) from {u1(λ*1), ..., uT(λ*T)}.
                                                                  Here v(u1(λ*1), ..., uT(λ*T)) denotes the combina-
                    e* = arg max(Pr(e | f ))              (1)     tion system which combines translations from the
                             e
                                                                  ensemble of the output of each ui(λ*i). We call
where Pr(e | f ) is the probability that e is the                 ui(λ*i) a member system of v(u1(λ*1), ..., uT(λ*T)).
translation of the given source string f. To model                As discussed in Section 1, the diversity among
the posterior probability Pr(e | f ) , most of the                the outputs of member systems is an important
state-of-the-art SMT systems utilize the log-                     factor to the success of system combination. To
linear model proposed by Och and Ney (2002),                      obtain diversified member systems, traditional
as follows,                                                       methods concentrate more on using structurally
                       exp(∑ m =1 λ m ⋅ hm ( f , e))
                                 M                                different member systems, that is u1≠ u2 ≠...≠
     Pr(e | f ) =                                         (2)     uT. However, this constraint condition cannot be
                    ∑ e ' exp(∑ m=1 λ m ⋅ hm( f , e '))
                                     M
                                                                  satisfied when multiple translation engines are
                                                                  not available.
where {hm( f, e ) | m = 1, ..., M} is a set of fea-                  In this paper, we argue that the diversified
tures, and λm is the feature weight corresponding                 member systems can also be generated from a
to the m-th feature. hm( f, e ) can be regarded as a              single engine u(λ*) by adjusting the weight vector
function that maps every pair of source string f                  λ* in a principled way. In this work, we assume
and target string e into a non-negative value, and                that u1 = u2 =...= uT = u. Our goal is to find a se-
λm can be viewed as the contribution of hm( f, e )                ries of λ*i and build a combined system from
to the overall score Pr(e | f ) .                                 {u(λ*i)}. To achieve this goal, we propose a
   In this paper, u denotes a log-linear model that
has M fixed features {h1( f ,e ), ..., hM( f ,e )}, λ =
{λ1, ..., λM} denotes the M parameters of u, and
u(λ) denotes a SMT system based on u with pa-                     1
                                                                    The data set used for weight training is generally called
rameters λ. Generally, λ is trained on a training                 development set or tuning set in the SMT field. In this paper,
                                                                  we use the term training set to emphasize the training of
                                                                  log-linear model.


                                                                740


boosting-based system combination method (Fig-                              ously the original BLEU is just a special case of
ure 1).                                                                     WBLEU when all the training samples are
   Like other boosting algorithms, such as                                  equally weighted.
AdaBoost (Freund and Schapire, 1997; Schapire,                                 As the weighted BLEU is used to measure the
2001), the basic idea of this method is to use                              translation accuracy on the training set, the error
weak systems (member systems) to form a strong                              rate is defined to be:
system (combined system) by repeatedly calling
                                                                                               ε t = 1 − WBLEU( E , R )                 (6)
weak system trainer on different distributions
over the training samples. However, since most                              3.2     Re-weighting
of the boosting algorithms are designed for the
classification problem that is very different from                          Another key point is the maintaining of the dis-
the translation problem in natural language proc-                           tribution Dt(i) over the training set. Initially all
essing, several key components have to be redes-                            the weights of training samples are set equally.
igned when boosting is adapted to SMT system                                On each round, we increase the weights of the
combination.                                                                samples that are relatively poorly translated by
                                                                            the current weak system so that the MERT-based
3.1        Training                                                         trainer can focus on the hard samples in next
In this work, Minimum Error Rate Training                                   round. The update rule is given in Equation 4
(MERT) proposed by Och (2003) is used to es-                                with two parameters α t and li in it.
timate feature weights λ over a series of training                             α t can be regarded as a measure of the im-
samples. As in other state-of-the-art SMT sys-                              portance that the t-th weak system gains in boost-
tems, BLEU is selected as the accuracy measure                              ing. The definition of α t guarantees that α t al-
to define the error function used in MERT. Since                            ways has a positive value3. A main effect of α t
the weights of training samples are not taken into                          is to scale the weight updating (e.g. a larger α t
account in BLEU2, we modify the original defi-                              means a greater update).
nition of BLEU to make it sensitive to the distri-                             li is the loss on the i-th sample. For each i, let
bution Dt(i) over the training samples. The modi-                           {ei1, ..., ein} be the n-best translation candidates
fied version of BLEU is called weighted BLEU                                produced by the system. The loss function is de-
(WBLEU) in this paper.                                                      fined to be:
   Let E = e1 ... em be the translations produced                                                            1 k
by the system, R = r1 ... rm be the reference trans-                               li = BLEU(ei * , ri ) −
                                                                                                             k
                                                                                                               ∑ j =1 BLEU(eij, ri) (7)
lations where ri = {ri1, ..., riN}, and Dt(i) be the
weight of the i-th training sample (fi, ri). The                            where BLEU(eij, ri) is the smoothed sentence-level
weighted BLEU metric has the following form:                                BLEU score (Liang et al., 2006) of the transla-
      WBLEU( E , R )                                                        tion e with respect to the reference translations ri,
                                                                            and ei* is the oracle translation which is selected
          ⎛         ⎧ ∑ m Dt (i ) min {| g1 (rij ) |} ⎫ ⎞                   from {ei1, ..., ein} in terms of BLEU(eij, ri). li can
          ⎜         ⎪   i =1      1≤ j ≤ N            ⎪⎟
    = exp ⎜ 1 − max ⎨1,                               ⎬ ×                   be viewed as a measure of the average cost that
                          ∑      D (i ) | g1 (ei ) | ⎪ ⎟⎟
                             m
          ⎜         ⎪                                                       we guess the top-k translation candidates instead
          ⎝         ⎩        i =1 t
                                                      ⎭⎠
                                                                            of the oracle translation. The value of li counts
                                     (                   )
                                                             1/ 4
           ⎛ m D (i ) g (ei )                                ⎞              for the magnitude of weight update, that is, a lar-
             ∑
                                          N
       4
           ⎜     =  t      n    I     U       g (rij )
                                          j =1 n             ⎟              ger li means a larger weight update on Dt(i). The
      ∏
               i  1
           ⎜                                                 ⎟      (5)
                      ∑
                        m
                             D (i ) g n (ei )                               definition of the loss function here is similar to
      n =1
           ⎜            i =1 t                               ⎟
           ⎝                                                 ⎠              the one used in (Chiang et al., 2008) where only
                                                                            the top-1 translation candidate (i.e. k = 1) is
where g n ( s ) is the multi-set of all n-grams in a
                                                                            taken into account.
string s. In this definition, n-grams in ei and {rij}
are weighted by Dt(i). If the i-th training sample                          3.3     System Combination Scheme
has a larger weight, the corresponding n-grams
will have more contributions to the overall score                           In the last step of our method, a strong transla-
WBLEU( E , R ) . As a result, the i-th training                             tion system v(u(λ*1), ..., u(λ*T)) is built from the
sample gains more importance in MERT. Obvi-
                                                                            3
                                                                              Note that the definition of α t here is different from that in
2
  In this paper, we use the NIST definition of BLEU where                   the original AdaBoost algorithm (Freund and Schapire,
the effective reference length is the length of the shortest                1997; Schapire, 2001) where α t is a negative number when
reference translation.                                                       ε t > 0.5 .

                                                                          741


ensemble of member systems {u(λ*1), ..., u(λ*T)}.               4   Optimization
In this work, a sentence-level combination
method is used to select the best translation from              If implemented naively, the translation speed of
the pool of the n-best outputs of all the member                the final translation system will be very slow.
systems.                                                        For a given input sentence, each member system
   Let H(u(λ*t)) (or Ht for short) be the set of the            has to encode it individually, and the translation
n-best translation candidates produced by the t-th              speed is inversely proportional to the number of
member system u(λ*t), and H(v) be the union set                 member systems generated by our method. For-
of all Ht (i.e. H (v) = U H t ). The final translation          tunately, with the thought of computation, there
                                                                are a number of optimizations that can make the
is generated from H(v) based on the following                   system much more efficient in practice.
scoring function:                                                  A simple solution is to run member systems in
      e* = arg max ∑ t =1 β t ⋅ φt (e) + ψ (e, H (v)) (8)
                         T                                      parallel when translating a new sentence. Since
             e∈H ( v )                                          all the member systems share the same data re-
                                                                sources, such as language model and translation
where φt (e) is the log-scaled model score of e in              table, we only need to keep one copy of the re-
the t-th member system, and β t is the corre-                   quired resources in memory. The translation
sponding feature weight. It should be noted that                speed just depends on the computing power of
 e ∈ Hi may not exist in any Hi ' ≠ i . In this case,           parallel computation environment, such as the
we can still calculate the model score of e in any              number of CPUs.
other member systems, since all the member sys-                    Furthermore, we can use joint decoding tech-
tems are based on the same model and share the                  niques to save the computation of the equivalent
same feature space. ψ (e, H (v)) is a consensus-                translation hypotheses among member systems.
based scoring function which has been success-                  In joint decoding of member systems, the search
fully adopted in SMT system combination (Duan                   space is structured as a translation hypergraph
et al., 2009; Hildebrand and Vogel, 2008; Li et                 where the member systems can share their trans-
al., 2009). The computation of ψ (e, H (v)) is
                                                                lation hypotheses. If more than one member sys-
based on a linear combination of a set of n-gram
                                                                tems share the same translation hypothesis, we
consensuses-based features.
                                                                just need to compute the corresponding feature
         ψ (e, H (v)) = ∑θ n+ ⋅ hn+ (e, H (v)) +                values only once, instead of repeating the com-
                             n                                  putation in individual decoders. In our experi-
                         ∑θ      −
                                 n   ⋅ hn− (e, H (v))   (9)     ments, we find that over 60% translation hy-
                             n                                  potheses can be shared among member systems
                                                                when the number of member systems is over 4.
    For each order of n-gram, hn+ (e, H (v)) and
                                                                This result indicates that promising speed im-
 hn− (e, H (v)) are defined to measure the n-gram
                                                                provement can be achieved by using the joint
agreement and disagreement between e and other
                                                                decoding and hypothesis sharing techniques.
translation candidates in H(v), respectively. θ n+
                                                                   Another method to speed up the system is to
and θ n− are the feature weights corresponding to
                                                                accelerate n-gram language model with n-gram
 hn+ (e, H (v)) and hn− (e, H (v)) . As hn+ (e, H (v)) and      caching techniques. In this method, a n-gram
 hn− (e, H (v)) used in our work are exactly the                cache is used to store the most frequently and
same as the features used in (Duan et al., 2009)                recently accessed n-grams. When a new n-gram
and similar to the features used in (Hildebrand                 is accessed during decoding, the cache is
and Vogel, 2008; Li et al., 2009), we do not pre-               checked first. If the required n-gram hits the
sent the detailed description of them in this paper.            cache, the corresponding n-gram probability is
    If p orders of n-gram are used in computing                 returned by the cached copy rather than re-
ψ (e, H (v)) , the total number of features in the              fetching the original data in language model. As
system combination will be T + 2 × p (T model-                  the translation speed of SMT system depends
score-based features defined in Equation 8 and                  heavily on the computation of n-gram language
 2 × p consensus-based features defined in Equa-                model, the acceleration of n-gram language
tion 9). Since all these features are combined                  model generally leads to substantial speed-up of
linearly, we use MERT to optimize them for the                  SMT system. In our implementation, the n-gram
combination model.                                              caching in general brings us over 30% speed im-
                                                                provement of the system.



                                                              742


5     Experiments                                             of the bilingual data and the Xinhua portion of
                                                              English Gigaword corpus. Berkeley Parser is
Our experiments are conducted on Chinese-to-                  used to generate the English parse trees for the
English translation in three SMT systems.                     rule extraction of the syntax-based system. The
5.1    Baseline Systems                                       data set used for weight training in boosting-
                                                              based system combination comes from NIST
The first SMT system is a phrase-based system                 MT03 evaluation set. To speed up MERT, all the
with two reordering models including the maxi-                sentences with more than 20 Chinese words are
mum entropy-based lexicalized reordering model                removed. The test sets are the NIST evaluation
proposed by Xiong et al. (2006) and the hierar-               sets of MT04, MT05 and MT06. The translation
chical phrase reordering model proposed by Gal-               quality is evaluated in terms of case-insensitive
ley and Manning (2008). In this system all                    NIST version BLEU metric. Statistical signifi-
phrase pairs are limited to have source length of             cant test is conducted using the bootstrap re-
at most 3, and the reordering limit is set to 8 by            sampling method proposed by Koehn (2004).
default4.                                                        Beam search and cube pruning (Huang and
   The second SMT system is an in-house reim-                 Chiang, 2007) are used to prune the search space
plementation of the Hiero system which is based               in all the three baseline systems. By default, both
on the hierarchical phrase-based model proposed               of the beam size and the size of n-best list are set
by Chiang (2005).                                             to 20.
   The third SMT system is a syntax-based sys-                   In the settings of boosting-based system com-
tem based on the string-to-tree model (Galley et              bination, the maximum number of iterations is
al., 2006; Marcu et al., 2006), where both the                set to 30, and k (in Equation 7) is set to 5. The n-
minimal GHKM and SPMT rules are extracted                     gram consensuses-based features (in Equation 9)
from the bilingual text, and the composed rules               used in system combination ranges from unigram
are generated by combining two or three minimal               to 4-gram.
GHKM and SPMT rules. Synchronous binariza-
tion (Zhang et al., 2006; Xiao et al., 2009) is per-          5.3   Evaluation of Translations
formed on each translation rule for the CKY-                  First we investigate the effectiveness of the
style decoding.                                               boosting-based system combination on the three
   In this work, baseline system refers to the sys-           systems.
tem produced by the boosting-based system                        Figures 2-5 show the BLEU curves on the de-
combination when the number of iterations (i.e.               velopment and test sets, where the X-axis is the
T ) is set to 1. To obtain satisfactory baseline per-         iteration number, and the Y-axis is the BLEU
formance, we train each SMT system for 5 times                score of the system generated by the boosting-
using MERT with different initial values of fea-              based system combination. The points at itera-
ture weights to generate a group of baseline can-             tion 1 stand for the performance of the baseline
didates, and then select the best-performing one              systems. We see, first of all, that all the three
from this group as the final baseline system (i.e.            systems are improved during iterations on the
the starting point in the boosting process) for the           development set. This trend also holds on the test
following experiments.                                        sets. After 5, 7 and 8 iterations, relatively stable
5.2    Experimental Setup                                     improvements are achieved by the phrase-based
                                                              system, the Hiero system and the syntax-based
Our bilingual data consists of 140K sentence                  system, respectively. The BLEU scores tend to
pairs in the FBIS data set5. GIZA++ is employed               converge to the stable values after 20 iterations
to perform the bi-directional word alignment be-              for all the systems. Figures 2-5 also show that the
tween the source and target sentences, and the                boosting-based system combination seems to be
final word alignment is generated using the inter-            more helpful to the phrase-based system than to
sect-diag-grow method. All the word-aligned                   the Hiero system and the syntax-based system.
bilingual sentence pairs are used to extract                  For the phrase-based system, it yields over 0.6
phrases and rules for the baseline systems. A 5-              BLEU point gains just after the 3rd iteration on
gram language model is trained on the target-side             all the data sets.
                                                                 Table 1 summarizes the evaluation results,
4
  Our in-house experimental results show that this system     where the BLEU scores at iteration 5, 10, 15, 20
performs slightly better than Moses on Chinese-to-English     and 30 are reported for the comparison. We see
translation tasks.
5
  LDC catalog number: LDC2003E14                              that the boosting-based system method stably ac-

                                                            743


                           BLEU on MT03 (dev.)                                                          BLEU on MT04 (test)
            38                                                                       38
                                           phrase-based                                                                 phrase-based
                                                   hiero                                                                        hiero
                                           syntax-based                                                                 syntax-based
            37                                                                       37




                                                                         BLEU4[%]
 BLEU4[%]




            36                                                                       36


            35                                                                       35


            34                                                                       34


            33                                                                       33
                 0   5       10       15     20       25      30                          0        5        10       15     20        25       30
                              iteration number                                                               iteration number
 Figure 2: BLEU scores on the development set                            Figure 3: BLEU scores on the test set of MT04
                           BLEU on MT05 (test)                                                          BLEU on MT06 (test)
            37                                                                       35
                                           phrase-based                                                                 phrase-based
                                                   hiero                                                                        hiero
                                           syntax-based                                                                 syntax-based
            36                                                                       34
 BLEU4[%]




                                                                         BLEU4[%]




            35                                                                       33


            34                                                                       32


            33                                                                       31


            32                                                                       30
                 0   5       10       15     20       25      30                          0        5        10       15     20        25       30
                              iteration number                                                               iteration number
 Figure 4: BLEU scores on the test set of MT05                            Figure 5: BLEU scores on the test set of MT06

                                  Phrase-based                                      Hiero                             Syntax-based
                         Dev.     MT04     MT05     MT06     Dev.     MT04                MT05     MT06      Dev.     MT04       MT05      MT06
Baseline                 33.21    33.68    32.68    30.59    33.42    34.30               33.24    30.62     35.84    35.71      35.11     32.43
Baseline+600best         33.32    33.93    32.84    30.76    33.48    34.46               33.39    30.75     35.95    35.88      35.23     32.58
Boosting-5Iterations     33.95*   34.32*   33.33*   31.33*   33.73    34.48               33.44    30.83     36.03    35.92      35.27     33.09
Boosting-10Iterations    34.14*   34.68*   33.42*   31.35*   33.75    34.65               33.75*   31.02     36.14    36.39*     35.47     33.15*
Boosting-15Iterations    33.99*   34.78*   33.46*   31.45*   34.03*   34.88*              33.98*   31.20*    36.36*   36.46*     35.53*    33.43*
Boosting-20Iterations    34.09*   35.11*   33.56*   31.45*   34.17*   35.00*              34.04*   31.29*    36.44*   36.79*     35.77*    33.36*
Boosting-30Iterations    34.12*   35.16*   33.76*   31.59*   34.05*   34.99*              34.05*   31.30*    36.52*   36.81*     35.71*    33.46*
Table 1: Summary of the results (BLEU4[%]) on the development and test sets. * = significantly better
than baseline (p < 0.05).

hieves significant BLEU improvements after 15                         systems, and thus confirms the fact we observed
iterations, and the highest BLEU scores are gen-                      in Figures 2-5.
erally yielded after 20 iterations.                                      We also investigate the impact of n-best list
   Also as shown in Table 1, over 0.7 BLEU                            size on the performance of baseline systems. For
point gains are obtained on the phrase-based sys-                     the comparison, we show the performance of the
tem after 10 iterations. The largest BLEU im-                         baseline systems with the n-best list size of 600
provement on the phrase-based system is over 1                        (Baseline+600best in Table 1) which equals to
BLEU point in most cases. These results reflect                       the maximum number of translation candidates
that our method is relatively more effective for                      accessed in the final combination system (combi-
the phrase-based system than for the other two                        ne 30 member systems, i.e. Boosing-30Iterations).


                                                                   744


                                    Diversity on MT03 (dev.)                                                      Diversity on MT04 (test)
                       40                                                                            35
                                                    phrase-based                                                                 phrase-based
                                                            hiero                                                                        hiero
                                                    syntax-based                                                                 syntax-based
                       35                                                                            30
 Diversity (TER[%])




                                                                               Diversity (TER[%])
                       30                                                                            25


                       25                                                                            20


                       20                                                                            15


                       15                                                                            10
                            0   5      10       15     20      25   30                                    0   5      10       15     20      25   30
                                        iteration number                                                              iteration number
                      Figure 6: Diversity on the development set                                    Figure 7: Diversity on the test set of MT04
                                    Diversity on MT05 (test)                                                      Diversity on MT06 (test)
                                                                                                     40
                                                    phrase-based                                                                 phrase-based
                       35                                   hiero                                                                        hiero
                                                    syntax-based                                                                 syntax-based
                                                                                                     35
                       30
 Diversity (TER[%])




                                                                               Diversity (TER[%])




                                                                                                     30
                       25
                                                                                                     25

                       20
                                                                                                     20

                       15
                                                                                                     15

                            0   5      10       15     20      25   30                                    0   5      10       15     20      25   30
                                        iteration number                                                              iteration number
                      Figure 8: Diversity on the test set of MT05                                   Figure 9: Diversity on the test set of MT06

As shown in Table 1, Baseline+600best obtains                              translation output, and thus reflects a larger di-
stable improvements over Baseline. It indicates                            versity between the two outputs. In this work, the
that the access to larger n-best lists is helpful to                       TER score for a given group of member systems
improve the performance of baseline systems.                               is calculated by averaging the TER scores be-
However, the improvements achieved by Base-                                tween the outputs of each pair of member sys-
line+600best are modest compared to the im-                                tems in this group.
provements achieved by Boosting-30Iterations.                                 Figures 6-9 show the curves of diversity on
These results indicate that the SMT systems can                            the development and test sets, where the X-axis
benefit more from the diversified outputs of                               is the iteration number, and the Y-axis is the di-
member systems rather than from larger n-best                              versity. The points at iteration 1 stand for the
lists produced by a single system.                                         diversities of baseline systems. In this work, the
                                                                           baseline’s diversity is the TER score of the group
5.4                     Diversity among Member Systems                     of baseline candidates that are generated in ad-
We also study the change of diversity among the                            vance (Section 5.1).
outputs of member systems during iterations.                                  We see that the diversities of all the systems
The diversity is measured in terms of the Trans-                           increase during iterations in most cases, though a
lation Error Rate (TER) metric proposed in                                 few drops occur at a few points. It indicates that
(Snover et al., 2006). A higher TER score means                            our method is very effective to generate diversi-
that more edit operations are performed if we                              fied member systems. In addition, the diversities
transform one translation output into another                              of baseline systems (iteration 1) are much lower


                                                                         745


than those of the systems generated by boosting                  (Freund, 1995; Freund and Schapire, 1997;
(iterations 2-30). Together with the results shown               Collins et al., 2002; Rudin et al., 2007), and has
in Figures 2-5, it confirms our motivation that                  been successfully adopted in natural language
the diversified translation outputs can lead to                  processing (NLP) applications, such as document
performance improvements over the baseline                       classification (Schapire and Singer, 2000) and
systems.                                                         named entity classification (Collins and Singer,
   Also as shown in Figures 6-9, the diversity of                1999). However, most of the previous work did
the Hiero system is much lower than that of the                  not study the issue of how to improve a single
phrase-based and syntax-based systems at each                    SMT engine using boosting algorithms. To our
individual setting of iteration number. This inter-              knowledge, the only work addressing this issue is
esting finding supports the observation that the                 (Lagarda and Casacuberta, 2008) in which the
performance of the Hiero system is relatively                    boosting algorithm was adopted in phrase-based
more stable than the other two systems as shown                  SMT. However, Lagarda and Casacuberta
in Figures 2-5. The relative lack of diversity in                (2008)’s method calculated errors over the
the Hiero system might be due to the spurious                    phrases that were chosen by phrase-based sys-
ambiguity in Hiero derivations which generally                   tems, and could not be applied to many other
results in very few different translations in trans-             SMT systems, such as hierarchical phrase-based
lation outputs (Chiang, 2007).                                   systems and syntax-based systems. Differing
                                                                 from Lagarda and Casacuberta’s work, we are
5.5      Evaluation of Oracle Translations                       concerned more with proposing a general
In this set of experiments, we evaluate the oracle               framework which can work with most of the cur-
performance on the n-best lists of the baseline                  rent SMT models and empirically demonstrating
systems and the combined systems generated by                    its effectiveness on various SMT systems.
boosting-based system combination. Our primary                      There are also some other studies on building
goal here is to study the impact of our method on                diverse translation systems from a single transla-
the upper-bound performance.                                     tion engine for system combination. The first
   Table 2 shows the results, where Base-                        attempt is (Macherey and Och, 2007). They em-
line+600best stands for the top-600 translation                  pirically showed that diverse translation systems
candidates generated by the baseline systems,                    could be generated by changing parameters at
and Boosting-30iterations stands for the ensem-                  early-stages of the training procedure. Following
ble of 30 member systems’ top-20 translation                     Macherey and Och (2007)’s work, Duan et al.
candidates. As expected, the oracle performance                  (2009) proposed a feature subspace method to
of Boosting-30Iterations is significantly higher                 build a group of translation systems from various
than that of Baseline+600best. This result indi-                 different sub-models of an existing SMT system.
cates that our method can provide much “better”                  However, Duan et al. (2009)’s method relied on
translation candidates for system combination                    the heuristics used in feature sub-space selection.
than enlarging the size of n-best list naively. It               For example, they used the remove-one-feature
also gives us a rational explanation for the sig-                strategy and varied the order of n-gram language
nificant improvements achieved by our method                     model to obtain a satisfactory group of diverse
as shown in Section 5.3.                                         systems. Compared to Duan et al. (2009)’s
                                                                 method, a main advantage of our method is that
 Data     Method                  Phrase-   Hiero    Syntax-     it can be applied to most of the SMT systems
 Set                              based              based
 Dev.     Baseline+600best        46.36     46.51    46.92
                                                                 without designing any heuristics to adapt it to the
          Boosting-30Iterations   47.78*    47.44*   48.70*      specified systems.
 MT04     Baseline+600best        43.94     44.52    46.88
          Boosting-30Iterations   45.97*    45.47*   49.40*      7    Discussion and Future Work
 MT05     Baseline+600best        42.32     42.47    45.21
          Boosting-30Iterations   44.82*    43.44*   47.02*      Actually the method presented in this paper is
 MT06     Baseline+600best        39.47     39.39    40.52
          Boosting-30Iterations   41.51*    40.10*   41.88*      doing something rather similar to Minimum
Table 2: Oracle performance of various systems.                  Bayes Risk (MBR) methods. A main difference
* = significantly better than baseline (p < 0.05).               lies in that the consensus-based combination
                                                                 method here does not model the posterior prob-
6       Related Work                                             ability of each hypothesis (i.e. all the hypotheses
                                                                 are assigned an equal posterior probability when
Boosting is a machine learning (ML) method that                  we calculate the consensus-based features).
has been well studied in the ML community

                                                               746


Greater improvements are expected if MBR                  David Chiang. 2007. Hierarchical phrase-based trans-
methods are used and consensus-based combina-               lation. Computational Linguistics, 33(2):201-228.
tion techniques smooth over noise in the MERT             David Chiang, Yuval Marton and Philip Resnik. 2008.
pipeline.                                                   Online Large-Margin Training of Syntactic and
   In this work, we use a sentence-level system             Structural Translation Features. In Proc. of
combination method to generate final transla-               EMNLP 2008, Honolulu, pages 224-233.
tions. It is worth studying other more sophisti-          Michael Collins and Yoram Singer. 1999. Unsuper-
cated alternatives, such as word-level and                  vised Models for Named Entity Classification. In
phrase-level system combination, to further im-             Proc. of EMNLP/VLC 1999, pages 100-110.
prove the system performance.
                                                          Michael Collins, Robert Schapire and Yoram Singer.
   Another issue is how to determine an appro-
                                                            2002. Logistic Regression, AdaBoost and Bregman
priate number of iterations for boosting-based              Distances. Machine Learning, 48(3): 253-285.
system combination. It is especially important
when our method is applied in the real-world              Brooke Cowan, Ivona Kučerová and Michael Collins.
applications. Our empirical study shows that the            2006. A discriminative model for tree-to-tree trans-
                                                            lation. In Proc. of EMNLP 2006, pages 232-241.
stable and satisfactory improvements can be
achieved after 6-8 iterations, while the largest          Yuan Ding and Martha Palmer. 2005. Machine trans-
improvements can be achieved after 20 iterations.           lation using probabilistic synchronous dependency
In our future work, we will study in-depth prin-            insertion grammars. In Proc. of ACL 2005, Ann
cipled ways to determine the appropriate number             Arbor, Michigan, pages 541-548.
of iterations for boosting-based system combina-          Nan Duan, Mu Li, Tong Xiao and Ming Zhou. 2009.
tion.                                                       The Feature Subspace Method for SMT System
                                                            Combination. In Proc. of EMNLP 2009, pages
8    Conclusions                                            1096-1104.

We have proposed a boosting-based system com-             Jason Eisner. 2003. Learning non-isomorphic tree
                                                             mappings for machine translation. In Proc. of ACL
bination method to address the issue of building
                                                             2003, pages 205-208.
a strong translation system from a group of weak
translation systems generated from a single SMT           Yoav Freund. 1995. Boosting a weak learning algo-
engine. We apply our method to three state-of-              rithm by majority. Information and Computation,
the-art SMT systems, and conduct experiments                121(2): 256-285.
on three NIST Chinese-to-English MT evalua-               Yoav Freund and Robert Schapire. 1997. A decision-
tions test sets. The experimental results show that         theoretic generalization of on-line learning and an
our method is very effective to improve the                 application to boosting. Journal of Computer and
translation accuracy of the SMT systems.                    System Sciences, 55(1):119-139.
                                                          Michel Galley, Jonathan Graehl, Kevin Knight,
Acknowledgements                                            Daniel Marcu, Steve DeNeefe, Wei Wang and
                                                            Ignacio Thayer. 2006. Scalable inferences and
This work was supported in part by the National             training of context-rich syntax translation models.
Science Foundation of China (60873091) and the              In Proc. of ACL 2006, Sydney, Australia, pages
Fundamental Research Funds for the Central                  961-968.
Universities (N090604008). The authors would
                                                          Michel Galley and Christopher D. Manning. 2008. A
like to thank the anonymous reviewers for their
                                                            Simple and Effective Hierarchical Phrase Reorder-
pertinent comments, Tongran Liu, Chunliang                  ing Model. In Proc. of EMNLP 2008, Hawaii,
Zhang and Shujie Yao for their valuable sugges-             pages 848-856.
tions for improving this paper, and Tianning Li
and Rushan Chen for developing parts of the               Almut Silja Hildebrand and Stephan Vogel. 2008.
                                                            Combination of machine translation systems via
baseline systems.
                                                            hypothesis selection from combined n-best lists. In
                                                            Proc. of the 8th AMTA conference, pages 254-261.
References
                                                          Liang Huang and David Chiang. 2007. Forest rescor-
David Chiang. 2005. A hierarchical phrase-based              ing: Faster decoding with integrated language
  model for statistical machine translation. In Proc.        models. In Proc. of ACL 2007, Prague, Czech Re-
  of ACL 2005, Ann Arbor, Michigan, pages 263-               public, pages 144-151.
  270.




                                                        747


Philipp Koehn, Franz Och and Daniel Marcu. 2003.            Robert Schapire. The boosting approach to machine
  Statistical Phrase-Based Translation. In Proc. of           learning: an overview. 2001. In Proc. of MSRI
  HLT-NAACL 2003, Edmonton, USA, pages 48-54.                 Workshop on Nonlinear Estimation and Classifica-
                                                              tion, Berkeley, CA, USA, pages 1-23.
Philipp Koehn. 2004. Statistical Significance Tests for
  Machine Translation Evaluation. In Proc. of               Matthew Snover, Bonnie Dorr, Richard Schwartz,
  EMNLP 2004, Barcelona, Spain, pages 388-395.                Linnea Micciulla and John Makhoul. 2006. A
                                                              Study of Translation Edit Rate with Targeted Hu-
Antonio Lagarda and Francisco Casacuberta. 2008.
                                                              man Annotation. In Proc. of the 7th AMTA confer-
  Applying Boosting to Statistical Machine Transla-
                                                              ence, pages 223-231.
  tion. In Proc. of the 12th EAMT conference, pages
  88-96.                                                    Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and
                                                              Ming Zhou. 2009. Better Synchronous Binarization
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li and
                                                              for Machine Translation. In Proc. of EMNLP 2009,
 Ming Zhou. 2009. Collaborative Decoding: Partial
                                                              Singapore, pages 362-370.
 Hypothesis Re-Ranking Using Translation Consen-
 sus between Decoders. In Proc. of ACL-IJCNLP               Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
 2009, Singapore, pages 585-592.                              mum Entropy Based Phrase Reordering Model for
                                                              Statistical Machine Translation. In Proc. of ACL
Percy Liang, Alexandre Bouchard-Côté, Dan Klein
                                                              2006, Sydney, pages 521-528.
  and Ben Taskar. 2006. An end-to-end discrimina-
  tive approach to machine translation. In Proc. of         Hao Zhang, Liang Huang, Daniel Gildea and Kevin
  COLING/ACL 2006, pages 104-111.                             Knight. 2006. Synchronous Binarization for Ma-
                                                              chine Translation. In Proc. of HLT-NAACL 2006,
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
                                                              New York, USA, pages 256- 263.
  String Alignment Template for Statistical Machine
  Translation. In Proc. of ACL 2006, pages 609-616.
Wolfgang Macherey and Franz Och. 2007. An Em-
 pirical Study on Computing Consensus Transla-
 tions from Multiple Machine Translation Systems.
 In Proc. of EMNLP 2007, pages 986-995.
Daniel Marcu, Wei Wang, Abdessamad Echihabi and
  Kevin Knight. 2006. SPMT: Statistical machine
  translation with syntactified target language
  phrases. In Proc. of EMNLP 2006, Sydney, Aus-
  tralia, pages 44-52.
Evgeny Matusov, Nicola Ueffing and Hermann Ney.
  2006. Computing consensus translation from mul-
  tiple machine translation systems using enhanced
  hypotheses alignment. In Proc. of EACL 2006,
  pages 33-40.
Franz Och and Hermann Ney. 2002. Discriminative
   Training and Maximum Entropy Models for Statis-
   tical Machine Translation. In Proc. of ACL 2002,
   Philadelphia, pages 295-302.
Franz Och. 2003. Minimum Error Rate Training in
   Statistical Machine Translation. In Proc. of ACL
   2003, Japan, pages 160-167.
Antti-Veikko Rosti, Spyros Matsoukas and Richard
  Schwartz. 2007. Improved Word-Level System
  Combination for Machine Translation. In Proc. of
  ACL 2007, pages 312-319.
Cynthia Rudin, Robert Schapire and Ingrid Daube-
  chies. 2007. Analysis of boosting algorithms using
  the smooth margin function. The Annals of Statis-
  tics, 35(6): 2723-2768.
Robert Schapire and Yoram Singer. 2000. BoosTexter:
  A boosting-based system for text categorization.
  Machine Learning, 39(2/3):135-168.

                                                          748

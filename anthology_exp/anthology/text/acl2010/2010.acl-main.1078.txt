               Metadata-Aware Measures for Answer Summarization
                       in Community Question Answering


                     Mattia Tomasoni ∗                                             Minlie Huang
                 Dept. of Information Technology                          Dept. Computer Science and Technology
               Uppsala University, Uppsala, Sweden                   Tsinghua University, Beijing 100084, China
         mattia.tomasoni.8371@student.uu.se                                  aihuang@tsinghua.edu.cn




                       Abstract                                    2006; Wang et al., 2009b; Suryanto et al., 2009).
                                                                   Interestingly, a great amount of information is em-
    This paper presents a framework for au-                        bedded in the metadata generated as a byprod-
    tomatically processing information com-                        uct of users’ action and interaction on Social Me-
    ing from community Question Answering                          dia. Much valuable information is contained in an-
    (cQA) portals with the purpose of gen-                         swers other than the chosen best one (Liu et al.,
    erating a trustful, complete, relevant and                     2008). Our work aims to show that such informa-
    succinct summary in response to a ques-                        tion can be successfully extracted and made avail-
    tion. We exploit the metadata intrinsically                    able by exploiting metadata to distill cQA content.
    present in User Generated Content (UGC)                        To this end, we casted the problem to an instance
    to bias automatic multi-document summa-                        of the query-biased multi-document summariza-
    rization techniques toward high quality in-                    tion task, where the question was seen as a query
    formation. We adopt a representation of                        and the available answers as documents to be sum-
    concepts alternative to n-grams and pro-                       marized. We mapped each characteristic that an
    pose two concept-scoring functions based                       ideal answer should present to a measurable prop-
    on semantic overlap. Experimental re-                          erty that we wished the final summary could ex-
    sults on data drawn from Yahoo! An-                            hibit:
    swers demonstrate the effectiveness of our
                                                                     • Quality to assess trustfulness in the source,
    method in terms of ROUGE scores. We
    show that the information contained in the                       • Coverage to ensure completeness of the in-
    best answers voted by users of cQA por-                            formation presented,
    tals can be successfully complemented by                         • Relevance to keep focused on the user’s in-
    our method.                                                        formation need and
                                                                     • Novelty to avoid redundancy.
1   Introduction                                                   Quality of the information was assessed via Ma-
Community Question Answering (cQA) portals                         chine Learning (ML) techniques under best an-
are an example of Social Media where the infor-                    swer supervision in a vector space consisting of
mation need of a user is expressed in the form of a                linguistic and statistical features about the answers
question for which a best answer is picked among                   and their authors. Coverage was estimated by se-
the ones generated by other users. cQA websites                    mantic comparison with the knowledge space of a
are becoming an increasingly popular complement                    corpus of answers to similar questions which had
to search engines: overnight, a user can expect a                  been retrieved through the Yahoo! Answers API 1 .
human-crafted, natural language answer tailored                    Relevance was computed as information overlap
to her specific needs. We have to be aware, though,                between an answer and its question, while Novelty
that User Generated Content (UGC) is often re-                     was calculated as inverse overlap with all other
dundant, noisy and untrustworthy (Jeon et al.,                     answers to the same question. A score was as-
     ∗
                                                                   signed to each concept in an answer according to
     The research was conducted while the first author was
                                                                      1
visiting Tsinghua University.                                             http://developer.yahoo.com/answers


                                                             760
         Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 760–769,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


the above properties. A score-maximizing sum-                           feature space to capture the following syntactic,
mary under a maximum coverage model was then                            behavioral and statistical properties:
computed by solving an associated Integer Linear
                                                                          • ϑ, length of answer a
Programming problem (Gillick and Favre, 2009;
McDonald, 2007). We chose to express concepts                             • ς, number of non-stopwords in a with a cor-
in the form of Basic Elements (BE), a semantic                              pus frequency larger than n (set to 5 in our
unit developed at ISI2 and modeled semantic over-                           experiments)
lap as intersection in the equivalence classes of                         • $, points awarded to user u according to the
two concepts (formal definitions will be given in                           Yahoo! Answers’ points system
section 2.3).                                                             • %, ratio of best answers posted by user u
   The objective of our work was to present what
                                                                        The features mentioned above determined a space
we believe is a valuable conceptual framework;
                                                                        Ψ; An answer a, in such feature space, assumed
more advance machine learning and summariza-
                                                                        the vectorial form:
tion techniques would most likely improve the per-
formances.                                                                            Ψa = ( ϑ, ς, $, % )
   The remaining of this paper is organized as fol-
lows. In the next section Quality, Coverage, Rel-                       Following the intuition that chosen best answers
evance and Novelty measures are presented; we                           (a? ) carry high quality information, we used su-
explain how they were calculated and combined                           pervised ML techniques to predict the probability
to generate a final summary of all answers to a                         of a to have been selected as a best answer a? . We
question. Experiments are illustrated in Section                        trained a Linear Regression classifier to learn the
3, where we give evidence of the effectiveness of                       weight vector W = (w1 , w2 , w3 , w4 ) that would
our method. We list related work in Section 5, dis-                     combine the above feature. Supervision was given
cuss possible alternative approaches in Section 4                       in the form of a training set T rQ of labeled pairs
and provide our conclusions in Section 6.                               defined as:
                                                                                    T rQ = {h Ψa , isbesta i}
2     The summarization framework
                                                                        isbesta was a boolean label indicating whether a
2.1    Quality as a ranking problem
                                                                        was an a? answer; the training set size was de-
Quality assessing of information available on So-                       termined experimentally and will be discussed in
cial Media had been studied before mainly as a                          Section 3.2. Although the value of isbesta was
binary classification problem with the objective of                     known for all answers, the output of the classifier
detecting low quality content. We, on the other                         offered us a real-valued prediction that could be
hand, treated it as a ranking problem and made                          interpreted as a quality score Q(Ψa ):
use of quality estimates with the novel intent of
successfully combining information from sources                            Q(Ψa ) ≈ P ( isbesta = 1 | a, u, T Au , )
with different levels of trustfulness and writing                                   ≈ P ( isbesta = 1 | Ψa )
ability. This is crucial when manipulating UGC,                                     = W T · Ψa                          (1)
which is known to be subject to particularly great
variance in credibility (Jeon et al., 2006; Wang                        The Quality measure for an answer a was approx-
et al., 2009b; Suryanto et al., 2009) and may be                        imated by the probability of such answer to be a
poorly written.                                                         best answer (isbesta = 1) with respect to its au-
   An answer a was given along with information                         thor u and the sets T Au and T Aq . It was calcu-
about the user u that authored it, the set T Aq (To-                    lated as dot product between the learned weight
tal Answers) of all answers to the same question q                      vector W and the feature vector for answer Ψa .
and the set T Au of all answers by the same user.                          Our decision to proceed in an unsupervised di-
Making use of results available in the literature                       rection came from the consideration that any use
(Agichtein et al., 2008) 3 , we designed a Quality                      of external human annotation would have made it
                                                                        impracticable to build an actual system on larger
   2
     Information Sciences Institute, University of Southern             scale. An alternative, completely unsupervised ap-
California, http://www.isi.edu
   3
     A long list of features is proposed; training a classifier         proach to quality detection that has not undergone
on all of them would no doubt increase the performances.                experimental analysis is discussed in Section 4.


                                                                  761


2.2   Bag-of-BEs and semantic overlap                             Given two concepts c and k:
The properties that remain to be discussed, namely                                 (
                                                                                     c≡k                       or
Coverage, Relevance and Novelty, are measures                               c ./ k
of semantic overlap between concepts; a concept                                      E c ∩ E k 6= ∅
is the smallest unit of meaning in a portion of
                                                                  We defined semantic overlap as occurring between
written text. To represent sentences and answers
                                                                  c and k if they were syntactically identical or if
we adopted an alternative approach to classical n-
                                                                  their equivalence classes E c and E k had at least
grams that could be defined bag-of-BEs. a BE
                                                                  one element in common. In fact, given the above
is “a head|modifier|relation triple representation
                                                                  definition of equivalence class and the transitivity
of a document developed at ISI” (Zhou et al.,
                                                                  of “≡” relation, we have that if the equivalence
2006). BEs are a strong theoretical instrument to
                                                                  classes of two concepts are not disjoint, then they
tackle the ambiguity inherent in natural language
                                                                  must bare the same meaning under the convention
that find successful practical applications in real-
                                                                  of some language L; in that case we said that c
world query-based summarization systems. Dif-
                                                                  semantically overlapped k. It is worth noting that
ferent from n-grams, they are variant in length and
                                                                  relation “./” is symmetric, transitive and reflexive;
depend on parsing techniques, named entity de-
                                                                  as a consequence all concepts with the same mean-
tection, part-of-speech tagging and resolution of
                                                                  ing are part of a same equivalence class. BE and
syntactic forms such as hyponyms, pronouns, per-
                                                                  equivalence class extraction were performed by
tainyms, abbreviation and synonyms. To each BE
                                                                  modifying the behavior of the BEwT-E-0.3 frame-
is associated a class of semantically equivalent
                                                                  work 4 . The framework itself is responsible for
BEs as result of what is called a transformation
                                                                  the operative definition of the “≈L ” relation and
of the original BE; the mentioned class uniquely
                                                                  the creation of the equivalence classes.
defines the concept. What seemed to us most re-
markable is that this makes the concept context-                  2.3    Coverage via concept importance
dependent. A sentence is defined as a set of con-
cepts and an answer is defined as the union be-                   In the scenario we proposed, the user’s informa-
tween the sets that represent its sentences.                      tion need is addressed in the form of a unique,
   The rest of this section gives formal definition               summarized answer; information that is left out of
of our model of concept representation and seman-                 the final summary will simply be unavailable. This
tic overlap. From a set-theoretical point of view,                raises the concern of completeness: besides ensur-
each concepts c was uniquely associated with a set                ing that the information provided could be trusted,
E c = {c1 , c2 . . . cm } such that:                              we wanted to guarantee that the posed question
                                                                  was being answered thoroughly. We adopted the
      ∀i, j   (ci ≈L c) ∧ (ci 6≡ c) ∧ (ci 6≡ cj )                 general definition of Coverage as the portion of
                                                                  relevant information about a certain subject that
In our model, the “≡” relation indicated syntac-                  is contained in a document (Swaminathan et al.,
tic equivalence (exact pattern matching), while the               2009). We proceeded by treating each answer
“≈L ” relation represented semantic equivalence                   to a question q as a separate document and we
under the convention of some language L (two                      retrieved through the Yahoo! Answers API a set
concepts having the same meaning). E c was de-                    T K q (Total Knowledge) of 50 answers 5 to ques-
fined as the set of semantically equivalent concepts              tions similar to q: the knowledge space of T K q
to c, called its equivalence class; each concept ci               was chosen to approximate the entire knowledge
in E c carried the same meaning (≈L ) of concept c                space related to the queried question q. We cal-
without being syntactically identical (≡); further-               culated Coverage as a function of the portion of
more, no two concepts i and j in the same equiva-                 answers in T K q that presented semantic overlap
lence class were identical.                                       with a.
                                                                     4
                                                                        The authors can be contacted regarding the possibil-
 “Climbing a tree to escape a black bear is pointless be-         ity of sharing the code of the modified version. Orig-
 cause they can climb very well.”                                 inal version available from http://www.isi.edu/
 BE = they|climb                                                  publications/licensed-sw/BE/index.html.
 E c = {climb|bears, bear|go up, climbing|animals,                    5
                                                                        such limit was imposed by the current version of the API.
         climber|instincts, trees|go up, claws|climb...}          Experiments with a greater corpus should be carried out in the
                                                                  future.


                                                            762


                     X
        C(a, q) =            γ(ci ) · tf (ci , a)   (2)         The Novelty measure N (c, q) of a concept c with
                     ci ∈a                                      respect to a question q was calculated as the ratio
                                                                of the cardinality of set T Aq,c over the cardinality
The Coverage measure for an answer a was cal-
                                                                of set T Aq ; T Aq,c was the subset of all those an-
culated as the sum of term frequency tf (ci , a) for
                                                                swers d in T Aq that contained at least one concept
concepts in the answer itself, weighted by a con-
                                                                k which presented semantical overlap with c.
cept importance function, γ(ci ), for concepts in
the total knowledge space T K q . γ(c) was defined              2.5     The concept scoring functions
as follows:
                                                                We have now determined how to calculate the
                 |T K q,c |          |T K q |                   scores for each property in formulas (1), (2), (4)
          γ(c) =            · log 2              (3)
                  |T K q |          |T K q,c |                  and (5); under the assumption that the Quality and
                                                                Coverage of a concept are the same of its answer,
where T K q,c = {d ∈ T K q : ∃k ∈ d, k ./ c}
                                                                every concept c part of an answer a to some ques-
The function γ(c) of concept c was calculated as                tion q, could be assigned a score vector as follows:
a function of the cardinality of set T K q and set
T K q,c , which was the subset of all those answers                   Φc = ( Q(Ψa ), C(a, q), R(c, q), N (c, q) )
d that contained at least one concept k which pre-
                                                                What we needed at this point was a function S
sented semantical overlap with c itself. A similar
                                                                of the above vector which would assign a higher
idea of knowledge space coverage is addressed by
                                                                score to concepts most worthy of being included
Swaminathan et al. (2009), from which formulas
                                                                in the final summary. Our intuition was that since
(2) and (3) were derived.
                                                                Quality, Coverage, Novelty and Relevance were
   A sensible alternative would be to estimate Cov-
                                                                all virtues properties, S needed to be monoton-
erage at the sentence level.
                                                                ically increasing with respect to all its dimen-
2.4   Relevance and Novelty via ./ relation                     sions. We designed two such functions. Func-
                                                                tion (6), which multiplied the scores, was based
To this point, we have addressed matters of trust-
                                                                on the probabilistic interpretation of each score as
fulness and completeness. Another widely shared
                                                                an independent event. Further empirical consid-
concern for Information Retrieval systems is Rel-
                                                                erations, brought us to later introduce a logarith-
evance to the query. We calculated relevance by
                                                                mic component that would discourage inclusion of
computing the semantic overlap between concepts
                                                                sentences shorter then a threshold t (a reasonable
in the answers and the question. Intuitively, we re-
                                                                choice for this parameter is a value around 20).
ward concepts that express meaning that could be
                                                                The score for concept c appearing in sentence sc
found in the question to be answered.
                                                                was calculated as:
                                    |q c |
                    R(c, q) =                       (4)                                 4
                                     |q|
                                                                                        Y
                                                                            S Π (c) =         (Φci ) · logt (length(sc ))   (6)
                                                                                        i=1
where   qc   = {k ∈ q : k ./ c}
                                                                A second approach that made use of human
The Relevance measure R(c, q) of a concept c
                                                                annotation to learn a vector of weights V =
with respect to a question q was calculated as the
                                                                (v1 , v2 , v3 , v4 ) that linearly combined the scores
ratio of the cardinality of set q c (containing all
                                                                was investigated. Analogously to what had been
concepts in q that semantically overlapped with c)
                                                                done with scoring function (6), the Φ space was
normalized by the total number of concepts in q.
                                                                augmented with a dimension representing the
   Another property we found desirable, was to
                                                                length of the answer.
minimize redundancy of information in the final
                                                                                  4
summary. Since all elements in T Aq (the set of                         Σ
                                                                                  X
all answers to q) would be used for the final sum-                    S (c) =           (Φci · vi ) + length(sc ) · v5      (7)
                                                                                  i=1
mary, we positively rewarded concepts that were
expressing novel meanings.                                      In order to learn the weight vector V that would
                                     |T Aq,c |                  combine the above scores, we asked three human
                N (c, q) = 1 −                      (5)         annotators to generate question-biased extractive
                                      |T Aq |
                                                                summaries based on all answers available for a
where T Aq,c = {d ∈ T Aq : ∃k ∈ d, k ./ c}                      certain question. We trained a Linear Regression


                                                          763


                                                                                   X
classifier with a set T rS of labeled pairs defined               maximize:              S(ci ) · xi                         (9)
as:                                                                                  i
                                                                                   X
                                                                  subject to:            length(j) · sj ≤ lengthM
      T rS = {h (Φc , length(sc )), includec i}                                    X
                                                                                     j

                                                                                         yj · occij ≥ xi      ∀i        (10)
                                                                                     j
includec was a boolean label that indicated
                                                                                   occij , xi , yj ∈ {0, 1}        ∀i, j
whether sc , the sentence containing c, had
been included in the human-generated summary;                                      occij = 1 if ci ∈ sj ,            ∀i, j
length(sc ) indicated the length of sentence sc .                                  xi = 1 if ci ∈ M,            ∀i
Questions and relative answers for the generation                                  yj = 1 if sj ∈ M,             ∀j
of human summaries were taken from the “filtered
dataset” described in Section 3.1.                            In the above program, M is the set of selected sen-
                                                              tences: M = {sj : yj = 1, ∀j}. The integer
   The concept score for the same BE in two sep-
                                                              variables xi and yj were equals to one if the corre-
arate answers is very likely to be different be-
                                                              sponding concept ci and sentence sj were included
cause it belongs to answers with their own Quality
                                                              in M . Similarly occij was equal to one if concept
and Coverage values: this only makes the scoring
                                                              ci was contained in sentence sj . We maximized
function context-dependent and does not interfere
                                                              the sum of scores S(ci ) (for S equals to S Π or S Σ )
with the calculation the Coverage, Relevance and
                                                              for each concept ci in the final summary M . We
Novelty measures, which are based on information
                                                              did so under the constraint that the total length of
overlap and will regard two BEs with overlapping
                                                              all sentences sj included in M must be less than
equivalence classes as being the same, regardless
                                                              the total expected length of the summary itself. In
of their score being different.
                                                              addition, we imposed a consistency constraint: if
                                                              a concept ci was included in M , then at least one
2.6    Quality constrained summarization                      sentence sj that contained the concept must also
                                                              be selected (constraint (10)). The described opti-
The previous sections showed how we quantita-                 mization problem was solved using lp solve 6 .
tively determined which concepts were more wor-                  We conclude with an empirical side note: since
thy of becoming part of the final machine sum-                solving the above can be computationally very de-
mary M . The final step was to generate the sum-              manding for large number of concepts, we found
mary itself by automatically selecting sentences              performance-wise very fruitful to skim about one
under a length constraint. Choosing this constraint           fourth of the concepts with lowest scores.
carefully demonstrated to be of crucial importance
during the experimental phase. We again opted                 3     Experiments
for a metadata-driven approach and designed the               3.1    Datasets and filters
length constraint as a function of the lengths of
                                                              The initial dataset was composed of 216,563 ques-
all answers to q (T Aq ) weighted by the respective
                                                              tions and 1,982,006 answers written by 171,676
Quality measures:
                                                              user in 100 categories from the Yahoo! Answers
                     X                                        portal7 . We will refer to this dataset as the “un-
       lengthM =             length(a) · Q(Ψa )   (8)         filtered version”. The metadata described in sec-
                    a∈T Aq                                    tion 2.1 was extracted and normalized; quality
                                                              experiments (Section 3.2) were then conducted.
The intuition was that the longer and the more                The unfiltered version was later reduced to 89,814
trustworthy answers to a question were, the more              question-answer pairs that showed statistical and
space was reasonable to allocate for information              linguistic properties which made them particularly
in the final, machine summarized answer M .                   adequate for our purpose. In particular, trivial, fac-
                                                              toid and encyclopedia-answerable questions were
   M was generated so as to maximize the scores
                                                                  6
of the concepts it included. This was done under a                  the version used was lp solve 5.5, available at http:
                                                              //lpsolve.sourceforge.net/5.5
maximum coverage model by solving the follow-                     7
                                                                    The reader is encouraged to contact the authors regarding
ing Integer Linear Programming problem:                       the availability of data and filters described in this Section.


                                                        764


removed by applying a series of patterns for the
identification of complex questions. The work by
Liu et al. (2008) indicates some categories of ques-
tions that are particularly suitable for summariza-
tion, but due to the lack of high-performing ques-
tion classifiers we resorted to human-crafted ques-
tion patterns. Some pattern examples are the fol-
lowing:

   •   {Why,What is the reason} [...]
   •   How {to,do,does,did} [...]
   •   How {is,are,were,was,will} [...]
   •   How {could,can,would,should} [...]                               Figure 1: Precision values (Y-axis) in detecting best an-
We also removed questions that showed statistical                       swers a? with increasing training set size (X-axis) for a Lin-
values outside of convenient ranges: the number of                      ear Regression classifier on the unfiltered dataset.
answers, length of the longest answer and length
of the sum of all answers (both absolute and nor-
                                                                        amount of training examples needed to success-
malized) were taken in consideration. In particular
                                                                        fully train a classifier for the quality assessing task.
we discarded questions with the following charac-
                                                                        The Linear Regression9 method was chosen to de-
teristics:
                                                                        termine the probability Q(Ψa ) of a to be a best an-
   • there were less than three answers 8                               swer to q; as explained in Section 2.1, those prob-
   • the longest answer was over 400 words                              abilities were interpreted as quality estimates. The
     (likely a copy-and-paste)                                          evaluation of the classifier’s output was based on
                                                                        the observation that given the set of all answers
   • the sum of the length of all answers outside
                                                                        T Aq relative to q and the best answer a? , a suc-
     of the (100, 1000) words interval
                                                                        cessfully trained classifier should be able to rank
   • the average length of answers was outside of                       a? ahead of all other answers to the same question.
     the (50, 300) words interval                                       More precisely, we defined Precision as follows:
                                                                                                                   ?
At this point a second version of the dataset                             |{q ∈ T rQ : ∀a ∈ T Aq , Q(Ψa ) > Q(Ψa )}|
was created to evaluate the summarization perfor-                                            |T rQ |
mance under scoring function (6) and (7); it was
generated by manually selecting questions that                          where the numerator was the number of questions
arouse subjective, human interest from the pre-                         for which the classifier was able to correctly rank
vious 89,814 question-answer pairs. The dataset                         a? by giving it the highest quality estimate in T Aq
size was thus reduced to 358 answers to 100 ques-                       and the denominator was the total number of ex-
tions that were manually summarized (refer to                           amples in the training set T rQ . Figure 1 shows the
Section 3.3). From now on we will refer to this                         precision values (Y-axis) in identifying best an-
second version of the dataset as the “filtered ver-                     swers as the size of T rQ increases (X-axis). The
sion”.                                                                  experiment started from a training set of size 100
                                                                        and was repeated adding 300 examples at a time
3.2    Quality assessing                                                until precision started decreasing. With each in-
In Section 2.1 we claimed to be able to identify                        crease in training set size, the experiment was re-
high quality content. To demonstrate it, we con-                        peated ten times and average precision values were
ducted a set of experiments on the original unfil-                      calculated. In all runs, training examples were
tered dataset to establish whether the feature space                    picked randomly from the unfiltered dataset de-
Ψ was powerful enough to capture the quality of                         scribed in Section 3.1; for details on T rQ see Sec-
answers; our specific objective was to estimate the                     tion 2.1. A training set of 12,000 examples was
                                                                        chosen for the summarization experiments.
   8
     Being too easy to summarize or not requiring any sum-
                                                                          9
marization at all, those questions wouldn’t constitute an valu-             Performed with Weka 3.7.0 available at http://www.
able test of the system’s ability to extract information.               cs.waikato.ac.nz/˜ml/weka


                                                                  765


      System         a? (baseline)   SΣ       SΠ
      ROUGE-1 R      51.7%           67.3%    67.4%
      ROUGE-1 P      62.2%           54.0%    71.2%
      ROUGE-1 F      52.9%           59.3%    66.1%
      ROUGE-2 R      40.5%           52.2%    58.8%
      ROUGE-2 P      49.0%           41.4%    63.1%
      ROUGE-2 F      41.6%           45.9%    57.9%
      ROUGE-L R      50.3%           65.1%    66.3%
      ROUGE-L P      60.5%           52.3%    70.7%
      ROUGE-L F      51.5%           57.3%    65.1%

Table 1: Summarization Evaluation on filtered dataset (re-
fer to Section 3.1 for details). ROUGE-L, ROUGE-1 and
                                                                   Figure 2: Increase in ROUGE-L, ROUGE-1 and ROUGE-
ROUGE-2 are presented; for each, Recall (R), Precision (P)
                                                                   2 performances of the S Π system as more measures are taken
and F-1 score (F) are given.
                                                                   in consideration in the scoring function, starting from Rele-
                                                                   vance alone (R) to the complete system (RQNC). F-1 scores
3.3     Evaluating answer summaries
                                                                   are given.
The objective of our work was to summarize an-
swers from cQA portals. Two systems were de-
                                                                   from the enforcement of a more stringent length
signed: Table 1 shows the performances using
                                                                   constraint than the one proposed in (8). Further
function S Σ (see equation (7)), and function S Π
                                                                   potential improvements on S Σ could be obtained
(see equation (6)). The chosen best answer a?
                                                                   by choosing a classifier able to learn a more ex-
was used as a baseline. We calculated ROUGE-1
                                                                   pressive underlying function.
and ROUGE-2 scores10 against human annotation
on the filtered version of the dataset presented in                   In order to determine what influence the single
Section 3.1. The filtered dataset consisted of 358                 measures had on the overall performance, we con-
answers to 100 questions. For each questions q,                    ducted a final experiment on the filtered dataset to
three annotators were asked to produce an extrac-                  evaluate (the S Π scoring function was used). The
tive summary of the information contained in T Aq                  evaluation was conducted in terms of F-1 scores of
by selecting sentences subject to a fixed length                   ROUGE-L, ROUGE-1 and ROUGE-2. First only
limit of 250 words. The annotation resulted in 300                 Relevance was tested (R) and subsequently Qual-
summaries (larger-scale annotation is still ongo-                  ity was added (RQ); then, in turn, Coverage (RQC)
ing). For the S Σ system, 200 of the 300 generated                 and Novelty (RQN); Finally the complete system
summaries were used for training and the remain-                   taking all measures in consideration (RQNC). Re-
ing were used for testing (see the definition of T rS              sults are shown in Figure 2. In general perfor-
Section 2.5). Cross-validation was conducted. For                  mances increase smoothly with the exception of
the S Π system, which required no training, all of                 ROUGE-2 score, which seems to be particularly
the 300 summaries were used as the test set.                       sensitive to Novelty: no matter what combination
                                                                   of measures is used (R alone, RQ, RQC), changes
   S Σ outperformed the baseline in Recall (R) but
                                                                   in ROUGE-2 score remain under one point per-
not in Precision (P); nevertheless, the combined F-
                                                                   centile. Once Novelty is added, performances rise
1 score (F) was sensibly higher (around 5 points
                                                                   abruptly to the system’s highest. A summary ex-
percentile). On the other hand, our S Π system
                                                                   ample, along with the question and the best an-
showed very consistent improvements of an order
                                                                   swer, is presented in Table 2.
of 10 to 15 points percentile over the baseline on
all measures; we would like to draw attention on
the fact that even if Precision scores are higher,                 4    Discussion and Future Directions
it is on Recall scores that greater improvements
                                                                   We conclude by discussing a few alternatives to
were achieved. This, together with the results ob-
                                                                   the approaches we presented. The lengthM con-
tained by S Σ , suggest performances could benefit
                                                                   straint for the final summary (Section 2.6), could
  10
     Available at http://berouge.com/default.                      have been determined by making use of external
aspx                                                               knowledge such as T K q : since T K q represents


                                                             766


  HOW TO PROTECT YOURSELF FROM A BEAR?                                                   space for questions is presented by Agichtein et
  http://answers.yahoo.com/question/index?qid=
  20060818062414AA7VldB                                                                  al. (2008) and could be used to rank the quality of
  ***BEST ANSWER***                                                                      questions in a way similar to how we ranked the
  Great question. I have done alot of trekking through California, Montana               quality of answers.
  and Wyoming and have met Black bears (which are quite dinky and placid
  but can go nuts if they have babies), and have been half an hour away from                The Quality assessing component itself could
  (allegedly) the mother of all grizzley s whilst on a trail through Glacier
  National park - so some other trekkerers told me... What the park wardens              be built as a module that can be adjusted to the
  say is SING, SHOUT, MAKE NOISE...do it loudly, let them know you
  are there..they will get out of the way, it is a surprised bear wot will go            kind of Social Media in use; the creation of cus-
  mental and rip your little legs off..No fun permission: anything that will             tomized Quality feature spaces would make it
  confuse them and stop them in their tracks...I have been told be an native
  american buddy that to keep a bottle of perfume in your pocket...throw it at           possible to handle different sources of UGC (fo-
  the ground near your feet and make the place stink: they have good noses,
  them bears, and a mega concentrated dose of Britney Spears Obsessive                   rums, collaborative authoring websites such as
  Compulsive is gonna give em something to think about...Have you got a                  Wikipedia, blogs etc.). A great obstacle is the lack
  rape alarm? Def take that...you only need to distract them for a second
  then they will lose interest..Stick to the trails is the most important thing,         of systematically available high quality training
  and talk to everyone you see when trekking: make sure others know where
  you are.                                                                               examples: a tentative solution could be to make
  ***SUMMARIZED ANSWER***                                                                use of clustering algorithms in the feature space;
  [...] In addition if the bear actually approaches you or charges you.. still
  stand your ground. Many times they will not actually come in contact                   high and low quality clusters could then be labeled
  with you, they will charge, almost touch you than run away. [...] The                  by comparison with examples of virtuous behav-
  actions you should take are different based on the type of bear. for ex-
  ample adult Grizzlies can t climb trees, but Black bears can even when                 ior (such as Wikipedia’s Featured Articles). The
  adults. They can not climb in general as thier claws are longer and not
  semi-retractable like a Black bears claws. [...] I truly disagree with the             quality of a document could then be estimated as a
  whole play dead approach because both Grizzlies and Black bears are
  oppurtunistic animals and will feed on carrion as well as kill and eat an-
                                                                                         function of distance from the centroid of the clus-
  imals. Although Black bears are much more scavenger like and tend not                  ter it belongs to. More careful estimates could take
  to kill to eat as much as they just look around for scraps. Grizzlies on the
  other hand are very accomplished hunters and will take down large prey                 the position of other clusters and the concentration
  animals when they want. [...] I have lived in the wilderness of Northern
  Canada for many years and I can honestly say that Black bears are not at
                                                                                         of nearby documents in consideration.
  all likely to attack you in most cases they run away as soon as they see or               Finally, in addition to the chosen best answer, a
  smell a human, the only places where Black bears are agressive is in parks
  with visitors that feed them, everywhere else the bears know that usually              DUC-styled query-focused multi-document sum-
  humans shoot them and so fear us. [...]
                                                                                         mary could be used as a baseline against which
                                                                                         the performances of the system can be checked.
Table 2: A summarized answer composed of five different
portions of text generated with the S Π scoring function; the                            5   Related Work
chosen best answer is presented for comparison. The rich-
ness of the content and the good level of readability make                               A work with a similar objective to our own is
it a successful instance of metadata-aware summarization of                              that of Liu et al. (2008), where standard multi-
information in cQA systems. Less satisfying examples in-                                 document summarization techniques are em-
clude summaries to questions that require a specific order of                            ployed along with taxonomic information about
sentences or a compromise between strongly discordant opin-                              questions. Our approach differs in two fundamen-
ions; in those cases, the summarized answer might lack logi-                             tal aspects: it took in consideration the peculiari-
cal consistency.                                                                         ties of the data in input by exploiting the nature of
                                                                                         UGC and available metadata; additionally, along
the total knowledge available about q, a coverage                                        with relevance, we addressed challenges that are
estimate of the final answers against it would have                                      specific to Question Answering, such as Cover-
been ideal. Unfortunately the lack of metadata                                           age and Novelty. For an investigation of Coverage
about those answers prevented us from proceeding                                         in the context of Search Engines, refer to Swami-
in that direction. This consideration suggests the                                       nathan et al. (2009).
idea of building T K q using similar answers in the                                         At the core of our work laid information trust-
dataset itself, for which metadata is indeed avail-                                      fulness, summarization techniques and alternative
able. Furthermore, similar questions in the dataset                                      concept representation. A general approach to
could have been used to augment the set of an-                                           the broad problem of evaluating information cred-
swers used to generate the final summary with an-                                        ibility on the Internet is presented by Akamine
swers coming from similar questions. Wang et al.                                         et al. (2009) with a system that makes use of
(2009a) presents a method to retrieve similar ques-                                      semantic-aware Natural Language Preprocessing
tions that could be worth taking in consideration                                        techniques. With analogous goals, but a focus
for the task. We suggest that the retrieval method                                       on UGC, are the papers of Stvilia et al. (2005),
could be made Quality-aware. A Quality feature                                           Mcguinness et al. (2006), Hu et al. (2007) and


                                                                                   767


Zeng et al. (2006), which present a thorough inves-          state-of-the-art summarization systems is ongoing.
tigation of Quality and trust in Wikipedia. In the
cQA domain, Jeon et al. (2006) presents a frame-             Acknowledgments
work to use Maximum Entropy for answer quality               This work was partly supported by the Chi-
estimation through non-textual features; with the            nese Natural Science Foundation under grant No.
same purpose, more recent methods based on the               60803075, and was carried out with the aid of
expertise of answerers are proposed by Suryanto              a grant from the International Development Re-
et al. (2009), while Wang et al. (2009b) introduce           search Center, Ottawa, Canada. We would like to
the idea of ranking answers taking their relation to         thank Prof. Xiaoyan Zhu, Mr. Yang Tang and Mr.
questions in consideration. The paper that we re-            Guillermo Rodriguez for the valuable discussions
gard as most authoritative on the matter is the work         and comments and for their support. We would
by Agichtein et al. (2008) which inspired us in the          also like to thank Dr. Chin-yew Lin and Dr. Eu-
design of the Quality feature space presented in             gene Agichtein from Emory University for sharing
Section 2.1.                                                 their data.
   Our approach merged trustfulness estimation
and summarization techniques: we adapted the au-
tomatic concept-level model presented by Gillick             References
and Favre (2009) to our needs; related work in               Eugene Agichtein, Carlos Castillo, Debora Donato,
multi-document summarization has been carried                  Aristides Gionis, and Gilad Mishne. 2008. Find-
out by Wang et al. (2008) and McDonald (2007).                 ing high-quality content in social media. In Marc
A relevant selection of approaches that instead                Najork, Andrei Z. Broder, and Soumen Chakrabarti,
                                                               editors, Proceedings of the International Conference
make use of ML techniques for query-biased sum-                on Web Search and Web Data Mining, WSDM 2008,
marization is the following: Wang et al. (2007),               Palo Alto, California, USA, February 11-12, 2008,
Metzler and Kanungo (2008) and Li et al. (2009).               pages 183–194. ACM.
An aspect worth investigating is the use of par-
                                                             Susumu Akamine, Daisuke Kawahara, Yoshikiyo
tially labeled or totally unlabeled data for sum-              Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-
marization in the work of Wong et al. (2008) and               hashi, and Yutaka Kidawara. 2009. Wisdom: a web
Amini and Gallinari (2002).                                    information credibility analysis system. In ACL-
   Our final contribution was to explore the use of            IJCNLP ’09: Proceedings of the ACL-IJCNLP 2009
                                                               Software Demonstrations, pages 1–4, Morristown,
Basic Elements document representation instead                 NJ, USA. Association for Computational Linguis-
of the widely used n-gram paradigm: in this re-                tics.
gard, we suggest the paper by Zhou et al. (2006).
                                                             Massih-Reza Amini and Patrick Gallinari. 2002. The
                                                              use of unlabeled data to improve supervised learning
6   Conclusions                                               for text summarization. In SIGIR ’02: Proceedings
                                                              of the 25th annual international ACM SIGIR con-
We presented a framework to generate trust-                   ference on Research and development in informa-
ful, complete, relevant and succinct answers to               tion retrieval, pages 105–112, New York, NY, USA.
questions posted by users in cQA portals. We                  ACM.
made use of intrinsically available metadata along           Dan Gillick and Benoit Favre. 2009. A scalable global
with concept-level multi-document summariza-                   model for summarization. In ILP ’09: Proceedings
tion techniques. Furthermore, we proposed an                   of the Workshop on Integer Linear Programming for
original use for the BE representation of concepts             Natural Langauge Processing, pages 10–18, Morris-
                                                               town, NJ, USA. Association for Computational Lin-
and tested two concept-scoring functions to com-               guistics.
bine Quality, Coverage, Relevance and Novelty
measures. Evaluation results on human annotated              Meiqun Hu, Ee-Peng Lim, Aixin Sun, Hady Wirawan
data showed that our summarized answers consti-               Lauw, and Ba-Quy Vuong. 2007. Measuring arti-
                                                              cle quality in wikipedia: models and evaluation. In
tute a solid complement to best answers voted by              CIKM ’07: Proceedings of the sixteenth ACM con-
the cQA users.                                                ference on Conference on information and knowl-
   We are in the process of building a system that            edge management, pages 243–252, New York, NY,
performs on-line summarization of large sets of               USA. ACM.
questions and answers from Yahoo! Answers.                   Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Larger-scale evaluation of results against other                Park. 2006. A framework to predict the quality of


                                                       768


  answers with non-textual features. In SIGIR ’06:                 the sixteenth ACM conference on Conference on in-
  Proceedings of the 29th annual international ACM                 formation and knowledge management, pages 555–
  SIGIR conference on Research and development in                  562, New York, NY, USA. ACM.
  information retrieval, pages 228–235, New York,
  NY, USA. ACM.                                                  Dingding Wang, Tao Li, Shenghuo Zhu, and Chris
                                                                   Ding. 2008. Multi-document summarization via
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,                   sentence-level semantic analysis and symmetric ma-
  and Yong Yu. 2009. Enhancing diversity, cover-                   trix factorization. In SIGIR ’08: Proceedings of the
  age and balance for summarization through struc-                 31st annual international ACM SIGIR conference on
  ture learning. In WWW ’09: Proceedings of the 18th               Research and development in information retrieval,
  international conference on World wide web, pages                pages 307–314, New York, NY, USA. ACM.
  71–80, New York, NY, USA. ACM.
                                                                 Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009a.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,                   A syntactic tree matching approach to finding sim-
  Dingyi Han, and Yong Yu. 2008. Understand-                       ilar questions in community-based qa services. In
  ing and summarizing answers in community-based                   SIGIR ’09: Proceedings of the 32nd international
  question answering services. In Proceedings of the               ACM SIGIR conference on Research and develop-
  22nd International Conference on Computational                   ment in information retrieval, pages 187–194, New
  Linguistics (Coling 2008), pages 497–504, Manch-                 York, NY, USA. ACM.
  ester, UK, August. Coling 2008 Organizing Com-
  mittee.                                                        Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang.
                                                                   2009b. Ranking community answers by modeling
Ryan T. McDonald. 2007. A study of global infer-                   question-answer relationships via analogical reason-
  ence algorithms in multi-document summarization.                 ing. In SIGIR ’09: Proceedings of the 32nd interna-
  In Giambattista Amati, Claudio Carpineto, and Gio-               tional ACM SIGIR conference on Research and de-
  vanni Romano, editors, ECIR, volume 4425 of Lec-                 velopment in information retrieval, pages 179–186,
  ture Notes in Computer Science, pages 557–564.                   New York, NY, USA. ACM.
  Springer.
                                                                 Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Ex-
Deborah L. Mcguinness, Honglei Zeng, Paulo Pin-                    tractive summarization using supervised and semi-
  heiro Da Silva, Li Ding, Dhyanesh Narayanan, and                 supervised learning. In COLING ’08: Proceedings
  Mayukh Bhaowal. 2006. Investigation into trust for               of the 22nd International Conference on Computa-
  collaborative information repositories: A wikipedia              tional Linguistics, pages 985–992, Morristown, NJ,
  case study. In In Proceedings of the Workshop on                 USA. Association for Computational Linguistics.
  Models of Trust for the Web, pages 3–131.
                                                                 Honglei Zeng, Maher A. Alhossaini, Li Ding, Richard
Donald Metzler and Tapas Kanungo. 2008. Ma-                        Fikes, and Deborah L. McGuinness. 2006. Com-
  chine learned sentence selection strategies for query-           puting trust from revision history. In PST ’06: Pro-
  biased summarization. In Proceedings of SIGIR                    ceedings of the 2006 International Conference on
  Learning to Rank Workshop.                                       Privacy, Security and Trust, pages 1–1, New York,
                                                                   NY, USA. ACM.
Besiki Stvilia, Michael B. Twidale, Linda C. Smith,
  and Les Gasser. 2005. Assessing information qual-              Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006.
  ity of a community-based encyclopedia. In Proceed-               Summarizing answers for complicated questions. In
  ings of the International Conference on Information              Proceedings of the Fifth International Conference
  Quality.                                                         on Language Resources and Evaluation (LREC),
                                                                   Genoa, Italy.
Maggy Anastasia Suryanto, Ee Peng Lim, Aixin Sun,
 and Roger H. L. Chiang. 2009. Quality-aware col-
 laborative question answering: methods and evalu-
 ation. In WSDM ’09: Proceedings of the Second
 ACM International Conference on Web Search and
 Data Mining, pages 142–151, New York, NY, USA.
 ACM.

Ashwin Swaminathan, Cherian V. Mathew, and Darko
  Kirovski. 2009. Essential pages. In WI-IAT ’09:
  Proceedings of the 2009 IEEE/WIC/ACM Interna-
  tional Joint Conference on Web Intelligence and In-
  telligent Agent Technology, pages 173–182, Wash-
  ington, DC, USA. IEEE Computer Society.

Changhu Wang, Feng Jing, Lei Zhang, and Hong-
  Jiang Zhang. 2007. Learning query-biased web
  page summarization. In CIKM ’07: Proceedings of


                                                           769

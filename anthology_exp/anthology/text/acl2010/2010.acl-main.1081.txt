                    Using Document Level Cross-Event Inference
                           to Improve Event Extraction


                    Shasha Liao                                        Ralph Grishman
                New York University                                  New York University
               715 Broadway, 7th floor                              715 Broadway, 7th floor
              New York, NY 10003 USA                               New York, NY 10003 USA
               liaoss@cs.nyu.edu                                   grishman@cs.nyu.edu




                                                               tie together. Sometimes it is difficult even for
                     Abstract                                  people to classify events from isolated sentences.
                                                               From the sentence:
    Event extraction is a particularly challenging
    type of information extraction (IE). Most                      (1) He left the company.
    current event extraction systems rely on local             it is hard to tell whether it is a Transport event in
    information at the phrase or sentence level.               ACE, which means that he left the place; or an
    However, this local context may be
                                                               End-Position event, which means that he retired
    insufficient to resolve ambiguities in
    identifying particular types of events;
                                                               from the company.
    information from a wider scope can serve to                    However, if we read the whole document, a
    resolve some of these ambiguities. In this                 clue like “he planned to go shopping before he
    paper, we use document level information to                went home” would give us confidence to tag it as
    improve the performance of ACE event                       a Transport event, while a clue like “They held a
    extraction. In contrast to previous work, we               party for his retirement” would lead us to tag it
    do not limit ourselves to information about                as an End-Position event.
    events of the same type, but rather use                        Such clues are evidence from the same event
    information about other types of events to                 type. However, sometimes another event type is
    make predictions or resolve ambiguities
                                                               also a good predictor. For example, if we find a
    regarding a given event. We learn such
                                                               Start-Position event like “he was named
    relationships from the training corpus and use
    them to help predict the occurrence of events              president three years ago”, we are also
    and event arguments in a text. Experiments                 confident to tag (1) as End-Position event.
    show that we can get 9.0% (absolute) gain in                   Event argument identification also shares this
    trigger (event) classification, and more than              benefit. Consider the following two sentences:
    8% gain for argument (role) classification in
    ACE event extraction.
                                                                   (2) A bomb exploded in Bagdad; seven
                                                                 people died while 11 were injured.
1    Introduction                                                  (3) A bomb exploded in Bagdad; the
                                                                 suspect got caught when he tried to escape.
The goal of event extraction is to identify
instances of a class of events in text. The ACE                If we only consider the local context of the
2005 event extraction task involved a set of 33                trigger “exploded”, it is hard to determine that
generic event types and subtypes appearing                     “seven people” is a likely Target of the Attack
frequently in the news. In addition to identifying             event in (2), or that the “suspect” is the Attacker
the event itself, it also identifies all of the                of the Attack event, because the structures of (2)
participants and attributes of each event; these               and (3) are quite similar. The only clue is from
are the entities that are involved in that event.              the semantic inference that a person who died
   Identifying an event and its participants and               may well have been a Target of the Attack event,
attributes is quite difficult because a larger field           and the person arrested is probably the Attacker
of view is often needed to understand how facts                of the Attack event. These may be seen as


                                                         789
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 789–797,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


examples of a broader textual inference problem,                       (coreferential) entity mentions.
and in general such knowledge is quite difficult
                                                                      Entity mention: a reference to an entity
to acquire and apply. However, in the present
                                                                       (typically, a noun phrase)
case we can take advantage of event extraction
to learn these rules in a simpler fashion, which                      Timex: a time expression including date,
we present below.                                                      time of the day, season, year, etc.
   Most current event extraction systems are                          Event mention: a phrase or sentence within
based on phrase or sentence level extraction.
                                                                       which an event is described, including
Several recent studies use high-level information                      trigger and arguments. An event mention
to aid local event extraction systems. For                             must have one and only one trigger, and can
example, Finkel et al. (2005), Maslennikov and
                                                                       have an arbitrary number of arguments.
Chua (2007), Ji and Grishman (2008), and
Patwardhan and Riloff (2007, 2009) tried to use                       Event trigger: the main word that most
discourse, document, or cross-document                                 clearly expresses an event occurrence. An
information to improve information extraction.                         ACE event trigger is generally a verb or a
   However, most of this research focuses on                           noun.
single event extraction, or focuses on high-level                     Event mention arguments (roles) 2 : the
information within a single event type, and does                       entity mentions that are involved in an
not consider information acquired from other                           event mention, and their relation to the
event types. We extend these approaches by                             event. For example, event Attack might
introducing cross-event information to enhance                         include participants like Attacker, Target, or
the performance of multi-event-type extraction                         attributes like Time_within and Place.
systems. Cross-event information is quite useful:                      Arguments will be taggable only when they
first, some events co-occur frequently, while                          occur within the scope of the corresponding
other events do not. For example, Attack, Die,                         event, typically the same sentence.
and Injure events very frequently occur together,
while Attack and Marry are less likely to                           Consider the sentence:
co-occur. Also, typical relations among the                           (4) Three murders occurred in France
arguments of different types of events can be                       today, including the senseless slaying of
helpful in predicting information to be extracted.                  Bob Cole and the assassination of Joe
For example, the Victim of a Die event is                           Westbrook. Bob was on his way home when
probably the Target of the Attack event. As a                       he was attacked…
result, we extend the observation that “a
document containing a certain event is likely to                  Event extraction depends on previous phases
contain more events of the same type”, and base                like name identification, entity mention
our approach on the idea that “a document                      classification and coreference. Table 1 shows the
containing a certain type of event is likely to                results of this preprocessing. Note that entity
contain instances of related events”. In this                  mentions that share the same EntityID are
paper, automatically extracted within-event and                coreferential and treated as the same object.
cross-event information is used to aid traditional
sentence level event extraction.                                   Entity(Time     head             Entity         Entity
                                                                   x) mention      word             ID             type
2    Task Description                                              0001-1-1        France           0001-1         GPE
Automatic Content Extraction (ACE) defines an                      0001-T1-1       Today            0001-T1        Timex
event as a specific occurrence involving                           0001-2-1        Bob Cole         0001-2         PER
participants1, and it annotates 8 types and 33                     0001-3-1        Joe              0001-3         PER
subtypes of events. We first present some ACE                                      Westbrook
terminology to understand this task more easily:                   0001-2-2        Bob              0001-2         PER
 Entity: an object or a set of objects in one                     0001-2-3        He              0001-2      PER
     of the semantic categories of interest,                       Table 1. An example of entities and entity mentions
     referred to in the document by one or more                                     and their types

1                                                              2
  See                                                             Note that we do not deal with event mention coreference
http://projects.ldc.upenn.edu/ace/docs/English-Events-         in this paper, so each event mention is treated as a separate
Guidelines_v5.4.3.pdf for a description of this task.          event.


                                                         790


   There are three Die events, which share the                to propagate consistent trigger classification and
same Place and Time roles, with different Victim              event arguments across sentences and
roles. And there is one Attack event sharing the              documents. Combining global evidence from
same Place and Time roles with the Die events.                related documents with local decisions, they
                                                              obtained an appreciable improvement in both
Event     Trigger                  Role                       event and event argument identification.
type                 Place       Victim     Time                 Patwardhan and Riloff (2009) proposed an
Die       murder     0001-1-1               0001-T1-1         event extraction model which consists of two
                                                              components: a model for sentential event
Die       death      0001-1-1    0001-2-1   0001-T1-1         recognition, which offers a probabilistic
                                                              assessment of whether a sentence is discussing a
Die       killing    0001-1-1    0001-3-1   0001-T1-1         domain-relevant event; and a model for
Event     Trigger                  Role                       recognizing plausible role fillers, which
type                 Place       Target     Time              identifies phrases as role fillers based upon the
Attack    attack     0001-1-1    0001-2-3   0001-T1-1         assumption that the surrounding context is
                                                              discussing a relevant event. This unified
      Table2. An example of event trigger and roles           probabilistic model allows the two components
                                                              to jointly make decisions based upon both the
   In this paper, we treat the 33 event subtypes              local evidence surrounding each phrase and the
as separate event types and do not consider the               “peripheral vision”.
hierarchical structure among them.                               Gupta and Ji (2009) used cross-event
                                                              information within ACE extraction, but only for
3      Related Work                                           recovering implicit time information for events.
Almost all the current ACE event extraction
                                                              4     Motivation
systems focus on processing one sentence at a
time (Grishman et al., 2005; Ahn, 2006; Hardy                 We analyzed the sentence-level baseline event
et al. 2006). However, there have been several                extraction, and found that many events are
studies using high-level information from a                   missing or spuriously tagged because the local
wider scope:                                                  information is not sufficient to make a confident
   Maslennikov and Chua (2007) use discourse                  decision. In some local contexts, it is easy to
trees and local syntactic dependencies in a                   identify an event; in others, it is hard to do so.
pattern-based framework to incorporate wider                  Thus, if we first tag the easier cases, and use
context to refine the performance of relation                 such knowledge to help tag the harder cases, we
extraction. They claimed that discourse                       might get better overall performance. In
information could filter noisy dependency paths               addition, global information can make the event
as well as increasing the reliability of                      tagging more consistent at the document level.
dependency path extraction.                                      Here are some examples. For trigger
   Finkel et al. (2005) used Gibbs sampling, a                classification:
simple Monte Carlo method used to perform
                                                                    The pro-reform director of Iran's
approximate inference in factored probabilistic
                                                                  biggest-selling daily newspaper and official
models. By using simulated annealing in place
                                                                  organ of Tehran's municipality has stepped
of Viterbi decoding in sequence models such as
                                                                  down following the appointment of a
HMMs, CMMs, and CRFs, it is possible to
                                                                  conservative …it was founded a decade ago
incorporate non-local structure while preserving
                                                                  … but a conservative city council was
tractable inference. They used this technique to
                                                                  elected in the February 28 municipal polls
augment an information extraction system with
                                                                  … Mahmud Ahmadi-Nejad, reported to be a
long-distance dependency models, enforcing
                                                                  hardliner among conservatives, was
label consistency and extraction template
                                                                  appointed mayor on Saturday …Founded
consistency constraints.
                                                                  by     former     mayor     Gholamhossein
   Ji and Grishman (2008) were inspired from
                                                                  Karbaschi, Hamshahri…
the hypothesis of “One Sense Per Discourse”
(Yarowsky, 1995); they extended the scope from
a single document to a cluster of topic-related
documents and employed a rule-based approach


                                                        791


     Figure 1. Conditional probability of the other 32 event types in documents where a Die event appears




  Figure 2. Conditional probability of the other 32 event types in documents where a Start-Org event appears


   The sentence level baseline system finds                    (men in pajamas), turn Iraq upside down
event triggers like “founded” (trigger of                      and find them.
Start-Org), “elected” (trigger of Elect), and
                                                              From this document, the local information is
“appointment” (trigger of Start-Position), which
                                                           not enough for our system to tag “Hassan” as
are easier to identify because these triggers have
                                                           the target of an Attack event, because it is quite
more specific meanings. However, it does not
                                                           far from the trigger “shot” and the syntax is
recognize the trigger “stepped” (trigger of
                                                           somewhat complex. However, it is easy to tag
End-Position) because in the training corpus
                                                           “she” as the Victim of a Die event, because it is
“stepped” does not always appear as an
                                                           the object of the trigger “killed”. As “she” and
End-Position event, and local context does not
                                                           “Hassan” are co-referred, we can use this easily
provide enough information for the MaxEnt
                                                           tagged argument to help identify the harder one.
model to tag it as a trigger. However, in the
document that contains related events like                 4.1      Trigger Consistency and Distribution
Start-Position, “stepped” is more likely to be
tagged as an End-Position event.                           Within a document, there is a strong trigger
   For argument classification, the cross-event            consistency: if one instance of a word triggers an
evidence from the document level is also useful:           event, other instances of the same word will
                                                           trigger events of the same type3.
    British officials say they believe Hassan                 There are also strong correlations among
  was a blindfolded woman seen being shot in               event types in a document. To see this we
  the head by a hooded militant on a video                 calculated the conditional probability (in the
  obtained but not aired by the Arab                       ACE corpus) of a certain event type appearing in
  television station Al-Jazeera. She would be              a document when another event type appears in
  the first foreign woman to die in the wave of            the same document.
  kidnappings in Iraq…she's been killed by
                                                           3
                                                               This is true over 99.4% of the time in the ACE corpus.


                                                     792


  Figure 3. Conditional probability of all possible roles in other event types for entities that are the Targets of
                  Attack events (roles with conditional probability below 0.002 are omitted)


      Event                       Cond. Prob.
                                                              4.2      Role Consistency and Distribution
      Attack                         0.714
      Transport                      0.507                    Normally one entity, if it appears as an argument
      Injure                         0.306                    of multiple events of the same type in a single
      Meet                           0.164                    document, is assigned the same role each time.4
      Arrest-Jail                    0.153                       There is also a strong relationship between the
      Sentence                       0.126                    roles when an entity participates in different
      Phone-Write                    0.111                    types of events in a single document. For
      End-Position                   0.116                    example, we checked all the entities in the ACE
      Trial-Hearing                  0.105                    corpus that appear as the Target role for an
      Convict                        0.100                    Attack event, and recorded the roles they were
 Table 3. Events co-occurring with die events with            assigned for other event types. Only 31 other
           conditional probability > 10%                      event-role combinations appeared in total (out of
                                                              237 possible with ACE annotation), and 3
   As there are 33 subtypes, there are potentially            clearly dominated. In Figure 3, we can see that
33⋅32/2=528 event pairs. However, only a few                  the most likely roles for the Target role of the
of these appear with substantial frequency. For               Attack event are the Victim role of the Die or
example, there are only 10 other event types that             Injure event and the Artifact role of the
occur in more than 10% of the documents in                    Transport event. The last of these corresponds to
which a die event appears. From Table 3, we can               troop movements prior to or in response to
see that Attack, Transport and Injure events                  attacks.
appear frequently with Die. We call these the
related event types for Die (see Figure 1 and                 5      Cross-event Approach
Table 3).                                                     In this section we present our approach to using
   The same thing happens for Start-Org events,               document-level event and role information to
although its distribution is quite different from             improve sentence-level ACE event extraction.
Die events. For Start-Org, there are more related                Our event extraction system is a two-pass
events like End-Org, Start-Position, and                      system where the sentence-level system is first
End-Position (Figure 2). But there are 12 other               applied to make decisions based on local
event types which never appear in documents                   information. Then the confident local
containing Start-Org events.                                  information is collected and gives an
   From the above, we can see that the                        approximate view of the content of the
distributions of different event types are quite              document. The document level system is finally
different, and these distributions might be good              applied to deal with the cases which the local
predictors for event extraction.


                                                              4
                                                                  This is true over 97% of the time in the ACE corpus.


                                                        793


system can’t handle, and achieve document                            We want to see if this value can be trusted as a
consistency.                                                         confidence score. To this end, we set different
                                                                     thresholds from 0.1 to 1.0 in the baseline system
5.1     Sentence-level Baseline System                               output, and only evaluate triggers, arguments or
We use a state-of-the-art English IE system as                       roles whose confidence score is above the
our baseline (Grishman et al. 2005). This system                     threshold. Results show that as the threshold is
extracts events independently for each sentence,                     raised, the precision generally increases and the
because the definition of event mention                              recall falls. This indicates that the value is
argument constrains them to appear in the same                       consistent and a useful indicator of
sentence. The system combines pattern matching                       event/argument confidence (see Figure 4).6
with statistical models. In the training process,
for every event mention in the ACE training
corpus, patterns are constructed based on the
sequences of constituent heads separating the
trigger and arguments. A set of Maximum
Entropy based classifiers are also trained:
 Argument Classifier: to distinguish
     arguments of a potential trigger from
     non-arguments;
 Role Classifier: to classify arguments by
     argument role.                                                      Figure 4. The performance of different confidence
 Reportable-Event Classifier (Trigger                                           thresholds in the baseline system
     Classifier): Given a potential trigger, an                                       on the development set
     event type, and a set of arguments, to
     determine whether there is a reportable                            To    acquire      confident    document-level
     event mention.                                                  information, we only collect triggers and roles
   In the test procedure, each document is                           tagged with high confidence. Thus, a trigger
scanned for instances of triggers from the                           threshold t_threshold and role threshold
training corpus. When an instance is found, the                      r_threshold are set to remove low confidence
system tries to match the environment of the                         triggers and arguments. Finally, a table with
trigger against the set of patterns associated with                  confident event information is built. For every
that trigger. This pattern-matching process, if                      event, we collect its trigger and event type; for
successful, will assign some of the mentions in                      every    argument,       we    use    co-reference
the sentence as arguments of a potential event                       information and record every entity and its role(s)
mention. The argument classifier is applied to                       in events of a certain type.
the remaining mentions in the sentence; for any                         To achieve document consistency, in cases
argument passing that classifier, the role                           where the baseline system assigns a word to
classifier is used to assign a role to it. Finally,                  triggers for more than one event type, if the
once all arguments have been assigned, the                           margin between the probability of the highest
reportable-event classifier is applied to the                        and the second highest scores is above a
potential event mention; if the result is                            threshold m_threshold, we only keep the event
successful, this event mention is reported.5                         type with highest score and record this in the
                                                                     confident-event table. Otherwise (if the margin is
5.2     Document-level Confident Information                         smaller) the event type assignments will be
        Collector                                                    recorded in a separate conflict table. The same
To use document-level information, we need to                        strategy is applied to argument/role conflicts.
collect information based on the sentence-level                      We will not use information in the conflict table
baseline system. As it is a statistically-based                      to infer the event type or argument/roles for
model, it can provide a value that indicates how                     other event mentions, because we cannot
likely it is that this word is a trigger, or that the
mention is an argument and has a particular role.                    6
                                                                       The trigger classification curve doesn’t follow the
                                                                     expected recall/precision trade-off, particularly at high
                                                                     thresholds. This is due, at least in part, to the fact that
5
  If the event arguments include some assigned by the                some events bypass the reportable-event classifier (trigger
pattern-matching process, the event mention is accepted              classifier) (see footnote 5). At high thresholds this is true of
unconditionally, bypassing the reportable- event classifier.         the bulk of the events.


                                                               794


confidently resolve the conflict. However, the                are reported in this document. The trigger
event type and argument/role assignments in the               classifier predicts whether a word is the trigger
conflict table will be included in the final output           of an event, and if so of what type, given the
because the local confidence for the individual               information (from the confident-event table)
assignments is high.                                          about other types of events in the document.
   As a result, we finally build two                          Each feature of this classifier is the conjunction
document-level confident-event tables: the event              of:
type table and the argument (role) table. A                      • The base form of the word
conflict table is also built but not used for further            • An event type
predictions (see Table 4).                                       • A binary indicator of whether this event
                                                                     type is present elsewhere in the document
                  Confident table                             (There are 33 event types and so 33 features for
                  Event type table                            each word).
Trigger              Event Type
  Met                     Meet
                                                              5.3.2 Document       Level    Argument      (Role)
  Exploded                Attack                              Classifier
  Went                    Transport
  Injured                 Injure                              The role classifier predicts whether a given
                                                              mention is an argument of a given event and, if
  Attacked                Attack
                                                              so, what role it takes on, again using information
  Died                    Die
                                                              from the confident-event table about other
                Argument role table                           events.
Entity ID        Event type         Role                         As noted above, we assume that the role of an
  0004-T2        Die                  Time Within             entity is unique for a specific event type,
  0004-6         Die                  Place                   although an entity can take on different roles for
  0004-4         Die                  Victim                  different event types. Thus, if there is a conflict
  0004-7         Die                  Agent                   in the document level table, the collector will
  0004-11        Attack               Target                  only keep the one with highest confidence, or
  0004-T3        Attack               Time Within             discard them all. As a result, every entity is
  0004-12        Attack               Place                   assigned a unique role with respect to a
  0004-10        Attack               Attacker
                                                              particular event type, or null if it is not an
                                                              argument of a certain event type.
                   Conflict table
                                                                 Each feature is the conjunction of:
Entity ID        Event type              Roles                   • The event type we are trying to assign an
 0004-8           Attack            Victim, Agent                    argument/role to.
Table 4. Example of document-level confident-event               • One of the 32 other event types
  table (event type and argument role entries) and               • The role of this entity with respect to the
                    conflict table
                                                                     other event type elsewhere in the
                                                                     document, or null if this entity is not an
5.3    Statistical Cross-event Classifiers                           argument of that type of event
To take advantage of cross-event relationships,
we train two additional MaxEnt classifiers – a                5.4   Document Level Event Tagging
document-level trigger and argument classifier –
                                                              At this point, the low-confidence triggers and
and then use these classifiers to infer additional
                                                              arguments (roles) have been removed and the
events and event arguments. In analyzing new
                                                              document-level confident-event table has been
text, the trigger classifier is first applied to tag
                                                              built; the new classifiers are now used to
an event, and then the argument (role) classifier
                                                              augment the confident tags that were previously
is applied to tag possible arguments and roles of
                                                              assigned based on local information.
this event.
                                                                 For trigger tagging, we only apply the
                                                              classifier to the words that do not have a
5.3.1 Document Level Trigger Classifier                       confident local labeling; if the trigger is already
                                                              in the document level confident-event table, we
From the document-level confident-event table,                will not re-tag it.
we have a rough view of what kinds of events


                                                        795


             performance               Trigger                      Argument                       Role
    system/human                    classification                 classification              classification
                                P         R          F            P        R        F        P         R          F
    Sentence-level            67.56    53.54    59.74           46.45     37.15   41.29   41.02      32.81     36.46
    baseline system
    Within-event-type         63.03    59.90    61.43           48.59     46.16   47.35   43.33      41.16     42.21
    rules
    Cross-event               68.71    68.87    68.79           50.85     49.72   50.28   45.06      44.05     44.55
    statistical model
    Human annotation1         59.2     59.4     59.3            60.0      69.4    64.4    51.6       59.5      55.3
    Human annotation2         69.2     75.0     72.0            62.7      85.4    72.3    54.1       73.7      62.4
                              Table 5. Overall performance on blind test data

   The argument/role tagger is then applied to all          annotators on 28 ACE newswire texts (a subset
events—those in the confident-event table and               of the blind test set).7
those newly tagged. For argument tagging, we                   From the results presented in Table 5, we can
only consider the entity mentions in the same               see that using the document level cross-event
sentence as the trigger word, because by the                information, we can improve the F score for
ACE event guidelines, the arguments of an event             trigger classification by 9.0%, argument
should appear within the same sentence as the               classification by 9.0%, and role classification by
trigger. For a given event, we re-tag the entity            8.1%. Recall improved sharply, demonstrating
mentions that have not already been assigned as             that cross-event information could recover
arguments of that event by the confident-event              information that is difficult for the
or conflict table.                                          sentence-level baseline to extract; precision also
                                                            improved over the baseline, although not as
6     Experiments                                           markedly.
                                                               Compared to the within-event-type rules, the
We followed Ji and Grishman (2008)’s
                                                            cross-event     model     yields    much      more
evaluation and randomly select 10 newswire
                                                            improvement        for   trigger    classification:
texts from the ACE 2005 training corpora as our
                                                            rule-based propagation gains 1.7% improvement
development set, which is used for parameter
                                                            while the cross-event model achieves a further
tuning, and then conduct a blind test on a
                                                            7.3% improvement. For argument and role
separate set of 40 ACE 2005 newswire texts. We
                                                            classification, the cross-event model also gains
use the rest of the ACE training corpus (549
                                                            3% and 2.3% above that obtained by the
documents) as training data for both the
                                                            rule-based propagation process.
sentence-level baseline event tagger and
document-level event tagger.                                7          Conclusion and Future Work
   To compare with previous work on
within-event propagation, we reproduced Ji and              We propose a document-level statistical model
Grishman (2008)’s approach for cross-sentence,              for event trigger and argument (role)
within-event-type          inference          (see          classification to achieve document level
“within-event-type rules” in Table 5). We                   within-event and cross-event consistency.
applied their within-document inference rules               Experiments     show      that    document-level
using the cross-sentence confident-event                    information can improve the performance of a
information. These rules basically serve to adjust          sentence-level baseline event extraction system.
trigger and argument classification to achieve                 The model presented here is a simple
document-wide consistency. This process treats              two-stage recognition process; nonetheless, it
each event type separately: information about               has proven sufficient to yield substantial
events of a given type is used to infer                     improvements in event recognition and event
information about other events of the same type.
                                                            7
   We report the overall Precision (P), Recall (R),           The final key was produced by review and adjudication
and F-Measure (F) on blind test data. In addition,          of the two annotations by a third annotator, which indicates
we also report the performance of two human                 that the event extraction task is quite difficult and human
                                                            agreement is not very high.


                                                      796


argument recognition. Richer models, such as
those based on joint inference, may produce
even greater gains. In addition, extending the
approach to cross-document information,
following (Ji and Grishman 2008), may be able
to further improve performance.

References
David Ahn. 2006. The stages of event extraction. In
  Proc. COLING/ACL 2006 Workshop on
  Annotating and Reasoning about Time and Events.
  Sydney, Australia.
J. Finkel, T. Grenager, and C. Manning. 2005.
   Incorporating   Non-local    Information     into
   Information Extraction Systems by Gibbs
   Sampling. In Proc. 43rd Annual Meeting of the
   Association for Computational Linguistics, pages
   363–370, Ann Arbor, MI, June.
Ralph Grishman, David Westbrook and Adam
  Meyers. 2005. NYU’s English ACE 2005 System
  Description. In Proc. ACE 2005 Evaluation
  Workshop, Gaithersburg, MD.
Prashant Gupta, Heng Ji. 2009. Predicting Unknown
   Time Arguments based on Cross-Event
   Propagation. In Proc. ACL-IJCNLP 2009.
Hilda Hardy, Vika Kanchakouskaya and Tomek
  Strzalkowski.      2006.   Automatic     Event
  Classification Using Surface Text Features. In
  Proc. AAAI06 Workshop on Event Extraction and
  Synthesis. Boston, MA.
H. Ji and R. Grishman. 2008. Refining Event
  Extraction through Cross-Document Inference. In
  Proc. ACL-08: HLT, pages 254–262, Columbus,
  OH, June.
M. Maslennikov and T. Chua. 2007. A Multi
  resolution Framework for Information Extraction
  from Free Text. In Proc. 45th Annual Meeting of
  the Association of Computational Linguistics,
  pages 592–599, Prague, Czech Republic, June.
S. Patwardhan and E. Riloff. 2007. Effective
  Information Extraction with Semantic Affinity
  Patterns and Relevant Regions. In Proc. Joint
  Conference on Empirical Methods in Natural
  Language Processing and Computational Natural
  Language Learning, 2007, pages 717–727, Prague,
  Czech Republic, June.
Patwardhan, S. and Riloff, E. 2009. A Unified Model
  of Phrasal and Sentential Evidence for Information
  Extraction. In Proc. Conference on Empirical
  Methods in Natural Language Processing 2009,
  (EMNLP-09).
David Yarowsky. 1995. Unsupervised Word Sense
  Disambiguation Rivaling Supervised Methods. In
  Proc. ACL 1995. Cambridge, MA.


                                                       797

                   Improving Statistical Machine Translation with
                             Monolingual Collocation

                          Zhanyi Liu1, Haifeng Wang2, Hua Wu2, Sheng Li1
                            1
                              Harbin Institute of Technology, Harbin, China
                                      2
                                        Baidu.com Inc., Beijing, China
                                       zhanyiliu@gmail.com
                            {wanghaifeng, wu_hua}@baidu.com
                                        lisheng@hit.edu.cn


                                                              researches used soft syntactic constraints to pre-
                       Abstract                              dict whether source phrase can be translated to-
                                                              gether (Marton and Resnik, 2008; Xiong et al.,
    This paper proposes to use monolingual                    2009). However, the constraints were learned
    collocations to improve Statistical Ma-                   from the parsed corpus, which is not available
    chine Translation (SMT). We make use                      for many languages.
    of the collocation probabilities, which are                  In this paper, we propose to use monolingual
    estimated from monolingual corpora, in                    collocations to improve SMT. We first identify
    two aspects, namely improving word                        potentially collocated words and estimate collo-
    alignment for various kinds of SMT sys-                   cation probabilities from monolingual corpora
    tems and improving phrase table for                       using a Monolingual Word Alignment (MWA)
    phrase-based SMT. The experimental re-                    method (Liu et al., 2009), which does not need
    sults show that our method improves the                   any additional resource or linguistic preprocess-
    performance of both word alignment and                    ing, and which outperforms previous methods on
    translation quality significantly. As com-                the same experimental data. Then the collocation
    pared to baseline systems, we achieve ab-                 information is employed to improve Bilingual
    solute improvements of 2.40 BLEU score                    Word Alignment (BWA) for various kinds of
    on a phrase-based SMT system and 1.76                     SMT systems and to improve phrase table for
    BLEU score on a parsing-based SMT                         phrase-based SMT.
    system.                                                      To improve BWA, we re-estimate the align-
                                                              ment probabilities by using the collocation prob-
1    Introduction                                             abilities of words in the same cept. A cept is the
                                                              set of source words that are connected to the
Statistical bilingual word alignment (Brown et al.            same target word (Brown et al., 1993). An
1993) is the base of most SMT systems. As com-                alignment between a source multi-word cept and
pared to single-word alignment, multi-word                    a target word is a many-to-one multi-word
alignment is more difficult to be identified. Al-             alignment.
though many methods were proposed to improve                     To improve phrase table, we calculate phrase
the quality of word alignments (Wu, 1997; Och                 collocation probabilities based on word colloca-
and Ney, 2000; Marcu and Wong, 2002; Cherry                   tion probabilities. Then the phrase collocation
and Lin, 2003; Liu et al., 2005; Huang, 2009),                probabilities are used as additional features in
the correlation of the words in multi-word                    phrase-based SMT systems.
alignments is not fully considered.                              The evaluation results show that the proposed
   In phrase-based SMT (Koehn et al., 2003), the              method in this paper significantly improves mul-
phrase boundary is usually determined based on                ti-word alignment, achieving an absolute error
the bi-directional word alignments. But as far as             rate reduction of 29%. The alignment improve-
we know, few previous studies exploit the collo-              ment results in an improvement of 2.16 BLEU
cation relations of the words in a phrase. Some               score on phrase-based SMT system and an im-
                                                              provement of 1.76 BLEU score on parsing-based
This work was partially done at Toshiba (China) Research      SMT system. If we use phrase collocation proba-
and Development Center.                                       bilities as additional features, the phrase-based

                                                           825
         Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825–833,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


SMT performance is further improved by 0.24                                         In the MWA method, the similar algorithm to
BLEU score.                                                                      bilingual word alignment is used to estimate the
   The paper is organized as follows: In section 2,                              parameters of the models, except that a word
we introduce the collocation model based on the                                  cannot be aligned to itself.
MWA method. In section 3 and 4, we show how                                         Figure 1 shows an example of the potentially
to improve the BWA method and the phrase ta-                                     collocated word pairs aligned by the MWA me-
ble using collocation models respectively. We                                    thod.
describe the experimental results in section 5, 6
                                                                                 The team leader plays a key role in the project undertaking.
and 7. Lastly, we conclude in section 8.

2       Collocation Model
Collocation is generally defined as a group of                                       The team leader plays a key role in the project undertaking.
words that occur together more often than by                                                        Figure 1. MWA Example
chance (McKeown and Radev, 2000). A colloca-
tion is composed of two words occurring as ei-
ther a consecutive word sequence or an inter-                                    2.2        Collocation probability
rupted word sequence in sentences, such as "by                                   Given the monolingual word aligned corpus, we
accident" or "take ... advice". In this paper, we                                calculate the frequency of two words aligned in
use the MWA method (Liu et al., 2009) for col-                                   the corpus, denoted as freq( wi , w j ) . We filtered
location extraction. This method adapts the bi-
lingual word alignment algorithm to monolingual                                  the aligned words occurring only once. Then the
scenario to extract collocations only from mono-                                 probability for each aligned word pair is esti-
lingual corpora. And the experimental results in                                 mated as follows:
(Liu et al., 2009) showed that this method                                                                              freq( wi , w j )
achieved higher precision and recall than pre-                                                    p ( wi | w j )                                     (2)
                                                                                                                       freq( w, w j )
vious methods on the same experimental data.                                                                          w

                                                                                                                       freq( wi , w j )
2.1        Monolingual word alignment                                                             p ( w j | wi )                                     (3)
                                                                                                                       freq( wi , w)
                                                                                                                      w
The monolingual corpus is first replicated to
generate a parallel corpus, where each sentence                                     In this paper, the words of collocation are
pair consists of two identical sentences in the                                  symmetric and we do not determine which word
same language. Then the monolingual word                                         is the head and which word is the modifier. Thus,
alignment algorithm is employed to align the                                     the collocation probability of two words is de-
potentially collocated words in the monolingual                                  fined as the average of both probabilities, as in
sentences.                                                                       Eq. (4).
   According to Liu et al. (2009), we employ the
                                                                                                                    p ( wi | w j )  p ( w j | wi )
MWA Model 3 (corresponding to IBM Model 3)                                                       r ( wi , w j )                                      (4)
to calculate the probability of the monolingual                                                                                   2
word alignment sequence, as shown in Eq. (1).                                       If we have multiple monolingual corpora to
                                      l                                          estimate the collocation probabilities, we interpo-
    p MW AModel 3 ( S , A | S )   n( i | wi ) 
                                 i 1                                            late the probabilities as shown in Eq. (5).
                                  l
                                                                         (1)
                                 t ( w j | wa j )  d ( j | a j , l )                                   r (wi , w j )    k rk (wi , w j )         (5)
                                 j 1                                                                                      k


   Where S  w1l is a monolingual sentence, i                                       k denotes the interpolation coefficient for
denotes the number of words that are aligned                                     the probabilities estimated on the kth corpus.
with wi . Since a word never collocates with itself,
                                                                                 3        Improving Statistical Bilingual Word
the       alignment             set      is   denoted     as
                                                                                          Alignment
 A  {(i, ai ) | i [1, l ] & ai  i} . Three kinds of prob-
abilities are involved in this model: word collo-                                We use the collocation information to improve
cation probability t ( w j | wa ) , position colloca-
                                          j
                                                                                 both one-directional and bi-directional bilingual
tion probability d ( j | a j , l ) and fertility probabili-                      word alignments. The alignment probabilities are
                                                                                 re-estimated by using the collocation probabili-
ty n(i | wi ) .                                                                 ties of words in the same cept.

                                                                               826


3.1    Improving one-directional                             bilingual       model to calculate the word alignment probabili-
       word alignment                                                        ty of a sentence pair, as shown in Eq. (9).
According to the BWA method, given a bilingual                                                                 exp(  i hi ( F , E , A))
sentence pair E  e1l and F  f1m , the optimal                                           pr (F , A | E )             i
                                                                                                                                               (9)
                                                                                                               exp(  i hi ( F , E , A))
alignment sequence A between E and F can be                                                                   A'           i

obtained as in Eq. (6).                                                         Here, hi ( F , E, A) and i denote features and
                  *
                A  arg max p( F , A | E )                           (6)     feature weights, respectively. We use two fea-
                            A                                                tures in this paper, namely alignment probabili-
   The method is implemented in a series of five                             ties and collocation probabilities.
models (IBM Models). IBM Model 1 only em-                                       Thus, we obtain the decision rule:
ploys the word translation model to calculate the
probabilities of alignments. In IBM Model 2,
                                                                                           A*  arg max {
                                                                                                      A
                                                                                                                i hi ( F , E, A)}           (10)
                                                                                                                   i
both the word translation model and position dis-
tribution model are used. IBM Model 3, 4 and 5                                  Based on the GIZA++ package 1 , we imple-
consider the fertility model in addition to the                              mented a tool for the improved BWA method.
word translation model and position distribution                             We first train IBM Model 4 and collocation
model. And these three models are similar, ex-                               model on bilingual corpus and monolingual cor-
cept for the word distortion models.                                         pus respectively. Then we employ the hill-
   One-to-one and many-to-one alignments could                               climbing algorithm (Al-Onaizan et al., 1999) to
be produced by using IBM models. Although the                                search for the optimal alignment sequence of a
fertility model is used to restrict the number of                            given sentence pair, where the score of an align-
source words in a cept and the position distortion                           ment sequence is calculated as in Eq. (10).
model is used to describe the correlation of the                                We note that Eq. (8) only deals with many-to-
positions of the source words, the quality of                                one alignments, but the alignment sequence of a
many-to-one alignments is lower than that of                                 sentence pair also includes one-to-one align-
one-to-one alignments.                                                       ments. To calculate the collocation probability of
   Intuitively, the probability of the source words                          the alignment sequence, we should also consider
aligned to a target word is not only related to the                          the collocation probabilities of such one-to-one
fertility ability and their relative positions, but                          alignments. To solve this problem, we use the
also related to lexical tokens of words, such as                             collocation probability of the whole source sen-
common phrase or idiom. In this paper, we use                                tence, r (F ) , as the collocation probability of
the collocation probability of the source words in                           one-word cept.
a cept to measure their correlation strength. Giv-
                                                                             3.2       Improving bi-directional bilingual word
en source words { f j | a j  i} aligned to ei , their
                                                                                       alignments
collocation probability is calculated as in Eq. (7).
                                                                             In word alignment models implemented in GI-
                                       i 1 i
                                  2   r ( f [ i ]k , f [ i ] g )           ZA++, only one-to-one and many-to-one word
         r ({ f j | a j  i}) 
                                       k 1 g  k 1
                                                                     (7)     alignment links can be found. Thus, some multi-
                                             i * (i  1)                   word units cannot be correctly aligned. The
                                                                             symmetrization method is used to effectively
  Here, f [i ]k and f[ i ] g denote the k th word and                        overcome this deficiency (Och and Ney, 2003).
g th word in { f j | a j  i} ; r ( f[ i ]k , f[ i ] g ) denotes             Bi-directional alignments are generally obtained
                                                                             from source-to-target alignments As 2t and target-
the collocation probability of f [i ]k and f[ i ] g , as
                                                                             to-source alignments At 2 s , using some heuristic
shown in Eq. (4).                                                            rules (Koehn et al., 2005). This method ignores
   Thus, the collocation probability of the align-                           the correlation of the words in the same align-
ment sequence of a sentence pair can be calcu-                               ment unit, so an alignment may include many
lated according to Eq. (8).                                                  unrelated words 2 , which influences the perfor-
                                   l                                         mances of SMT systems.
              r ( F , A | E )   r ({ f j | a j  i})               (8)
                                  i 1


  Based on maximum entropy framework, we                                     1
                                                                                 http://www.fjoch.com/GIZA++.html
                                                                             2
combine the collocation model and the BWA                                        In our experiments, a multi-word unit may include up to
                                                                                 40 words.


                                                                           827


   In order to solve the above problem, we incor-                                                                                     Chinese       English
porate the collocation probabilities into the bi-                                                                Corpora
                                                                                                                                       words         words
directional word alignment process.                                                                          Bilingual corpus          6.3M          8.5M
   Given alignment sets As 2t and At 2 s . We can
                                                                                                          Additional monolingual
obtain the union As t  As 2t  At 2s . The source                                                                                    312M             203M
                                                                                                                  corpora
sentence f             1
                        m
                             can be segmented into m cepts
     m
                                                                                                                 Table 1. Statistics of training data
f   1      . The target sentence e1l can also be seg-
mented into l  cepts e1l  . The words in the same
cept can be a consecutive word sequence or an                                                         word pair calculated according to Eq. (4). For the
interrupted word sequence.                                                                            phrase only including one word, we set a fixed
    Finally, the optimal alignments A between                                                         collocation probability that is the average of the
                                                                                                      collocation probabilities of the sentences on a
 f 1 and e1l  can be obtained from As t using the
    m
                                                                                                      development set. These collocation probabilities
following decision rule.                                                                              are incorporated into the phrase-based SMT sys-
          (e1l ' , f1m' , A )*                                                                        tem as features.
                                                                                     3      (11)
            arg max{
               A  As t
                                  p(ei , f j )            1
                                                                 r (ei ) 2  r ( f j ) }            5       Experiments on Word Alignment
                            ( ei , f j )A
                                                                                                      5.1      Experimental settings
   Here, r ( f j ) and r (ei ) denote the collocation
                                                                                                      We use a bilingual corpus, FBIS (LDC2003E14),
probabilities of the words in the source language                                                     to train the IBM models. To train the collocation
and target language respectively, which are cal-                                                      models, besides the monolingual parts of FBIS,
culated by using Eq. (7). p(ei , f j ) denotes the                                                    we also employ some other larger Chinese and
word translation probability that is calculated                                                       English monolingual corpora, namely, Chinese
according to Eq. (12).  i denotes the weights of                                                     Gigaword (LDC2007T38), English Gigaword
these probabilities.                                                                                  (LDC2007T07), UN corpus (LDC2004E12), Si-
                                                                                                      norama corpus (LDC2005T10), as shown in Ta-
                                      ( p (e | f )  p ( f | e)) / 2                                ble 1.
                                   eei f  f j
               p (ei , f j )                                                                (12)        Using these corpora, we got three kinds of col-
                                                       | ei | * | f j |                               location models:
      p(e | f ) and p( f | e) are the source-to-target                                                     CM-1: the training data is the additional mo-
and target-to-source translation probabilities                                                              nolingual corpora;
trained from the word aligned bilingual corpus.                                                            CM-2: the training data is either side of the bi-
                                                                                                            lingual corpus;
4           Improving Phrase Table                                                                         CM-3: the interpolation of CM-1 and CM-2.
Phrase-based SMT system automatically extracts                                                           To investigate the quality of the generated
bilingual phrase pairs from the word aligned bi-                                                      word alignments, we randomly selected a subset
lingual corpus. In such a system, an idiomatic                                                        from the bilingual corpus as test set, including
expression may be split into several fragments,                                                       500 sentence pairs. Then word alignments in the
and the phrases may include irrelevant words. In                                                      subset were manually labeled, referring to the
this paper, we use the collocation probability to                                                     guideline of the Chinese-to-English alignment
measure the possibility of words composing a                                                          (LDC2006E93), but we made some modifica-
phrase.                                                                                               tions for the guideline. For example, if a preposi-
   For each bilingual phrase pair automatically                                                       tion appears after a verb as a phrase aligned to
extracted from word aligned corpus, we calculate                                                      one single word in the corresponding sentence,
the collocation probabilities of source phrase and                                                    then they are glued together.
target phrase respectively, according to Eq. (13).                                                       There are several different evaluation metrics
                                             n 1 n                                                   for word alignment (Ahrenberg et al., 2000). We
                                        2   r ( wi , w j )
                                             i 1 j i 1                                             use precision (P), recall (R) and alignment error
                       r ( w1n )                                                            (13)
                                                  n * (n  1)                                         ratio (AER), which are similar to those in Och
                                                                                                      and Ney (2000), except that we consider each
    Here, w1n denotes a phrase with n words;                                                          alignment as a sure link.
r ( wi , w j ) denotes the collocation probability of a


                                                                                                    828


                                   Single word alignments Multi-word alignments
                         Experiments
                                     P       R      AER    P       R      AER
                   Baseline         0.77   0.45     0.43  0.23    0.71    0.65
                              CM-1 0.70    0.50     0.42  0.35    0.86    0.50
         Improved BWA methods CM-2 0.73    0.48     0.42  0.36    0.89    0.49
                              CM-3 0.73    0.48     0.41  0.39    0.78    0.47
                                   Table 2. English-to-Chinese word alignment results


                中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。


 China's science and technology research has made achievements which have gained the attention of the people of the world .




                中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。
                zhong-guo de     ke-xue-ji-shu      yan-jiu   qu-de    le   xu-duo ling shi-ren   zhu-mu   de   cheng-jiu .
                 china     DE     science and       research obtain   LE     many   let   common attract DE achievement .
                                   technology                                              people attention


    Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and
 the improved BWA method using CM-3. " " denotes the alignments of our method; " " denotes
                               the alignments of the baseline method.


                                                                            English-to-Chinese word alignment. We only
                           | S g  Sr |
                     P                                         (14)        evaluate the English-to-Chinese word alignment
                                | Sg |                                      here. GIZA++ with the default settings is used as
                                                                            the baseline method. The evaluation results in
                              | S g  Sr |
                         R                                     (15)        Table 2 indicate that the performances of our
                                 | Sr |                                     methods on single word alignments are close to
                                2* | S g  S r |
                                                                            that of the baseline method. For multi-word
                AER  1                                        (16)        alignments, our methods significantly outper-
                                 | S g |  | Sr |                           form the baseline method in terms of both preci-
   Where, S g and S r denote the automatically                              sion and recall, achieving up to 18% absolute
                                                                            error rate reduction.
generated alignments and the reference align-                                  Although the size of the bilingual corpus is
ments.                                                                      much smaller than that of additional monolingual
   In order to tune the interpolation coefficients                          corpora, our methods using CM-1 and CM-2
in Eq. (5) and the weights of the probabilities in                          achieve comparable performances. It is because
Eq. (11), we also manually labeled a develop-                               CM-2 and the BWA model are derived from the
ment set including 100 sentence pairs, in the                               same resource. By interpolating CM1 and CM2,
same manner as the test set. By minimizing the                              i.e. CM-3, the error rate of multi-word alignment
AER on the development set, the interpolation                               results is further reduced.
coefficients of the collocation probabilities on                               Figure 2 shows an example of word alignment
CM-1 and CM-2 were set to 0.1 and 0.9. And the                              results generated by the baseline method and the
weights of probabilities were set as 1  0.6 ,                             improved method using CM-3. In this example,
 2  0.2 and  3  0.2 .                                                   our method successfully identifies many-to-one
                                                                            alignments such as "the people of the world
5.2   Evaluation results                                                    世人". In our collocation model, the collocation
One-directional alignment results                                           probability of "the people of the world" is much
                                                                            higher than that of "people world". And our me-
  To train a Chinese-to-English SMT system,
                                                                            thod is also effective to prevent the unrelated
we need to perform both Chinese-to-English and

                                                                       829


                             Single word alignments Multi-word alignments             All alignments
        Experiments
                               P       R      AER    P       R      AER              P       R   AER
           Baseline           0.84   0.43     0.42  0.18    0.74    0.70            0.52 0.45 0.51
                WA-1          0.80   0.51     0.37  0.30    0.89    0.55            0.58 0.51 0.45
    Our methods WA-2          0.81   0.50     0.37  0.33    0.81    0.52            0.62 0.50 0.44
                WA-3          0.78   0.56     0.34  0.44    0.88    0.41            0.63 0.54 0.40
                              Table 3. Bi-directional word alignment results


words from being aligned. For example, in the                         Experiments            BLEU (%)
baseline alignment "has made ... have       取得",                       Baseline                29.62
"have" and "has" are unrelated to the target word,
                                                                              CM-1             30.85
while our method only generated "made           取
得", this is because that the collocation probabili-                      WA-1 CM-2             31.28
ties of "has/have" and "made" are much lower                                  CM-3             31.48
than that of the whole source sentence.                                       CM-1             31.00
Bi-directional alignment results                             Our methods WA-2 CM-2             31.33
   We build a bi-directional alignment baseline                               CM-3             31.51
in two steps: (1) GIZA++ is used to obtain the                                CM-1             31.43
source-to-target and target-to-source alignments;                        WA-3 CM-2             31.62
(2) the bi-directional alignments are generated by                            CM-3             31.78
using "grow-diag-final". We use the methods
proposed in section 3 to replace the correspond-             Table 4. Performances of Moses using the dif-
ing steps in the baseline method. We evaluate               ferent bi-directional word alignments (Signifi-
three methods:                                                 cantly better than baseline with p < 0.01)
  WA-1: one-directional alignment method pro-
   posed in section 3.1 and grow-diag-final;            6      Experiments on Phrase-Based SMT
  WA-2: GIZA++ and the bi-directional bilin-
   gual word alignments method proposed in              6.1     Experimental settings
   section 3.2;                                         We use FBIS corpus to train the Chinese-to-
  WA-3: both methods proposed in section 3.             English SMT systems. Moses (Koehn et al., 2007)
   Here, CM-3 is used in our methods. The re-           is used as the baseline phrase-based SMT system.
sults are shown in Table 3.                             We use SRI language modeling toolkit (Stolcke,
   We can see that WA-1 achieves lower align-           2002) to train a 5-gram language model on the
ment error rate as compared to the baseline me-         English sentences of FBIS corpus. We used the
thod, since the performance of the improved one-        NIST MT-2002 set as the development set and
directional alignment method is better than that        the NIST MT-2004 test set as the test set. And
of GIZA++. This result indicates that improving         Koehn's implementation of minimum error rate
one-directional word alignment results in bi-           training (Och, 2003) is used to tune the feature
directional word alignment improvement.                 weights on the development set.
   The results also show that the AER of WA-2              We use BLEU (Papineni et al., 2002) as eval-
is lower than that of the baseline. This is because     uation metrics. We also calculate the statistical
the proposed bi-directional alignment method            significance differences between our methods
can effectively recognize the correct alignments        and the baseline method by using paired boot-
from the alignment union, by leveraging colloca-        strap re-sample method (Koehn, 2004).
tion probabilities of the words in the same cept.       6.2     Effect of improved word alignment on
   Our method using both methods proposed in                    phrase-based SMT
section 3 produces the best alignment perfor-
mance, achieving 11% absolute error rate reduc-         We investigate the effectiveness of the improved
tion.                                                   word alignments on the phrase-based SMT sys-
                                                        tem. The bi-directional alignments are obtained

                                                      830


             T1:    We must adopt effective measures in order to avoid problems .


                    我们     必须      采取      有效 措施 才能 避免 出 问题 。
                    wo-men bi-xu   cai-qu you-xiao cuo-shi cai-neng bi-mian chu   wen-ti   .
                    we     must     use   effective measure can      avoid out    problem .


              T2:   We must adopt effective measures can we avoid out of the question .

      Figure 3. Example of the translations generated by the baseline system and the system where the
                                phrase collocation probabilities are added


             Experiments              BLEU (%)             Figure 3 shows an example: T1 is generated
                                                        by the system where the phrase collocation prob-
                Moses                    29.62
                                                        abilities are used and T2 is generated by the
  + Phrase collocation probability       30.47          baseline system. In this example, since the collo-
   + Improved word alignments                           cation probability of "出 问题" is much higher
                                         32.02
  + Phrase collocation probability                      than that of "问题 。", our method tends to split
                                                        "出 问题 。" into "(出 问题) (。)", rather than
   Table 5. Performances of Moses employing
our proposed methods (Significantly better than         "(出) (问题 。)". For the phrase "才能 避免" in
            baseline with p < 0.01)                     the source sentence, the collocation probability
                                                        of the translation "in order to avoid" is higher
                                                        than that of the translation "can we avoid". Thus,
using the same methods as those shown in Table          our method selects the former as the translation.
3. Here, we investigate three different collocation     Although the phrase "我们 必须 采取 有效 措
models for translation quality improvement. The         施" in the source sentence has the same transla-
results are shown in Table 4.                           tion "We must adopt effective measures", our
   From the results of Table 4, it can be seen that     method splits this phrase into two parts "我们 必
the systems using the improved bi-directional           须" and "采取 有效 措施", because two parts
alignments achieve higher quality of translation        have higher collocation probabilities than the
than the baseline system. If the same alignment         whole phrase.
method is used, the systems using CM-3 got the             We also investigate the performance of the
highest BLEU scores. And if the same colloca-           system employing both the word alignment im-
tion model is used, the systems using WA-3              provement and phrase table improvement me-
achieved the higher scores. These results are           thods. From the results in Table 5, it can be seen
consistent with the evaluations of word align-          that the quality of translation is future improved.
ments as shown in Tables 2 and 3.                       As compared with the baseline system, an abso-
6.3    Effect of phrase collocation probabili-          lute improvement of 2.40 BLEU score is
       ties                                             achieved. And this result is also better than the
                                                        results shown in Table 4.
To investigate the effectiveness of the method
proposed in section 4, we only use the colloca-         7     Experiments on Parsing-Based SMT
tion model CM-3 as described in section 5.1. The
results are shown in Table 5. When the phrase           We also investigate the effectiveness of the im-
collocation probabilities are incorporated into the     proved word alignments on the parsing-based
SMT system, the translation quality is improved,        SMT system, Joshua (Li et al., 2009). In this sys-
achieving an absolute improvement of 0.85               tem, the Hiero-style SCFG model is used
BLEU score. This result indicates that the collo-       (Chiang, 2007), without syntactic information.
cation probabilities of phrases are useful in de-       The rules are extracted only based on the FBIS
termining the boundary of phrase and predicting         corpus, where words are aligned by "MW-3 &
whether phrases should be translated together,          CM-3". And the language model is the same as
which helps to improve the phrase-based SMT             that in Moses. The feature weights are tuned on
performance.                                            the development set using the minimum error


                                                      831


           Experiments             BLEU (%)               Alignment Systems. In Proceedings of the Second
                                                          International Conference on Language Resources
              Joshua                 30.05                and Evaluation, pp. 1255-1261.
    + Improved word alignments       31.81
                                                      Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
   Table 6. Performances of Joshua using the dif-       Knight, John Lafferty, Dan Melamed, Franz-Josef
ferent word alignments (Significantly better than       Och, David Purdy, Noah A. Smith, and David Ya-
                                                        rowsky. 1999. Statistical Machine Translation. Fi-
             baseline with p < 0.01)
                                                        nal Report. In Johns Hopkins University Workshop.
                                                      Peter F. Brown, Stephen A. Della Pietra, Vincent J.
rate training method. We use the same evaluation        Della Pietra, and Robert. L. Mercer. 1993. The Ma-
measure as described in section 6.1.                    thematics of Statistical Machine Translation: Pa-
   The translation results on Joshua are shown in       rameter estimation. Computational Linguistics,
                                                        19(2): 263-311.
Table 6. The system using the improved word
alignments achieves an absolute improvement of        Colin Cherry and Dekang Lin. 2003. A Probability
1.76 BLEU score, which indicates that the im-           Model to Improve Word Alignment. In Proceed-
provements of word alignments are also effective        ings of the 41st Annual Meeting of the Association
to improve the performance of the parsing-based         for Computational Linguistics, pp. 88-95.
SMT systems.                                          David Chiang. 2007. Hierarchical Phrase-Based
                                                        Translation. Computational Linguistics, 33(2):
8    Conclusion                                         201-228.

We presented a novel method to use monolingual        Fei Huang. 2009. Confidence Measure for Word
collocations to improve SMT. We first used the           Alignment. In Proceedings of the 47th Annual
                                                         Meeting of the ACL and the 4th IJCNLP, pp. 932-
MWA method to identify potentially collocated
                                                         940.
words and estimate collocation probabilities only
from monolingual corpora, no additional re-           Philipp Koehn. 2004. Statistical Significance Tests for
source or linguistic preprocessing is needed.           Machine Translation Evaluation. In Proceedings of
Then the collocation information was employed           the 2004 Conference on Empirical Methods in
to improve BWA for various kinds of SMT sys-            Natural Language Processing, pp. 388-395.
tems and to improve phrase table for phrase-          Philipp Koehn, Amittai Axelrod, Alexandra Birch
based SMT.                                              Mayne, Chris Callison-Burch, Miles Osborne, and
   To improve BWA, we re-estimate the align-            David Talbot. 2005. Edinburgh System Description
ment probabilities by using the collocation prob-       for the 2005 IWSLT Speech Translation Evalua-
abilities of words in the same cept. To improve         tion. In Processings of the International Workshop
                                                        on Spoken Language Translation 2005.
phrase table, we calculate phrase collocation
probabilities based on word collocation probabil-     Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
ities. Then the phrase collocation probabilities        Statistical Phrase-based Translation. In Proceed-
are used as additional features in phrase-based         ings of the Human Language Technology Confe-
SMT systems.                                            rence and the North American Association for
                                                        Computational Linguistics, pp. 127-133.
   The evaluation results showed that the pro-
posed method significantly improved word              Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
alignment, achieving an absolute error rate re-         Callison-Burch, Marcello Federico, Nicola Bertoldi,
duction of 29% on multi-word alignment. The             Brooke Cowan, Wade Shen, Christine Moran Ri-
improved word alignment results in an improve-          chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
                                                        Constantin, and Evan Herbst. 2007. Moses: Open
ment of 2.16 BLEU score on a phrase-based
                                                        Source Toolkit for Statistical Machine Translation.
SMT system and an improvement of 1.76 BLEU              In Proceedings of the 45th Annual Meeting of the
score on a parsing-based SMT system. When we            ACL, Poster and Demonstration Sessions, pp. 177-
also used phrase collocation probabilities as ad-       180.
ditional features, the phrase-based SMT perfor-
                                                      Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-
mance is finally improved by 2.40 BLEU score
                                                        nitkevitch, Sanjeev Khudanpur, Lane Schwartz,
as compared with the baseline system.                   Wren Thornton, Jonathan Weese, and Omar Zaidan.
                                                        2009. Demonstration of Joshua: An Open Source
Reference                                               Toolkit for Parsing-based Machine Translation. In
Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein,       Proceedings of the 47th Annual Meeting of the As-
  and Jorg Tiedemann. 2000. Evaluation of Word


                                                    832


  sociation for Computational Linguistics, Software
  Demonstrations, pp. 25-28.
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear
  Models for Word Alignment. 2005. In Proceedings
  of the 43rd Annual Meeting of the Association for
  Computational Linguistics, pp. 459-466.
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.
  2009. Collocation Extraction Using Monolingual
  Word Alignment Method. In Proceedings of the
  2009 Conference on Empirical Methods in Natural
  Language Processing, pp. 487-495.
Daniel Marcu and William Wong. 2002. A Phrase-
  Based, Joint Probability Model for Statistical Ma-
  chine Translation. In Proceedings of the 2002 Con-
  ference on Empirical Methods in Natural Lan-
  guage Processing, pp. 133-139.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
  Constraints for Hierarchical Phrase-Based Transla-
  tion. In Proceedings of the 46st Annual Meeting of
  the Association for Computational Linguistics, pp.
  1003-1011.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
  Collocations. In Robert Dale, Hermann Moisl, and
  Harold Somers (Ed.), A Handbook of Natural Lan-
  guage Processing, pp. 507-523.
Franz Josef Och and Hermann Ney. 2000. Improved
   Statistical Alignment Models. In Proceedings of
   the 38th Annual Meeting of the Association for
   Computational Linguistics, pp. 440-447.
Franz Josef Och. 2003. Minimum Error Rate Training
   in Statistical Machine Translation. In Proceedings
   of the 41st Annual Meeting of the Association for
   Computational Linguistics, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A Syste-
   matic Comparison of Various Statistical Alignment
   Models. Computational Linguistics, 29(1): 19-52.
Kishore Papineni, Salim Roukos, Todd Ward, and
  Weijing Zhu. 2002. BLEU: A Method for Auto-
  matic Evaluation of Machine Translation. In Pro-
  ceedings of 40th annual meeting of the Association
  for Computational Linguistics, pp. 311-318.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
  guage Modeling Toolkit. In Proceedings for the In-
  ternational Conference on Spoken Language
  Processing, pp. 901-904.
Dekai Wu. 1997. Stochastic Inversion Transduction
  Grammars and Bilingual Parsing of Parallel Cor-
  pora. Computational Linguistics, 23(3): 377-403.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
  2009. A Syntax-Driven Bracketing Model for
  Phrase-Based Translation. In Proceedings of the
  47th Annual Meeting of the ACL and the 4th
  IJCNLP, pp. 315-323.



                                                        833

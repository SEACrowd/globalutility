    Creating Robust Supervised Classifiers via Web-Scale N-gram Data

       Shane Bergsma                            Emily Pitler                               Dekang Lin
      University of Alberta               University of Pennsylvania                       Google, Inc.
     sbergsma@ualberta.ca                  epitler@seas.upenn.edu                      lindek@google.com




                      Abstract                                   2. How well do web-based models perform on
                                                                    new domains or when labeled data is scarce?
   In this paper, we systematically assess the
   value of using web-scale N-gram data in                        We address these questions on two generation
   state-of-the-art supervised NLP classifiers.                and two analysis tasks, using both existing N-gram
   We compare classifiers that include or ex-                  data and a novel web-scale N-gram corpus that
   clude features for the counts of various                    includes part-of-speech information (Section 2).
   N-grams, where the counts are obtained                      While previous work has combined web-scale fea-
   from a web-scale auxiliary corpus. We                       tures with other features in specific classification
   show that including N-gram count features                   problems (Modjeska et al., 2003; Yang et al.,
   can advance the state-of-the-art accuracy                   2005; Vadas and Curran, 2007b), we provide a
   on standard data sets for adjective order-                  multi-task, multi-domain comparison.
   ing, spelling correction, noun compound                        Some may question why supervised approaches
   bracketing, and verb part-of-speech dis-                    are needed at all for generation problems. Why
   ambiguation. More importantly, when op-                     not solely rely on direct evidence from a giant cor-
   erating on new domains, or when labeled                     pus? For example, for the task of prenominal ad-
   training data is not plentiful, we show that                jective ordering (Section 3), a system that needs
   using web-scale N-gram features is essen-                   to describe a ball that is both big and red can sim-
   tial for achieving robust performance.                      ply check that big red is more common on the web
                                                               than red big, and order the adjectives accordingly.
1 Introduction                                                    It is, however, suboptimal to only use N-gram
                                                               data. For example, ordering adjectives by direct
Many NLP systems use web-scale N-gram counts
                                                               web evidence performs 7% worse than our best
(Keller and Lapata, 2003; Nakov and Hearst,
                                                               supervised system (Section 3.2). No matter how
2005; Brants et al., 2007). Lapata and Keller
                                                               large the web becomes, there will always be plau-
(2005) demonstrate good performance on eight
                                                               sible constructions that never occur. For example,
tasks using unsupervised web-based models. They
                                                               there are currently no pages indexed by Google
show web counts are superior to counts from a
                                                               with the preferred adjective ordering for bedrag-
large corpus. Bergsma et al. (2009) propose un-
                                                               gled 56-year-old [professor]. Also, in a particu-
supervised and supervised systems that use counts
                                                               lar domain, words may have a non-standard usage.
from Google’s N-gram corpus (Brants and Franz,
                                                               Systems trained on labeled data can learn the do-
2006). Web-based models perform particularly
                                                               main usage and leverage other regularities, such as
well on generation tasks, where systems choose
                                                               suffixes and transitivity for adjective ordering.
between competing sequences of output text (such
                                                                  With these benefits, systems trained on labeled
as different spellings), as opposed to analysis
                                                               data have become the dominant technology in aca-
tasks, where systems choose between abstract la-
                                                               demic NLP. There is a growing recognition, how-
bels (such as part-of-speech tags or parse trees).
                                                               ever, that these systems are highly domain de-
   In this work, we address two natural and related
                                                               pendent. For example, parsers trained on anno-
questions which these previous studies leave open:
                                                               tated newspaper text perform poorly on other gen-
  1. Is there a benefit in combining web-scale                 res (Gildea, 2001). While many approaches have
     counts with the features used in state-of-the-            adapted NLP systems to specific domains (Tsu-
     art supervised approaches?                                ruoka et al., 2005; McClosky et al., 2006; Blitzer


                                                         865
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


et al., 2007; Daumé III, 2007; Rimell and Clark,             of the N-gram data and its counts remain constant.
2008), these techniques assume the system knows               We always optimize the SVM’s (L2) regulariza-
on which domain it is being used, and that it has             tion parameter on the in-domain development set.
access to representative data in that domain. These           We present results with L2-SVM, but achieve sim-
assumptions are unrealistic in many real-world sit-           ilar results with L1-SVM and logistic regression.
uations; for example, when automatically process-
ing a heterogeneous collection of web pages. How              2.2 Tasks and Labeled Data
well do supervised and unsupervised NLP systems               We study two generation tasks: prenominal ad-
perform when used uncustomized, out-of-the-box                jective ordering (Section 3) and context-sensitive
on new domains, and how can we best design our                spelling correction (Section 4), followed by two
systems for robust open-domain performance?                   analysis tasks: noun compound bracketing (Sec-
   Our results show that using web-scale N-gram               tion 5) and verb part-of-speech disambiguation
data in supervised systems advances the state-of-             (Section 6). In each section, we provide refer-
the-art performance on standard analysis and gen-             ences to the origin of the labeled data. For the
eration tasks. More importantly, when operating               out-of-domain Gutenberg and Medline data used
out-of-domain, or when labeled data is not plen-              in Sections 3 and 4, we generate examples our-
tiful, using web-scale N-gram data not only helps             selves.1 We chose Gutenberg and Medline in order
achieve good performance – it is essential.                   to provide challenging, distinct domains from our
                                                              training corpora. Our Gutenberg corpus consists
2 Experiments and Data                                        of out-of-copyright books, automatically down-
2.1   Experimental Design                                     loaded from the Project Gutenberg website. 2 The
                                                              Medline data consists of a large collection of on-
We evaluate the benefit of N-gram data on multi-              line biomedical abstracts. We describe how la-
class classification problems. For each task, we              beled adjective and spelling examples are created
have some labeled data indicating the correct out-            from these corpora in the corresponding sections.
put for each example. We evaluate with accuracy:
the percentage of examples correctly classified in            2.3 Web-Scale Auxiliary Data
test data. We use one in-domain and two out-of-
                                                              The most widely-used N-gram corpus is the
domain test sets for each task. Statistical signifi-
                                                              Google 5-gram Corpus (Brants and Franz, 2006).
cance is assessed with McNemar’s test, p<0.01.
                                                                 For our tasks, we also use Google V2: a new
   We provide results for unsupervised approaches
                                                              N-gram corpus (also with N-grams of length one-
and the majority-class baseline for each task.
                                                              to-five) that we created from the same one-trillion-
   For our supervised approaches, we represent the
                                                              word snapshot of the web as the Google 5-gram
examples as feature vectors, and learn a classi-
                                                              Corpus, but with several enhancements. These in-
fier on the training vectors. There are two fea-
                                                              clude: 1) Reducing noise by removing duplicate
ture classes: features that use N-grams (N- GM )
                                                              sentences and sentences with a high proportion
and those that do not (L EX ). N- GM features are
                                                              of non-alphanumeric characters (together filtering
real-valued features giving the log-count of a par-
                                                              about 80% of the source data), 2) pre-converting
ticular N-gram in the auxiliary web corpus. L EX
                                                              all digits to the 0 character to reduce sparsity for
features are binary features that indicate the pres-
                                                              numeric expressions, and 3) including the part-of-
ence or absence of a particular string at a given po-
                                                              speech (POS) tag distribution for each N-gram.
sition in the input. The name L EX emphasizes that
                                                              The source data was automatically tagged with
they identify specific lexical items. The instantia-
                                                              TnT (Brants, 2000), using the Penn Treebank tag
tions of both types of features depend on the task
                                                              set. Lin et al. (2010) provide more details on the
and are described in the corresponding sections.
   Each classifier is a linear Support Vector Ma-                1
                                                                     http://webdocs.cs.ualberta.ca/∼bergsma/Robust/
chine (SVM), trained using LIBLINEAR (Fan et al.,             provides our Gutenberg corpus, a link to Medline, and also
                                                              the generated examples for both Gutenberg and Medline.
2008) on the standard domain. We use the one-vs-                  2
                                                                    www.gutenberg.org. All books just released in 2009 and
all strategy when there are more than two classes             thus unlikely to occur in the source data for our N-gram cor-
(in Section 4). We plot learning curves to mea-               pus (from 2006). Of course, with removal of sentence dupli-
                                                              cates and also N-gram thresholding, the possible presence of
sure the accuracy of the classifier when the num-             a test sentence in the massive source data is unlikely to affect
ber of labeled training examples varies. The size             results. Carlson et al. (2008) reach a similar conclusion.


                                                        866


N-gram data and N-gram search tools.                                     The input to the system is a pair of adjectives,
   The third enhancement is especially relevant                       (a1 , a2 ), ordered alphabetically. The task is to
here, as we can use the POS distribution to collect                   classify this order as correct (the positive class) or
counts for N-grams of mixed words and tags. For                       incorrect (the negative class). Since both classes
example, we have developed an N-gram search en-                       are equally likely, the majority-class baseline is
gine that can count how often the adjective un-                       around 50% on each of the three test sets.
precedented precedes another adjective in our web
corpus (113K times) and how often it follows one                      3.1 Supervised Adjective Ordering
(11K times). Thus, even if we haven’t seen a par-                     3.1.1 L EX features
ticular adjective pair directly, we can use the posi-                 Our adjective ordering model with L EX features is
tional preferences of each adjective to order them.                   a novel contribution of this paper.
   Early web-based models used search engines to                         We begin with two features for each pair: an in-
collect N-gram counts, and thus could not use cap-                    dicator feature for a1 , which gets a feature value of
italization, punctuation, and annotations such as                     +1, and an indicator feature for a2 , which gets a
part-of-speech (Kilgarriff and Grefenstette, 2003).                   feature value of −1. The parameters of the model
Using a POS-tagged web corpus goes a long way                         are therefore weights on specific adjectives. The
to addressing earlier criticisms of web-based NLP.                    higher the weight on an adjective, the more it is
                                                                      preferred in the first position of a pair. If the alpha-
3 Prenominal Adjective Ordering                                       betic ordering is correct, the weight on a1 should
Prenominal adjective ordering strongly affects text                   be higher than the weight on a2 , so that the clas-
readability. For example, while the unprecedented                     sifier returns a positive score. If the reverse order-
statistical revolution is fluent, the statistical un-                 ing is preferred, a2 should receive a higher weight.
precedented revolution is not. Many NLP systems                       Training the model in this setting is a matter of as-
need to handle adjective ordering robustly. In ma-                    signing weights to all the observed adjectives such
chine translation, if a noun has two adjective mod-                   that the training pairs are maximally ordered cor-
ifiers, they must be ordered correctly in the tar-                    rectly. The feature weights thus implicitly produce
get language. Adjective ordering is also needed                       a linear ordering of all observed adjectives. The
in Natural Language Generation systems that pro-                      examples can also be regarded as rank constraints
duce information from databases; for example, to                      in a discriminative ranker (Joachims, 2002). Tran-
convey information (in sentences) about medical                       sitivity is achieved naturally in that if we correctly
patients (Shaw and Hatzivassiloglou, 1999).                           order pairs a ≺ b and b ≺ c in the training set,
   We focus on the task of ordering a pair of adjec-                  then a ≺ c by virtue of the weights on a and c.
tives independently of the noun they modify and                          While exploiting transitivity has been shown
achieve good performance in this setting. Follow-                     to improve adjective ordering, there are many
ing the set-up of Malouf (2000), we experiment                        conflicting pairs that make a strict linear order-
on the 263K adjective pairs Malouf extracted from                     ing of adjectives impossible (Malouf, 2000). We
the British National Corpus (BNC). We use 90%                         therefore provide an indicator feature for the pair
of pairs for training, 5% for testing, and 5% for                     a1 a2 , so the classifier can memorize exceptions
development. This forms our in-domain data.3                          to the linear ordering, breaking strict order tran-
   We create out-of-domain examples by tokeniz-                       sitivity. Our classifier thus operates along the lines
ing Medline and Gutenberg (Section 2.2), then                         of rankers in the preference-based setting as de-
POS-tagging them with CRFTagger (Phan, 2006).                         scribed in Ailon and Mohri (2008).
We create examples from all sequences of two ad-                         Finally, we also have features for all suffixes of
jectives followed by a noun. Like Malouf (2000),                      length 1-to-4 letters, as these encode useful infor-
we assume that edited text has adjectives ordered                     mation about adjective class (Malouf, 2000). Like
fluently. We extract 13K and 9.1K out-of-domain                       the adjective features, the suffix features receive a
pairs from Gutenberg and Medline, respectively. 4                     value of +1 for adjectives in the first position and
                                                                      −1 for those in the second.
   3
      BNC is not a domain per se (rather a balanced corpus),
but has a style and vocabulary distinct from our OOD data.            3.1.2 N- GM features
    4
      Like Malouf (2000), we convert our pairs to lower-case.
Since the N-gram data includes case, we merge counts from             Lapata and Keller (2005) propose a web-based
the upper and lower case combinations.                                approach to adjective ordering: take the most-


                                                                867


 System                                  IN       O1       O2                             100
 Malouf (2000)                          91.5     65.6     71.6                            95
 web c(a1 , a2 ) vs. c(a2 , a1 )        87.1     83.7     86.0
                                                                                          90
 SVM with N- GM features                90.0     85.8     88.5




                                                                           Accuracy (%)
                                                                                          85
 SVM with L EX features                 93.0     70.0     73.9
 SVM with N- GM + L EX                  93.7     83.6     85.4                            80
                                                                                          75
Table 1: Adjective ordering accuracy (%). SVM                                             70                  N-GM+LEX
and Malouf (2000) trained on BNC, tested on                                               65                      N-GM
BNC (IN), Gutenberg (O1), and Medline (O2).                                                                        LEX
                                                                                          60
                                                                                                100     1e3        1e4       1e5
frequent order of the words on the web, c(a1 , a2 )                                              Number of training examples
vs. c(a2 , a1 ). We adopt this as our unsupervised
approach. We merge the counts for the adjectives                          Figure 1: In-domain learning curve of adjective
occurring contiguously and separated by a comma.                          ordering classifiers on BNC.
   These are indubitably the most important N- GM
features; we include them but also other, tag-based                                       100
                                                                                             N-GM+LEX
counts from Google V2. Raw counts include cases                                           95     N-GM
                                                                                                  LEX
where one of the adjectives is not used as a mod-                                         90
                                                                           Accuracy (%)
ifier: “the special present was” vs. “the present                                         85
special issue.” We include log-counts for the                                             80
following, more-targeted patterns: 5 c(a1 a2 N.*),                                        75
c(a2 a1 N.*), c(DT a1 a2 N.*), c(DT a2 a1 N.*).                                           70
We also include features for the log-counts of
                                                                                          65
each adjective preceded or followed by a word
                                                                                          60
matching an adjective-tag: c(a1 J.*), c(J.* a1 ),                                               100     1e3        1e4       1e5
c(a2 J.*), c(J.* a2 ). These assess the positional                                               Number of training examples
preferences of each adjective. Finally, we include
the log-frequency of each adjective. The more fre-                        Figure 2: Out-of-domain learning curve of adjec-
quent adjective occurs first 57% of the time.                             tive ordering classifiers on Gutenberg.
   As in all tasks, the counts are features in a clas-
sifier, so the importance of the different patterns is                    examples, all differences are highly significant.
weighted discriminatively during training.                                   Out-of-domain, L EX ’s accuracy drops a shock-
                                                                          ing 23% on Gutenberg and 19% on Medline (Ta-
3.2    Adjective Ordering Results
                                                                          ble 1). Malouf (2000)’s system fares even worse.
In-domain, with both feature classes, we set a                            The overlap between training and test pairs helps
strong new standard on this data: 93.7% accuracy                          explain. While 59% of the BNC test pairs were
for the N- GM +L EX system (Table 1). We trained                          seen in the training corpus, only 25% of Gutenberg
and tested Malouf (2000)’s program on our data;                           and 18% of Medline pairs were seen in training.
our L EX classifier, which also uses no auxiliary                            While other ordering models have also achieved
corpus, makes 18% fewer errors than Malouf’s                              “very poor results” out-of-domain (Mitchell,
system. Our web-based N- GM model is also su-                             2009), we expected our expanded set of L EX fea-
perior to the direct evidence web-based approach                          tures to provide good generalization on new data.
of Lapata and Keller (2005), scoring 90.0% vs.                            Instead, L EX is very unreliable on new domains.
87.1% accuracy. These results show the benefit                               N- GM features do not rely on specific pairs in
of our new lexicalized and web-based features.                            training data, and thus remain fairly robust cross-
   Figure 1 gives the in-domain learning curve.                           domain. Across the three test sets, 84-89% of
With fewer training examples, the systems with                            examples had the correct ordering appear at least
N- GM features strongly outperform the L EX -only                         once on the web. On new domains, the learned
system. Note that with tens of thousands of test                          N- GM system maintains an advantage over the un-
    5
      In this notation, capital letters (and regular expressions)         supervised c(a1 , a2 ) vs. c(a2 , a1 ), but the differ-
are matched against tags while a1 and a2 match words.                     ence is reduced. Note that training with 10-fold


                                                                    868


cross validation, the N- GM system can achieve up            System                                    IN     O1     O2
to 87.5% on Gutenberg (90.0% for N- GM + L EX ).             Baseline                                 66.9   44.6   60.6
   The learning curve showing performance on                 Lapata and Keller (2005)                 88.4   78.0   87.4
Gutenberg (but still training on BNC) is particu-            Bergsma et al. (2009)                    94.8   87.7   94.2
larly instructive (Figure 2, performance on Med-             SVM with N- GM features                  95.7   92.1   93.9
line is very similar). The L EX system performs              SVM with L EX features                   95.2   85.8   91.0
much worse than the web-based models across                  SVM with N- GM + L EX                    96.5   91.9   94.8
all training sizes. For our top in-domain sys-
tem, N- GM + L EX , as you add more labeled ex-             Table 2: Spelling correction accuracy (%). SVM
amples, performance begins decreasing out-of-               trained on NYT, tested on NYT (IN) and out-of-
domain. The system disregards the robust N-gram             domain Gutenberg (O1) and Medline (O2).
counts as it is more and more confident in the L EX                         100
features, and it suffers the consequences.
                                                                            95
4 Context-Sensitive Spelling Correction
                                                                            90




                                                             Accuracy (%)
We now turn to the generation problem of context-
                                                                            85
sensitive spelling correction. For every occurrence
of a word in a pre-defined set of confusable words                          80
(like peace and piece), the system must select the
most likely word from the set, flagging possible                            75                      N-GM+LEX
                                                                                                        N-GM
usage errors when the predicted word disagrees                                                           LEX
                                                                            70
with the original. Contextual spell checkers are                                  100       1e3           1e4       1e5
one of the most widely used NLP technologies,                                           Number of training examples
reaching millions of users via compressed N-gram
models in Microsoft Office (Church et al., 2007).           Figure 3: In-domain learning curve of spelling
   Our in-domain examples are from the New York             correction classifiers on NYT.
Times (NYT) portion of Gigaword, from Bergsma
et al. (2009). They include the 5 confusion sets            4.1 Supervised Spelling Correction
where accuracy was below 90% in Golding and                 Our L EX features are typical disambiguation fea-
Roth (1999). There are 100K training, 10K devel-            tures that flag specific aspects of the context. We
opment, and 10K test examples for each confusion            have features for the words at all positions in
set. Our results are averages across confusion sets.        a 9-word window (called collocation features by
   Out-of-domain examples are again drawn from              Golding and Roth (1999)), plus indicators for a
Gutenberg and Medline. We extract all instances             particular word preceding or following the con-
of words that are in one of our confusion sets,             fusable word. We also include indicators for all
along with surrounding context. By assuming the             N-grams, and their position, in a 9-word window.
extracted instances represent correct usage, we la-            For N- GM count features, we follow Bergsma
bel 7.8K and 56K out-of-domain test examples for            et al. (2009). We include the log-counts of all
Gutenberg and Medline, respectively.                        N-grams that span the confusable word, with each
   We test three unsupervised systems: 1) Lapata            word in the confusion set filling the N-gram pat-
and Keller (2005) use one token of context on the           tern. These features do not use part-of-speech.
left and one on the right, and output the candidate         Following Bergsma et al. (2009), we get N-gram
from the confusion set that occurs most frequently          counts using the original Google N-gram Corpus.
in this pattern. 2) Bergsma et al. (2009) measure              While neither our L EX nor N- GM features are
the frequency of the candidates in all the 3-to-5-          novel on their own, they have, perhaps surpris-
gram patterns that span the confusable word. For            ingly, not yet been evaluated in a single model.
each candidate, they sum the log-counts of all pat-
terns filled with the candidate, and output the can-        4.2 Spelling Correction Results
didate with the highest total. 3) The baseline pre-         The N- GM features outperform the L EX features,
dicts the most frequent member of each confusion            95.7% vs. 95.2% (Table 2). Together, they
set, based on frequencies in the NYT training data.         achieve a very strong 96.5% in-domain accuracy.


                                                      869


This is 2% higher than the best unsupervised ap-              System                                  IN     O1        O2
proach (Bergsma et al., 2009). Web-based models               Baseline                               70.5   66.8      84.1
again perform well across a range of training data            Dependency model                       74.7   82.8      84.4
sizes (Figure 3).                                             SVM with N- GM features                89.5   81.6      86.2
    The error rate of L EX nearly triples on Guten-           SVM with L EX features                 81.1   70.9      79.0
berg and almost doubles on Medline (Table 2). Re-             SVM with N- GM + L EX                  91.6   81.6      87.4
moving N- GM features from the N- GM + L EX sys-
                                                             Table 3: NC-bracketing accuracy (%). SVM
tem, errors increase around 75% on both Guten-
berg and Medline. The L EX features provide no               trained on WSJ, tested on WSJ (IN) and out-of-
help to the combined system on Gutenberg, while              domain Grolier (O1) and Medline (O2).
they do help significantly on Medline. Note the                              100
learning curves for N- GM +L EX on Gutenberg and                                N-GM+LEX
                                                                             95     N-GM
Medline (not shown) do not display the decrease                                      LEX
                                                                             90
that we observed in adjective ordering (Figure 2).




                                                              Accuracy (%)
                                                                             85
    Both the baseline and L EX perform poorly on
                                                                             80
Gutenberg. The baseline predicts the majority
class from NYT, but it’s not always the majority                             75
class in Gutenberg. For example, while in NYT                                70
site occurs 87% of the time for the (cite, sight,                            65
site) confusion set, sight occurs 90% of the time in                         60
Gutenberg. The L EX classifier exploits this bias as                               10           100             1e3
it is regularized toward a more economical model,                                       Number of labeled examples
but the bias does not transfer to the new domain.
                                                             Figure 4: In-domain NC-bracketer learning curve
5 Noun Compound Bracketing
                                                             from sections 0-22 of the Treebank as training, 72
About 70% of web queries are noun phrases (Barr              from section 24 for development and 95 from sec-
et al., 2008) and methods that can reliably parse            tion 23 as a test set. As out-of-domain data, we
these phrases are of great interest in NLP. For              use 244 NCs from Grolier Encyclopedia (Lauer,
example, a web query for zebra hair straightener             1995a) and 429 NCs from Medline (Nakov, 2007).
should be bracketed as (zebra (hair straightener)),             The majority class baseline is left-bracketing.
a stylish hair straightener with zebra print, rather
than ((zebra hair) straightener), a useless product          5.1 Supervised Noun Bracketing
since the fur of zebras is already quite straight.           Our L EX features indicate the specific noun at
   The noun compound (NC) bracketing task is                 each position in the compound, plus the three pairs
usually cast as a decision whether a 3-word NC               of nouns and the full noun triple. We also add fea-
has a left or right bracketing. Most approaches are          tures for the capitalization pattern of the sequence.
unsupervised, using a large corpus to compare the               N- GM features give the log-count of all subsets
statistical association between word pairs in the            of the compound. Counts are from Google V2.
NC. The adjacency model (Marcus, 1980) pro-                  Following Nakov and Hearst (2005), we also in-
poses a left bracketing if the association between           clude counts of noun pairs collapsed into a single
words one and two is higher than between two                 token; if a pair occurs often on the web as a single
and three. The dependency model (Lauer, 1995a)               unit, it strongly indicates the pair is a constituent.
compares one-two vs. one-three. We include de-                  Vadas and Curran (2007a) use simpler features,
pendency model results using PMI as the associ-              e.g. they do not use collapsed pair counts. They
ation measure; results were lower with the adja-             achieve 89.9% in-domain on WSJ and 80.7% on
cency model.                                                 Grolier. Vadas and Curran (2007b) use compara-
   As in-domain data, we use Vadas and Curran                ble features to ours, but do not test out-of-domain.
(2007a)’s Wall-Street Journal (WSJ) data, an ex-
tension of the Treebank (which originally left NPs           5.2 Noun Compound Bracketing Results
flat). We extract all sequences of three consec-             N- GM systems perform much better on this task
utive common nouns, generating 1983 examples                 (Table 3). N- GM +L EX is statistically significantly


                                                       870


better than L EX on all sets. In-domain, errors                       examples from the Brown portion of the Treebank
more than double without N- GM features. L EX                         and 6296 examples from tagged Medline abstracts
performs poorly here because there are far fewer                      in the PennBioIE corpus (Kulick et al., 2004).
training examples. The learning curve (Figure 4)                         The majority class baseline is to choose VBD.
looks much like earlier in-domain curves (Fig-
ures 1 and 3), but truncated before L EX becomes                      6.1 Supervised Verb Disambiguation
competitive. The absence of a sufficient amount of                    There are two orthogonal sources of information
labeled data explains why NC-bracketing is gen-                       for predicting VBN/VBD: 1) the noun-verb pair,
erally regarded as a task where corpus counts are                     and 2) the context around the pair. Both N- GM
crucial.                                                              and L EX features encode both these sources.
   All web-based models (including the depen-
dency model) exceed 81.5% on Grolier, which                           6.1.1   L EX features
is the level of human agreement (Lauer, 1995b).                       For 1), we use indicators for the noun and verb,
N- GM + L EX is highest on Medline, and close                         the noun-verb pair, whether the verb is on an in-
to the 88% human agreement (Nakov and Hearst,                         house list of said-verb (like warned, announced,
2005). Out-of-domain, the L EX approach per-                          etc.), whether the noun is capitalized and whether
forms very poorly, close to or below the base-                        it’s upper-case. Note that in training data, 97.3%
line accuracy. With little training data and cross-                   of capitalized nouns are followed by a VBD and
domain usage, N-gram features are essential.                          98.5% of said-verbs are VBDs. For 2), we provide
                                                                      indicator features for the words before the noun
6 Verb Part-of-Speech Disambiguation                                  and after the verb.
Our final task is POS-tagging. We focus on one                        6.1.2   N- GM features
frequent and difficult tagging decision: the distinc-
                                                                      For 1), we characterize a noun-verb relation via
tion between a past-tense verb (VBD) and a past
                                                                      features for the pair’s distribution in Google V2.
participle (VBN). For example, in the troops sta-
                                                                      Characterizing a word by its distribution has a
tioned in Iraq, the verb stationed is a VBN; troops
                                                                      long history in NLP; we apply similar techniques
is the head of the phrase. On the other hand, for
                                                                      to relations, like Turney (2006), but with a larger
the troops vacationed in Iraq, the verb vacationed
                                                                      corpus and richer annotations. We extract the 20
is a VBD and also the head. Some verbs make the
                                                                      most-frequent N-grams that contain both the noun
distinction explicit (eat has VBD ate, VBN eaten),
                                                                      and the verb in the pair. For each of these, we con-
but most require context for resolution.
                                                                      vert the tokens to POS-tags, except for tokens that
   Conflating VBN/VBD is damaging because it af-
                                                                      are among the most frequent 100 unigrams in our
fects downstream parsers and semantic role la-
                                                                      corpus, which we include in word form. We mask
belers. The task is difficult because nearby POS
                                                                      the noun of interest as N and the verb of interest
tags can be identical in both cases. When the
                                                                      as V. This converted N-gram is the feature label.
verb follows a noun, tag assignment can hinge on
                                                                      The value is the pattern’s log-count. A high count
world-knowledge, i.e., the global lexical relation
                                                                      for patterns like (N that V), (N have V) suggests
between the noun and verb (E.g., troops tends to
                                                                      the relation is a VBD, while patterns (N that were
be the object of stationed but the subject of vaca-
                                                                      V), (N V by), (V some N) indicate a VBN. As al-
tioned).6 Web-scale N-gram data might help im-
                                                                      ways, the classifier learns the association between
prove the VBN/VBD distinction by providing rela-
                                                                      patterns and classes.
tional evidence, even if the verb, noun, or verb-
noun pair were not observed in training data.                            For 2), we use counts for the verb’s context co-
   We extract nouns followed by a VBN/VBD in the                      occurring with a VBD or VBN tag. E.g., we see
WSJ portion of the Treebank (Marcus et al., 1993),                    whether VBD cases like troops ate or VBN cases
getting 23K training, 1091 development and 1130                       like troops eaten are more frequent. Although our
                                                                      corpus contains many VBN/VBD errors, we hope
test examples from sections 2-22, 24, and 23, re-
                                                                      the errors are random enough for aggregate counts
spectively. For out-of-domain data, we get 21K
                                                                      to be useful. The context is an N-gram spanning
   6
     HMM-style taggers, like the fast TnT tagger used on our          the VBN/VBD. We have log-count features for all
web corpus, do not use bilexical features, and so perform es-
pecially poorly on these cases. One motivation for our work           five such N-grams in the (previous-word, noun,
was to develop a fast post-processor to fix VBN/VBD errors.           verb, next-word) quadruple. The log-count is in-


                                                                871


 System                                 IN     O1        O2          93.0% for L EX . With two views of an example,
 Baseline                              89.2   85.2      79.6         L EX is more likely to have domain-neutral fea-
 ContextSum                            92.5   91.1      90.4         tures to draw on. Data sparsity is reduced.
 SVM with N- GM features               96.1   93.4      93.8            Also, the Treebank provides an atypical num-
 SVM with L EX features                95.8   93.4      93.0         ber of labeled examples for analysis tasks. In a
 SVM with N- GM + L EX                 96.4   93.5      94.0         more typical situation with less labeled examples,
                                                                     N- GM strongly dominates L EX , even when two
Table 4: Verb-POS-disambiguation accuracy (%)
                                                                     views are used. E.g., with 2285 training exam-
trained on WSJ, tested on WSJ (IN) and out-of-                       ples, N- GM +L EX is statistically significantly bet-
domain Brown (O1) and Medline (O2).                                  ter than L EX on both out-of-domain sets.
                100                                                     All systems, however, perform log-linearly with
                       N-GM (N,V+context)                            training size. In other tasks we only had a handful
                        LEX (N,V+context)
                              N-GM (N,V)                             of N- GM features; here there are 21K features for
                95             LEX (N,V)                             the distributional patterns of N,V pairs. Reducing
 Accuracy (%)




                                                                     this feature space by pruning or performing trans-
                90
                                                                     formations may improve accuracy in and out-of-
                                                                     domain.
                85
                                                                     7 Discussion and Future Work
                80
                         100          1e3         1e4                Of all classifiers, L EX performs worst on all cross-
                         Number of training examples                 domain tasks. Clearly, many of the regularities
                                                                     that a typical classifier exploits in one domain do
Figure 5: Out-of-domain learning curve of verb                       not transfer to new genres. N- GM features, how-
disambiguation classifiers on Medline.                               ever, do not depend directly on training examples,
                                                                     and thus work better cross-domain. Of course, us-
dexed by the position and length of the N-gram.                      ing web-scale N-grams is not the only way to cre-
We include separate count features for contexts                      ate robust classifiers. Counts from any large auxil-
matching the specific noun and for when the noun                     iary corpus may also help, but web counts should
token can match any word tagged as a noun.                           help more (Lapata and Keller, 2005). Section 6.2
   ContextSum: We use these context counts in an                     suggests that another way to mitigate domain-
unsupervised system, ContextSum. Analogously                         dependence is having multiple feature views.
to Bergsma et al. (2009), we separately sum the                         Banko and Brill (2001) argue “a logical next
log-counts for all contexts filled with VBD and                      step for the research community would be to di-
then VBN, outputting the tag with the higher total.                  rect efforts towards increasing the size of anno-
                                                                     tated training collections.” Assuming we really do
6.2             Verb POS Disambiguation Results                      want systems that operate beyond the specific do-
As in all tasks, N- GM +L EX has the best in-domain                  mains on which they are trained, the community
accuracy (96.4%, Table 4). Out-of-domain, when                       also needs to identify which systems behave as in
N-grams are excluded, errors only increase around                    Figure 2, where the accuracy of the best in-domain
14% on Medline and 2% on Brown (the differ-                          system actually decreases with more training ex-
ences are not statistically significant). Why? Fig-                  amples. Our results suggest better features, such
ure 5, the learning curve for performance on Med-                    as web pattern counts, may help more than ex-
line, suggests some reasons. We omit N- GM +L EX                     panding training data. Also, systems using web-
from Figure 5 as it closely follows N- GM .                          scale unlabeled data will improve automatically as
   Recall that we grouped the features into two                      the web expands, without annotation effort.
views: 1) noun-verb (N,V) and 2) context. If we                         In some sense, using web counts as features
use just (N,V) features, we do see a large drop out-                 is a form of domain adaptation: adapting a web
of-domain: L EX (N,V) lags N- GM (N,V) even us-                      model to the training domain. How do we ensure
ing all the training examples. The same is true us-                  these features are adapted well and not used in
ing only context features (not shown). Using both                    domain-specific ways (especially with many fea-
views, the results are closer: 93.8% for N- GM and                   tures to adapt, as in Section 6)? One option may


                                                               872


be to regularize the classifier specifically for out-         Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
of-domain accuracy. We found that adjusting the                 Och, and Jeffrey Dean. 2007. Large language mod-
                                                                els in machine translation. In EMNLP.
SVM misclassification penalty (for more regular-
ization) can help or hurt out-of-domain. Other                Thorsten Brants. 2000. TnT – a statistical part-of-
regularizations are possible. In each task, there               speech tagger. In ANLP.
are domain-neutral unsupervised approaches. We
                                                              Andrew Carlson, Tom M. Mitchell, and Ian Fette.
could encode these systems as linear classifiers                2008. Data analysis project: Leveraging massive
with corresponding weights. Rather than a typical               textual corpora using n-gram statistics. Technial Re-
SVM that minimizes the weight-norm ||w|| (plus                  port CMU-ML-08-107.
the slacks), we could regularize toward domain-               Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
neutral weights. This regularization could be opti-             Compressing trigram language models with Golomb
mized on creative splits of the training data.                  coding. In EMNLP-CoNLL.

8 Conclusion                                                  Hal Daumé III. 2007. Frustratingly easy domain adap-
                                                                tation. In ACL.
We presented results on tasks spanning a range of
                                                              Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
NLP research: generation, disambiguation, pars-                 Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
ing and tagging. Using web-scale N-gram data                    A library for large linear classification. Journal of
improves accuracy on each task. When less train-                Machine Learning Research, 9.
ing data is used, or when the system is used on a             Dan Gildea. 2001. Corpus variation and parser perfor-
different domain, N-gram features greatly improve               mance. In EMNLP.
performance. Since most supervised NLP systems
do not use web-scale counts, further cross-domain             Andrew R. Golding and Dan Roth. 1999. A Winnow-
                                                                based approach to context-sensitive spelling correc-
evaluation may reveal some very brittle systems.                tion. Machine Learning, 34(1-3):107–130.
Continued effort in new domains should be a pri-
ority for the community going forward.                        Thorsten Joachims. 2002. Optimizing search engines
                                                                using clickthrough data. In KDD.
Acknowledgments
                                                              Frank Keller and Mirella Lapata. 2003. Using the web
We gratefully acknowledge the Center for Lan-                   to obtain frequencies for unseen bigrams. Computa-
guage and Speech Processing at Johns Hopkins                    tional Linguistics, 29(3):459–484.
University for hosting the workshop at which part             Adam Kilgarriff and Gregory Grefenstette. 2003. In-
of this research was conducted.                                 troduction to the special issue on the Web as corpus.
                                                                Computational Linguistics, 29(3):333–347.

References                                                    Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
                                                                Ryan McDonald, Martha Palmer, Andrew Schein,
Nir Ailon and Mehryar Mohri. 2008. An efficient re-             Lyle Ungar, Scott Winters, and Pete White. 2004.
  duction of ranking to classification. In COLT.                Integrated annotation for biomedical information ex-
                                                                traction. In BioLINK 2004: Linking Biological Lit-
Michele Banko and Eric Brill. 2001. Scaling to very             erature, Ontologies and Databases.
  very large corpora for natural language disambigua-
  tion. In ACL.                                               Mirella Lapata and Frank Keller. 2005. Web-based
                                                                models for natural language processing. ACM
Cory Barr, Rosie Jones, and Moira Regelson. 2008.               Transactions on Speech and Language Processing,
  The linguistic structure of English web-search                2(1):1–31.
  queries. In EMNLP.
                                                              Mark Lauer. 1995a. Corpus statistics meet the noun
Shane Bergsma, Dekang Lin, and Randy Goebel.                   compound: Some empirical results. In ACL.
  2009. Web-scale N-gram models for lexical disam-
  biguation. In IJCAI.                                        Mark Lauer. 1995b. Designing Statistical Language
                                                               Learners: Experiments on Compound Nouns. Ph.D.
John Blitzer, Mark Dredze, and Fernando Pereira.               thesis, Macquarie University.
  2007. Biographies, bollywood, boom-boxes and
  blenders: Domain adaptation for sentiment classi-           Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
  fication. In ACL.                                             David Yarowsky, Shane Bergsma, Kailash Patil,
                                                                Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Thorsten Brants and Alex Franz. 2006. The Google                Dalwani, and Sushant Narsale. 2010. New tools for
  Web 1T 5-gram Corpus Version 1.1. LDC2006T13.                 web-scale N-grams. In LREC.


                                                        873


Robert Malouf. 2000. The order of prenominal adjec-
  tives in natural language generation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary
  Marcinkiewicz. 1993. Building a large annotated
  corpus of English: The Penn Treebank. Computa-
  tional Linguistics, 19(2):313–330.
Mitchell P. Marcus. 1980. Theory of Syntactic Recog-
  nition for Natural Languages. MIT Press, Cam-
  bridge, MA, USA.
David McClosky, Eugene Charniak, and Mark John-
  son. 2006. Reranking and self-training for parser
  adaptation. In COLING-ACL.
Margaret Mitchell. 2009. Class-based ordering of
 prenominal modifiers. In 12th European Workshop
 on Natural Language Generation.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
  sim. 2003. Using the Web in machine learning for
  other-anaphora resolution. In EMNLP.
Preslav Nakov and Marti Hearst. 2005. Search engine
  statistics beyond the n-gram: Application to noun
  compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
  plicit Training Set: Application to Noun Compound
  Syntax and Semantics. Ph.D. thesis, University of
  California, Berkeley.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
  POS Tagger. crftagger.sourceforge.net.
Laura Rimell and Stephen Clark. 2008. Adapting a
  lexicalized-grammar parser to contrasting domains.
  In EMNLP.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
  dering among premodifiers. In ACL.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
  Tomoko Ohta, John McNaught, Sophia Ananiadou,
  and Jun’ichi Tsujii. 2005. Developing a robust part-
  of-speech tagger for biomedical text. In Advances in
  Informatics.
Peter D. Turney. 2006. Similarity of semantic rela-
  tions. Computational Linguistics, 32(3):379–416.
David Vadas and James R. Curran. 2007a. Adding
  noun phrase structure to the Penn Treebank. In ACL.
David Vadas and James R. Curran. 2007b. Large-scale
  supervised models for noun phrase bracketing. In
  PACLING.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
  Improving pronoun resolution using statistics-based
  semantic compatibility information. In ACL.




                                                        874

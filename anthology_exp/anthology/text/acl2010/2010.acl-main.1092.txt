                                          String Extension Learning

                                                    Jeffrey Heinz
                                                University of Delaware
                                                Newark, Delaware, USA
                                                 heinz@udel.edu



                         Abstract                                       (Strictly Local, SL) (McNaughton and Papert,
                                                                        1971; Rogers and Pullum, to appear), the Piece-
       This paper provides a unified, learning-                         wise Testable (PT) languages (Simon, 1975), the
       theoretic analysis of several learnable                          Piecewise Testable languages in the Strict Sense
       classes of languages discussed previously                        (Strictly Piecewise, SP) (Rogers et al., 2009), the
       in the literature. The analysis shows that                       Strongly Testable languages (Beauquier and Pin,
       for these classes an incremental, globally                       1991), the Definite languages (Brzozowski, 1962),
       consistent, locally conservative, set-driven                     and the Finite languages, among others. To our
       learner always exists. Additionally, the                         knowledge, this is the first analysis which identi-
       analysis provides a recipe for constructing                      fies the common structural elements of these lan-
       new learnable classes. Potential applica-                        guage classes which allows them to be identifiable
       tions include learnable models for aspects                       in the limit from positive data: each language class
       of natural language and cognition.                               induces a natural partition over all logically possi-
                                                                        ble strings and each language in the class is the
1 Introduction
                                                                        union of finitely many blocks of this partition.
The problem of generalizing from examples to                               One consequence of this analysis is a recipe
patterns is an important one in linguistics and                         for constructing new learnable classes. One no-
computer science. This paper shows that many                            table case is the Strictly Piecewise (SP) languages,
disparate language classes, many previously dis-                        which was originally motivated for two reasons:
cussed in the literature, have a simple, natural                        the learnability properties discussed here and its
and interesting (because non-enumerative) learner                       ability to describe long-distance dependencies in
which exactly identifies the class in the limit from                    natural language phonology (Heinz, 2007; Heinz,
distribution-free, positive evidence in the sense of                    to appear). Later this class was discovered to have
Gold (Gold, 1967).1 These learners are called                           several independent characterizations and form
String Extension Learners because each string in                        the basis of another subregular hierarchy (Rogers
the language can be mapped (extended) to an ele-                        et al., 2009).
ment of the grammar, which in every case, is con-                          It is expected string extension learning will have
ceived as a finite set of elements. These learners                      applications in linguistic and cognitive models. As
have desirable properties: they are incremental,                        mentioned, the SP languages already provide a
globally consistent, and locally conservative.                          novel hypothesis of how long-distance dependen-
   Classes previously discussed in the litera-                          cies in sound patterns are learned. Another exam-
ture which are string extension learnable in-                           ple is the Strictly Local (SL) languages which are
clude the Locally Testable (LT) languages, the                          the categorical, symbolic version of n-gram mod-
Locally Testable Languages in the Strict Sense                          els, which are widely used in natural language pro-
   1
     The allowance of negative evidence (Gold, 1967) or re-             cessing (Jurafsky and Martin, 2008). Since the SP
stricting the kinds of texts the learner is required to succeed         languages also admit a probabilistic variant which
on (i.e. non-distribution-free evidence) (Gold, 1967; Horn-             describe an efficiently estimable class of distribu-
ing, 1969; Angluin, 1988) admits the learnability of the class
of recursively enumerable languages. Classes of languages               tions (Heinz and Rogers, 2010), it is plausible to
learnable in the harder, distribution-free, positive-evidence-          expect the other classes will as well, though this is
only settings are due to structural properties of the language
classes that permit generalization (Angluin, 1980b; Blumer
                                                                        left for future research.
et al., 1989). That is the central interest here.                          String extension learners are also simple, mak-


                                                                  897
           Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 897–906,
                    Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


ing them accessible to linguists without a rigorous             The content of a text is defined below.
mathematical background.
   This paper is organized as follow. §2 goes                   content(t) =
over basic notation and definitions. §3 defines                       {w ∈ Σ∗ : ∃n ∈   N such that t(n) = w}
string extension grammars, languages, and lan-
guage classes and proves some of their fundamen-                  A text t is a positive text for a language L iff
tal properties. §4 defines string extension learn-            content(t) = L. Thus there is only one text t for
ers and proves their behavior. §5 shows how im-               the empty language: for all i, t(i) = #.
portant subregular classes are string extension lan-              A learner is a function φ which maps ini-
guage classes. §6 gives examples of nonregular                tial finite sequences of texts to grammars,
and infinite language classes which are string ex-            i.e. φ : SEQ → G. The elements of G (the gram-
tension learnable. §7 summarizes the results, and             mars) generate languages in some well-defined
discusses lines of inquiry for future research.               way. A learner converges on a text t iff there exists

2 Preliminaries
                                                              i ∈ N     and a grammar G such that for all j > i,
                                                              φ(t[j]) = G.
This section establishes notation and recalls basic               For any grammar G, the language it generates is
definitions for formal languages, the paradigm of             denoted L(G). A learner φ identifies a language
identification in the limit from positive data (Gold,         L in the limit iff for any positive text t for L, φ
1967). Familiarity with the basic concepts of sets,           converges on t to grammar G and L(G) = L. Fi-
functions, and sequences is assumed.                          nally, a learner φ identifies a class of languages L
   For some set A, P(A) denotes the set of all                in the limit iff for any L ∈ L, φ identifies L in
subsets of A and Pf in (A) denotes the set of all             the limit. Angluin (1980b) provides necessary and
finite subsets of A. If f is a function such that             sufficient properties of language classes which are
f : A → B then let f ⋄ (a) = {f (a)}. Thus,                   identifiable in the limit from positive data.
f ⋄ : A → P(B) (note f ⋄ is not surjective). A                    A learner φ of language class L is globally con-
set π of nonempty subsets of S is a partition of S            sistent iff for each i and for all texts t for some
iff the elements of π (called blocks) are pairwise            L ∈ L, content(t[i]) ⊆ L(φ(t[i])). A learner φ is
disjoint and their union equals S.                            locally conservative iff for each i and for all texts
   Σ denotes a fixed finite set of symbols, the al-           t for some L ∈ L, whenever φ(t[i]) 6= φ(t[i − 1]),
phabet. Let Σn , Σ≤n , Σ∗ , Σ+ denote all strings             it is the case that t(i) 6∈ L(φ([i−1])). These terms
formed over this alphabet of length n, of length              are from Jain et al. (2007). Also, learners which
less than or equal to n, of any finite length, and            do not depend on the order of the text are called
of any finite length strictly greater than zero, re-          set-driven (Jain et al., 1999, p. 99).
spectively. The term word is used interchangeably
with string. The range of a string w is the set               3 Grammars and Languages
of symbols which are in w. The empty string is                Consider some set A. A string extension function
the unique string of length zero denoted λ. Thus              is a total function f : Σ∗ → Pf in (A). It is not
range(λ) = ∅. The length of a string u is de-                 required that f be onto. Denote the class of func-
noted by |u|, e.g. |λ| = 0. A language L is                   tions which have this general form SEF.
some subset of Σ∗ . The reverse of a language
                                                                 Each string extension function is naturally as-
Lr = {wr : w ∈ L}.
                                                              sociated with some formal class of grammars and
   Gold (1967) establishes a learning paradigm
                                                              languages. These functions, grammars, and lan-
known as identification in the limit from positive
                                                              guages are called string extension functions, gram-
data. A text is an infinite sequence whose ele-
                                                              mars, and languages, respectively.
ments are drawn from Σ∗ ∪ {#} where # rep-
resents a non-expression. The ith element of t is             Definition 1 Let f ∈ SEF.
denoted t(i), and t[i] denotes the finite sequence
t(0), t(1), . . . t(i). Following Jain et al. (1999),           1. A grammar is a finite subset of A.
let SEQ denote the set of all possible finite se-
                                                                2. The language of grammar G is
quences:

      SEQ = {t[i] : t is a text and i ∈   N}                             Lf (G) = {w ∈ Σ∗ : f (w) ⊆ G}


                                                        898


  3. The class of languages obtained by all possi-           result can now be proved, used in the next section
     ble grammars is                                         on learning.2
                                                             Theorem 3 For any finite L0 ⊆ Σ∗ , L =
            Lf = {Lf (G) : G ∈ Pf in (A)}
                                                             L(f (L0 )) is the smallest language in Lf contain-
                                                             ing L0 .
The subscript f is omitted when it is understood
from context.                                                Proof: Clearly L0 ⊆ L. Suppose L′ ∈ Lf and
    A function f ∈ SEF naturally induces a par-              L0 ⊆ L′ . It follows directly from Lemma 1 that
tition πf over Σ∗ . Strings u and v are equivalent           L ⊆ L′ (since f (L) = f (L0 ) ⊆ f (L′ )).    2
(u ∼f v) iff f (u) = f (v).
                                                             4 String Extension Learning
Theorem 1 Every language L ∈ Lf is a finite
union of blocks of πf .                                      Learning string extension classes is simple. The
                                                             initial hypothesis of the learner is the empty gram-
Proof: Follows directly from the definition of ∼f
                                                             mar. The learner’s next hypothesis is obtained by
and the finiteness of string extension grammars. 2
                                                             applying function f to the current observation and
We return to this result in §6.                              taking the union of that set with the previous one.
Theorem 2 Lf is closed under intersection.                   Definition 2 For all f ∈ SEF and for all t ∈
Proof: We show L1 ∩L2 = L(G1 ∩G2 ). Consider                 SEQ, define φf as follows:
any word w belonging to L1 and L2 . Then f (w)                           
is a subset of G1 and of G2 . Thus f (w) ⊆ G1 ∩                           ∅                                   if i = −1
                                                             φf (t[i]) =   φ (t[i − 1])                        if t(i) = #
G2 , and therefore w ∈ L(G1 ∩ G2 ). The other                             f
                                                                           φf (t[i − 1]) ∪ f (t(i))            otherwise
inclusion follows similarly.                  2
String extension language classes are not in gen-               By convention, the initial state of the grammar
eral closed under union or reversal (counterexam-            is given by φ(t[−1]) = ∅. The learner φf exem-
ples to union closure are given in §5.1 and to re-           plifies string extension learning. Each individual
versal closure in §6.)                                       string in the text reveals, by extension with f , as-
   It is useful to extend the domain of the function         pects of the canonical grammar for L ∈ Lf .
f from strings to languages.
                                                             Theorem 4 φf is globally consistent, locally con-
                            [                                servative, and set-driven.
                  f (L) =      f (w)             (1)
                          w∈L                                Proof: Global consistness and local conservative-
                                                             ness follow immediately from Definition 2. For
   An element g of grammar G for language L =                set-drivenness, witness (by Definition 2) it is the
Lf (G) is useful iff g ∈ f (L). An element is use-
less if it is not useful. A grammar with no useless
                                                                                                           N
                                                             case that for any text t and any i ∈ , φ(t[i]) =
                                                             f (content(t[i])).                               2
elements is called canonical.
                                                                The key to the proof that φf identifies Lf in the
Remark 1 Fix a function f ∈ SEF. For every
                                                             limit from positive data is the finiteness of G for
L ∈ Lf , there is a canonical grammar, namely
                                                             all L(G) ∈ L. The idea is that there is a point
f (L). In other words, L = L(f (L)).
                                                             in the text in which every element of the grammar
Lemma 1 Let L, L′ ∈ Lf . L ⊆ L′ iff f (L) ⊆                  has been seen because (1) there are only finitely
f (L′ )                                                      many useful elements of G, and (2) the learner is
Proof: (⇒) Suppose L ⊆ L′ and consider any                   guaranteed to see a word in L which yields (via f )
g ∈ f (L). Since g is useful, there is a w ∈ L such          each element of G at some point (since the learner
that g ∈ f (w). But f (w) ⊆ f (L′ ) since w ∈ L′ .           receives a positive text for L). Thus at this point
   (⇐) Suppose f (L) ⊆ f (L′ ) and consider any                  2
                                                                   The requirement in Theorem 3 that L0 be finite can be
w ∈ L. Then f (w) ⊆ f (L) so by transitivity,                dropped if the qualifier “in Lf ” be dropped as well. This
f (w) ⊆ f (L′ ). Therefore w ∈ L′ .               2          can be seen when one considers the identity function and the
                                                             class of finite languages. (The identity function is a string
The significance of this result is that as the gram-         extension function, see §6.) In this case, id(Σ∗ ) = Σ∗ , but
                                                             Σ∗ is not a member of Lf in . However since the interest here
mar G monotonically increases, the language                  is learners which generalize on the basis of finite experience,
L(G) monotonically increases too. The following              Theorem 3 is sufficient as is.


                                                       899


the learner φ is guaranteed to have converged to                and Rogers et al. (2009) for an introduction to the
the target G as no additional words will add any                subregular hierarchies, as well as their relevance
more elements to the learner’s grammar.                         to linguistics and cognition.
Lemma 2 For all L ∈ Lf , there is a finite sample               5.1 K-factor languages
S such that L is the smallest language in Lf con-
                                                                The k-factors of a word are the contiguous subse-
taining S. S is called a characteristic sample of L
                                                                quences of length k in w. Consider the following
in Lf (S is also called a tell-tale).
                                                                string extension function.
Proof: For L ∈ Lf , construct the sample S as                                                N
                                                                Definition 3 For some k ∈ , let
follows. For each g ∈ f (L), choose some word
w ∈ L such that g ∈ f (w). Since f (L) is finite                  f ack (w) =
(Remark 1), S is finite. Clearly f (S) = f (L) and                    {x ∈ Σk : ∃u, v ∈ Σ∗
thus L = L(f (S)). Therefore, by Theorem 3, L is                        such that w = uxv} when k ≤ |w| and
the smallest language in Lf containing S.        2                    {w} otherwise
Theorem 5 Fix f ∈ SEF. Then φf identifies Lf
                                                                   Following the earlier definitions, for some k, a
in the limit.
                                                                grammar G is a subset of Σ≤k and a word w be-
Proof: For any L ∈ Lf , there is a characteristic fi-           longs to the language of G iff f ack (w) ⊆ G.
nite sample S for L (Lemma 2). Thus for any text t              Example 1 Let Σ = {a, b} and consider gram-
for L, there is i such that S ⊆ content(t[i]). Thus             mars G = {λ, a, aa, ab, ba}. Then L(G) =
for any j > i, φ(t(j)) is the smallest language                 {λ, a} ∪ {w : |w| ≥ 2 and w 6∈ Σ∗ bbΣ∗ }. The 2-
in Lf containing S by Theorem 3 and Lemma 2.                    factor bb is a prohibited 2-factor for L(G). Clearly,
Thus, φ(t(j)) = f (S) = f (L).                     2            L(G) ∈ Lf ac2 .
   An immediate corollary is the efficiency of φf                  Languages in Lf ack make distinctions based on
in the length of the sample, provided f is efficient            which k-factors are permitted or prohibited. Since
in the length of the string (de la Higuera, 1997).              f ack ∈ SEF , it follows immediately from the
Corollary 1 φf is efficient in the length of the                results in §§3-4 that the k-factor languages are
sample iff f is efficiently computable in the length            closed under intersection, and each has a char-
of a string.                                                    acteristic sample. For example, a characteristic
                                                                sample for the 2-factor language in Example 1 is
   To summarize: string extension grammars are
                                                                {λ, a, ab, ba, aa}; i.e. the canonical grammar it-
finite subsets of some set A. The class of lan-
                                                                self. It follows from Theorem 5 that the class of
guages they generate are determined by a func-
                                                                k-factor languages is identifiable in the limit by
tion f which maps strings to finite subsets of A
                                                                φf ack . The learner φf ac2 with a text from the lan-
(chunks of grammars). Since the size of the canon-
                                                                guage in Example 1 is illustrated in Table 1.
ical grammars is finite, a learner which develops a
                                                                   The class Lf ack is not closed under
grammar on the basis of the observed words and
                                                                union.       For example for k = 2, con-
the function f identifies this class exactly in the
                                                                sider L1         =      L({λ, a, b, aa, bb, ba}) and
limit from positive data. It also follows that if f
                                                                L2 = L({λ, a, b, aa, ab, bb}). Then L1 ∪ L2
is efficient in the length of the string then φf is ef-
                                                                excludes string aba, but includes ab and ba, which
ficient in the length of the sample and that φf is
                                                                is not possible for any L ∈ Lf ack .
globally consistent, locally conservative, and set-
                                                                   K-factors are used to define other language
driven. It is striking that such a natural and gen-
                                                                classes, such as the Strictly Local and Lo-
eral framework for generalization exists and that,
                                                                cally Testable languages (McNaughton and Pa-
as will be shown, a variety of language classes can
                                                                pert, 1971), discussed in §5.4 and §5.5.
be expressed given the choice of f .
                                                                5.2 Strictly k-Piecewise languages
5 Subregular examples
                                                                The Strictly k-Piecewise (SPk ) languages (Rogers
This section shows how classes which make up                    et al., 2009) can be defined with a function whose
the subregular hierarchies (McNaughton and Pa-                  co-domain is P(Σ≤k ). However unlike the func-
pert, 1971) are string extension language classes.              tion f ack , the function SPk , does not require that
Readers are referred to Rogers and Pullum (2007)                the k-length subsequences be contiguous.


                                                          900


                            i    t(i)     f ac2 (t(i))       Grammar G          L(G)
                           -1                                ∅                  ∅
                            0    aaaa     {aa}               {aa}               aaa∗
                            1    aab      {aa, ab}           {aa, ab}           aaa∗ ∪ aaa∗ b
                            2    a        {a}                {a, aa, ab}        aa∗ ∪ aa∗ b
                           ...

Table 1: The learner φf ac2 with a text from the language in Example 1. Boldtype indicates newly added
elements to the grammar.


   A string u = a1 . . . ak is a subsequence of                    of length at most k and u is in L, then v is in L as
string w iff ∃ v0 , v1 , . . . vk ∈ Σ∗ such that w =               well (Simon, 1975; Simon, 1993; Lothaire, 2005).
v0 a1 v1 . . . ak vk . The empty string λ is a subse-                 A language L is said to be Piecewise-Testable
quence of every string. When u is a subsequence                    (PT) if it is k-Piecewise Testable for some k ∈ .       N
of w we write u ⊑ w.                                               If k is fixed, the k-Piecewise Testable languages
Definition 4 For some k ∈        N,                                are identifiable in the limit from positive data
                                                                   (Garcı́a and Ruiz, 1996; Garcı́a and Ruiz, 2004).
          SPk (w) = {u ∈ Σ≤k : u ⊑ w}                              More recently, the Piecewise Testable languages
                                                                   has been shown to be linearly separable with a
   In other words, SPk (w) returns all subse-                      subsequence kernel (Kontorovich et al., 2008).
quences, contiguous or not, in w up to length k.                      The k-Piecewise Testable languages can also
Thus, for some k, a grammar G is a subset of Σ≤k .                 be described with the function SPk⋄ . Recall that
Following Definition 1, a word w belongs to the                    f ⋄ (a) = {f (a)}. Thus functions SPk⋄ define
language of G only if SP2 (w) ⊆ G.3                                grammars as a finite list of sets of subsequences
                                                                   up to length k that may occur in words in the lan-
Example 2 Let Σ = {a, b} and consider the
                                                                   guage. This reflects the fact that the k-Piecewise
grammar G = {λ, a, b, aa, ab, ba}. Then L(G) =
                                                                   Testable languages are the boolean closure of the
Σ∗ \(Σ∗ bΣ∗ bΣ∗ ).
                                                                   Strictly k-Piecewise languages.4
As seen from Example 2, SP languages encode
long-distance dependencies. In Example 2, L pro-                   5.4 Strictly k-Local languages
hibits a b from following another b in a word, no                  To define the Strictly k-Local languages, it is nec-
matter how distant. Table 2 illustrates φSP2 learn-                essary to make a pointwise extension to the defini-
ing the language in Example 2.                                     tions in §3.
   Heinz (2007,2009a) shows that consonantal
                                                                   Definition 5 For sets A1 , . . . , An , suppose for
harmony patterns in natural language are describ-
                                                                   each i, fi : Σ∗ → Pf in (Ai ), and let f =
able by such SP2 languages and hypothesizes
                                                                   (f1 , . . . , fn ).
that humans learn them in the way suggested by
φSP2 . Strictly 2-Piecewise languages have also                      1. A grammar G is a tuple (G1 , . . . , Gn ) where
been used in models of reading comprehension                            G1 ∈ Pf in (A1 ), . . . , Gn ∈ Pf in (An ).
(Whitney, 2001; Grainger and Whitney, 2004;
Whitney and Cornelissen, 2008) as well as text                       2. If for any w ∈ Σ∗ , each fi (w) ⊆ Gi for all
classification(Lodhi et al., 2002; Cancedda et al.,                     1 ≤ i ≤ n, then f (w) is a pointwise subset
2003) (see also (Shawe-Taylor and Christianini,                         of G, written f (w) ⊆
                                                                                            · G.
2005, chap. 11)).
                                                                     3. The language of grammar G is
5.3    K-Piecewise Testable languages
                                                                                   Lf (G) = {w : f (w) ⊆
                                                                                                       · G}
A language L is k-Piecewise Testable iff when-
ever strings u and v have the same subsequences
                                                                     4. The class of languages obtained by all such
   3
    In earlier work, the function SP2 has been described                possible grammars G is Lf .
as returning the set of precedence relations in w, and the
                                                                      4
language class LSP2 was called the precedence languages                 More generally, it is not hard to show that Lf ⋄ is the
(Heinz, 2007; Heinz, to appear).                                   boolean closure of Lf .


                                                             901


                  i    t(i)    SP2 (t(i))          Grammar G                   Language of G
                 -1                                ∅                           ∅
                  0    aaaa    {λ, a, aa}          {λ, a, aa}                  a∗
                  1    aab     {λ, a, b, aa, ab}   {λ, a, aa, b, ab}           a∗ ∪ a∗ b
                  2    baa     {λ, a, b, aa, ba}   {λ, a, b, aa, ab, ba}       Σ∗ \(Σ∗ bΣ∗ bΣ∗ )
                  3    aba     {λ, a, b, ab, ba}   {λ, a, b, aa, ab, ba}       Σ∗ \(Σ∗ bΣ∗ bΣ∗ )
                 ...

Table 2: The learner φSP2 with a text from the language in Example 2. Boldtype indicates newly added
elements to the grammar.


   These definitions preserve the learning results            only if LRIk (w) ⊆   · G. Clearly, LLRIk = k-
of §4. Note that the characteristic sample of L ∈             SL, and henceforth we refer to this class as k-SL.
Lf will be the union of the characteristic samples            Since, for fixed k, LRIk ∈ SEF, all of the learn-
of each fi and the language Lf (G) is the intersec-           ing results in §4 apply.
tion of Lfi (Gi ).
   Locally k-Testable Languages in the Strict                 5.5 Locally k-Testable languages
Sense (Strictly k-Local) have been studied by sev-            The Locally k-testable languages (k-LT) are orig-
eral researchers (McNaughton and Papert, 1971;                inally defined in McNaughton and Papert (1971)
Garcia et al., 1990; Caron, 2000; Rogers and Pul-             and are the subject of several studies (Brzozowski
lum, to appear), among others. We follow the                  and Simon, 1973; McNaughton, 1974; Kim et
definitions from (McNaughton and Papert, 1971,                al., 1991; Caron, 2000; Garcı́a and Ruiz, 2004;
p. 14), effectively encoded in the following func-            Rogers and Pullum, to appear).
tions.                                                           A language L is k-testable iff for all w1 , w2 ∈
                       N
Definition 6 Fix k ∈ . Then the (left-edge) pre-              Σ∗ such that |w1 | ≥ k and |w2 | ≥ k, and
fix of length k, the (right-edge) suffix of length k,         LRIk (w1 ) = LRIk (w2 ) then either both w1 , w2
and the interior k-factors of a word w are                    belong to L or neither do. Clearly, every language
                                                              in k-SL belongs to k-LT. However k-LT prop-
Lk (w) = {u ∈ Σk : ∃v ∈ Σ∗ such that w = uv}
                                                              erly include k-SL because a k-testable language
Rk (w) = {u ∈ Σk : ∃v ∈ Σ∗ such that w = vu}                  only distinguishes words whenever LRIk (w1 ) 6=
      Ik (w) = f ack (w)\(Lk (w) ∪ Rk (w))                    LRIk (w2 ). It is known that the k-LT languages
                                                              are the boolean closure of the k-SL (McNaughton
Example 3 Suppose w = abcba. Then L2 (w) =
                                                              and Papert, 1971).
{ab}, R2 (w) = {ba} and I2 (w) = {bc, cb}.
                                                                 The function LRIk⋄ exactly expresses k-testable
Example 4 Suppose |w| = k. Then Lk (w) =                      languages. Informally, each word w is mapped
Rk (w) = {w} and Ik (w) = ∅.                                  to a set containing a single element, this element
Example 5 Suppose |w| is less than k.          Then           is the triple LRIk (w). Thus a grammar G is a
Lk (w) = Rk (w) = ∅ and Ik (w) = {w}.                         subset of the triples used to define k-SL. Clearly,
   A language L is k-Strictly Local (k-SL) iff for            LLRIk⋄ = k-LT since it is the boolean closure of
all w ∈ L, there exist sets L, R, and I such                  LLRIk . Henceforth we refer to LLRIk⋄ as the k-
that w ∈ L iff Lk (w) ⊆ L, Rk (w) ⊆ R, and                    Locally Testable (k-LT) languages.
Ik (w) ⊆ I. McNaughton and Papert note that if
w is of length less than k than L may be perfectly            5.6 Generalized subsequence languages
arbitrary about w.                                            Here we introduce generalized subsequence func-
   This can now be expressed as the string exten-             tions, a general class of functions to which the
sion function:                                                SPk and f ack functions belong. Like those
                                                              functions, generalized subsequence functions map
      LRIk (w) = (Lk (w), Rk (w), Ik (w))
                                                              words to a set of subsequences found within the
   Thus for some k, a grammar G is triple formed              words. These functions are instantiated by a vec-
by taking subsets of Σk , Σk , and Σ≤k , respec-              tor whose number of coordinates determine how
tively. A word w belongs to the language of G                 many times a subsequence may be discontiguous


                                                        902


and whose coordinate values determine the length                              grammar G         Language of G
of each contiguous part of the subsequence.                                   ∅                 ∅
Definition 7 For some n ∈               N, let ~v =
                                                                              {0}               an bn
                                         N
hv0 , v1 , . . . , vn i, where each vi ∈ . P Let k be
                                                                              {1}
                                                                              {0, 1}
                                                                                                Σ∗ \an bn
                                                                                                Σ∗
the length of the subsequences; i.e. k = n0 vi .
                                                                   Table 3: The language class Lf from Example 9
 f~v (w) =
      {u ∈ Σk : ∃x0 , . . . , xn , u0 , . . . , un+1 ∈ Σ∗
                                                                     In the examples considered so far, the enumera-
      such that w = u0 x0 u1 x1 , . . . , un xn un+1
                                                                  tion of the blocks is essentially encoded in partic-
      and |xi | = vi for all 0 ≤ i ≤ n}                           ular substrings (or tuples of substrings). However,
      when k ≤ |w|, and{w} otherwise                              much less clever enumerations are available.
                                                                  Example 9 Let A = {0,1} and consider the fol-
                                                                  lowing function:
   The following examples help make the general-
                                                                                                 iff w ∈ an bn
                                                                                        
ized subsequence functions clear.                                                           0
                                                                              f (w) =
Example 6 Let ~v = h2i. Then fh2i = f ac2 . Gen-                                            1    otherwise
erally, fhki = f ack .
                                                                  The function f belongs to SEF because it is maps
Example 7 Let ~v = h1, 1i. Then fh1,1i = SP2 .                    strings to a finite co-domain. Lf has four lan-
Generally, if ~v = h1, . . . 1i with |~v | = k. Then              guages shown in Table 3.
f~v = SPk .
                                                                  The language class in Example 9 is not regular be-
Example 8 Let ~v = h3, 2, 1i and a, b, c, d, e, f ∈               cause it includes the well-known context-free lan-
Σ. Then Lfh3,2,1i includes languages which                        guage an bn . This collection of languages is also
prohibit strings w which contain subsequences                     not closed under reversal.
abcdef where abc and de must be contiguous in                        There are also infinite language classes that are
w and abcdef is a subsequence of w.                               string extension language classes. Arguably the
                                                                  simplest example is the class of finite languages,
   Generalized subsequence languages make dif-
                                                                  denoted Lf in .
ferent kinds of distinctions to be made than PT and
LT languages. For example, the language in Ex-                    Example 10 Consider the function id which
ample 8 is neither k-LT nor k′ -PT for any values                 maps words in Σ∗ to their singleton sets, i.e.
k, k′ . Generalized subsequence languages prop-                   id(w) = {w}.5 A grammar G is then a finite
erly include the k-SP and k-SL classes (Exam-                     subset of Σ∗ , and so L(G) is just a finite set of
ples 6 and 7), and the boolean closure of the sub-                words in Σ∗ ; in fact, L(G) = G. It follows that
sequence languages (f~v⋄ ) properly includes the LT               Lid = Lf in .
and PT classes.                                                   It can be easily seen that the function id induces
   Since for any ~v , f~v and f~v⋄ are string extension           the trivial partition over Σ∗ , and languages are
functions the learning results in §4 apply. Note                  just finite unions of these blocks. The learner φid
that f~v (w) is computable in time O(|w|k ) where k               makes no generalizations at all, and only remem-
is the length of the maximal subsequences deter-                  bers what it has observed.
mined by ~v .                                                        There are other more interesting infinite string
                                                                  extension classes. Here is one relating to the
6 Other examples                                                  Parikh map (Parikh, 1966). For all a ∈ Σ, let
This section provides examples of infinite and                    fa (w) be the set containing n where n is the num-
nonregular language classes that are string exten-                ber of times the letter a occurs in the string w. For
sion learnable. Recall from Theorem 1 that string                     5
                                                                        Strictly speaking, this is not the identity function per
extension languages are finite unions of blocks of                se, but it is as close to the identity function as one can get
the partition of Σ∗ induced by f . Assuming the                   since string extension functions are defined as mappings from
                                                                  strings to sets. However, once the domain of the function is
blocks of this partition can be enumerated, the
                                          N
                                                                  extended (Equation 1), then it follows that id is the identity
range of f can be construed as Pf in ( ).                         function when its argument is a set of strings.


                                                            903


example fa (babab) = {2}. Thus fa is a total func-             this class is essentially two learners: φLRI3 and
tion mapping strings to singleton sets of natural              φP2 , operating simultaneously.6 The learners
numbers, so it is a string extension function. This            make predictions about generalizations, which can
function induces an infinite partition of Σ∗ , where           be tested in artificial language learning experi-
the words in any particular block have the same                ments on adults and infants (Rogers and Pullum, to
number of letters a. It is convenient to enumerate             appear; Chambers et al., 2002; Onishi et al., 2003;
the blocks according to how many occurrences of                Cristiá and Seidl, 2008).7
the letter a may occur in words within the block.                 For theoretical computer science, it remains an
Hence, B0 is the block whose words have no oc-                 open question what property holds of functions
currences of a, B1 is the block whose words have               f in SEF to ensure that Lf is regular, context-
one occurrence of a, and so on.                                free, or context-sensitive. For known subregular
                                                   N
   In this case, a grammar G is a finite subset of ,           classes, there are constructions that provide deter-
e.g. {2, 3, 4}. L(G) is simply those words which               ministic automata that suggest the relevant prop-
have either 2, 3, or 4, occurrences of the letter a.           erties. (See, for example, Garcia et al. (1990) and
Thus Lfa is an infinite class, which contains lan-             Garica and Ruiz (1996).)
guages of infinite size, which is easily identified in            Also, Timo Kötzing and Samuel Moelius (p.c.)
the limit from positive data by φfa .                          suggest that the results here may be generalized
   This section gave examples of nonregular and                along the following lines. Instead of defining the
nonfinite string extension classes by pursuing the             function f as a map from strings to finite subsets,
implications of Theorem 1, which established that              let f be a function from strings to elements of a
f ∈ SEF partition Σ∗ into blocks of which lan-                 lattice. A grammar G is an element of the lattice
guages are finite unions thereof. The string exten-            and the language of the G are all strings w such
sion function f provides an effective way of en-               that f maps w to a grammar less than G. Learners
coding all languages L in Lf because f (L) en-                 φf are defined as the least upper bound of its cur-
codes a finite set, the grammar.                               rent hypothesis and the grammar to which f maps
                                                               the current word.8 Kasprzik and Kötzing (2010)
7 Conclusion and open questions                                develop this idea and demonstrate additional prop-
                                                               erties of string extension classes and learning, and
One contribution of this paper is a unified way of             show that the pattern languages (Angluin, 1980a)
thinking about many formal language classes, all               form a string extension class.9
of which have been shown to be identifiable in                    Also, hyperplane learning (Clark et al., 2006a;
the limit from positive data by a string extension             Clark et al., 2006b) and function-distinguishable
learner. Another contribution is a recipe for defin-           learning (Fernau, 2003) similarly associate lan-
ing classes of languages identifiable in the limit             guage classes with functions. How those analyses
from positive data by this kind of learner.                    relate to the current one remains open.
   As shown, these learners have many desirable                   Finally, since the stochastic counterpart of k-
properties. In particular, they are globally consis-           SL class is the n-gram model, it is plausible that
tent, locally conservative, and set-driven. Addi-              probabilistic string extension language classes can
tionally, the learner is guaranteed to be efficient            form the basis of new natural language process-
in the size of the sample, provided the function f             ing techniques. (Heinz and Rogers, 2010) show
itself is efficient in the length of the string.                   6
                                                                     This learner resembles what learning theorists call par-
   Several additional questions of interest remain             allel learning (Case and Moelius, 2007) and what cognitive
open for theoretical linguistics, theoretical com-             scientists call modular learning (Gallistel and King, 2009).
                                                                   7
puter science, and computational linguistics.                        I conjecture that morphological and syntactic patterns
                                                               are generally not amenable to a string extension learning
   For theoretical linguistics, it appears that the            analysis because these patterns appear to require a paradigm,
string extension function f = (LRI3 , P2 ), which              i.e. a set of data points, before any conclusion can be confi-
defines a class of languages which obey restric-               dently drawn about the generating grammar. Stress patterns
                                                               also do not appear to be amenable to a string extension learn-
tions on both contiguous subsequences of length                ing (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009).
3 and on discontiguous subsequences of length 2,                   8
                                                                     See also Lange et al. (2008, Theorem 15) and Case et al.
provides a good first approximation to the seg-                (1999, pp.101-103).
                                                                   9
                                                                     The basic idea is to consider the lattice L = hLf in , ⊇i.
mental phonotactic patterns in natural languages               Each element of L is a finite set of strings representing the
(Heinz, 2007). The string extension learner for                intersection of all pattern languages consistent with this set.


                                                         904


how to efficiently estimate k-SP distributions, and            John Case, Sanjay Jain, Steffen Lange, and Thomas
it is conjectured that the other string extension lan-           Zeugmann. 1999. Incremental concept learning for
                                                                 bounded data mining. Information and Computa-
guage classes can be recast as classes of distri-
                                                                 tion, 152:74–110.
butions, which can also be successfully estimated
from positive evidence.                                        Kyle E. Chambers, Kristine H. Onishi, and Cynthia
                                                                 Fisher. 2002. Learning phonotactic constraints from
Acknowledgments                                                  brief auditory experience. Cognition, 83:B13–B23.

This work was supported by a University of                     Alexander Clark, Christophe Costa Florêncio, and
                                                                 Chris Watkins. 2006a. Languages as hyperplanes:
Delaware Research Fund grant during the 2008-                    grammatical inference with string kernels. In Pro-
2009 academic year. I would like to thank John                   ceedings of the European Conference on Machine
Case, Alexander Clark, Timo Kötzing, Samuel                     Learning (ECML), pages 90–101.
Moelius, James Rogers, and Edward Stabler for                  Alexander Clark, Christophe Costa Florêncio, Chris
valuable discussion. I would also like to thank                  Watkins, and Mariette Serayet. 2006b. Planar
Timo Kötzing for careful reading of an earlier                  languages and learnability. In Proceedings of the
draft and for catching some errors. Remaining er-                8th International Colloquium on Grammatical Infer-
                                                                 ence (ICGI), pages 148–160.
rors are my responsibility.
                                                               Alejandrina Cristiá and Amanda Seidl. 2008. Phono-
                                                                 logical features in infants phonotactic learning: Ev-
References                                                       idence from artificial grammar learning. Language,
                                                                 Learning, and Development, 4(3):203–227.
Dana Angluin. 1980a. Finding patterns common to
  a set of strings. Journal of Computer and System             Colin de la Higuera. 1997. Characteristic sets for poly-
  Sciences, 21:46–62.                                            nomial grammatical inference. Machine Learning,
                                                                 27:125–138.
Dana Angluin. 1980b. Inductive inference of formal
  languages from positive data. Information Control,           Matt Edlefsen, Dylan Leeman, Nathan Myers,
  45:117–135.                                                   Nathaniel Smith, Molly Visscher, and David Well-
Dana Angluin. 1988. Identifying languages from                  come. 2008. Deciding strictly local (SL) lan-
                                                                guages. In Jon Breitenbucher, editor, Proceedings
  stochastic examples. Technical Report 614, Yale
  University, New Haven, CT.                                    of the Midstates Conference for Undergraduate Re-
                                                                search in Computer Science and Mathematics, pages
D. Beauquier and J.E. Pin. 1991. Languages and scan-            66–73.
   ners. Theoretical Computer Science, 84:3–21.
                                                               Henning Fernau. 2003. Identification of function dis-
Anselm Blumer, Andrzej Ehrenfeucht, David Haus-                  tinguishable languages. Theoretical Computer Sci-
  sler, and Manfred K. Warmuth. 1989. Learnability               ence, 290:1679–1711.
  and the Vapnik-Chervonenkis dimension. J. ACM,
  36(4):929–965.                                               C.R. Gallistel and Adam Philip King. 2009. Memory
                                                                 and the Computational Brain. Wiley-Blackwell.
J.A. Brzozowski and I. Simon. 1973. Characterization
   of locally testable events. Discrete Math, 4:243–           Pedro Garcı́a and José Ruiz. 1996. Learning k-
   271.                                                          piecewise testable languages from positive data. In
                                                                 Laurent Miclet and Colin de la Higuera, editors,
J.A. Brzozowski. 1962. Canonical regular expres-                 Grammatical Interference: Learning Syntax from
   sions and minimal state graphs for definite events. In        Sentences, volume 1147 of Lecture Notes in Com-
   Mathematical Theory of Automata, pages 529–561.               puter Science, pages 203–210. Springer.
   New York.
                                                               Pedro Garcı́a and José Ruiz. 2004. Learning k-testable
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and                and k-piecewise testable languages from positive
  Jean-Michel Renders. 2003. Word-sequence ker-                  data. Grammars, 7:125–140.
  nels.  Journal of Machine Learning Research,
  3:1059–1082.                                                 Pedro Garcia, Enrique Vidal, and José Oncina. 1990.
                                                                 Learning locally testable languages in the strict
Pascal Caron. 2000. Families of locally testable lan-            sense. In Proceedings of the Workshop on Algorith-
  guages. Theoretical Computer Science, 242:361–                 mic Learning Theory, pages 325–338.
  376.
                                                               E.M. Gold. 1967. Language identification in the limit.
John Case and Sam Moelius. 2007. Parallelism                     Information and Control, 10:447–474.
  increases iterative learning power. In 18th An-
  nual Conference on Algorithmic Learning Theory               J. Grainger and C. Whitney. 2004. Does the huamn
  (ALT07), volume 4754 of Lecture Notes in Artificial             mnid raed wrods as a wlohe? Trends in Cognitive
  Intelligence, pages 49–63. Springer-Verlag, Berlin.             Science, 8:58–59.


                                                         905


Jeffrey Heinz and James Rogers. 2010. Estimating               Kristine H. Onishi, Kyle E. Chambers, and Cynthia
   strictly piecewise distributions. In Proceedings of           Fisher. 2003. Infants learn phonotactic regularities
   the ACL.                                                      from brief auditory experience. Cognition, 87:B69–
                                                                 B77.
Jeffrey Heinz. 2007. The Inductive Learning of
   Phonotactic Patterns. Ph.D. thesis, University of           R. J. Parikh. 1966. On context-free languages. Journal
   California, Los Angeles.                                       of the ACM, 13, 570581., 13:570–581.
Jeffrey Heinz. 2009. On the role of locality in learning       James Rogers and Geoffrey Pullum. to appear. Aural
   stress patterns. Phonology, 26(2):303–351.                    pattern recognition experiments and the subregular
                                                                 hierarchy. Journal of Logic, Language and Infor-
Jeffrey Heinz. to appear. Learning long distance                 mation.
   phonotactics. Linguistic Inquiry.
                                                               James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
J. J. Horning. 1969. A Study of Grammatical Infer-               sen, Molly Visscher, David Wellcome, and Sean
   ence. Ph.D. thesis, Stanford University.                      Wibel. 2009. On languages piecewise testable in
                                                                 the strict sense. In Proceedings of the 11th Meeting
Sanjay Jain, Daniel Osherson, James S. Royer, and
                                                                 of the Assocation for Mathematics of Language.
  Arun Sharma. 1999. Systems That Learn: An In-
  troduction to Learning Theory (Learning, Develop-            John Shawe-Taylor and Nello Christianini. 2005. Ker-
  ment and Conceptual Change). The MIT Press, 2nd                nel Methods for Pattern Analysis. Cambridge Uni-
  edition.                                                       versity Press.
Sanjay Jain, Steffen Lange, and Sandra Zilles. 2007.           Imre Simon. 1975. Piecewise testable events. In Au-
  Some natural conditions on incremental learning.               tomata Theory and Formal Languages, pages 214–
  Information and Computation, 205(11):1671–1684.                222.
Daniel Jurafsky and James Martin. 2008. Speech                 Imre Simon. 1993. The product of rational lan-
  and Language Processing: An Introduction to Nat-               guages. In ICALP ’93: Proceedings of the 20th
  ural Language Processing, Speech Recognition, and              International Colloquium on Automata, Languages
  Computational Linguistics. Prentice-Hall, Upper                and Programming, pages 430–444, London, UK.
  Saddle River, NJ, 2nd edition.                                 Springer-Verlag.
Anna Kasprzik and Timo Kötzing. to appear. String             Carol Whitney and Piers Cornelissen. 2008. SE-
  extension learning using lattices. In Proceedings of           RIOL reading. Language and Cognitive Processes,
  the 4th International Conference on Language and               23:143–164.
  Automata Theory and Applications (LATA 2010),
  Trier, Germany.                                              Carol Whitney. 2001. How the brain encodes the or-
                                                                 der of letters in a printed word: the SERIOL model
S.M. Kim, R. McNaughton, and R. McCloskey. 1991.                 and selective literature review. Psychonomic Bul-
  A polynomial time algorithm for the local testabil-            letin Review, 8:221–243.
  ity problem of deterministic finite automata. IEEE
  Trans. Comput., 40(10):1087–1093.
Leonid (Aryeh) Kontorovich, Corinna Cortes, and
  Mehryar Mohri. 2008. Kernel methods for learn-
  ing languages.    Theoretical Computer Science,
  405(3):223 – 236. Algorithmic Learning Theory.
Steffen Lange, Thomas Zeugmann, and Sandra Zilles.
   2008. Learning indexed families of recursive lan-
   guages from positive data: A survey. Theoretical
   Computer Science, 397:194–232.
H. Lodhi, N. Cristianini, J. Shawe-Taylor, and
  C. Watkins. 2002. Text classification using string
  kernels. Journal of Machine Language Research,
  2:419–444.
M. Lothaire, editor. 2005. Applied Combinatorics on
  Words. Cmbridge University Press, 2nd edition.
Robert McNaughton and Seymour Papert.             1971.
  Counter-Free Automata. MIT Press.
R. McNaughton. 1974. Algebraic decision procedures
   for local testability. Math. Systems Theory, 8:60–76.


                                                         906

      Cross-Language Document Summarization Based on Machine
                   Translation Quality Prediction

                         Xiaojun Wan, Huiying Li and Jianguo Xiao
    Institute of Compute Science and Technology, Peking University, Beijing 100871, China
         Key Laboratory of Computational Linguistics (Peking University), MOE, China
            {wanxiaojun,lihuiying,xiaojianguo}@icst.pku.edu.cn


                                                             ceived little attention in the past years. A
                     Abstract                                straightforward way for cross-language docu-
                                                             ment summarization is to translate the summary
                                                             from the source language to the target language
                                                             by using machine translation services. However,
    Cross-language document summarization is a               though machine translation techniques have been
    task of producing a summary in one language              advanced a lot, the machine translation quality is
    for a document set in a different language. Ex-
                                                             far from satisfactory, and in many cases, the
    isting methods simply use machine translation
    for document translation or summary transla-
                                                             translated texts are hard to understand. Therefore,
    tion. However, current machine translation               the translated summary is likely to be hard to
    services are far from satisfactory, which re-            understand by readers, i.e., the summary quality
    sults in that the quality of the cross-language          is likely to be very poor. For example, the trans-
    summary is usually very poor, both in read-              lated Chinese sentence for an ordinary English
    ability and content. In this paper, we propose           sentence (“It is also Mr Baker who is making the
    to consider the translation quality of each sen-         most of presidential powers to dispense lar-
    tence in the English-to-Chinese cross-language           gesse.”) by using Google Translate is “同时，也
    summarization process. First, the translation
    quality of each English sentence in the docu-            是贝克是谁提出了对总统权力免除最慷慨。”.
    ment set is predicted with the SVM regression            The translated sentence is hard to understand
    method, and then the quality score of each sen-          because it contains incorrect translations and it is
    tence is incorporated into the summarization             very disfluent. If such sentences are selected into
    process. Finally, the English sentences with             the summary, the quality of the summary would
    high translation quality and high informative-           be very poor.
    ness are selected and translated to form the                In order to address the above problem, we
    Chinese summary. Experimental results dem-               propose to consider the translation quality of the
    onstrate the effectiveness and usefulness of the         English sentences in the summarization process.
    proposed approach.
                                                             In particular, the translation quality of each Eng-
                                                             lish sentence is predicted by using the SVM re-
                                                             gression method, and then the predicted MT
1    Introduction                                            quality score of each sentence is incorporated
                                                             into the sentence evaluation process, and finally
Given a document or document set in one source               both informative and easy-to-translate sentences
language, cross-language document summariza-                 are selected and translated to form the Chinese
tion aims to produce a summary in a different                summary.
target language. In this study, we focus on Eng-                An empirical evaluation is conducted to evalu-
lish-to-Chinese document summarization for the               ate the performance of machine translation qual-
purpose of helping Chinese readers to quickly                ity prediction, and a user study is performed to
understand the major content of an English docu-             evaluate the cross-language summary quality.
ment or document set. This task is very impor-               The results demonstrate the effectiveness of the
tant in the field of multilingual information ac-            proposed approach.
cess.                                                           The rest of this paper is organized as follows:
   Till now, most previous work focuses on                   Section 2 introduces related work. The system is
monolingual document summarization, but                      overviewed in Section 3. In Sections 4 and 5, we
cross-language document summarization has re-                present the detailed algorithms and evaluation


                                                         917
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917–926,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


results of machine translation quality prediction        based summarization methods usually assign
and cross-language summarization, respectively.          each sentence a saliency score and then rank the
We discuss in Section 6 and conclude this paper          sentences in a document or document set.
in Section 7.                                               For single document summarization, the sen-
                                                         tence score is usually computed by empirical
2     Related Work                                       combination of a number of statistical and lin-
                                                         guistic feature values, such as term frequency,
2.1    Machine Translation Quality Prediction
                                                         sentence position, cue words, stigma words,
Machine translation evaluation aims to assess the        topic signature (Luhn 1969; Lin and Hovy, 2000).
correctness and quality of the translation. Usu-         The summary sentences can also be selected by
ally, the human reference translation is provided,       using machine learning methods (Kupiec et al.,
and various methods and metrics have been de-            1995; Amini and Gallinari, 2002) or graph-based
veloped for comparing the system-translated text         methods (ErKan and Radev, 2004; Mihalcea and
and the human reference text. For example, the           Tarau, 2004). Other methods include mutual re-
BLEU metric, the NIST metric and their relatives         inforcement principle (Zha 2002; Wan et al.,
are all based on the idea that the more shared           2007).
substrings the system-translated text has with the          For multi-document summarization, the cen-
human reference translation, the better the trans-       troid-based method (Radev et al., 2004) is a typi-
lation is. Blatz et al. (2003) investigate training      cal method, and it scores sentences based on
sentence-level confidence measures using a vari-         cluster centroids, position and TFIDF features.
ety of fuzzy match scores. Albrecht and Hwa              NeATS (Lin and Hovy, 2002) makes use of new
(2007) rely on regression algorithms and refer-          features such as topic signature to select impor-
ence-based features to measure the quality of            tant sentences. Machine Learning based ap-
sentences.                                               proaches have also been proposed for combining
   Transition evaluation without using reference         various sentence features (Wong et al., 2008).
translations has also been investigated. Quirk           The influences of input difficulty on summariza-
(2004) presents a supervised method for training         tion performance have been investigated in
a sentence level confidence measure on transla-          (Nenkova and Louis, 2008). Graph-based meth-
tion output using a human-annotated corpus.              ods have also been used to rank sentences in a
Features derived from the source sentence and            document set. For example, Mihalcea and Tarau
the target sentence (e.g. sentence length, perplex-      (2005) extend the TextRank algorithm to com-
ity, etc.) and features about the translation proc-      pute sentence importance in a document set.
ess are leveraged. Gamon et al. (2005) investi-          Cluster-level information has been incorporated
gate the possibility of evaluating MT quality and        in the graph model to better evaluate sentences
fluency at the sentence level in the absence of          (Wan and Yang, 2008). Topic-focused or query
reference translations, and they can improve on          biased multi-document summarization has also
the correlation between language model perplex-          been investigated (Wan et al., 2006). Wan et al.
ity scores and human judgment by combing these           (2010) propose the EUSUM system for extract-
perplexity scores with class probabilities from a        ing easy-to-understand English summaries for
machine-learned classifier. Specia et al. (2009)         non-native readers.
use the ICM theory to identify the threshold to             Several pilot studies have been performed for
map a continuous predicted score into “good” or          the cross-language summarization task by simply
“bad” categories. Chae and Nenkova (2009) use            using document translation or summary transla-
surface syntactic features to assess the fluency of      tion. Leuski et al. (2003) use machine translation
machine translation results.                             for English headline generation for Hindi docu-
   In this study, we further predict the translation     ments. Lim et al. (2004) propose to generate a
quality of an English sentence before the ma-            Japanese summary without using a Japanese
chine translation process, i.e., we do not leverage      summarization system, by first translating Japa-
reference translation and the target sentence.           nese documents into Korean documents, and
                                                         then extracting summary sentences by using Ko-
2.2    Document Summarization                            rean summarizer, and finally mapping Korean
Document summarization methods can be gener-             summary sentences to Japanese summary sen-
ally categorized into extraction-based methods           tences. Chalendar et al. (2005) focuses on se-
and abstraction-based methods. In this paper, we         mantic analysis and sentence generation tech-
focus on extraction-based methods. Extraction-           niques for cross-language summarization. Orasan


                                                       918


and Chiorean (2008) propose to produce summa-          However, an informative and fluent English sen-
ries with the MMR method from Romanian news            tence is likely to be translated into an uninforma-
articles and then automatically translate the          tive and disfluent Chinese sentence, and there-
summaries into English. Cross language query           fore, this sentence cannot be selected into the
based summarization has been investigated in           summary.
(Pingali et al., 2007), where the query and the           In order to address the above problem of exist-
documents are in different languages. Other re-        ing methods, our proposed approach takes into
lated work includes multilingual summarization         account a novel factor of each sentence for cross-
(Lin et al., 2005), which aims to create summa-        language summary extraction. Each English sen-
ries from multiple sources in multiple languages.      tence is associated with a score indicating its
Siddharthan and McKeown (2005) use the in-             translation quality. An English sentence with
formation redundancy in multilingual input to          high translation quality score is more likely to be
correct errors in machine translation and thus         selected into the original English summary, and
improve the quality of multilingual summaries.         such English summary can be translated into a
                                                       better Chinese summary. Figure 1 gives the ar-
3   The Proposed Approach                              chitecture of our proposed approach.
Previous methods for cross-language summariza-
tion usually consist of two steps: one step for                                  English
summarization and one step for translation. Dif-                                Sentences
ferent order of the two steps can lead to the fol-
lowing two basic English-to-Chinese summariza-                  Sentence                          Sentence
tion methods:                                                  MT Quality                     Informativeness
   Late Translation (LateTrans): Firstly, an                   Prediction                        Evaluation
English summary is produced for the English                     MT quality score
                                                                                      Informativeness score
document set by using existing summarization
methods. Then, the English summary is auto-                                      English
matically translated into the corresponding Chi-                                Summary
                                                                                Extraction
nese summary by using machine translation ser-
vices.                                                                        English summary
   Early Translation (EarlyTrans): Firstly, the                                 EN-to-CN
English documents are translated into Chinese                                    Machine
documents by using machine translation services.                                Translation
Then, a Chinese summary is produced for the
translated Chinese documents.                                               Chinese Summary
   Generally speaking, the LateTrans method has              Figure 1: Architecture of our proposed ap-
a few advantages over the EarlyTrans method:                                  proach
   1) The LateTrans method is much more effi-             Seen from the figure, our proposed approach
cient than the EarlyTrans method, because only a       consists of four main steps: 1) The machine
very few summary sentences are required to be          translation quality score of each English sentence
translated in the LateTrans method, whereas all        is predicted by using regression methods; 2) The
the sentences in the documents are required to be      informativeness score of each English sentence is
translated in the EarlyTrans method.                   computed by using existing methods; 3) The
   2) The LateTrans method is deemed to be             English summary is produced by making use of
more effective than the EarlyTrans method, be-         both the machine translation quality score and
cause the translation errors of the sentences have     the informativeness score; 4) The extracted Eng-
great influences on the summary sentence extrac-       lish summary is translated into Chinese summary
tion in the EarlyTrans method.                         by using machine translation services.
   Thus in this study, we adopt the LateTrans             In this study, we adopt Google Translate1 for
method as our baseline method. We also adopt           English-to-Chinese translation. Google Translate
the late translation strategy for our proposed ap-     is one of the state-of-the-art commercial machine
proach.                                                translation systems used today. It applies statisti-
   In the baseline method, a translated Chinese        cal learning techniques to build a translation
sentence is selected into the summary because
the original English sentence is informative.          1
                                                           http://translate.google.com/translate_t


                                                     919


model based on both monolingual text in the tar-                           w T f ( xi ) + b − y i ≤ ε + ξ i
get language and aligned text consisting of ex-                           y i − w T f ( x i ) − b ≤ ε + ξ i*
amples of human translations between the lan-
guages.                                                                   ε , ξ i , ξ i* ≥ 0, i = 1,..., n.
   The first step and the evaluation results will be        The constant C>0 is a parameter for determin-
described in Section 4, and the other steps and          ing the trade-off between the flatness of f and the
the evaluation results will be described together        amount up to which deviations larger than ε are
in Section 5.                                            tolerated.
                                                            In the experiments, we use the LIBSVM tool
4     Machine Translation Quality Predic-                (Chang and Lin, 2001) with the RBF kernel for
      tion                                               the task, and we use the parameter selection tool
                                                         of 10-fold cross validation via grid search to find
4.1    Methodology                                       the best parameters on the training set with re-
In this study, machine translation (MT) quality          spect to mean squared error (MSE), and then use
reflects both the translation accuracy and the flu-      the best parameters to train on the whole training
ency of the translated sentence. An English sen-         set.
tence with high MT quality score is likely to be            We use the following two groups of features
translated into an accurate and fluent Chinese           for each sentence: the first group includes several
sentence, which can be easily read and under-            basic features, and the second group includes
stand by Chinese readers. The MT quality pre-            several parse based features 2 . They are all de-
diction is a task of mapping an English sentence         rived based on the source English sentence.
to a numerical value corresponding to a quality             The basic features are as follows:
level. The larger the value is, the more accurately      1) Sentence length: It refers to the number of
and fluently the sentence can be translated into              words in the sentence.
Chinese sentence.                                        2) Sub-sentence number: It refers to the num-
   As introduced in Section 2.1, several related              ber of sub-sentences in the sentence. We
work has used regression and classification                   simply use the punctuation marks as indica-
methods for MT quality prediction without refer-              tors of sub-sentences.
ence translations. In our approach, the MT qual-         3) Average sub-sentence length: It refers to
ity of each sentence in the documents is also pre-            the average number of words in the sub-
dicted without reference translations. The differ-            sentences within the sentence.
ence between our task and previous work is that          4) Percentage of nouns and adjectives: It re-
previous work can make use of both features in                fers to the percentage of noun words or ad-
source sentence and features in target sentence,              jective words in the in the sentence.
while our task only leverages features in source         5) Number of question words: It refers to the
sentence, because in the late translation strategy,           number of question words (who, whom,
the English sentences in the documents have not               whose, when, where, which, how, why, what)
been translated yet at this step.                             in the sentence.
   In this study, we adopt the ε-support vector re-         We use the Stanford Lexicalized Parser (Klein
gression (ε-SVR) method (Vapnik 1995) for the            and Manning, 2002) with the provided English
sentence-level MT quality prediction task. The           PCFG model to parse a sentence into a parse tree.
SVR algorithm is firmly grounded in the frame-           The output tree is a context-free phrase structure
work of statistical learning theory (VC theory).         grammar representation of the sentence. The
The goal of a regression algorithm is to fit a flat      parse features are then selected as follows:
function to the given training data points.              1) Depth of the parse tree: It refers to the
   Formally, given a set of training data points              depth of the generated parse tree.
D={(xi,yi)| i=1,2,…,n} ⊂ Rd×R, where xi is input         2) Number of SBARs in the parse tree:
feature vector and yi is associated score, the goal           SBAR is defined as a clause introduced by a
is to fit a function f which approximates the rela-           (possibly empty) subordinating conjunction.
tion inherited between the data set points. The               It is an indictor of sentence complexity.
standard form is:
                              n        n
                     1
           min * w T w + C ∑ ξ i + C ∑ ξ i*
          w ,b ,ξ ,ξ 2      i =1     i =1
                                                         2
                                                             Other features, including n-gram frequency, perplexity
Subject to                                                   features, etc., are not useful in our study. MT features are
                                                             not used because Google Translate is used as a black box.


                                                       920


3) Number of NPs in the parse tree: It refers         scores for the test sentences: Yˆ = { yˆi | i = 1,...n} , and
    to the number of noun phrases in the parse        the manually assigned scores for the sentences:
    tree.                                             Y = { yi | i = 1,...n} , the MSE of the prediction result
4) Number of VPs in the parse tree: It refers
    to the number of verb phrases in the parse        is defined as
                                                                                             n
                                                                                  1
    tree.                                                             MSE (Yˆ ) =
                                                                                  n
                                                                                            ∑ ( yˆ     i   − yi ) 2
  All the above feature values are scaled by us-                                            i =1

ing the provided svm-scale program.                      Pearson’s Correlation Coefficient (ρ): This
  At this step, each English sentence si can be       metric is a measure of whether the trends of pre-
associated with a MT quality score TransScore(si)     diction values matched the trends for human-
predicted by the ε-SVR method. The score is fi-       labeled data. The coefficient between Y and Yˆ is
nally normalized by dividing by the maximum           defined as
score.                                                                           n

                                                                                ∑(y    i   − y )( yˆ i − yˆ )
4.2     Evaluation                                                         ρ=   i =1

                                                                                           ns y s yˆ
4.2.1    Evaluation Setup
                                                      where y and ŷ are the sample means of Y and
In the experiments, we first constructed the gold-    Yˆ , s and s ˆ are the sample standard deviations
                                                              y        y
standard dataset in the following way:
   DUC2001 provided 309 English news articles         of Y and Yˆ .
for document summarization tasks, and the arti-
                                                      4.2.2        Evaluation Results
cles were grouped into 30 document sets. The
news articles were selected from TREC-9. We           Table 1 shows the prediction results. We can see
chose five document sets (d04, d05, d06, d08,         that the overall results are promising. And the
d11) with 54 news articles out of the DUC2001         correlation is moderately high. The results are
document sets. The documents were then split          acceptable because we only make use of the fea-
into sentences and we used 1736 sentences for         tures derived from the source sentence. The re-
evaluation. All the sentences were automatically      sults guarantee that the use of MT quality scores
translated into Chinese sentences by using the        in the summarization process is feasible.
Google Translate service.                                We can also see that both the basic features
   Two Chinese college students were employed         and the parse features are beneficial to the over-
for data annotation. They read the original Eng-      all prediction results.
lish sentence and the translated Chinese sentence,
and then manually labeled the overall translation                  Feature Set                     MSE                 ρ
quality score for each sentence, separately. The                   Basic features                  0.709              0.399
translation quality is an overall measure for both
the translation accuracy and the readability of the                Parse features                  0.702              0.395
translated sentence. The score ranges between 1                     All features                   0.683              0.433
and 5, and 1 means “very bad”, and 5 means                            Table 1: Prediction results
“very good”, and 3 means “normal”. The correla-
tion between the two sets of labeled scores is        5     Cross-Language Document Summari-
0.646. The final translation quality score was the          zation
average of the scores provided by the two anno-
tators.                                               5.1         Methodology
   After annotation, we randomly separated the        In this section, we first compute the informative-
labeled sentence set into a training set of 1428      ness score for each sentence. The score reflect
sentences and a test set of 308 sentences. We         how the sentence expresses the major topic in the
then used the LIBSVM tool for training and test-      documents. Various existing methods can be
ing.                                                  used for computing the score. In this study, we
   Two metrics were used for evaluating the pre-      adopt the centroid-based method.
diction results. The two metrics are as follows:         The centroid-based method is the algorithm
   Mean Square Error (MSE): This metric is a          used in the MEAD system. The method uses a
measure of how correct each of the prediction         heuristic and simple way to sum the sentence
values is on average, penalizing more severe er-      scores computed based on different features. The
rors more heavily. Given the set of prediction        score for each sentence is a linear combination of

                                                  921


the weights computed based on the following                           highly overlapping with other highly scored sen-
three features:                                                       tences, and finally the informative, novel, and
   Centroid-based Weight. The sentences close                         easy-to-translate sentences are chosen into the
to the centroid of the document set are usually                       English summary.
more important than the sentences farther away.                         Finally, the sentences in the English summary
And the centroid weight C(si) of a sentence si is                     are translated into the corresponding Chinese
calculated as the cosine similarity between the                       sentences by using Google Translate, and the
sentence text and the concatenated text for the                       Chinese summary is formed.
whole document set D. The weight is then nor-
malized by dividing the maximal weight.                               5.2     Evaluation
   Sentence Position. The leading several sen-                        5.2.1    Evaluation Setup
tences of a document are usually important. So
we calculate for each sentence a weight to reflect                 In this experiment, we used the document sets
its position priority as P(si)=1-(i-1)/n, where i is               provided by DUC2001 for evaluation. As men-
the sequence of the sentence si and n is the total                 tioned in Section 4.2.1, DUC2001 provided 30
number of sentences in the document. Obviously,                    English document sets for generic multi-
i ranges from 1 to n.                                              document summarization. The average document
   First Sentence Similarity. Because the first                    number per document set was 10. The sentences
sentence of a document is very important, a sen-                   in each article have been separated and the sen-
tence similar to the first sentence is also impor-                 tence information has been stored into files. Ge-
tant. Thus we use the cosine similarity value be-                  neric reference English summaries were pro-
tween a sentence and the corresponding first sen-                  vided by NIST annotators for evaluation. In our
tence in the same document as the weight F(si)                     study, we aimed to produce Chinese summaries
for sentence si.                                                   for the English document sets. The summary
   After all the above weights are calculated for                  length was limited to five sentences, i.e. each
each sentence, we sum all the weights and get the                  summary consisted of five sentences.
overall score for the sentence as follows:                               The DUC2001 dataset was divided into the
  InfoScore ( s i ) = α ⋅ C ( s i ) + β ⋅ P ( s i ) + γ ⋅ F ( s i )following two datasets:
                                                                         Ideal Dataset: We have manually labeled the
where α, β and γ are parameters reflecting the
                                                                   MT quality scores for the sentences in five
importance of different features. We empirically
                                                                   document sets (d04-d11), and we directly used
set α=β=γ=1.
                                                                   the manually labeled scores in the summarization
   After the informativeness scores for all sen-
                                                                   process. The ideal dataset contained these five
tences are computed, the score of each sentence
                                                                   document sets.
is normalized by dividing by the maximum score.
                                                                         Real Dataset: The MT quality scores for the
   After we obtain the MT quality score and the
                                                                   sentences in the remaining 25 document sets
informativeness score of each sentence in the
                                                                   were automatically predicted by using the
document set, we linearly combine the two
                                                                   learned SVM regression model. And we used the
scores to get the overall score of each sentence.
                                                                   automatically predicted scores in the summariza-
   Formally, let TransScore(si)∈[0,1] and Info-
                                                                   tion process. The real dataset contained these 25
Score(si)∈[0,1] denote the MT quality score and                    document sets.
the informativeness score of sentence si, the                         We performed two evaluation procedures: one
overall score of the sentence is:                                               based on the ideal dataset to validate the
OverallSco re( si ) = (1 − λ ) × InfoScore ( si ) + λ × TransScore ( si )       feasibility of the proposed approach, and
where λ∈[0,1] is a parameter controlling the                                    the other based on the real dataset to
influences of the two factors. If λ is set to 0, the               demonstrate        the effectiveness of the proposed
summary is extracted without considering the                       approach       in real applications.
MT quality factor. In the experiments, we em-                            To date, various methods and metrics have
pirically set the parameter to 0.3 in order to bal-                been       developed for English summary evaluation
ance the two factors of content informativeness                    by      comparing     system summary with reference
and translation quality.                                           summary,        such  as the pyramid method (Nenkova
   For multi-document summarization, some sen-                     et     al., 2007)   and  the ROUGE metrics (Lin and
tences are highly overlapping with each other,                     Hovy,       2003).  However,   such methods or metrics
and thus we apply the same greedy algorithm in                     cannot be directly used for evaluating Chinese
(Wan et al., 2006) to penalize the sentences                       summary without reference Chinese summary.


                                                                  922


Instead, we developed an evaluation protocol as           averaged across the documents sets and across
follows:                                                  the subjects.
   The evaluation was based on human scoring.
Four Chinese college students participated in the         5.2.2     Evaluation Results
evaluation as subjects. We have developed a               Table 2 shows the evaluation results on the ideal
friendly tool for helping the subjects to evaluate        dataset with 5 document sets. We can see that
each Chinese summary from the following three             based on the manually labeled MT quality scores,
aspects:                                                  the Chinese summaries produced by our pro-
   Content: This aspect indicates how much a              posed approach are significantly better than that
summary reflects the major content of the docu-           produced by the baseline approach over all three
ment set. After reading a summary, each user can          aspects. All subjects agree that our proposed ap-
select a score between 1 and 5 for the summary.           proach can produce more informative and easy-
1 means “very uninformative” and 5 means                  to-read Chinese summaries than the baseline ap-
“very informative”.                                       proach.
   Readability: This aspect indicates the read-              Table 3 shows the evaluation results on the
ability level of the whole summary. After reading         real dataset with 25 document sets. We can see
a summary, each user can select a score between           that based on the automatically predicted MT
1 and 5 for the summary. 1 means “hard to read”,          quality scores, the Chinese summaries produced
and 5 means “easy to read”.                               by our proposed approach are significantly better
   Overall: This aspect indicates the overall             than that produced by the baseline approach over
quality of a summary. After reading a summary,            the readability aspect and the overall aspect. Al-
each user can select a score between 1 and 5 for          most all subjects agree that our proposed ap-
the summary. 1 means “very bad”, and 5 means              proach can produce more easy-to-read and high-
“very good”.                                              quality Chinese summaries than the baseline ap-
   We performed the evaluation procedures on              proach.
the ideal dataset and the read dataset, separately.          Comparing the evaluation results in the two
During each evaluation procedure, we compared             tables, we can find that the performance differ-
our proposed approach (λ=0.3) with the baseline           ence between the two approaches on the ideal
approach without considering the MT quality               dataset is bigger than that on the real dataset, es-
factor (λ=0). And the two summaries produced              pecially on the content aspect. The results dem-
by the two systems for the same document set              onstrate that the more accurate the MT quality
were presented in the same interface, and then            scores are, the more significant the performance
the four subjects assigned scores to each sum-            improvement is.
mary after they read and compared the two                   Overall, the proposed approach is effective to
summaries. And the assigned scores were finally           produce good-quality Chinese summaries for
                                                          English document sets.

                                Baseline Approach                       Proposed Approach
                          content readability overall             content readability overall
             Subject1       3.2         2.6       2.8               3.4         3.0       3.4
             Subject2       3.0         3.2       3.2               3.4         3.6       3.4
             Subject3       3.4         2.8       3.2               3.6         3.8       3.8
             Subject4       3.2         3.0       3.2               3.8         3.8       3.8
             Average        3.2         2.9       3.1              3.55*       3.55*      3.6*
                    Table 2: Evaluation results on the ideal dataset (5 document sets)
                                 Baseline Approach                       Proposed Approach
                          content    readability overall          content readability    overall
            Subject1        2.64         2.56      2.60             2.80        3.24       2.96
            Subject2        3.60         2.76      3.36             3.52        3.28       3.64
            Subject3        3.52         3.72      3.44             3.56        3.80       3.48
            Subject4        3.16         2.96      3.12             3.16        3.44       3.52
            Average         3.23         3.00      3.13             3.26        3.44*      3.40*
                    Table 3: Evaluation results on the real dataset (25 document sets)
(* indicates the difference between the average score of the proposed approach and that of the baseline approach
                                     is statistically significant by using t-test.)


                                                      923


                                                                         Document set 2: D54 from the real dataset
5.2.3      Example Analysis                                              Summary by baseline approach:
In this section, we give two running examples to                         s1: 两个加州11月6日投票的主张，除其他限制外，全州成员及
                                                                         州议员的条件。(0.57)
better show the effectiveness of our proposed                            (Two propositions on California's Nov. 6 ballot would, among other
approach. The Chinese sentences and the original                         things, limit the terms of statewide officeholders and state legisla-
English sentences in the summary are presented                           tors.)
                                                                         s2: 原因之一是任期限制将开放到现在的政治职务任职排除了
together. The normalized MT quality score for                            许多人的职业生涯。(0.36)
each sentence is also given at the end of the Chi-                       (One reason is that term limits would open up politics to many
nese sentence.                                                           people now excluded from office by career incumbents.)
                                                                         s3: 建议限制国会议员及州议员都很受欢迎，越来越多的条件
                                                                         是，根据专家和投票。(0.20)
Document set 1: D04 from the ideal dataset                               (Proposals to limit the terms of members of Congress and of state
Summary by baseline approach:                                            legislators are popular and getting more so, according to the pundits
s1: 预计美国的保险公司支付，估计在佛罗里达州的73亿美元                                           and the polls.)
（37亿英镑），作为安德鲁飓风的结果-迄今为止最昂贵的灾                                             s4: 国家法规的酒吧首先从运行时间为国会候选人已举行了加
难曾经面临产业。(0.56)                                                           入的资格规定了宪法规定，并已失效。(0.24)
(US INSURERS expect to pay out an estimated Dollars 7.3bn                (State statutes that bar first-time candidates from running for Con-
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far       gress have been held to add to the qualifications set forth in the
the costliest disaster the industry has ever faced. )                    Constitution and have been invalidated.)
s2: 有越来越多的迹象表明安德鲁飓风，不受欢迎的，因为它                                            s5: 另一个论点是，公民的同时，不断进入新的华盛顿国会将
的佛罗里达和路易斯安那州的受灾居民，最后可能不伤害到连                                              面临流动更好的结果，比政府的任期较长的代表提供的。(0.20)
任的布什总统竞选。(0.67)                                                          (Another argument is that a citizen Congress with its continuing
(THERE are growing signs that Hurricane Andrew, unwelcome as             flow of fresh faces into Washington would result in better govern-
it was for the devastated inhabitants of Florida and Louisiana, may      ment than that provided by representatives with lengthy tenure.)
in the end do no harm to the re-election campaign of President           Summary by proposed approach:
George Bush.)                                                            s1: 两个加州 11 月 6 日投票的主张，除其他限制外，全州成员
s3: 一般事故发生后，英国著名保险公司昨日表示，保险索赔                                            及州议员的条件。(0.57)
的安德鲁飓风所引发的成本也高达4000万美元&#39;。 (0.44)                                      (Two propositions on California's Nov. 6 ballot would, among other
(GENERAL ACCIDENT said yesterday that insurance claims                   things, limit the terms of statewide officeholders and state legisla-
arising from Hurricane Andrew could 'cost it as much as Dollars          tors.)
40m'.)                                                                   s2: 原因之一是任期限制将开放到现在的政治职务任职排除了
s4: 在巴哈马，政府发言人麦库里说，4人死亡已离岛东部群岛                                           许多人的职业生涯。(0.36)
报告。 (0.56)                                                               (One reason is that term limits would open up politics to many
(In the Bahamas, government spokesman Mr Jimmy Curry said                people now excluded from office by career incumbents.)
four deaths had been reported on outlying eastern islands.)              s3: 另一个论点是，公民的同时，不断进入新的华盛顿国会将
s5: 新奥尔良的和1.6万人，是特别脆弱，因为该市位于海平面                                          面临流动更好的结果，比政府的任期较长的代表提供的。(0.20)
以下，有密西西比河通过其中心的运行和一个大型湖泊立即向                                              (Another argument is that a citizen Congress with its continuing
北方。(0.44)                                                                flow of fresh faces into Washington would result in better govern-
(New Orleans, with a population of 1.6m, is particularly vulnerable      ment than that provided by representatives with lengthy tenure.)
because the city lies below sea level, has the Mississippi River         s4: 有两个国会任期限制，经济学家，至少公共选择那些劝
running through its centre and a large lake immediately to the north.)   说，要充分理解充分的理由。(0.39)
                                                                         (There are two solid reasons for congressional term limitation that
Summary by proposed approach:                                            economists, at least those of the public-choice persuasion, should
s1: 预计美国的保险公司支付，估计在佛罗里达州的73亿美元                                           fully appreciate.)
（37亿英镑），作为安德鲁飓风的结果-迄今为止最昂贵的灾                                             s5: 与国会的问题的根源是，除非有重大丑闻，几乎是不可能
难曾经面临产业。(0.56)                                                           战胜现任。(0.47)
(US INSURERS expect to pay out an estimated Dollars 7.3bn                (The root of the problems with Congress is that, barring major
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far       scandal, it is almost impossible to defeat an incumbent.)
the costliest disaster the industry has ever faced.)
s2: 有越来越多的迹象表明安德鲁飓风，不受欢迎的，因为它                                            6     Discussion
的佛罗里达和路易斯安那州的受灾居民，最后可能不伤害到连
任的布什总统竞选。(0.67)                                                          In this study, we adopt the late translation strat-
(THERE are growing signs that Hurricane Andrew, unwelcome as             egy for cross-document summarization. As men-
it was for the devastated inhabitants of Florida and Louisiana, may
in the end do no harm to the re-election campaign of President           tioned earlier, the late translation strategy has
George Bush.)                                                            some advantages over the early translation strat-
s3: 在巴哈马，政府发言人麦库里说，4人死亡已离岛东部群岛                                           egy. However, in the early translation strategy,
报告。(0.56)
(In the Bahamas, government spokesman Mr Jimmy Curry said                we can use the features derived from both the
four deaths had been reported on outlying eastern islands.)              source English sentence and the target Chinese
s4: 在首当其冲的损失可能会集中在美国的保险公司，业内分                                            sentence to improve the MT quality prediction
析人士昨天说。 (0.89)
(The brunt of the losses are likely to be concentrated among US
                                                                         results.
insurers, industry analysts said yesterday.)                                Overall, the framework of our proposed ap-
s5: 在北迈阿密，损害是最小的。(1.0)                                                   proach can be easily adapted for cross-document
(In north Miami, damage is minimal.)
                                                                         summarization with the early translation strategy.


                                                                      924


And an empirical comparison between the two                    tence simplification, and bilingual generation. In
strategies is left as our future work.                         Workshop on Crossing Barriers in Text Summari-
   Though this study focuses on English-to-                    zation Research, 5th International Conference on
Chinese document summarization, cross-                         Recent Advances in Natural Language Processing
                                                               (RANLP2005).
language summarization tasks for other lan-
guages can also be solved by using our proposed             C.-C. Chang and C.-J. Lin. 2001. LIBSVM : a library
                                                               for support vector machines. Software available at
approach.
                                                               http://www.csie.ntu.edu.tw/~cjlin/libsvm
7    Conclusion and Future Work                             G. ErKan, D. R. Radev. LexPageRank. 2004. Prestige
                                                               in Multi-Document Text Summarization. In Pro-
In this study we propose a novel approach to ad-               ceedings of EMNLP2004.
dress the cross-language document summariza-                M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
tion task. Our proposed approach predicts the                  level MT evaluation without reference translations:
MT quality score of each English sentence and                  beyond language modeling. In Proceedings of
then incorporates the score into the summariza-                EAMT2005.
tion process. The user study results verify the             D. Klein and C. D. Manning. 2002. Fast Exact Infer-
                                                               ence with a Factored Model for Natural Language
effectiveness of the approach.
                                                               Parsing. In Proceedings of NIPS2002.
   In future work, we will manually translate
                                                            J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable
English reference summaries into Chinese refer-
                                                               Document Summarizer. In Proceedings of
ence summaries, and then adopt the ROUGE                       SIGIR1995.
metrics to perform automatic evaluation of the
                                                            A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och,
extracted Chinese summaries by comparing them                  E. Hovy. 2003. Cross-lingual C*ST*RD: English
with the Chinese reference summaries. Moreover,                access to Hindi information. ACM Transactions on
we will further improve the sentence’s MT qual-                Asian Language Information Processing, 2(3):
ity by using sentence compression or sentence                  245-269.
reduction techniques.                                       J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multi-
                                                               document summarization using cross-language
Acknowledgments                                                texts. In Proceedings of NTCIR-4.
                                                            C. Y. Lin, E. Hovy. 2000. The Automated Acquisition
This work was supported by NSFC (60873155),
                                                               of Topic Signatures for Text Summarization. In
Beijing Nova Program (2008B03), NCET                           Proceedings of the 17th Conference on Computa-
(NCET-08-0006), RFDP (20070001059) and                         tional Linguistics.
National       High-tech       R&D        Program           C..-Y. Lin and E.. H. Hovy. 2002. From Single to
(2008AA01Z421). We thank the students for                      Multi-document Summarization: A Prototype Sys-
participating in the user study. We also thank the             tem and its Evaluation. In Proceedings of ACL-02.
anonymous reviewers for their useful comments.              C.-Y. Lin and E.H. Hovy. 2003. Automatic Evalua-
                                                               tion of Summaries Using N-gram Co-occurrence
References                                                     Statistics. In Proceedings of HLT-NAACL -03.
J. Albrecht and R. Hwa. 2007. A re-examination of           C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilingual
   machine learning approaches for sentence-level mt           summarization evaluation 2005: automatic evalua-
   evaluation. In Proceedings of ACL2007.                      tion report. In Proceedings of MSE (ACL-2005
                                                               Workshop).
M. R. Amini, P. Gallinari. 2002. The Use of Unla-
   beled Data to Improve Supervised Learning for            H. P. Luhn. 1969. The Automatic Creation of litera-
   Text Summarization. In Proceedings of SIGIR2002.            ture Abstracts. IBM Journal of Research and De-
                                                               velopment, 2(2).
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C.
   Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.          R. Mihalcea, P. Tarau. 2004. TextRank: Bringing
   2003. Confidence estimation for statistical machine         Order into Texts. In Proceedings of EMNLP2004.
   translation. Johns Hopkins Summer Workshop Fi-           R. Mihalcea and P. Tarau. 2005. A language inde-
   nal Report.                                                 pendent algorithm for single and multiple docu-
J. Chae and A. Nenkova. 2009. Predicting the fluency           ment summarization. In Proceedings of IJCNLP-05.
   of text with shallow structural features: case studies   A. Nenkova and A. Louis. 2008. Can you summarize
   of machine translation and human-written text. In           this? Identifying correlates of input difficulty for
   Proceedings of EACL2009.                                    generic multi-document summarization. In Pro-
G. de Chalendar, R. Besançon, O. Ferret, G. Grefen-            ceedings of ACL-08:HLT.
   stette, and O. Mesnard. 2005. Crosslingual summa-        A. Nenkova, R. Passonneau, and K. McKeown. 2007.
   rization with thematic extraction, syntactic sen-           The Pyramid method: incorporating human content
                                                               selection variation in summarization evaluation.

                                                        925


   ACM Transactions on Speech and Language Proc-
   essing (TSLP), 4(2).
C. Orasan, and O. A. Chiorean. 2008. Evaluation of a
   Crosslingual Romanian-English Multi-document
   Summariser. In Proceedings of 6th Language Re-
   sources and Evaluation Conference (LREC2008).
P. Pingali, J. Jagarlamudi and V. Varma. 2007. Ex-
   periments in cross language query focused multi-
   document summarization. In Workshop on Cross
   Lingual Information Access Addressing the Infor-
   mation Need of Multilingual Societies in
   IJCAI2007.
C. Quirk. 2004. Training a sentence-level machine
   translation confidence measure. In Proceedings of
   LREC2004.
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004.
   Centroid-based summarization of multiple docu-
   ments. Information Processing and Management,
   40: 919-938.
A. Siddharthan and K. McKeown. 2005. Improving
   multilingual summarization: using redundancy in
   the input to correct MT errors. In Proceedings of
   HLT/EMNLP-2005.
L. Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C.
   Saunders. 2009. Improving the Confidence of Ma-
   chine Translation Quality Estimates. In MT Summit
   2009 (Machine Translation Summit XII).
V. Vapnik. 1995. The Nature of Statistical Learning
   Theory. Springer.
X. Wan, H. Li and J. Xiao. 2010. EUSUM: extracting
   easy-to-understand English summaries for non-
   native readers. In Proceedings of SIGIR2010.
X. Wan, J. Yang and J. Xiao. 2006. Using cross-
   document random walks for topic-focused multi-
   documetn summarization. In Proceedings of
   WI2006.
X. Wan and J. Yang. 2008. Multi-document summari-
   zation using cluster-based link analysis. In Pro-
   ceedings of SIGIR-08.
X. Wan, J. Yang and J. Xiao. 2007. Towards an Itera-
   tive Reinforcement Approach for Simultaneous
   Document Summarization and Keyword Extraction.
   In Proceedings of ACL2007.
K.-F. Wong, M. Wu and W. Li. 2008. Extractive sum-
   marization using supervised and semi-supervised
   learning. In Proceedings of COLING-08.
H. Y. Zha. 2002. Generic Summarization and Key-
   phrase Extraction Using Mutual Reinforcement
   Principle and Sentence Clustering. In Proceedings
   of SIGIR2002.




                                                   926

         Bayesian Synchronous Tree-Substitution Grammar Induction
                 and its Application to Sentence Compression


                                Elif Yamangil and Stuart M. Shieber
                                         Harvard University
                                    Cambridge, Massachusetts, USA
                              {elif, shieber}@seas.harvard.edu




                      Abstract                                 chronous tree-substitution (Eisner, 2003) or tree-
                                                               adjoining grammars may better capture the pair-
    We describe our experiments with training                  ings.
    algorithms for tree-to-tree synchronous                       In this work, we explore techniques for inducing
    tree-substitution grammar (STSG) for                       synchronous tree-substitution grammars (STSG)
    monolingual translation tasks such as                      using as a testbed application extractive sentence
    sentence compression and paraphrasing.                     compression. Learning an STSG from aligned
    These translation tasks are characterized                  trees is tantamount to determining a segmentation
    by the relative ability to commit to parallel              of the trees into elementary trees of the grammar
    parse trees and availability of word align-                along with an alignment of the elementary trees
    ments, yet the unavailability of large-scale               (see Figure 1 for an example of such a segmenta-
    data, calling for a Bayesian tree-to-tree                  tion), followed by estimation of the weights for the
    formalism. We formalize nonparametric                      extracted tree pairs.1 These elementary tree pairs
    Bayesian STSG with epsilon alignment in                    serve as the rules of the extracted grammar. For
    full generality, and provide a Gibbs sam-                  SCFG, segmentation is trivial — each parent with
    pling algorithm for posterior inference tai-               its immediate children is an elementary tree — but
    lored to the task of extractive sentence                   the formalism then restricts us to deriving isomor-
    compression. We achieve improvements                       phic tree pairs. STSG is much more expressive,
    against a number of baselines, including                   especially if we allow some elementary trees on
    expectation maximization and variational                   the source or target side to be unsynchronized, so
    Bayes training, illustrating the merits of                 that insertions and deletions can be modeled, but
    nonparametric inference over the space of                  the segmentation and alignment problems become
    grammars as opposed to sparse parametric                   nontrivial.
    inference with a fixed grammar.                               Previous approaches to this problem have
                                                               treated the two steps — grammar extraction and
1   Introduction
                                                               weight estimation — with a variety of methods.
Given an aligned corpus of tree pairs, we might                One approach is to use word alignments (where
want to learn a mapping between the paired trees.              these can be reliably estimated, as in our testbed
Such induction of tree mappings has application                application) to align subtrees and extract rules
in a variety of natural-language-processing tasks              (Och and Ney, 2004; Galley et al., 2004) but
including machine translation, paraphrase, and                 this leaves open the question of finding the right
sentence compression. The induced tree map-                    level of generality of the rules — how deep the
pings can be expressed by synchronous grammars.                rules should be and how much lexicalization they
Where the tree pairs are isomorphic, synchronous               should involve — necessitating resorting to heuris-
context-free grammars (SCFG) may suffice, but in               tics such as minimality of rules, and leading to
general, non-isomorphism can make the problem                     1
                                                                    Throughout the paper we will use the word STSG to re-
of rule extraction difficult (Galley and McKeown,              fer to the tree-to-tree version of the formalism, although the
2007). More expressive formalisms such as syn-                 string-to-tree version is also commonly used.


                                                         937
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937–947,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


large grammars. Once a given set of rules is ex-               possibility of searching over the infinite space of
tracted, weights can be imputed using a discrimi-              grammars (and, in machine translation, possible
native approach to maximize the (joint or condi-               word alignments), thus side-stepping the narrow-
tional) likelihood or the classification margin in             ness problem outlined above as well.
the training data (taking or not taking into account              In this work, we use an extension of the afore-
the derivational ambiguity). This option leverages             mentioned models of generative segmentation for
a large amount of manual domain knowledge en-                  STSG induction, and describe an algorithm for
gineering and is not in general amenable to latent             posterior inference under this model that is tai-
variable problems.                                             lored to the task of extractive sentence compres-
                                                               sion. This task is characterized by the availabil-
   A simpler alternative to this two step approach
                                                               ity of word alignments, providing a clean testbed
is to use a generative model of synchronous
                                                               for investigating the effects of grammar extraction.
derivation and simultaneously segment and weight
                                                               We achieve substantial improvements against a
the elementary tree pairs to maximize the prob-
                                                               number of baselines including EM, support vector
ability of the training data under that model; the
                                                               machine (SVM) based discriminative training, and
simplest exemplar of this approach uses expecta-
                                                               variational Bayes (VB). By comparing our method
tion maximization (EM) (Dempster et al., 1977).
                                                               to a range of other methods that are subject dif-
This approach has two frailties. First, EM search
                                                               ferentially to the two problems, we can show that
over the space of all possible rules is computation-
                                                               both play an important role in performance limi-
ally impractical. Second, even if such a search
                                                               tations, and that our method helps address both as
were practical, the method is degenerate, pushing
                                                               well. Our results are thus not only encouraging for
the probability mass towards larger rules in order
                                                               grammar estimation using sparse priors but also il-
to better approximate the empirical distribution of
                                                               lustrate the merits of nonparametric inference over
the data (Goldwater et al., 2006; DeNero et al.,
                                                               the space of grammars as opposed to sparse para-
2006). Indeed, the optimal grammar would be one
                                                               metric inference with a fixed grammar.
in which each tree pair in the training data is its
own rule. Therefore, proposals for using EM for                   In the following, we define the task of extrac-
this task start with a precomputed subset of rules,            tive sentence compression and the Bayesian STSG
and with EM used just to assign weights within                 model, and algorithms we used for inference and
this grammar. In summary, previous methods suf-                prediction. We then describe the experiments in
fer from problems of narrowness of search, having              extractive sentence compression and present our
to restrict the space of possible rules, and overfit-          results in contrast with alternative algorithms. We
ting in preferring overly specific grammars.                   conclude by giving examples of compression pat-
                                                               terns learned by the Bayesian method.
   We pursue the use of hierarchical probabilistic
models incorporating sparse priors to simultane-               2    Sentence compression
ously solve both the narrowness and overfitting
                                                               Sentence compression is the task of summarizing a
problems. Such models have been used as gener-
                                                               sentence while retaining most of the informational
ative solutions to several other segmentation prob-
                                                               content and remaining grammatical (Jing, 2000).
lems, ranging from word segmentation (Goldwa-
                                                               In extractive sentence compression, which we fo-
ter et al., 2006), to parsing (Cohn et al., 2009; Post
                                                               cus on in this paper, an order-preserving subset of
and Gildea, 2009) and machine translation (DeN-
                                                               the words in the sentence are selected to form the
ero et al., 2008; Cohn and Blunsom, 2009; Liu
                                                               summary, that is, we summarize by deleting words
and Gildea, 2009). Segmentation is achieved by
                                                               (Knight and Marcu, 2002). An example sentence
introducing a prior bias towards grammars that are
                                                               pair, which we use as a running example, is the
compact representations of the data, namely by en-
                                                               following:
forcing simplicity and sparsity: preferring simple
rules (smaller segments) unless the use of a com-                  • Like FaceLift, much of ATM’s screen perfor-
plex rule is evidenced by the data (through repeti-                  mance depends on the underlying applica-
tion), and thus mitigating the overfitting problem.                  tion.
A Dirichlet process (DP) prior is typically used
to achieve this interplay. Interestingly, sampling-                • ATM’s screen performance depends on the
based nonparametric inference further allows the                     underlying application.


                                                         938


    Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression.


where the underlined words were deleted. In su-                can be used to delete a subtree rooted at PP. We
pervised sentence compression, the goal is to gen-             use square bracketed indices to represent the align-
eralize from a parallel training corpus of sentences           ment γ of frontier nodes — NP[1] aligns with
(source) and their compressions (target) to unseen             NP[1] , VP[2] aligns with VP[2] , NP[] aligns with
sentences in a test set to predict their compres-              the special symbol  denoting a deletion from the
sions. An unsupervised setup also exists; meth-                source tree. Symmetrically -aligned target nodes
ods for the unsupervised problem typically rely                are used to represent insertions into the target tree.
on language models and linguistic/discourse con-               Similarly, the rule
straints (Clarke and Lapata, 2006a; Turner and
                                                                    NP /  → (NP (NN FaceLift)) / 
Charniak, 2005). Because these methods rely on
dynamic programming to efficiently consider hy-                can be used to continue deriving the deleted sub-
potheses over the space of all possible compres-               tree. See Figure 1 for an example of how an STSG
sions of a sentence, they may be harder to extend              with these rules would operate in synchronously
to general paraphrasing.                                       generating our example sentence pair.
                                                                  STSG is a convenient choice of formalism for
3    The STSG Model                                            a number of reasons. First, it eliminates the iso-
                                                               morphism and strong independence assumptions
Synchronous tree-substitution grammar is a for-                of SCFGs. Second, the ability to have rules deeper
malism for synchronously generating a pair of                  than one level provides a principled way of model-
non-isomorphic source and target trees (Eisner,                ing lexicalization, whose importance has been em-
2003). Every grammar rule is a pair of elemen-                 phasized (Galley and McKeown, 2007; Yamangil
tary trees aligned at the leaf level at their frontier         and Nelken, 2008). Third, we may have our STSG
nodes, which we will denote using the form                     operate on trees instead of sentences, which allows
                                                               for efficient parsing algorithms, as well as provid-
                  cs /ct → es /et ,      γ                     ing syntactic analyses for our predictions, which is
                                                               desirable for automatic evaluation purposes.
(indices s for source, t for target) where cs , ct are            A straightforward extension of the popular EM
root nonterminals of the elementary trees es , et re-          algorithm for probabilistic context free grammars
spectively and γ is a 1-to-1 correspondence be-                (PCFG), the inside-outside algorithm (Lari and
tween the frontier nodes in es and et . For example,           Young, 1990), can be used to estimate the rule
the rule                                                       weights of a given unweighted STSG based on a
      S / S → (S (PP (IN Like) NP[] ) NP[1] VP[2] ) /         corpus of parallel parse trees t = t1 , . . . , tN where
                (S NP[1] VP[2] )                               tn = tn,s /tn,t for n = 1, . . . , N . Similarly, an


                                                         939


Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a
target node (top row in blue), and split/merge a deletion rule via aligning with  (bottom row in red).


extension of the Viterbi algorithm is available for            a derivation sequence (where dn is the number of
finding the maximum probability derivation, use-               rules used in the derivation), consulting Gc when-
ful for predicting the target analysis tN +1,t for a           ever an elementary tree pair with root c is to be
test instance tN +1,s . (Eisner, 2003) However, as             sampled.
noted earlier, EM is subject to the narrowness and
                                                                      iid
overfitting problems.                                             e ∼ Gc ,         for all e whose root label is c

3.1   The Bayesian generative process                          Given the derivation sequence en , a tree pair tn is
                                                               determined, that is,
Both of these issues can be addressed by taking
a nonparametric Bayesian approach, namely, as-
                                                                              
                                                                                 1      en,1 , . . . , en,dn derives tn
suming that the elementary tree pairs are sampled               p(tn | en ) =
                                                                                 0      otherwise.
from an independent collection of Dirichlet pro-                                                                      (2)
cess (DP) priors. We describe such a process for                  The hyperparameters αc can be incorporated
sampling a corpus of tree pairs t.                             into the generative model as random variables;
   For all pairs of root labels c = cs /ct that we             however, we opt to fix these at various constants
consider, where up to one of cs or ct can be  (e.g.,          to investigate different levels of sparsity.
S / S, NP / ), we sample a sparse discrete distribu-             For the base distribution P0 (· | c) there are a
tion Gc over infinitely many elementary tree pairs             variety of choices; we used the following simple
e = es /et sharing the common root c from a DP                 scenario. (We take c = cs /ct .)

            Gc ∼ DP(αc , P0 (· | c))              (1)          Synchronous rules For the case where neither cs
                                                                   nor ct are the special symbol , the base dis-
where the DP has the concentration parameter αc                    tribution first generates es and et indepen-
controlling the sparsity of Gc , and the base dis-                 dently, and then samples an alignment be-
tribution P0 (· | c) is a distribution over novel el-              tween the frontier nodes. Given a nontermi-
ementary tree pairs that we describe more fully                    nal, an elementary tree is generated by first
shortly.                                                           making a decision to expand the nontermi-
   We then sample a sequence of elementary tree                    nal (with probability βc ) or to leave it as a
pairs to serve as a derivation for each observed de-               frontier node (1 − βc ). If the decision to ex-
rived tree pair. For each n = 1, . . . , N , we sam-               pand was made, we sample an appropriate
ple elementary tree pairs en = en,1 , . . . , en,dn in             rule from a PCFG which we estimate ahead


                                                         940


      of time from the training corpus. We expand                  inference procedure that we describe next. It also
      the nonterminal using this rule, and then re-                makes clear DP’s inductive bias to reuse elemen-
      peat the same procedure for every child gen-                 tary tree pairs.
      erated that is a nonterminal until there are no                 We use Gibbs sampling (Geman and Geman,
      generated nonterminal children left. This is                 1984), a Markov chain Monte Carlo (MCMC)
      done independently for both es and et . Fi-                  method, to sample from the posterior (3). A
      nally, we sample an alignment between the                    derivation e of the corpus t is completely specified
      frontier nodes uniformly at random out of all                by an alignment between the source nodes and the
      possible alingments.                                         corresponding target nodes (as well as  on either
                                                                   side), which we take to be the state of the sampler.
Deletion/insertion rules If ct = , that is, we
                                                                   We start at a random derivation of the corpus, and
    have a deletion rule, we need to generate
                                                                   at every iteration resample a derivation by amend-
    e = es /. (The insertion rule case is symmet-
                                                                   ing the current one through local changes made
    ric.) The base distribution generates es using
                                                                   at the node level, in the style of Goldwater et al.
    the same process described for synchronous
                                                                   (2006).
    rules above. Then with probability 1 we align
                                                                      Our sampling updates are extensions of those
    all frontier nodes in es with . In essence,
                                                                   used by Cohn and Blunsom (2009) in MT, but are
    this process generates TSG rules, rather than
                                                                   tailored to our task of extractive sentence compres-
    STSG rules, which are used to cover deleted
                                                                   sion. In our task, no target node can align with
    (or inserted) subtrees.
                                                                    (which would indicate a subtree insertion), and
   This simple base distribution does nothing to                   barring unary branches no source node i can align
enforce an alignment between the internal nodes                    with two different target nodes j and j 0 at the same
of es and et . One may come up with more sophis-                   time (indicating a tree expansion). Rather, the
ticated base distributions. However the main point                 configurations of interest are those in which only
of the base distribution is to encode a control-                   source nodes i can align with , and two source
lable preference towards simpler rules; we there-                  nodes i and i0 can align with the same target node
fore make the simplest possible assumption.                        j. Thus, the alignments of interest are not arbitrary
                                                                   relations, but (partial) functions from nodes in es
3.2   Posterior inference via Gibbs sampling                       to nodes in et or . We therefore sample in the
Assuming fixed hyperparameters α = {αc } and                       direction from source to target. In particular, we
β = {βc }, our inference problem is to find the                    visit every tree pair and each of its source nodes i,
posterior distribution of the derivation sequences                 and update its alignment by selecting between and
e = e1 , . . . , eN given the observations t =                     within two choices: (a) unaligned, (b) aligned with
t1 , . . . , tN . Applying Bayes’ rule, we have                    some target node j or . The number of possibil-
                                                                   ities j in (b) is significantly limited, firstly by the
             p(e | t) ∝ p(t | e)p(e)                  (3)          word alignment (for instance, a source node dom-
                                                                   inating a deleted subspan cannot be aligned with
where p(t | e) is a 0/1 distribution (2) which does
                                                                   a target node), and secondly by the current align-
not depend on Gc , and p(e) can be obtained by
                                                                   ment of other nearby aligned source nodes. (See
collapsing Gc for all c.
                                                                   Cohn and Blunsom (2009) for details of matching
   Consider repeatedly generating elementary tree
                                                                   spans under tree constraints.)2
pairs e1 , . . . , ei , all with the same root c, iid from
Gc . Integrating over Gc , the ei become depen-                       2
                                                                        One reviewer was concerned that since we explicitly dis-
dent. The conditional prior of the i-th elementary                 allow insertion rules in our sampling procedure, our model
tree pair given previously generated ones e<i =                    that generates such rules wastes probability mass and is there-
                                                                   fore “deficient”. However, we regard sampling as a separate
e1 , . . . , ei−1 is given by                                      step from the data generation process, in which we can for-
                                                                   mulate more effective algorithms by using our domain knowl-
                            nei + αc P0 (ei | c)                   edge that our data set was created by annotators who were
       p(ei | e<i ) =                                 (4)          instructed to delete words only. Also, disallowing insertion
                                i − 1 + αc
                                                                   rules in the base distribution unnecessarily complicates the
where nei denotes the number of times ei occurs                    definition of the model, whereas it is straightforward to de-
                                                                   fine the joint distribution of all (potentially useful) rules and
in e<i . Since the collapsed model is exchangeable                 then use domain knowledge to constrain the support of that
in the ei , this formula forms the backbone of the                 distribution during inference, as we do here. In fact, it is pos-


                                                             941


   More formally, let eM be the elementary tree                          tion (MPD), which we define as
pair rooted at the closest aligned ancestor i0 of
node i when it is unaligned; and let eA and eB                                 e∗ = argmax p(e | ts , t)
                                                                                       e
be the elementary tree pairs rooted at i0 and i re-                                        X
                                                                                  = argmax    p(e | ts , e)p(e | t)
spectively when i is aligned with some target node                                            e       e
j or . Then, by exchangeability of the elementary
trees sharing the same root label, and using (4), we                     where e denotes a derivation for t = ts /tt . (We
have                                                                     suppress the N + 1 subscripts for brevity.) We
                                                                         approximate this objective first by substituting
                           neM + αcM P0 (eM | cM )                       δeMAP (e) for p(e | t) and secondly using a finite
      p(unalign) =                                 (5)                   STSG model for the infinite p(e | ts , eMAP ), which
                                 n cM + α cM
                           neA + αcA P0 (eA | cA )                       we obtain simply by normalizing the rule counts in
p(align with j) =                                  (6)                   eMAP . We use dynamic programming for parsing
                                ncA + αcA
                                                                         under this finite model (Eisner, 2003).3
                           neB + αcB P0 (eB | cB )
                      ×                            (7)                     Unfortunately, this approach does not ensure
                                n cB + α cB                              that the test instances are parsable, since ts may
                                                                         include unseen structure or novel words. A work-
where the counts ne· , nc· are with respect to the                       around is to include all zero-count context free
current derivation of the rest of the corpus; except                     copy rules such as
for neB , ncB we also make sure to account for hav-                            NP / NP → (NP NP[1] PP[2] ) / (NP NP[1] PP[2] )
ing generated eA . See Figure 2 for an illustration                            NP /  → (NP NP[] PP[] ) / 
of the sampling updates.
                                                                         in order to smooth our finite model. We used
   It is important to note that the sampler described
                                                                         Laplace smoothing (adding 1 to all counts) as it
can move from any derivation to any other deriva-
                                                                         gave us interpretable results.
tion with positive probability (if only, for example,
by virtue of fully merging and then resegment-                           4    Evaluation
ing), which guarantees convergence to the poste-
rior (3). However some of these transition prob-                         We compared the Gibbs sampling compressor
abilities can be extremely small due to passing                          (GS) against a version of maximum a posteriori
through low probability states with large elemen-                        EM (with Dirichlet parameter greater than 1) and
tary trees; in turn, the sampling procedure is prone                     a discriminative STSG based on SVM training
to local modes. In order to counteract this and to                       (Cohn and Lapata, 2008) (SVM). EM is a natural
improve mixing we used simulated annealing. The                          benchmark, while SVM is also appropriate since
probability mass function (5-7) was raised to the                        it can be taken as the state of the art for our task.4
power 1/T with T dropping linearly from T = 5                               We used a publicly available extractive sen-
to T = 0. Furthermore, using a final tempera-                            tence compression corpus: the Broadcast News
ture of zero, we recover a maximum a posteriori                          compressions corpus (BNC) of Clarke and Lap-
(MAP) estimate which we denote eMAP .                                    ata (2006a). This corpus consists of 1370 sentence
                                                                         pairs that were manually created from transcribed
                                                                         Broadcast News stories. We split the pairs into
3.3    Prediction                                                        training, development, and testing sets of 1000,
                                                                             3
We discuss the problem of predicting a target tree                             We experimented with MPT using Monte Carlo integra-
tN +1,t that corresponds to a source tree tN +1,s                        tion over possible derivations; the results were not signifi-
                                                                         cantly different from those using MPD.
unseen in the observed corpus t. The maximum                                 4
                                                                               The comparison system described by Cohn and Lapata
probability tree (MPT) can be found by consid-                           (2008) attempts to solve a more general problem than ours,
ering all possible ways to derive it. However a                          abstractive sentence compression. However, given the nature
                                                                         of the data that we provided, it can only learn to compress
much simpler alternative is to choose the target                         by deleting words. Since the system is less specialized to the
tree implied by the maximum probability deriva-                          task, their model requires additional heuristics in decoding
                                                                         not needed for extractive compression, which might cause a
                                                                         reduction in performance. Nonetheless, because the compar-
sible to prove that our approach is equivalent up to a rescaling         ison system is a generalization of the extractive SVM com-
of the concentration parameters. Since we fit these parame-              pressor of Cohn and Lapata (2007), we do not expect that the
ters to the data, our approach is equivalent.                            results would differ qualitatively.


                                                                   942


                        SVM      EM       GS                tions with annealing. All hyperparameters αc , βc
                                                            were held constant at α, β for simplicity and were
   Precision            55.60   58.80    58.94
                                                            fit using grid-search over α ∈ [10−6 , 106 ], β ∈
   Recall               53.37   56.58    64.59
                                                            [10−3 , 0.5]. The model chosen for testing was
   Relational F1        54.46   57.67    61.64
                                                            (α, β) = (100, 0.1).
   Compression rate     59.72   64.11    65.52
                                                               As an automated metric of quality, we compute
                                                            F-score based on grammatical relations (relational
Table 1: Precision, recall, relational F1 and com-          F1, or RelF1) (Riezler et al., 2003), by which the
pression rate (%) for various systems on the 200-           consistency between the set of predicted grammat-
sentence BNC test set. The compression rate for             ical relations and those from the gold standard is
the gold standard was 65.67%.                               measured, which has been shown by Clarke and
                                                            Lapata (2006b) to correlate reliably with human
                 SVM      EM       GS     Gold              judgments. We also conducted a small human sub-
                                                            jective evaluation of the grammaticality and infor-
  Grammar         2.75† 2.85∗ 3.69         4.25
                                                            mativeness of the compressions generated by the
  Importance      2.85  2.67∗ 3.41         3.82
                                                            various methods.
  Comp. rate     68.18 64.07 67.97        62.34
                                                            4.1   Automated evaluation
Table 2: Average grammar and importance scores              For all three systems we obtained predictions for
for various systems on the 20-sentence subsam-              the test set and used the Stanford parser to extract
ple. Scores marked with ∗ are significantly dif-            grammatical relations from predicted trees and the
ferent than the corresponding GS score at α < .05           gold standard. We computed precision, recall,
and with † at α < .01 according to post-hoc Tukey           RelF1 (all based on grammatical relations), and
tests. ANOVA was significant at p < .01 both for            compression rate (percentage of the words that are
grammar and importance.                                     retained), which we report in Table 1. The results
                                                            for GS are averages over five independent runs.
                                                            EM gives a strong baseline since it already uses
170, and 200 pairs, respectively. The corpus was            rules that are limited in depth and number of fron-
parsed using the Stanford parser (Klein and Man-            tier nodes by stipulation, helping with the overfit-
ning, 2003).                                                ting we have mentioned, surprisingly outperform-
   In our experiments with the publicly available           ing its discriminative counterpart in both precision
SVM system we used all except paraphrasal rules             and recall (and consequently RelF1). GS however
extracted from bilingual corpora (Cohn and Lap-             maintains the same level of precision as EM while
ata, 2008). The model chosen for testing had pa-            improving recall, bringing an overall improvement
rameter for trade-off between training error and            in RelF1.
margin set to C = 0.001, used margin rescaling,
and Hamming distance over bags of tokens with               4.2   Human evaluation
brevity penalty for loss function. EM used a sub-           We randomly subsampled our 200-sentence test
set of the rules extracted by SVM, namely all rules         set for 20 sentences to be evaluated by human
except non-head deleting compression rules, and             judges through Amazon Mechanical Turk. We
was initialized uniformly. Each EM instance was             asked 15 self-reported native English speakers for
characterized by two parameters: α, the smooth-             their judgments of GS, EM, and SVM output sen-
ing parameter for MAP-EM, and δ, the smooth-                tences and the gold standard in terms of grammat-
ing parameter for augmenting the learned gram-              icality (how fluent the compression is) and impor-
mar with rules extracted from unseen data (add-             tance (how much of the meaning of and impor-
(δ − 1) smoothing was used), both of which were             tant information from the original sentence is re-
fit to the development set using grid-search over           tained) on a scale of 1 (worst) to 5 (best). We re-
(1, 2]. The model chosen for testing was (α, δ) =           port in Table 2 the average scores. EM and SVM
(1.0001, 1.01).                                             perform at very similar levels, which we attribute
   GS was initialized at a random derivation. We            to using the same set of rules, while GS performs
sampled the alignments of the source nodes in ran-          at a level substantially better than both, and much
dom order. The sampler was run for 5000 itera-              closer to human performance in both criteria. The


                                                      943


                                                           mars to find one that bests represents the corpus.
                                                           It is possible to introduce DP-like sparsity in EM
                                                           using variational Bayes (VB) training. We exper-
                                                           iment with this next in order to understand how
                                                           dominant the two factors are. The VB algorithm
                                                           requires a simple update to the M-step formulas
                                                           for EM where the expected rule counts are normal-
                                                           ized, such that instead of updating the rule weight
                                                           in the t-th iteration as in the following

                                                                           t+1      nc,e + α − 1
                                                                          θc,e =
                                                                                   nc,. + Kα − K

                                                           where nc,e represents the expected count of rule
                                                           c → e, and K is the total number of ways
                                                           to rewrite c, we now take into account our
                                                           DP(αc , P0 (· | c)) prior in (1), which, when
                                                           truncated to a finite grammar, reduces to a
                                                           K-dimensional Dirichlet prior with parameter
                                                           αc P0 (· | c). Thus in VB we perform a variational
                                                           E-step with the subprobabilities given by

                                                                   t+1      exp (Ψ(nc,e + αc P0 (e | c)))
                                                                  θc,e =
                                                                                exp (Ψ(nc,. + αc ))

                                                           where Ψ denotes the digamma function. (Liu and
                                                           Gildea, 2009) (See MacKay (1997) for details.)
                                                           Hyperparameters were handled the same way as
                                                           for GS.
                                                              Instead of selecting a single model on the devel-
                                                           opment set, here we provide the whole spectrum of
                                                           models and their performances in order to better
                                                           understand their comparative behavior. In Figure
                                                           3 we plot RelF1 on the test set versus compres-
                                                           sion rate and compare GS, EM, and VB (β = 0.1
                                                           fixed, (α, δ) ranging in [10−6 , 106 ]×(1, 2]). Over-
                                                           all, we see that GS maintains roughly the same
Figure 3: RelF1, precision, recall plotted against         level of precision as EM (despite its larger com-
compression rate for GS, EM, VB.                           pression rates) while achieving an improvement in
                                                           recall, consequently performing at a higher RelF1
                                                           level. We note that VB somewhat bridges the gap
human evaluation indicates that the superiority of         between GS and EM, without quite reaching GS
the Bayesian nonparametric method is underap-              performance. We conclude that the mitigation of
preciated by the automated evaluation metric.              the two factors (narrowness and overfitting) both
                                                           contribute to the performance gain of GS.5
4.3   Discussion
The fact that GS performs better than EM can be            4.4   Example rules learned
attributed to two reasons: (1) GS uses a sparse            In order to provide some insight into the grammar
prior and selects a compact representation of the          extracted by GS, we list in Tables (3) and (4) high
data (grammar sizes ranged from 4K-7K for GS
                                                               5
compared to a grammar of about 35K rules for                     We have also experimented with VB with parametric in-
                                                           dependent symmetric Dirichlet priors. The results were sim-
EM). (2) GS does not commit to a precomputed               ilar to EM with the exception of sparse priors resulting in
grammar and searches over the space of all gram-           smaller grammars and slightly improving performance.


                                                     944


         (ROOT (S CC[] NP[1] VP[2] .[3] ))                             / (ROOT (S NP[1] VP[2] .[3] ))
         (ROOT (S NP[1] ADVP[] VP[2] (. .)))                           / (ROOT (S NP[1] VP[2] (. .)))
         (ROOT (S ADVP[] (, ,) NP[1] VP[2] (. .)))                     / (ROOT (S NP[1] VP[2] (. .)))
         (ROOT (S PP[] (, ,) NP[1] VP[2] (. .)))                       / (ROOT (S NP[1] VP[2] (. .)))
         (ROOT (S PP[] ,[] NP[1] VP[2] .[3] ))                        / (ROOT (S NP[1] VP[2] .[3] ))
         (ROOT (S NP[] (VP VBP[] (SBAR (S NP[1] VP[2] ))) .[3] ))     / (ROOT (S NP[1] VP[2] .[3] ))
         (ROOT (S ADVP[] NP[1] (VP MD[2] VP[3] ) .[4] ))               / (ROOT (S NP[1] (VP MD[2] VP[3] ) .[4] ))
         (ROOT (S (SBAR (IN as) S[] ) ,[] NP[1] VP[2] .[3] ))         / (ROOT (S NP[1] VP[2] .[3] ))
         (ROOT (S S[] (, ,) CC[] (S NP[1] VP[2] ) .[3] ))             / (ROOT (S NP[1] VP[2] .[3] ))
         (ROOT (S PP[] NP[1] VP[2] .[3] ))                             / (ROOT (S NP[1] VP[2] .[3] ))
         (ROOT (S S[1] (, ,) CC[] S[2] (. .)))                         / (ROOT (S NP[1] VP[2] (. .)))
         (ROOT (S S[] ,[] NP[1] ADVP[2] VP[3] .[4] ))                 / (ROOT (S NP[1] ADVP[2] VP[3] .[4] ))
         (ROOT (S (NP (NP NNP[] (POS ’s)) NNP[1] NNP[2] )              / (ROOT (S (NP NNP[1] NNP[2] )
            (VP (VBZ reports)) .[3] ))                                       (VP (VBZ reports)) .[3] ))

    Table 3: High probability ROOT / ROOT compression rules from the final state of the sampler.
              (S NP[1] ADVP[] VP[2] )                                        / (S NP[1] VP[2] )
              (S INTJ[] (, ,) NP[1] VP[2] (. .))                             / (S NP[1] VP[2] (. .))
              (S (INTJ (UH Well)) ,[] NP[1] VP[2] .[3] )                     / (S NP[1] VP[2] .[3] )
              (S PP[] (, ,) NP[1] VP[2] )                                    / (S NP[1] VP[2] )
              (S ADVP[] (, ,) S[1] (, ,) (CC but) S[2] .[3] )                / (S S[1] (, ,) (CC but) S[2] .[3] )
              (S ADVP[] NP[1] VP[2] )                                        / (S NP[1] VP[2] )
              (S NP[] (VP VBP[] (SBAR (IN that) (S NP[1] VP[2] ))) (. .))   / (S NP[1] VP[2] (. .))
              (S NP[] (VP VBZ[] ADJP[] SBAR[1] ))                          / S[1]
              (S CC[] PP[] (, ,) NP[1] VP[2] (. .))                         / (S NP[1] VP[2] (. .))
              (S NP[] (, ,) NP[1] VP[2] .[3] )                               / (S NP[1] VP[2] .[3] )
              (S NP[1] (, ,) ADVP[] (, ,) VP[2] )                            / (S NP[1] VP[2] )
              (S CC[] (NP PRP[1] ) VP[2] )                                   / (S (NP PRP[1] ) VP[2] )
              (S ADVP[] ,[] PP[] ,[] NP[1] VP[2] .[3] )                   / (S NP[1] VP[2] .[3] )
              (S ADVP[] (, ,) NP[1] VP[2] )                                  / (S NP[1] VP[2] )

          Table 4: High probability S / S compression rules from the final state of the sampler.


probability subtree-deletion rules expanding cate-               gate the effects of sparse priors and nonparamet-
gories ROOT / ROOT and S / S, respectively. Of                   ric inference over the space of grammars. We
especial interest are deep lexicalized rules such as             showed that, despite its degeneracy, expectation
                                                                 maximization is a strong baseline when given a
                                                                 reasonable grammar. However, Gibbs-sampling–
                                                                 based nonparametric inference achieves improve-
                                                                 ments against this baseline. Our investigation with
                                                                 variational Bayes showed that the improvement is
                                                                 due both to finding sparse grammars (mitigating
                                                                 overfitting) and to searching over the space of all
                                                                 grammars (mitigating narrowness). Overall, we
a pattern of compression used many times in the                  take these results as being encouraging for STSG
BNC in sentence pairs such as “NPR’s Anne Gar-                   induction via Bayesian nonparametrics for mono-
rels reports” / “Anne Garrels reports”. Such an                  lingual translation tasks. The future for this work
informative rule with nontrivial collocation (be-                would involve natural extensions such as mixing
tween the possessive marker and the word “re-                    over the space of word alignments; this would al-
ports”) would be hard to extract heuristically and               low application to MT-like tasks where flexible
can only be extracted by reasoning across the                    word reordering is allowed, such as abstractive
training examples.                                               sentence compression and paraphrasing.

5   Conclusion
We explored nonparametric Bayesian learning                      References
of non-isomorphic tree mappings using Dirich-                    James Clarke and Mirella Lapata. 2006a. Constraint-
let process priors. We used the task of extrac-                    based sentence compression: An integer program-
tive sentence compression as a testbed to investi-                 ming approach. In Proceedings of the 21st Interna-


                                                           945


  tional Conference on Computational Linguistics and            Jason Eisner. 2003. Learning non-isomorphic tree
  44th Annual Meeting of the Association for Compu-                mappings for machine translation. In ACL ’03: Pro-
  tational Linguistics, pages 144–151, Sydney, Aus-                ceedings of the 41st Annual Meeting on Associa-
  tralia, July. Association for Computational Linguis-             tion for Computational Linguistics, pages 205–208,
  tics.                                                            Morristown, NJ, USA. Association for Computa-
                                                                   tional Linguistics.
James Clarke and Mirella Lapata. 2006b. Models
  for sentence compression: A comparison across do-             Michel Galley and Kathleen McKeown. 2007. Lex-
  mains, training requirements and evaluation mea-                icalized Markov grammars for sentence compres-
  sures. In Proceedings of the 21st International Con-            sion. In Human Language Technologies 2007:
  ference on Computational Linguistics and 44th An-               The Conference of the North American Chapter of
  nual Meeting of the Association for Computational               the Association for Computational Linguistics; Pro-
  Linguistics, pages 377–384, Sydney, Australia, July.            ceedings of the Main Conference, pages 180–187,
  Association for Computational Linguistics.                      Rochester, New York, April. Association for Com-
                                                                  putational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
  model of syntax-directed tree to string grammar in-           Michel Galley, Mark Hopkins, Kevin Knight, and
  duction. In EMNLP ’09: Proceedings of the 2009                  Daniel Marcu. 2004. What’s in a translation
  Conference on Empirical Methods in Natural Lan-                 rule? In Daniel Marcu Susan Dumais and Salim
  guage Processing, pages 352–361, Morristown, NJ,                Roukos, editors, HLT-NAACL 2004: Main Proceed-
  USA. Association for Computational Linguistics.                 ings, pages 273–280, Boston, Massachusetts, USA,
                                                                  May 2 - May 7. Association for Computational Lin-
Trevor Cohn and Mirella Lapata. 2007. Large mar-                  guistics.
  gin synchronous generation and its application to
  sentence compression. In Proceedings of the Con-              S. Geman and D. Geman. 1984. Stochastic Relaxation,
  ference on Empirical Methods in Natural Language                 Gibbs Distributions and the Bayesian Restoration of
  Processing and on Computational Natural Lan-                     Images. pages 6:721–741.
  guage Learning, pages 73–82, Prague. Association
  for Computational Linguistics.                                Sharon Goldwater, Thomas L. Griffiths, and Mark
                                                                  Johnson. 2006. Contextual dependencies in un-
Trevor Cohn and Mirella Lapata. 2008. Sentence                    supervised word segmentation. In Proceedings of
  compression beyond word deletion. In COLING                     the 21st International Conference on Computational
  ’08: Proceedings of the 22nd International Confer-              Linguistics and 44th Annual Meeting of the Associa-
  ence on Computational Linguistics, pages 137–144,               tion for Computational Linguistics, pages 673–680,
  Manchester, United Kingdom. Association for Com-                Sydney, Australia, July. Association for Computa-
  putational Linguistics.                                         tional Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-                   Hongyan Jing. 2000. Sentence reduction for auto-
  som. 2009. Inducing compact but accurate tree-                  matic text summarization. In Proceedings of the
  substitution grammars. In NAACL ’09: Proceed-                   sixth conference on Applied natural language pro-
  ings of Human Language Technologies: The 2009                   cessing, pages 310–315, Morristown, NJ, USA. As-
  Annual Conference of the North American Chap-                   sociation for Computational Linguistics.
  ter of the Association for Computational Linguistics,
  pages 548–556, Morristown, NJ, USA. Association               Dan Klein and Christopher D. Manning. 2003. Fast
  for Computational Linguistics.                                  exact inference with a factored model for natural
                                                                  language parsing. In Advances in Neural Informa-
A. Dempster, N. Laird, and D. Rubin. 1977. Max-                   tion Processing Systems 15 (NIPS, pages 3–10. MIT
  imum likelihood from incomplete data via the EM                 Press.
  algorithm. Journal of the Royal Statistical Society,
  39 (Series B):1–38.                                           Kevin Knight and Daniel Marcu. 2002. Summa-
                                                                  rization beyond sentence extraction: a probabilis-
John DeNero, Dan Gillick, James Zhang, and Dan                    tic approach to sentence compression. Artif. Intell.,
  Klein. 2006. Why generative phrase models under-                139(1):91–107.
  perform surface heuristics. In StatMT ’06: Proceed-
  ings of the Workshop on Statistical Machine Trans-            K. Lari and S. J. Young. 1990. The estimation of
  lation, pages 31–38, Morristown, NJ, USA. Associ-               stochastic context-free grammars using the Inside-
  ation for Computational Linguistics.                            Outside algorithm. Computer Speech and Lan-
                                                                  guage, 4:35–56.
John DeNero, Alexandre Bouchard-Côté, and Dan
  Klein. 2008. Sampling alignment structure under               Ding Liu and Daniel Gildea. 2009. Bayesian learn-
  a Bayesian translation model. In EMNLP ’08: Pro-                ing of phrasal tree-to-string templates. In Proceed-
  ceedings of the Conference on Empirical Methods in              ings of the 2009 Conference on Empirical Methods
  Natural Language Processing, pages 314–323, Mor-                in Natural Language Processing, pages 1308–1317,
  ristown, NJ, USA. Association for Computational                 Singapore, August. Association for Computational
  Linguistics.                                                    Linguistics.


                                                          946


David J.C. MacKay. 1997. Ensemble learning for hid-
  den markov models. Technical report, Cavendish
  Laboratory, Cambridge, UK.
Franz Josef Och and Hermann Ney. 2004. The align-
  ment template approach to statistical machine trans-
  lation. Comput. Linguist., 30(4):417–449.
Matt Post and Daniel Gildea. 2009. Bayesian learning
 of a tree substitution grammar. In Proceedings of the
 ACL-IJCNLP 2009 Conference Short Papers, pages
 45–48, Suntec, Singapore, August. Association for
 Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and
   Annie Zaenen. 2003. Statistical sentence condensa-
   tion using ambiguity packing and stochastic disam-
   biguation methods for lexical-functional grammar.
   In NAACL ’03: Proceedings of the 2003 Conference
   of the North American Chapter of the Association
   for Computational Linguistics on Human Language
   Technology, pages 118–125, Morristown, NJ, USA.
   Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
   vised and unsupervised learning for sentence com-
   pression. In ACL ’05: Proceedings of the 43rd An-
   nual Meeting on Association for Computational Lin-
   guistics, pages 290–297, Morristown, NJ, USA. As-
   sociation for Computational Linguistics.

Elif Yamangil and Rani Nelken. 2008. Mining
   wikipedia revision histories for improving sentence
   compression. In Proceedings of ACL-08: HLT,
   Short Papers, pages 137–140, Columbus, Ohio,
   June. Association for Computational Linguistics.




                                                         947

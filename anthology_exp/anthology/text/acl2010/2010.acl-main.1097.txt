                       Contextualizing Semantic Representations
                       Using Syntactically Enriched Vector Models

                   Stefan Thater and Hagen Fürstenau and Manfred Pinkal
                            Department of Computational Linguistics
                                       Saarland University
                           {stth, hagenf, pinkal}@coli.uni-saarland.de


                      Abstract                                formation retrieval (Manning et al., 2008), word-
                                                              sense discrimination (Schütze, 1998) and disam-
    We present a syntactically enriched vec-                  biguation (McCarthy and Carroll, 2003), to name
    tor model that supports the computation                   but a few.
    of contextualized semantic representations                   Standard vector-space models have serious lim-
    in a quasi compositional fashion. It em-                  itations, however: While semantic information is
    ploys a systematic combination of first- and              typically encoded in phrases and sentences, distri-
    second-order context vectors. We apply                    butional semantics, in sharp contrast to logic-based
    our model to two different tasks and show                 semantics, does not offer any natural concept of
    that (i) it substantially outperforms previ-              compositionality that would allow the semantics
    ous work on a paraphrase ranking task, and                of a complex expression to be computed from the
    (ii) achieves promising results on a word-                meaning of its parts. A different, but related prob-
    sense similarity task; to our knowledge, it is            lem is caused by word-sense ambiguity and con-
    the first time that an unsupervised method                textual variation of usage. Frequency counts of
    has been applied to this task.                            context words for a given target word provide in-
                                                              variant representations averaging over all different
1   Introduction                                              usages of the target word. There is no obvious way
In the logical paradigm of natural-language seman-            to distinguish the different senses of e.g. acquire
tics originating from Montague (1973), semantic               in different contexts, such as acquire knowledge or
structure, composition and entailment have been               acquire shares.
modelled to an impressive degree of detail and                   Several approaches for word-sense disambigua-
formal consistency. These approaches, however,                tion in the framework of distributional semantics
lack coverage and robustness, and their impact                have been proposed in the literature (Schütze, 1998;
on realistic natural-language applications is lim-            McCarthy and Carroll, 2003). In contrast to these
ited: The logical framework suffers from over-                approaches, we present a method to model the mu-
specificity, and is inappropriate to model the per-           tual contextualization of words in a phrase in a com-
vasive vagueness, ambivalence, and uncertainty                positional way, guided by syntactic structure. To
of natural-language semantics. Also, the hand-                some extent, our method resembles the approaches
crafting of resources covering the huge amounts               proposed by Mitchell and Lapata (2008) and Erk
of content which are required for deep semantic               and Padó (2008). We go one step further, however,
processing is highly inefficient and expensive.               in that we employ syntactically enriched vector
   Co-occurrence-based semantic vector models of-             models as the basic meaning representations, as-
fer an attractive alternative. In the standard ap-            suming a vector space spanned by combinations
proach, word meaning is represented by feature                of dependency relations and words (Lin, 1998).
vectors, with large sets of context words as dimen-           This allows us to model the semantic interaction
sions, and their co-occurrence frequencies as val-            between the meaning of a head word and its de-
ues. Semantic similarity information can be ac-               pendent at the micro-level of relation-specific co-
quired using unsupervised methods at virtually no             occurrence frequencies. It turns out that the benefit
cost, and the information gained is soft and gradual.         to precision is considerable.
Many NLP tasks have been modelled successfully                   Using syntactically enriched vector models
using vector-based models. Examples include in-               raises problems of different kinds: First, the use


                                                         948
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


of syntax increases dimensionality and thus may           that are similar to p and a, and takes the centroid
cause data sparseness (Padó and Lapata, 2007).            of these words’ vectors to be the representation of
Second, the vectors of two syntactically related          the complex expression p(a).
words, e.g., a target verb acquire and its direct ob-      Mitchell and Lapata (2008), henceforth M&L,
ject knowledge, typically have different syntactic      propose a general framework in which meaning rep-
environments, which implies that their vector repre-    resentations for complex expressions are computed
sentations encode complementary information and         compositionally by combining the vector represen-
there is no direct way of combining the information     tations of the individual words of the complex ex-
encoded in the respective vectors.                      pression. They focus on the assessment of different
   To solve these problems, we build upon pre-          operations combining the vectors of the subexpres-
vious work (Thater et al., 2009) and propose to         sions. An important finding is that component-wise
use syntactic second-order vector representations.      multiplication outperforms the more common addi-
Second-order vector representations in a bag-of-        tion method. Although their composition method
words setting were first used by Schütze (1998);        is guided by syntactic structure, the actual instanti-
in a syntactic setting, they also feature in Dligach    ations of M&L’s framework are insensitive to syn-
and Palmer (2008). For the problem at hand, the         tactic relations and word-order, assigning identical
use of second-order vectors alleviates the sparse-      representation to dog bites man and man bites dog
ness problem, and enables the definition of vector      (see Erk and Padó (2008) for a discussion). Also,
space transformations that make the distributional      they use syntax-free bag-of-words-based vectors as
information attached to words in different syntactic    basic representations of word meaning.
positions compatible. Thus, it allows vectors for
                                                             Erk and Padó (2008), henceforth E&P, represent
a predicate and its arguments to be combined in a
                                                          the meaning of a word w through a collection of
compositional way.
                                                          vectors instead of a single vector: They assume
   We conduct two experiments to assess the suit-
                                                          selectional preferences and inverse selectional pref-
ability of our method. Our first experiment is car-
                                                          erences to be constitutive parts of the meaning in
ried out on the SemEval 2007 lexical substitution
                                                          addition to the meaning proper. The interpretation
task dataset (McCarthy and Navigli, 2007). It will
                                                          of a word p in context a is a combination of p’s
show that our method significantly outperforms
                                                          meaning with the (inverse) selectional preference
other unsupervised methods that have been pro-
                                                          of a. Thus, a verb meaning does not combine di-
posed in the literature to rank words with respect
                                                          rectly with the meaning of its object noun, as on
to their semantic similarity in a given linguistic
                                                          the M&L account, but with the centroid of the vec-
context. In a second experiment, we apply our
                                                          tors of the verbs to which the noun can stand in an
model to the “word sense similarity task” recently
                                                          object relation. Clearly, their approach is sensitive
proposed by Erk and McCarthy (2009), which is
                                                          to syntactic structure. Their evaluation shows that
a refined variant of a word-sense disambiguation
                                                          their model outperforms the one proposed by M&L
task. The results show a substantial positive effect.
                                                          on a lexical substitution task (see Section 4). The
Plan of the paper. We will first review related           basic vectors, however, are constructed in a word
work in Section 2, before presenting our model in         space similar to the one of the M&L approach.
Section 3. In Sections 4 and 5 we evaluate our             In Thater et al. (2009), henceforth TDP, we took
model on the two different tasks. Section 6 con-        up the basic idea from E&P of exploiting selec-
cludes.                                                 tional preference information for contextualization.
                                                        Instead of using collections of different vectors,
2   Related Work
                                                        we incorporated syntactic information by assuming
Several approaches to contextualize vector repre-       a richer internal structure of the vector represen-
sentations of word meaning have been proposed.          tations. In a small case study, moderate improve-
One common approach is to represent the mean-           ments over E&P on a lexical substitution task could
ing of a word a in context b simply as the sum, or      be shown. In the present paper, we formulate a
centroid of a and b (Landauer and Dumais, 1997).        general model of syntactically informed contextu-
  Kintsch (2001) considers a variant of this simple     alization and show how to apply it to a number a
model. By using vector representations of a predi-      of representative lexical substitution tasks. Eval-
cate p and an argument a, Kintsch identifies words      uation shows significant improvements over TDP


                                                    949


                                                                            introduce another kind of vectors capturing infor-
    gainVB    skillNN         acquireVB       buy-backNN     purchaseVB
                  conj, 2
                                                                            mations about all words that can be reached with
                                                nn, 1
         obj, 5             obj, 3   obj, 6             obj, 7
                                                                            two steps in the co-occurrence graph. Such a path
             knowlegeNN                   shareNN                           is characterized by two dependency relations and
                                                                            two words, i.e., a quadruple (r, w0 , r0 , w00 ), whose
                                                                            weight is the product of the weights of the two
Figure 1: Co-occurrence graph of a small sample                             edges used in the path. To avoid overly sparse vec-
corpus of dependency trees.                                                 tors we generalize over the “middle word” w0 and
                                                                            build our second-order vectors on the dimensions
and E&P.                                                                    corresponding to triples (r, r0 , w00 ) of two depen-
                                                                            dency relations and one word at the end of the two-
3     The model                                                             step path. For instance, the second-order vector for
                                                                            acquire is
In this section, we present our method of contex-
tualizing semantic vector representations. We first                                         h15(OBJ,OBJ−1 ,gain) ,
give an overview of the main ideas, which is fol-                                            6(OBJ,CONJ−1 ,skill) ,
lowed by a technical description of first-order and                                          6(OBJ,OBJ−1 ,buy-back) ,
second-order vectors (Section 3.2) and the contex-                                           42(OBJ,OBJ−1 ,purchase) , . . .i
tualization operation (Section 3.3).
                                                                            In this simple example, the values are the prod-
3.1     Overview                                                            ucts of the edge weights on each of the paths. The
                                                                            method of computation is detailed in Section 3.2.
Our model employs vector representations for
                                                                            Note that second order vectors in particular con-
words and expressions containing syntax-specific
                                                                            tain paths of the form (r, r−1 , w0 ), relating a verb
first and second order co-occurrences information.
                                                                            w to other verbs w0 which are possible substitution
   The basis for the construction of both kinds of
                                                                            candidates.
vector representations are co-occurrence graphs.
                                                                               With first- and second-order vectors we can
Figure 1 shows the co-occurrence graph of a small
                                                                            now model the interaction of semantic informa-
sample corpus of dependency trees: Words are
                                                                            tion within complex expressions. Given a pair
represented as nodes in the graph, possible depen-
                                                                            of words in a particular grammatical relation like
dency relations between them are drawn as labeled
                                                                            acquire knowledge, we contextualize the second-
edges, with weights corresponding to the observed
                                                                            order vector of acquire with the first-order vec-
frequencies. From this graph, we can directly read
                                                                            tor of knowledge. We let the first-order vector
off the first-order vector for every word w: the vec-
                                                                            with its selectional preference information act as a
tor’s dimensions correspond to pairs (r, w0 ) of a
                                                                            kind of weighting filter on the second-order vector,
grammatical relation and a neighboring word, and
                                                                            and thus refine the meaning representation of the
are assigned the frequency count of (w, r, w0 ).
                                                                            verb. The actual operation we will use is point-
   The noun knowledge, for instance, would be rep-
                                                                            wise multiplication, which turned out to be the
resented by the following vector:
                                                                            best-performing one for our purpose. Interestingly,
                                                                            Mitchell and Lapata (2008) came to the same result
    h5(O BJ−1 ,gain) , 2(C ONJ−1 ,skill) , 3(O BJ−1 ,acquire) , . . .i
                                                                            in a different setting.
This vector talks about the possible dependency                                In our example, we obtain a new second-order
heads of knowledge and thus can be seen as the                              vector for acquire in the context of knowledge:
(inverse) selectional preference of knowledge (see                                           h75(OBJ,OBJ−1 ,gain) ,
Erk and Padó (2008)).                                                                         12(OBJ,CONJ−1 ,skill) ,
   As soon as we want to compute a meaning rep-                                               0(OBJ,OBJ−1 ,buy-back) ,
resentation for a phrase like acquire knowledge                                               0(OBJ,OBJ−1 ,purchase) , . . .i
from the verb acquire together with its direct ob-
ject knowledge, we are facing the problem that                                  Note that all dimensions that are not “licensed” by
verbs have different syntactic neighbors than nouns,                            the argument knowledge are filtered out as they are
hence their first-order vectors are not easily com-                             multiplied with 0. Also, contextualisation of ac-
parable. To solve this problem we additionally                                  quire with the argument share instead of knowledge


                                                                          950


would have led to a very different vector, which                              For example, if w is a verb, r = O BJ and r0 =
reflects the fact that the two argument nouns induce                        O BJ−1 (i.e., the inverse object relation), then the
different readings of the inherently ambiguous ac-                          coefficients of ~er,r0 ,w00 in [[w]] would characterize
quire.                                                                      the distribution of verbs w00 which share objects
                                                                            with w.
3.2   First and second-order vectors
Assuming a set W of words and a set R of depen-                                 3.3    Composition
dency relation labels, we consider a Euclidean vec-                             Both first and second-order vectors are defined for
tor space V1 spanned by the set of orthonormal                                  lexical expressions only. In order to represent the
basis vectors {~er,w0 | r ∈ R, w0 ∈ W }, i.e., a vector                         meaning of complex expressions we need to com-
space whose dimensions correspond to pairs of a re-                             bine the vectors for grammatically related words
lation and a word. Recall that any vector of V1 can                             in a given sentence. Given two words w and w0 in
be represented as a finite sum of the form ∑ ai~er,w0                           relation r we contextualize the second-order vector
with appropriate scalar factors ai . In this vector                             of w with the r-lifted first-order vector of w0 :
space we define the first-order vector [w] of a word
w as follows:                                                                                 [[wr:w0 ]] = [[w]] × Lr ([w0 ])
                    [w] =     ∑     ω(w, r, w0 ) ·~er,w0
                             r∈R                                            Here × may denote any operator on V2 . The ob-
                            w0 ∈W                                           jective is to incorporate (inverse) selectional pref-
where ω is a function that assigns the dependency                           erence information from the context (r, w0 ) in such
triple (w, r, w0 ) a corresponding weight. In the sim-                      a way as to identify the correct word sense of w.
plest case, ω would denote the frequency in a cor-                          This suggests that the dimensions of [[w]] should
pus of dependency trees of w occurring together                             be filtered so that only those compatible with the
with w0 in relation r. In the experiments reported be-                      context remain. A more flexible approach than
low, we use pointwise mutual information (Church                            simple filtering, however, is to re-weight those di-
and Hanks, 1990) instead as it proved superior to                           mensions with context information. This can be
raw frequency counts:                                                       expressed by pointwise vector multiplication (in
                                                                            terms of the given basis of V2 ). We therefore take
                                          p(w, w0 | r)
          pmi(w, r, w0 ) = log                                              × to be pointwise multiplication.
                                        p(w | r)p(w0 | r)
                                                                               To contextualize (the vector of) a word w with
   We further consider a similarly defined vec-                             multiple words w1 , . . . , wn and corresponding rela-
tor space V2 , spanned by an orthonormal basis                              tions r1 , . . . , rn , we compute the sum of the results
{~er,r0 ,w0 | r, r0 ∈ R, w0 ∈ W }. Its dimensions there-                    of the pairwise contextualizations of the target vec-
fore correspond to triples of two relations and a                           tor with the vectors of the respective dependents:
word. Evidently this is a higher dimensional space
                                                                                                                         n
than V1 , which therefore can be embedded into
                                                                                           [[wr1 :w1 ,...,rn :wn ]] =   ∑ [[wr :w ]]
V2 by the “lifting maps” Lr : V1 ,→ V2 defined by                                                                       k=1
                                                                                                                               k   k

Lr (~er0 ,w0 ) := ~er,r0 ,w0 (and by linear extension there-
fore on all vectors of V1 ). Using these lifting maps                           4     Experiments: Ranking Paraphrases
we define the second-order vector [[w]] of a word w
as                                                                          In this section, we evaluate our model on a para-
              [[w]] = ∑ ω(w, r, w0 ) · Lr [w0 ]
                                                                           phrase ranking task. We consider sentences with
                         r∈R                                                an occurrence of some target word w and a list of
                        w0 ∈W
                                                                            paraphrase candidates w1 , . . . , wk such that each of
Substituting the definitions of Lr and [w0 ], this                          the wi is a paraphrase of w for some sense of w.
yields                                                                      The task is to decide for each of the paraphrase
                                        !                                   candidates wi how appropriate it is as a paraphrase
[[w]] =    ∑           ∑      ω(w, r, w0 )ω(w0 , r0 , w00 ) ~er,r0 ,w00     of w in the given context. For instance, buy, pur-
          r,r0 ∈R     w0 ∈W                                                 chase and obtain are all paraphrases of acquire, in
          w00 ∈W
                                                                            the sense that they can be substituted for acquire in
which shows the generalization over w0 in form of                           some contexts, but purchase and buy are not para-
the inner sum.                                                              phrases of acquire in the first sentence of Table 1.


                                                                          951


 Sentence                                                         Paraphrases
 Teacher education students will acquire the knowl- gain 4; amass 1; receive 1; obtain 1
 edge and skills required to [. . . ]
 Ontario Inc. will [. . . ] acquire the remaining IXOS buy 3; purchase 1; gain 1; get 1; procure 2; obtain 1
 shares [. . . ]

                       Table 1: Two examples from the lexical substitution task data set


4.1   Resources                                                 variant of average precision.
                                                                   Average precision (Buckley and Voorhees, 2000)
We use a vector model based on dependency trees
                                                                is a measure commonly used to evaluate systems
obtained from parsing the English Gigaword corpus
                                                                that return ranked lists of results. Generalized aver-
(LDC2003T05). The corpus consists of news from
                                                                age precision (GAP) additionally rewards the cor-
several newswire services, and contains over four
                                                                rect order of positive cases w.r.t. their gold standard
million documents. We parse the corpus using the
                                                                weight. We define average precision first:
Stanford parser1 (de Marneffe et al., 2006) and a
non-lexicalized parser model, and extract over 1.4                              Σni=1 xi pi                     Σik=1 xk
billion dependency triples for about 3.9 million                         AP =                          pi =
                                                                                    R                               i
words (lemmas) from the parsed corpus.
   To evaluate the performance of our model, we                 where xi is a binary variable indicating whether
use various subsets of the SemEval 2007 lexical                 the ith item as ranked by the model is in the gold
substitution task (McCarthy and Navigli, 2007)                  standard or not, R is the size of the gold standard,
dataset. The complete dataset contains 10 instances             and n is the number of paraphrase candidates to
for each of 200 target words—nouns, verbs, adjec-               be ranked. If we take xi to be the gold standard
tives and adverbs—in different sentential contexts.             weight of the ith item or zero if it is not in the
Systems that participated in the task had to generate           gold standard, we can define generalized average
paraphrases for every instance, and were evaluated              precision as follows:
against a gold standard containing up to 10 possible                                          ∑ni=1 I(xi ) pi
paraphrases for each of the individual instances.                                GAP =
                                                                                              ∑Ri=1 I(yi )yi
   There are two natural subtasks in generating
paraphrases: identifying paraphrase candidates and              where I(xi ) = 1 if xi is larger than zero, zero oth-
ranking them according to the context. We follow                erwise, and yi is the average weight of the ideal
E&P and evaluate it only on the second subtask:                 ranked list y1 , . . . , yi of gold standard paraphrases.
we extract paraphrase candidates from the gold                     As a second scoring method, we use precision
standard by pooling all annotated gold-standard                 out of ten (P10 ). The measure is less discriminative
paraphrases for all instances of a verb in all con-             than GAP. We use it because we want to compare
texts, and use our model to rank these paraphrase               our model with E&P. P10 measures the percentage
candidates in specific contexts. Table 1 shows two              of gold-standard paraphrases in the top-ten list of
instances of the target verb acquire together with              paraphrases as ranked by the system, and can be
its paraphrases in the gold standard as an example.             defined as follows (McCarthy and Navigli, 2007):
The paraphrases are attached with weights, which
                                                                                         Σs∈M T G f (s)
correspond to the number of times they have been                                 P10 =                  ,
given by different annotators.                                                            Σs∈G f (s)
                                                                where M is the list of 10 paraphrase candidates top-
4.2   Evaluation metrics                                        ranked by the model, G is the corresponding anno-
To evaluate the performance of our method we use                tated gold-standard data, and f (s) is the weight of
generalized average precision (Kishida, 2005), a                the individual paraphrases.
   1 We use version 1.6 of the parser. We modify the depen-       4.3   Experiment 1: Verb paraphrases
dency trees by “folding” prepositions into the edge labels to
make the relation between a head word and the head noun of        In our first experiment, we consider verb para-
a prepositional phrase explicit.                                  phrases using the same controlled subset of the


                                                            952


lexical substitution task data that had been used by             tence, i.e., a second order vector for the verb that
TDP in an earlier study. We compare our model                    is contextually constrained by the first order vec-
to various baselines and the models of TDP and                   tors of all its arguments, and compare them to the
E&P, and show that our new model substantially                   unconstrained (second-order) vectors of each para-
outperforms previous work.                                       phrase candidate, using cosine similarity.3 For the
                                                                 first sentence in Table 1, for example, we compute
Dataset. The dataset is identical to the one used                [[acquireSUBJ:student,OBJ:knowledge ]] and compare it to
by TDP and has been constructed in the same way                  [[gain]], [[amass]], [[buy]], [[purchase]] and so on.
as the dataset used by E&P: it contains those gold-
standard instances of verbs that have—according                  Baselines. We evaluate our model against a ran-
to the analyses produced by the MiniPar parser                   dom baseline and two variants of our model: One
(Lin, 1993)—an overtly realized subject and object.              variant (“2nd order uncontexualized”) simply uses
Gold-standard paraphrases that do not occur in the               contextually unconstrained second-order vectors
parsed British National Corpus are removed.2 In                  to rank paraphrase candidates. Comparing the full
total, the dataset contains 162 instances for 34 dif-            model to this variant will show how effective our
ferent verbs. On average, target verbs have 20.5                 method of contextualizing vectors is. The sec-
substitution candidates; for individual instances of             ond variant (“1st order contextualized”) represents
a target verb, an average of 3.9 of the substitution             verbs in context by their first order vectors that
candidates are annotated as correct paraphrases.                 specify how often the verb co-occurs with its argu-
Below, we will refer to this dataset as “LST/SO.”                ments in the parsed Gigaword corpus. We compare
                                                                 our model to this baseline to demonstrate the bene-
Experimental procedure. To compute the vec-
                                                                 fit of (contextualized) second-order vectors. As for
tor space, we consider only a subset of the complete
                                                                 the full model, we use pmi values rather than raw
set of dependency triples extracted from the parsed
                                                                 frequency counts as co-occurrence statistics.
Gigaword corpus. We experimented with various
strategies, and found that models which consider             Results. For the LST/SO dataset, the generalized
all dependency triples exceeding certain pmi- and            average precision, averaged over all instances in the
frequency thresholds perform best.                           dataset, is 45.94%, and the average P10 is 73.11%.
   Since the dataset is rather small, we use a four-            Table 2 compares our model to the random base-
fold cross-validation method for parameter tuning:           line, the two variants of our model, and previous
We divide the dataset into four subsets, test vari-          work. As can be seen, our model improves about
ous parameter settings on one subset and use the             8% in terms of GAP and almost 7% in terms of
parameters that perform best (in terms of GAP) to            P10 upon the two variants of our model, which in
evaluate the model on the three other subsets. We            turn perform 10% above the random baseline. We
consider the following parameters: pmi-thresholds            conclude that both the use of second-order vectors,
for the dependency triples used in the computa-              as well as the method used to contextualize them,
tion of the first- and second-order vectors, and             are very effective for the task under consideration.
frequency thresholds. The parameters differ only
                                                                The table also compares our model to the model
slightly between the four subsets, and the general
                                                             of TDP and two different instantiations of E&P’s
tendency is that good results are obtained if a low
                                                             model. The results for these three models are cited
pmi-threshold (≤ 2) is applied to filter dependency
                                                             from Thater et al. (2009). We can observe that
triples used in the computation of the second-order
                                                             our model improves about 9% in terms of GAP
vectors, and a relatively high pmi-threshold (≥ 4)
                                                             and about 7% in terms of P10 upon previous work.
to filter dependency triples in the computation of
                                                             Note that the results for the E&P models are based
the first-order vectors. Good performing frequency
                                                                 3 Note that the context information is the same for both
thresholds are 10 or 15. The threshold values for
context vectors are slightly different: a medium             words. With our choice of pointwise multiplication for the
                                                             composition operator × we have (~v1 × ~w) ·~v2 =~v1 · (~v2 × ~w).
pmi-threshold between 2 and 4 and a low frequency            Therefore the choice of which word is contextualized does not
threshold of 3.                                              strongly influence their cosine similarity, and contextualizing
                                                             both should not add any useful information. On the contrary
   To rank paraphrases in context, we compute con-           we found that it even lowers performance. Although this
textualized vectors for the verb in the input sen-           could be repaired by appropriately modifying the operator ×,
                                                             for this experiment we stick with the easier solution of only
   2 Both   TDP and E&P use the British National Corpus.     contextualizing one of the words.


                                                           953


  Model                            GAP      P10               POS    Instances    M1       M2      Baseline
  Random baseline                  26.03    54.25             Noun     535       46.38    42.54     30.01
  E&P (add, object)                29.93    66.20              Adj     508       39.41    43.21     28.32
  E&P (min, subject & object)      32.22    64.86              Adv     284       48.19    51.43     37.25
  TDP                              36.54    63.32
  1st order contextualized         36.09    59.35       Table 3: GAP-scores for non-verb paraphrases us-
                                                        ing two different methods.
  2nd order uncontextualized       37.65    66.32
  Full model                       45.94    73.11
                                                          information.
         Table 2: Results of Experiment 1                    We therefore propose an alternative method to
                                                          rank non-verb paraphrases: We take the second-
on a reimplementation of E&P’s original model—            order vector of the target’s head and contextually
the P10 -scores reported by Erk and Padó (2009)           constrain it by the first order vector of the target.
range between 60.2 and 62.3, over a slightly lower        For instance, if we want to rank the paraphrase
random baseline.                                          candidates hint and star for the noun lead in the
                                                          sentence
   According to a paired t-test the differences are
statistically significant at p < 0.01.
                                                          (1) Meet for coffee early, swap leads and get per-
Performance on the complete dataset. To find                  mission to contact if possible.
out how our model performs on less controlled
                                                        we compute [[swapOBJ:lead ]] and compare it to the
datasets, we extracted all instances from the lexical
                                                        lifted first-order vectors of all paraphrase candi-
substitution task dataset with a verb target, exclud-
                                                        dates, LOBJ ([hint]) and LOBJ ([star]), using cosine
ing only instances which could not be parsed by
                                                        similarity.
the Stanford parser, or in which the target was mis-
tagged as a non-verb by the parser. The resulting          To evaluate the performance of the two methods,
dataset contains 496 instances. As for the LST/SO       we extract all instances from the lexical substitution
dataset, we ignore all gold-standard paraphrases        task dataset with a nominal, adjectival, or adverbial
that do not occur in the parsed (Gigaword) corpus.      target, excluding instances with incorrect parse or
                                                        no parse at all. As before, we ignore gold-standard
   If we use the best-performing parameters from
                                                        paraphrases that do not occur in the parsed Giga-
the first experiment, we obtain a GAP score of
                                                        word corpus.
45.17% and a P10 -score of 75.43%, compared to
random baselines of 27.42% (GAP) and 58.83%                The results are shown in Table 3, where “M1”
(P10 ). The performance on this larger dataset is       refers to the method we used before on verbs, and
thus almost the same compared to our results for        “M2” refers to the alternative method described
the more controlled dataset. We take this as evi-       above. As one can see, M1 achieves better results
dence that our model is quite robust w.r.t. different   than M2 if applied to nouns, while M2 is better
realizations of a verb’s subcategorization frame.       than M1 if applied to adjectives and adverbs. The
                                                        second result is unsurprising, as adjectives and ad-
4.4   Experiment 2: Non-verb paraphrases                verbs often have no dependents at all.
                                                           We can observe that the performance of our
We now apply our model to parts of speech (POS)
                                                        model is similarly strong on non-verbs. GAP scores
other than verbs. The main difference between
                                                        on nouns (using M1) and adverbs are even higher
verbs on the one hand, and nouns, adjectives, and
                                                        than those on verbs. We take these results to show
adverbs on the other hand, is that verbs typically
                                                        that our model can be successfully applied to all
come with a rich context—subject, object, and so
                                                        open word classes.
on—while non-verbs often have either no depen-
dents at all or only closed class dependents such as      5    Experiment: Ranking Word Senses
determiners which provide only limited contextual
informations, if any at all. While we can apply the     In this section, we apply our model to a different
same method as before also to non-verbs, we might       word sense ranking task: Given a word w in context,
expect it to work less well due to limited contextual   the task is to decide to what extent the different


                                                    954


WordNet (Fellbaum, 1998) senses of w apply to                  Word   Present paper   WN-Freq       Combined
this occurrence of w.
                                                                ask      0.344          0.369         0.431
Dataset. We use the dataset provided by Erk and                 add      0.256          0.164         0.270
McCarthy (2009). The dataset contains ordinal                   win      0.236          0.343         0.381
judgments of the applicability of WordNet senses
on a 5 point scale, ranging from completely differ-        average       0.279          0.291         0.361
ent to identical for eight different lemmas in 50
different sentential contexts. In this experiment,       Table 4: Correlation of model predictions and hu-
we concentrate on the three verbs in the dataset:        man judgments
ask, add and win.
Experimental procedure. Similar to Pennac-                 combining our rankings with those of the frequency
chiotti et al. (2008), we represent different word         baseline, by simply computing the average ranks
senses by the words in the corresponding synsets.          of those two models. The results are shown in the
For each word sense, we compute the centroid of            third column. Performance is significantly higher
the second-order vectors of its synset members.            than for both the original model and the frequency-
Since synsets tend to be small (they even may con-         informed baseline. This shows that our model cap-
tain only the target word itself), we additionally         tures an additional kind of information, and thus
add the centroid of the sense’s hypernyms, scaled          can be used to improve the frequency-based model.
down by the factor 10 (chosen as a rough heuristic
without any attempt at optimization).                      6    Conclusion
   We apply the same method as in Section 4.3:
                                                         We have presented a novel method for adapting
For each instance in the dataset, we compute the
                                                         the vector representations of words according to
second-order vector of the target verb, contextually
                                                         their context. In contrast to earlier approaches, our
constrain it by the first-order vectors of the verb’s
                                                         model incorporates detailed syntactic information.
arguments, and compare the resulting vector to
                                                         We solved the problems of data sparseness and
the vectors that represent the different WordNet
                                                         incompatibility of dimensions which are inherent in
senses of the verb. The WordNet senses are then
                                                         this approach by modeling contextualization as an
ranked according to the cosine similarity between
                                                         interplay between first- and second-order vectors.
their sense vector and the contextually constrained
                                                            Evaluating on the SemEval 2007 lexical substitu-
target verb vector.
                                                         tion task dataset, our model performs substantially
   To compare the predicted ranking to the gold-
                                                         better than all earlier approaches, exceeding the
standard ranking, we use Spearman’s ρ, a standard
                                                         state of the art by around 9% in terms of general-
method to compare ranked lists to each other. We
                                                         ized average precision and around 7% in terms of
compute ρ between the similarity scores averaged
                                                         precision out of ten. Also, our system is the first un-
over all three annotators and our model’s predic-
                                                         supervised method that has been applied to Erk and
tions. Based on agreement between human judges,
                                                         McCarthy’s (2009) graded word sense assignment
Erk and McCarthy (2009) estimate an upper bound
                                                         task, showing a substantial positive correlation with
ρ of 0.544 for the dataset.
                                                         the gold standard. We further showed that a weakly
Results. Table 4 shows the results of our exper-         supervised heuristic, making use of WordNet sense
iment. The first column shows the correlation of         ranks, can be significantly improved by incorporat-
our model’s predictions with the human judgments         ing information from our system.
from the gold-standard, averaged over all instances.        We studied the effect that context has on target
All correlations are significant (p < 0.001) as tested   words in a series of experiments, which vary the
by approximate randomization (Noreen, 1989).             target word and keep the context constant. A natu-
   The second column shows the results of a              ral objective for further research is the influence of
frequency-informed baseline, which predicts the          varying contexts on the meaning of target expres-
ranking based on the order of the senses in Word-        sions. This extension might also shed light on the
Net. This (weakly supervised) baseline outper-           status of the modelled semantic process, which we
forms our unsupervised model for two of the three        have been referring to in this paper as “contextu-
verbs. As a final step, we explored the effect of        alization”. This process can be considered one of


                                                     955


mutual disambiguation, which is basically the view        Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
of E&P. Alternatively, one can conceptualize it as          tronic Lexical Database. Bradford Book.
semantic composition: in particular, the head of a        Walter Kintsch. 2001. Predication. Cognitive Science,
phrase incorporates semantic information from its           25:173–202.
dependents, and the final result may to some extent
                                                          Kazuaki Kishida. 2005. Property of average precision
reflect the meaning of the whole phrase.                    and its generalization: An examination of evaluation
   Another direction for further study will be the          indicator for information retrieval experiments. NII
generalization of our model to larger syntactic con-        Technical Report.
texts, including more than only the direct neighbors
                                                          Thomas K. Landauer and Susan T. Dumais. 1997.
in the dependency graph, ultimately incorporating           A solution to plato’s problem: The latent semantic
context information from the whole sentence in a            analysis theory of acquisition, induction, and rep-
recursive fashion.                                          resentation of knowledge. Psychological Review,
                                                            104(2):211–240.
Acknowledgments. We would like to thank Ed-
uard Hovy and Georgiana Dinu for inspiring discus-        Dekang Lin. 1993. Principle-based parsing without
                                                            overgeneration. In Proceedings of the 31st Annual
sions and helpful comments. This work was sup-              Meeting of the Association for Computational Lin-
ported by the Cluster of Excellence “Multimodal             guistics, pages 112–120, Columbus, OH, USA.
Computing and Interaction”, funded by the Ger-
man Excellence Initiative, and the project SALSA,         Dekang Lin. 1998. Automatic retrieval and clustering
                                                            of similar words. In Proceedings of the 36th Annual
funded by DFG (German Science Foundation).                  Meeting of the Association for Computational Lin-
                                                            guistics and 17th International Conference on Com-
                                                            putational Linguistics, Volume 2, pages 768–774.
References
                                                          Christopher D. Manning, Prabhakar Raghavan, and
Chris Buckley and Ellen M. Voorhees. 2000. Evaluat-         Hinrich Schütze. 2008. Introduction to Information
  ing evaluation measure stability. In Proceedings of       Retrieval. Cambridge University Press.
  the 23rd Annual International ACM SIGIR Confer-
  ence on Research and Development in Information         Diana McCarthy and John Carroll. 2003. Disam-
  Retrieval, pages 33–40, Athens, Greece.                   biguating nouns, verbs, and adjectives using auto-
                                                            matically acquired selectional preferences. Compu-
Kenneth W. Church and Patrick Hanks. 1990. Word
                                                            tational Linguistics, 29(4):639–654.
  association, mutual information and lexicography.
  Computational Linguistics, 16(1):22–29.                 Diana McCarthy and Roberto Navigli. 2007. SemEval-
Marie-Catherine de Marneffe, Bill MacCartney, and           2007 Task 10: English Lexical Substitution Task. In
 Christopher D. Manning. 2006. Generating typed             Proc. of SemEval, Prague, Czech Republic.
 dependency parses from phrase structure parses. In       Jeff Mitchell and Mirella Lapata. 2008. Vector-based
 Proceedings of the fifth international conference on        models of semantic composition. In Proceedings
 Language Resources and Evaluation (LREC 2006),              of ACL-08: HLT, pages 236–244, Columbus, OH,
 pages 449–454, Genoa, Italy.                                USA.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
 mantic features for verb sense disambiguation. In        Richard Montague. 1973. The proper treatment of
 Proceedings of ACL-08: HLT, Short Papers, pages            quantification in ordinary English. In Jaakko Hin-
 29–32, Columbus, OH, USA.                                  tikka, Julius Moravcsik, and Patrick Suppes, editors,
                                                            Approaches to Natural Language, pages 221–242.
Katrin Erk and Diana McCarthy. 2009. Graded word            Dordrecht.
  sense assignment. In Proceedings of the 2009 Con-
  ference on Empirical Methods in Natural Language        Eric W. Noreen. 1989. Computer-intensive Methods
  Processing, pages 440–449, Singapore.                      for Testing Hypotheses: An Introduction. John Wi-
                                                             ley and Sons Inc.
Katrin Erk and Sebastian Padó. 2008. A structured
  vector space model for word meaning in context. In      Sebastian Padó and Mirella Lapata.            2007.
  Proceedings of the 2008 Conference on Empirical           Dependency-based construction of semantic space
  Methods in Natural Language Processing, Honolulu,         models. Computational Linguistics, 33(2):161–199.
  HI, USA.
                                                          Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Katrin Erk and Sebastian Padó. 2009. Paraphrase as-        Danilo Croce, and Michael Roth. 2008. Automatic
  sessment in structured vector space: Exploring pa-       induction of framenet lexical units. In Proceedings
  rameters and datasets. In Proc. of the Workshop          of the 2008 Conference on Empirical Methods in
  on Geometrical Models of Natural Language Seman-         Natural Language Processing, pages 457–465, Hon-
  tics, Athens, Greece.                                    olulu, HI, USA.


                                                    956


Hinrich Schütze. 1998. Automatic word sense discrim-
  ination. Computational Linguistics, 24(1):97–124.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
   2009. Ranking paraphrases in context. In Proceed-
   ings of the 2009 Workshop on Applied Textual Infer-
   ence, pages 44–47, Singapore.




                                                     957

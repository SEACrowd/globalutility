     Optimising Information Presentation for Spoken Dialogue Systems

       Verena Rieser             Oliver Lemon                                          Xingkun Liu
   University of Edinburgh   Heriot-Watt University                                Heriot-Watt University
 Edinburgh, United Kingdom Edinburgh, United Kingdom                             Edinburgh, United Kingdom
verena.rieser@ed.ac.uk o.lemon@hw.ac.uk                                             x.liu@hw.ac.uk



                      Abstract                                good overview of the search results) versus keep-
                                                              ing the utterances short and understandable.
    We present a novel approach to Informa-                      In the following we show that IP for SDS can
    tion Presentation (IP) in Spoken Dialogue                 be treated as a data-driven joint optimisation prob-
    Systems (SDS) using a data-driven statis-                 lem, and that this outperforms a supervised model
    tical optimisation framework for content                  of human ‘wizard’ behaviour on a particular IP
    planning and attribute selection. First we                task (presenting sets of search results to a user).
    collect data in a Wizard-of-Oz (WoZ) ex-                     A similar approach has been applied to the
    periment and use it to build a supervised                 problem of Referring Expression Generation in di-
    model of human behaviour. This forms                      alogue (Janarthanam and Lemon, 2010).
    a baseline for measuring the performance
    of optimised policies, developed from this                1.1   Previous work on Information
    data using Reinforcement Learning (RL)                          Presentation in SDS
    methods. We show that the optimised poli-                 Broadly speaking, IP for SDS can be divided into
    cies significantly outperform the baselines               two main steps: 1) IP strategy selection and 2)
    in a variety of generation scenarios: while               Content or Attribute Selection. Prior work has
    the supervised model is able to attain up to              presented a variety of IP strategies for structur-
    87.6% of the possible reward on this task,                ing information (see examples in Table 1). For ex-
    the RL policies are significantly better in 5             ample, the SUMMARY strategy is used to guide the
    out of 6 scenarios, gaining up to 91.5% of                user’s “focus of attention”. It draws the user’s at-
    the total possible reward. The RL policies                tention to relevant attributes by grouping the cur-
    perform especially well in more complex                   rent results from the database into clusters, e.g.
    scenarios. We are also the first to show                  (Polifroni and Walker, 2008; Demberg and Moore,
    that adding predictive “lower level” fea-                 2006). Other studies investigate a COMPARE strat-
    tures (e.g. from the NLG realiser) is im-                 egy, e.g. (Walker et al., 2007; Nakatsu, 2008),
    portant for optimising IP strategies accord-              while most work in SDS uses a RECOMMEND strat-
    ing to user preferences. This provides new                egy, e.g. (Young et al., 2007). In a previous proof-
    insights into the nature of the IP problem                of-concept study (Rieser and Lemon, 2009) we
    for SDS.                                                  show that each of these strategies has its own
                                                              strengths and drawbacks, dependent on the partic-
1   Introduction
                                                              ular context in which information needs to be pre-
Work on evaluating SDS suggests that the Infor-               sented to a user. Here, we will also explore pos-
mation Presentation (IP) phase is the primary con-            sible combinations of the strategies, for example
tributor to dialogue duration (Walker et al., 2001),          SUMMARY followed by RECOMMEND , e.g. (Whittaker
and as such, is a central aspect of SDS design.               et al., 2002), see Figure 1.
During this phase the system returns a set of items              Prior work on Content or Attribute Selection
(“hits”) from a database, which match the user’s              has used a “Summarize and Refine” approach (Po-
current search constraints. An inherent problem               lifroni and Walker, 2008; Polifroni and Walker,
in this task is the trade-off between presenting              2006; Chung, 2004). This method employs utility-
“enough” information to the user (for example                 based attribute selection with respect to how each
helping them to feel confident that they have a               attribute (e.g. price or food type in restaurant


                                                        1009
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


search) of a set of items helps to narrow down            mend one of them. The IP module has to decide
the user’s goal to a single item. Related work ex-        which action to take next, how many attributes to
plores a user modelling approach, where attributes        mention, and when to stop generating.
are ranked according to user preferences (Dem-
berg and Moore, 2006; Winterboer et al., 2007).
Our data collection (see Section 3) and training en-
vironment incorporate these approaches.
   The work in this paper is the first to ap-
ply a data-driven method to this whole decision
space (i.e. combinations of Information Presenta-
tion strategies as well as attribute selection), and to
show the utility of both lower-level features (e.g.
from the NLG realiser) and higher-level features
(e.g. from Dialogue Management) for this prob-            Figure 1: Possible Information Presentation struc-
lem. Previous work has only focused on individual         tures (X=stop generation)
aspects of the problem (e.g. how many attributes
to generate, or when to use a SUMMARY), using a
pipeline model for SDS with DM features as input,         3     Wizard-of-Oz data collection
and where NLG has no knowledge of lower level
features (e.g. behaviour of the realiser). In Section     In an initial Wizard-of-Oz (WoZ) study, we asked
4.3 we show that lower level features significantly       humans (our “wizards”) to produce good IP ac-
influence users’ ratings of IP strategies. In the fol-    tions in different dialogue contexts, when interact-
lowing we use a Reinforcement Learning (RL) as a          ing in spoken dialogues with other humans (the
statistical planning framework (Sutton and Barto,         “users”), who believed that they were talking to an
1998) to explore the contextual features for mak-         automated SDS. The wizards were experienced re-
ing these decisions, and propose a new joint opti-        searchers in SDS and were familiar with the search
misation method for IP strategies combining con-          domain (restaurants in Edinburgh). They were in-
tent structuring and attribute selection.                 structed to select IP structures and attributes for
                                                          NLG so as to most efficiently allow users to find a
2   NLG as planning under uncertainty                     restaurant matching their search constraints. They
                                                          also received prior training on this task.
We follow the overall framework of NLG as plan-              The task for the wizards was to decide which
ning under uncertainty (Lemon, 2008; Rieser and           IP structure to use next (see Section 3.2 for a
Lemon, 2009; Lemon, 2010), where each NLG ac-             list of IP strategies to choose from), which at-
tion is a sequential decision point, based on the         tributes to mention (e.g. cuisine, price range, lo-
current dialogue context and the expected long-           cation, food quality, and/or service quality), and
term utility or “reward” of the action. Other re-         whether to stop generating, given varying num-
cent approaches describe this task as planning, e.g.      bers of database matches, varying prompt reali-
(Koller and Petrick, 2008), or as contextual de-          sations, and varying user behaviour. Wizard ut-
cision making according to a cost function (van           terances were synthesised using a state-of-the-art
Deemter, 2009), but not as a statistical planning         text-to-speech engine. The user speech input was
problem, where uncertainty in the stochastic envi-        delivered to the wizard using Voice Over IP. Figure
ronment is explicitly modelled. Below, we apply           2 shows the web-based interface for the wizard.
this framework to Information Presentation strate-
gies in SDS using Reinforcement Learning, where           3.1    Experimental Setup and Data collection
the example task is to present a set of search results    We collected 213 dialogues with 18 subjects and 2
(e.g. restaurants) to users. In particular, we con-       wizards (Liu et al., 2009). Each user performed a
sider 7 possible policies for structuring the content     total of 12 tasks, where no task set was seen twice
(see Figure 1): Recommending one single item,             by any one wizard. The majority of users were
comparing two items, summarising all of them,             from a range of backgrounds in a higher educa-
or ordered combinations of those actions, e.g. first      tion institute, in the age range 20-30, native speak-
summarise all the retrieved items and then recom-         ers of English, and none had prior experience of


                                                      1010


Figure 2: Wizard interface. [A:] The wizard selects attribute values as specified by the user’s query. [B:] The retrieved
database items are presented in an ordered list. We use a User Modelling approach for ranking the restaurants, see e.g. (Polifroni
and Walker, 2008). [C:] The wizard then chooses which strategy and which attributes to generate next, by clicking radio buttons.
The attribute/s specified in the last user query are pre-selected by default. The strategies can only be combined in the orders as
specified in Figure 1. [D:] An utterance is automatically generated by the NLG realiser every time the wizard selects a strategy,
and is displayed in an intermediate text panel. [E:] The wizard can decide to add the generated utterance to the final output
panel or to start over again. The text in the final panel is sent to the user via TTS, once the wizard decides to stop generating.

      Strategy            Example utterance
      SUMMARY       no    I found 26 restaurants, which have Indian cuisine. 11 of the restaurants are in the expensive price
      UM                  range. Furthermore, 10 of the restaurants are in the cheap price range and 5 of the restaurants
                          are in the moderate price range.
      SUMMARY     UM      26 restaurants meet your query. There are 10 restaurants which serve Indian food and are in the
                          cheap price range. There are also 16 others which are more expensive.
      COMPARE       by    The restaurant called Kebab Mahal is an Indian restaurant. It is in the cheap price range. And
      Item                the restaurant called Saffrani, which is also an Indian restaurant, is in the moderate price range.
      COMPARE       by    The restaurant called Kebab Mahal and the restaurant called Saffrani are both Indian restaurants.
      Attribute           However, Kebab Mahal is in the cheap price range while Saffrani is moderately priced.
      RECOMMEND           The restaurant called Kebab Mahal has the best overall quality amongst the matching restau-
                          rants. It is an Indian restaurant, and it is in the cheap price range.

Table 1: Example realisations, generated when the user provided cuisine=Indian, and where the
wizard has also selected the additional attribute price for presentation to the user.


Spoken Dialogue Systems. After each task the                       for more details.
user answered a questionnaire on a 6 point Lik-
ert scale, regarding the perceived generation qual-                3.2     NLG Realiser
ity in that task. The wizards’ IP strategies were                  In the Wizard-of-Oz environment we implemented
highly ranked by the users on average (4.7), and                   a NLG realiser for the chosen IP structures and
users were able to select a restaurant in 98.6% of                 attribute choices, in order to realise the wizards’
the cases. No significant difference between the                   choices in real time. This generator is based on
wizards was observed.                                              data from the stochastic sentence planner SPaRKy
   The data contains 2236 utterances in total: 1465                (Stent et al., 2004). We replicated the variation ob-
wizard utterances and 771 user utterances. We au-                  served in SPaRKy by analysing high-ranking ex-
tomatically extracted 81 features (e.g #sentences,                 ample outputs (given the highest possible score
#DBhits, #turns, #ellipsis)1 from the XML logfiles                 by the SPaRKy judges) and implemented the vari-
after each dialogue. Please see (Rieser et al., 2009)              ance using dynamic sentence generation. The real-
  1
    The full corpus and list of features is available at           isations vary in sentence aggregation, aggregation
https://www.classic-project.org/corpora/                           operators (e.g. ‘and’, period, or ellipsis), contrasts


                                                              1011


(e.g. ‘however’, ‘on the other hand’) and referring                IF (dbHits <= 9)& (prevNLG = summary):
expressions (e.g. ‘it’, ‘this restaurant’) used. The                  THEN nlgStrategy=compare;
                                                                   IF (dbHits = 1):
length of an utterance also depends on the num-                       THEN nlgStrategy= Recommend;
ber of attributes chosen, i.e. the more attributes the             IF(prevNLG=summaryRecommend)&(dbHits>=10):
                                                                      THEN nlgStrategy= Recommend;
longer the utterance. All of these variations were                 ELSE nlgStrategy=summary;
logged.
   In particular, we realised the following IP strate-          Figure 3: Rules learned by JRip for the wizard
gies (see examples in Table 1):                                 model (‘dbHits’= number of database matches,
                                                                ‘prevNLG’= previous NLG action)
   •   SUMMARY    of all matching restaurants with
       or without a User Model (UM), following
       (Polifroni and Walker, 2008). The approach                  The features selected by this model were only
       using a UM assumes that the user has cer-                “high-level” features, i.e. the input (previous ac-
       tain preferences (e.g. cheap) and only tells             tion, number of database hits) that an IP module
       him about the relevant items, whereas the                receives as input from a Dialogue Manager (DM).
       approach with no UM lists all the matching               We further analysed the importance of different
       items.                                                   features using feature ranking and selection meth-
                                                                ods (Rieser et al., 2009), finding that the human
   •   COMPARE the top 2 restaurants by Item (i.e.
                                                                wizards in this specific setup did not pay signifi-
       listing all the attributes for the first item and
                                                                cant attention to any lower level features, e.g. from
       then for the other) or by Attribute (i.e. di-
                                                                surface realisation, although the generated output
       rectly comparing the different attribute val-
                                                                was displayed to them (see Figure 2).
       ues).
                                                                   Nevertheless, note that the supervised model
   •   RECOMMEND the top-ranking restaurant (ac-                achieves up to 87.6% of the possible reward on
       cording to UM).                                          this task, as we show in Section 5.2, and so can
                                                                be considered a serious baseline against which to
   Note that there was no discernible pattern in                measure performance. Below, we will show that
the data about the wizards’ decisions between                   Reinforcement Learning (RL) produces a signifi-
the UM/no UM and the byItem/byAttribute ver-                    cant improvement over the strategies present in the
sions of the strategies. In this study we will                  original data, especially in cases where RL has ac-
therefore concentrate on the higher level decisions             cess to “lower level” features of the context.
(SUMMARY vs. COMPARE vs. RECOMMEND) and model
these different realisations as noise in the realiser.          4       The Simulation / Learning
                                                                        Environment
3.3    Supervised Baseline strategy
                                                                Here we “bootstrap” a simulated training environ-
We analysed the WoZ data to explore the best-                   ment from the WoZ data, following (Rieser and
rated strategies (the top scoring 50%, n = 205)                 Lemon, 2008).
that were employed by humans for this task. Here
we used a variety of Supervised Learning meth-                  4.1      User Simulations
ods to create a model of the highly rated wizard                User Simulations are commonly used to train
behaviour. Please see (Rieser et al., 2009) for fur-            strategies for Dialogue Management, see for ex-
ther details. The best performing method was Rule               ample (Young et al., 2007). A user simulation for
Induction (JRip). 2 The model achieved an accu-                 NLG is very similar, in that it is a predictive model
racy of 43.19% which is significantly (p < .001)                of the most likely next user act. 4 However, this
better than the majority baseline of always choos-              NLG predicted user act does not actually change
ing SUMMARY (34.65%). 3 The resulting rule set is               the overall dialogue state (e.g. by filling slots) but
shown in Figure 3.                                              it only changes the generator state. In other words,
   2                                                                4
     The WEKA implementation of (Cohen, 1995)’s RIPPER.              Similar to the internal user models applied in recent
   3
     Note that the low accuracy is due to data sparsity and     work on POMDP (Partially Observable Markov Decision
diverse behaviour of the wizards. However, in (Rieser et al.,   Process) dialogue managers (Young et al., 2007; Henderson
2009) we show that this model is significantly different from   and Lemon, 2008; Gasic et al., 2008) for estimation of user
the policy learned using the worse scoring 50%.                 act probabilities.


                                                            1012


the NLG user simulation tells us what the user is                     We evaluate the performance of these models
most likely to do next, if we were to stop generat-                by measuring dialogue similarity to the original
ing now.                                                           data, based on the Kullback-Leibler (KL) diver-
  We are most interested in the following user re-                 gence, as also used by, e.g. (Cuayáhuitl et al.,
actions:                                                           2005; Jung et al., 2009; Janarthanam and Lemon,
                                                                   2009). We compare the raw probabilities as ob-
  1. select: the user chooses one of the pre-                      served in the data with the probabilities generated
     sented items, e.g. “Yes, I’ll take that one.”.                by our n-gram models using different discounting
     This reply type indicates that the Informa-                   techniques for each context, see table 2. All the
     tion Presentation was sufficient for the user                 models have a small divergence from the origi-
     to make a choice.                                             nal data (especially the bi-gram model), suggest-
  2. addInfo: The user provides more at-                           ing that they are reasonable simulations for train-
     tributes, e.g. “I want something cheap.”. This                ing and testing NLG policies.
     reply type indicates that the user has more                      The absolute discounting method for the bi-
     specific requests, which s/he wants to specify                gram model is most dissimilar to the data, as is the
     after being presented with the current infor-                 WittenBell method for the tri-gram model, i.e. the
     mation.                                                       models using these discounting methods have the
  3. requestMoreInfo: The user asks for                            highest KL score. The best performing methods
     more information, e.g. “Can you recommend                     (i.e. most similar to the original data), are linear
     me one?”, “What is the price range of the                     discounting for the bi-gram model and GoodTur-
     last item?”. This reply type indicates that the               ing for the tri-gram. We use the most similar user
     system failed to present the information the                  models for system training, and the most dissimi-
     user was looking for.                                         lar user models for testing NLG policies, in order
                                                                   to test whether the learned policies are robust and
  4. askRepeat: The user asks the system to                        adaptive to unseen dialogue contexts.
     repeat the same message again, e.g. “Can you
     repeat?”. This reply type indicates that the                     discounting method   bi-gram US     tri-gram US
     utterance was either too long or confusing for                   WittenBell              0.086           0.512
     the user to remember, or the TTS quality was                     GoodTuring              0.086           0.163
     not good enough, or both.                                        absolute                0.091           0.246
  5. silence: The user does not say anything.                         linear                  0.011           0.276
     In this case it is up to the system to take ini-
     tiative.                                                      Table 2: Kullback-Leibler divergence for the dif-
  6. hangup: The user closes the interaction.                      ferent User Simulations (US)

   We build user simulations using n-gram mod-
els of system (s) and user (u) acts, as first
introduced by (Eckert et al., 1997). In or-                        4.2   Database matches and “Focus of
der to account for data sparsity, we apply dif-                          attention”
ferent discounting (“smoothing”) techniques in-                    An important task of Information Presentation is
cluding back-off, using the CMU Statistical Lan-                   to support the user in choosing between all the
guage Modelling toolkit (Clarkson and Rosen-                       available items (and ultimately in selecting the
feld, 1997). We construct a bi-gram model5                         most suitable one) by structuring the current infor-
for the users’ reactions to the system’s IP struc-                 mation returned from the database, as explained in
ture decisions (P (au,t |IPs,t )), and a tri-gram                  Section 1.1. We therefore model the user’s “fo-
(i.e. IP structure + attribute choice) model for                   cus of attention” as a feature in our learning ex-
predicting user reactions to the system’s com-                     periments. This feature reflects how the differ-
bined IP structure and attribute selection deci-                   ent IP strategies structure information with dif-
sions: P (au,t |IPs,t , attributess,t ).                           ferent numbers of attributes. We implement this
   5
                                                                   shift of the user’s focus analogously to discover-
    Where au,t is the predicted next user action at time t,
IPs,t was the system’s Information Presentation action at t,       ing the user’s goal in Dialogue Management: ev-
and attributess,t is the attributes selected by the system at t.   ery time the predicted next user act is to add in-


                                                               1013


formation (addInfo), we infer that the user is                   5   Reinforcement Learning experiments
therefore only interested in a subset of the previ-
                                                                 We now formulate the problem as a Markov De-
ously presented results and so the system will fo-
                                                                 cision Process (MDP), where states are NLG di-
cus on this new subset of database items in the rest
                                                                 alogue contexts and actions are NLG decisions.
of the generated utterance. For example, the user’s
                                                                 Each state-action pair is associated with a transi-
focus after the SUMMARY (with UM) in Table 1 is
                                                                 tion probability, which is the probability of mov-
DBhits = 10, since the user is only interested in
                                                                 ing from state s at time t to state s0 at time t + 1 af-
cheap, Indian places.
                                                                 ter having performed action a when in state s. This
4.3    Data-driven Reward function                               transition probability is computed by the environ-
                                                                 ment model (i.e. the user simulation and realiser),
The reward/evaluation function is constructed
                                                                 and explicitly captures the uncertainty in the gen-
from the WoZ data, using a stepwise linear regres-
                                                                 eration environment. This is a major difference
sion, following the PARADISE framework (Walker
                                                                 to other non-statistical planning approaches. Each
et al., 2000). This model selects the features
                                                                 transition is also associated with a reinforcement
which significantly influenced the users’ ratings
                                                                 signal (or “reward”) rt+1 describing how good the
for the NLG strategy in the WoZ questionnaire.
                                                                 result of action a was when performed in state s.
We also assign a value to the user’s reactions
                                                                 The aim of the MDP is to maximise long-term ex-
(valueU serReaction), similar to optimising task
                                                                 pected reward of its decisions, resulting in a policy
success for DM (Young et al., 2007). This reflects
                                                                 which maps each possible state to an appropriate
the fact that good IP strategies should help the
                                                                 action in that state.
user to select an item (valueU serReaction =
                                                                    We treat IP as a hierarchical joint optimisation
+100) or provide more constraints addInfo
                                                                 problem, where first one of the IP structures (1-
(valueU serReaction = ±0), but the user should
                                                                 3) is chosen and then the number of attributes is
not do anything else (valueU serReaction =
                                                                 decided, as shown in Figure 4. At each genera-
−100). The regression in equation 1 (R2 =
                                                                 tion step, the MDP can choose 1-5 attributes (e.g.
.26) indicates that users’ ratings are influenced by
                                                                 cuisine, price range, location, food quality, and/or
higher level and lower level features: Users like to
                                                                 service quality). Generation stops as soon as the
be focused on a small set of database hits (where
                                                                 user is predicted to select an item, i.e. the IP task
#DBhits ranges over [1-100]), which will enable
                                                                 is successful. (Note that the same constraint is op-
them to choose an item (valueU serReaction),
                                                                 erational for the WoZ baseline.)
while keeping the IP utterances short (where
#sentence is in the range [2-18]):
                                                                                                            
                                                                                  SUMMARY 
                                                                      ACTION: IP: COMPARE
                                                                                               attr: 1-5       
                                                                                                                 
                                                                                 RECOMMEND                    
                                                                                                                
       Reward = (−1.2) × #DBhits                          (1)                                                
                                                                               attributes: 1-15
                                                                                                                
                                                                                                                
       +(.121) × valueU serReaction                                                                          
                                                                             sentence: 2-18                   
                   −(1.43) × #sentence
                                                                                                             
                                                                                                                
                                                                             dbHitsFocus: 1-100               
                                                                      STATE: 
                                                                              
                                                                                           
                                                                                                     
                                                                                                                
Note that the worst possible reward for an NLG
                                                                      
                                                                             userSelect:  0,1                 
                                                                                                                 
                                                                                                             
move is therefore (−1.20 × 100) − (.121 × 100) −                               userAddInfo: 0,1
                                                                                                              
                                                                                                              
(18 × 1.43) = −157.84. This is achieved by pre-
                                                                                         
                                                                                userElse: 0,1
senting 100 items to the user in 18 sentences6 , in
such a way that the user ends the conversation un-               Figure 4: State-Action space for the RL-NLG
successfully. The top possible reward is achieved                problem
in the rare cases where the system can immedi-
ately present 1 item to the user using just 2 sen-                  States are represented as sets of NLG dia-
tences, and the user then selects that item, i.e. Re-            logue context features. The state space comprises
ward = −(1.20 × 1) + (.121 × 100) − (2 × 1.43) =                 “lower-level” features about the realiser behaviour
8.06                                                             (two discrete features representing the number of
   6
                                                                 attributes and sentences generated so far) and three
    Note that the maximum possible number of sentences
generated by the realizer is 18 for the full IP sequence SUM -   binary features representing the user’s predicted
MARY + COMPARE + RECOMMEND using all the attributes.             next action, as well as “high-level” features pro-


                                                             1014


vided by the DM (e.g. current database hits in the      2.1. IPS+Attr choice, Template realiser:
user’s focus (dbHitsFocus)). We trained the                  Predicted next user action varies according
policy using the SHARSHA algorithm (Shapiro and              to tri-gram (P (au,t |IPs,t , attributess,t ))
Langley, 2002) with linear function approximation            model; Number of sentences per IP structure
(Sutton and Barto, 1998), and the simulation envi-           set to default.
ronment described in Section 4. The policy was
trained for 60,000 iterations.                          2.2. IPS+Attr choice, Template realiser+Focus model:
                                                             Tri-gram user simulation with Template re-
5.1   Experimental Set-up                                    aliser and Focus of attention model with
                                                             respect to #DBhits and #attributes as
We compare the learned strategies against the WoZ
                                                             described in section 4.2.
baseline as described in Section 3.3. For attribute
selection we choose a majority baseline (randomly       2.3. IPS+Attr choice, Stochastic realiser: Tri-
choosing between 3 or 4 attributes) since the at-            gram user simulation with sentence/attribute
tribute selection models learned by Supervised               relationship according to Stochastic realiser
Learning on the WoZ data didn’t show significant             as described in Section 3.2.
improvements.
   For training, we used the user simulation model      2.4. IPS+Attr choice, Stochastic realiser+Focus:
most similar to the data, see Section 4.1. For               i.e. the full model = Predicted next user ac-
testing, we test with the different user simulation          tion varies according to tri-gram model+
model (the one which is most dissimilar to the               Focus of attention model + Sentence/attribute
data).                                                       relationship according to stochastic realiser.
   We first investigate how well IP structure (with-
                                                        5.2    Results
out attribute choice) can be learned in increas-
ingly complex generation scenarios. A genera-           We compare the average final reward (see Equa-
tion scenario is a combination of a particular kind     tion 1) gained by the baseline against the trained
of NLG realiser (template vs. stochastic) along         RL policies in the different scenarios for each
with different levels of variation introduced by cer-   1000 test runs, using a paired samples t-test. The
tain features of the dialogue context. In general,      results are shown in Table 3. In 5 out of 6 scenar-
the stochastic realiser introduces more variation       ios the RL policy significantly (p < .001) outper-
in lower level features than the template-based re-     forms the supervised baseline. We also report on
aliser. The Focus model introduces more varia-          the percentage of the top possible reward gained
tion with respect to #DBhits and #attributes as de-     by the individual policies, and the raw percentage
scribed in Section 4.2. We therefore investigate        improvement of the RL policy. Note that the best
the following cases:                                    possible (100%) reward can only be gained in rare
                                                        cases (see Section 4.3).
1.1. IP structure choice, Template realiser:               The learned RL policies show that lower level
     Predicted next user action varies according to     features are important in gaining significant im-
     the bi-gram model (P (au,t |IPs,t )); Number       provements over the baseline. The more complex
     of sentences and attributes per IP strategy is     the scenario, the harder it is to gain higher rewards
     set by defaults, reflecting a template-based       for the policies in general (as more variation is in-
     realiser.                                          troduced), but the relative improvement in rewards
                                                        also increases with complexity: the baseline does
1.2. IP structure choice, Stochastic realiser:          not adapt well to the variations in lower level fea-
     IP structure where number of attributes per        tures whereas RL learns to adapt to the more chal-
     NLG turn is given at the beginning of each         lenging scenarios. 7
     episode (e.g. set by the DM); Sentence gen-           An overview of the range of different IP strate-
     eration according to the SPaRKy stochastic         gies learned for each setup can be found in Table 4.
     realiser model as described in Section 3.2.        Note that these strategies are context-dependent:
                                                        the learner chooses how to proceed dependent on
   We then investigate different scenarios for             7
                                                            Note, that the baseline does reasonably well in scenarios
jointly optimising IP structure (IPS) and attribute     with variation introduced by only higher level features (e.g.
selection (Attr) decisions.                             scenario 2.2).


                                                    1015


                                 Wizard Baseline                          RL % - Baseline %
                Scenario                           RL average Reward
                                 average Reward                           = % improvement
                     1.1          -15.82(±15.53)    -9.90***(±15.38)     89.2% - 85.6%= 3.6%
                     1.2          -19.83(±17.59)   -12.83***(±16.88)     87.4% - 83.2%= 4.2%
                     2.1          -12.53(±16.31)    -6.03***(±11.89)     91.5% - 87.6%= 3.9%
                     2.2          -14.15(±16.60)     -14.18(±18.04)      86.6% - 86.6%= 0.0%
                     2.3          -17.43(±15.87)    -9.66***(±14.44)     89.3% - 84.6%= 4.7%
                     2.4          -19.59(±17.75)   -12.78***(±15.83)     87.4% - 83.3%= 4.1%

Table 3: Test results for 1000 dialogues, where *** denotes that the RL policy is significantly (p < .001)
better than the Baseline policy.


the features in the state space at each generation        and produced by the stochastic sentence re-
step.                                                     aliser. It learns to generate the whole sequence
 Scenario   strategies learned
                                                          (SUMMARY+COMPARE+RECOMMEND) if #attributes is
            RECOMMEND                                     low (<3), because the overall generated utterance
            COMPARE
 1.1
            COMPARE + RECOMMEND                           (final #sentences) is still relatively short. Other-
            SUMMARY
            SUMMARY + COMPARE                             wise the policy is similar to the one for scenario
            SUMMARY + RECOMMEND
            SUMMARY + COMPARE + RECOMMEND .               1.1.
            RECOMMEND
            COMPARE                                          The RL policies for jointly optimising IP strat-
            COMPARE + RECOMMEND
 1.2        SUMMARY                                       egy and attribute selection learn to select the num-
            SUMMARY + COMPARE
            SUMMARY + RECOMMEND                           ber of attributes according to the generation sce-
            SUMMARY + COMPARE + RECOMMEND .
            RECOMMEND (5)                                 narios 2.1-2.4. For example, the RL policy learned
            SUMMARY (2)
            SUMMARY (2)+ COMPARE (4)                      for scenario 2.1 generates a RECOMMEND with 5 at-
 2.1
            SUMMARY (2)+ COMPARE (1)
            SUMMARY (2)+ COMPARE (4)+ RECOMMEND (5)       tributes if the database hits are low (<13). Oth-
            SUMMARY (2)+ COMPARE (1)+ RECOMMEND (5)
                                                          erwise, it will start with a SUMMARY using 2 at-
            RECOMMEND (5)
 2.2        SUMMARY (4)                                   tributes. If the user is predicted to narrow down
            SUMMARY (4)+ RECOMMEND (5)
            RECOMMEND (2)                                 his focus after the SUMMARY, the policy continues
            SUMMARY (1)
 2.3        SUMMARY (1)+ COMPARE (4)                      with a COMPARE using 1 attribute only, otherwise it
            SUMMARY (1)+ COMPARE (1)
            SUMMARY (1)+ COMPARE (4)+ RECOMMEND (2)       helps the user by presenting 4 attributes. It then
            RECOMMEND (2)
            SUMMARY (2)
                                                          continues with RECOMMEND(5), and stops as soon
 2.4
            SUMMARY (2)+ COMPARE (4)
            SUMMARY (2)+ RECOMMEND (2)
                                                          as the user is predicted to select one item.
            SUMMARY (2)+ COMPARE (4)+ RECOMMEND (2)          The learned policy for scenario 2.1 generates
            SUMMARY (2)+ COMPARE (1)+ RECOMMEND (2)
                                                          5.85 attributes per NLG turn on average (i.e. the
Table 4: RL strategies learned for the different sce-     cumulative number of attributes generated in the
narios, where (n) denotes the number of attributes        whole NLG sequence, where the same attribute
generated.                                                may be repeated within the sequence). This strat-
                                                          egy primarily adapts to the variations from the user
   For example, the RL policy for scenario 1.1            simulation (tri-gram model). For scenario 2.2 the
learned to start with a SUMMARY if the initial num-       average number of attributes is higher (7.15) since
ber of items returned from the database is high           the number of attributes helps to narrow down the
(>30). It will then stop generating if the user is        user’s focus via the DBhits/attribute relationship
predicted to select an item. Otherwise, it contin-        specified in section 4.2. For scenario 2.3 fewer
ues with a RECOMMEND. If the number of database           attributes are generated on average (3.14), since
items is low, it will start with a COMPARE and then       here the number of attributes influences the sen-
continue with a RECOMMEND, unless the user selects        tence realiser, i.e. fewer attributes results in fewer
an item. Also see Table 4. Note that the WoZ strat-       sentences, but does not impact the user’s focus.
egy behaves as described in Figure 3.                     In scenario 2.4 all the conditions mentioned above
   In addition, the RL policy for scenario 1.2            influence the learned policy. The average number
learns to adapt to a more complex scenario:               of attributes selected is still low (3.19).
the number of attributes requested by the DM                 In comparison, the average (cumulative) num-


                                                      1016


ber of attributes for the WoZ baseline is 7.10. The          We are now working on testing the learned poli-
WoZ baseline generates all the possible IP struc-         cies with real users, outside of laboratory condi-
tures (with 3 or 4 attributes) but is restricted to use   tions, using a restaurant-guide SDS, deployed as a
only “high-level” features (see Figure 3). By beat-       VOIP service. Previous work in SDS has shown
ing this baseline we show the importance of the           that results for Dialogue Management obtained
“lower-level” features. Nevertheless, this wizard         with simulated users are able to transfer to eval-
policy achieves up to 87.6% of the possible reward        uations with real users (Lemon et al., 2006).
on this task, and so can be considered a serious             This methodology provides new insights into
baseline against which to measure performance.            the nature of the IP problem, which has previously
   The only case (scenario 2.2) where RL does not         been treated as a module following dialogue man-
improve significantly over the baseline is where          agement with no access to lower-level context fea-
lower level features do not play an important role        tures. The data-driven planning method applied
for learning good strategies: scenario 2.2 is only        here promises a significant upgrade in the perfor-
sensitive to higher level features (DBhits).              mance of generation modules, and thereby of Spo-
                                                          ken Dialogue Systems in general.
6   Conclusion
                                                          Acknowledgments
We have presented a new data-driven method for
Information Presentation (IP) in Spoken Dialogue          The research leading to these results has received
Systems using a statistical optimisation frame-           funding from the European Community’s Seventh
work for content structure planning and attribute         Framework Programme (FP7/2007-2013) under
selection. This work is the first to apply a data-        grant agreement no. 216594 (CLASSiC project
driven optimisation method to the IP decision             www.classic-project.org) and from the
space, and to show the utility of both lower-level        EPSRC, project no. EP/G069840/1.
and higher-level features for this problem.
   We collected data in a Wizard-of-Oz (WoZ)
experiment and showed that human “wizards”                References
mostly pay attention to ‘high-level’ features from        Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Dialogue Management. The WoZ data was used                  Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
to build statistical models of user reactions to            dicting how it sounds: Re-ranking alternative in-
                                                            puts to TTS using latent variables (forthcoming). In
IP strategies, and a data-driven reward function
                                                            Proc. of Interspeech/ICSLP, Special Session on Ma-
for Reinforcement Learning (RL). We show that               chine Learning for Adaptivity in Spoken Dialogue
lower level features significantly influence users’         Systems.
ratings of IP strategies. We compared a model of
human behaviour (the ‘human wizard baseline’)             Grace Chung. 2004. Developing a flexible spoken dia-
                                                            log system using simulation. In Proc. of the Annual
against policies optimised using Reinforcement              Meeting of the Association for Computational Lin-
Learning, in a variety of scenarios. Our optimised          guistics (ACL).
policies significantly outperform the IP structuring
and attribute selection present in the WoZ data, es-      P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
pecially when performing in complex generation               cal Language Modeling Using the CMU-Cambridge
                                                             Toolkit. In Proc. of ESCA Eurospeech.
scenarios which require adaptation to, e.g. number
of database results, utterance length, etc. While         William W. Cohen. 1995. Fast effective rule induction.
the human wizards were able to attain up to 87.6%           In Proceedings of the 12th International Conference
of the possible reward on this task, the RL poli-           on Machine Learning (ICML).
cies are significantly better in 5 out of 6 scenarios,
                                                          Heriberto Cuayáhuitl, Steve Renals, Oliver Lemon, and
gaining up to 91.5% of the total possible reward.           Hiroshi Shimodaira. 2005. Human-computer dia-
   We have also shown that adding predictive                logue simulation using hidden markov models. In
“lower level” features, e.g. from the NLG realiser          Proc. of the IEEE workshop on Automatic Speech
and a user reaction model, is important for learn-          Recognition and Understanding (ASRU).
ing optimal IP strategies according to user pref-         Vera Demberg and Johanna D. Moore. 2006. Infor-
erences. Future work could include the predicted            mation presentation in spoken dialogue systems. In
TTS quality (Boidin et al., 2009) as a feature.             Proceedings of EACL.


                                                      1017


W. Eckert, E. Levin, and R. Pieraccini. 1997. User       Joseph Polifroni and Marilyn Walker. 2008. Inten-
  modeling for spoken dialogue system evaluation. In        sional Summaries as Cooperative Responses in Di-
  Proc. of the IEEE workshop on Automatic Speech            alogue Automation and Evaluation. In Proceedings
  Recognition and Understanding (ASRU).                     of ACL.

M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,         Verena Rieser and Oliver Lemon. 2008. Learn-
  B. Thomson, and S. Young. 2008. Training and             ing Effective Multimodal Dialogue Strategies from
  Evaluation of the HIS POMDP Dialogue System in           Wizard-of-Oz data: Bootstrapping and Evaluation.
  Noise. In Proc. of SIGdial Workshop on Discourse         In Proc. of ACL.
  and Dialogue.                                          Verena Rieser and Oliver Lemon. 2009. Natural Lan-
                                                           guage Generation as Planning Under Uncertainty for
James Henderson and Oliver Lemon. 2008. Mixture            Spoken Dialogue Systems. In Proc. of EACL.
  Model POMDPs for Efficient Handling of Uncer-
  tainty in Dialogue Management. In Proc. of ACL.        Verena Rieser, Xingkun Liu, and Oliver Lemon. 2009.
                                                           Optimal Wizard NLG Behaviours in Context. Tech-
Srinivasan Janarthanam and Oliver Lemon. 2009. A           nical report, Deliverable 4.2, CLASSiC Project.
   Two-tier User Simulation Model for Reinforcement
   Learning of Adaptive Referring Expression Genera-     Dan Shapiro and P. Langley. 2002. Separating skills
   tion Policies. In Proc. of SIGdial.                     from preference: Using learning to program by re-
                                                           ward. In Proc. of the 19th International Conference
Srini Janarthanam and Oliver Lemon. 2010. Learn-           on Machine Learning (ICML).
   ing to adapt to unknown users: Referring expression   Amanda Stent, Rashmi Prasad, and Marilyn Walker.
   generation in spoken dialogue systems. In Proceed-     2004. Trainable sentence planning for complex in-
   ings of ACL.                                           formation presentation in spoken dialog systems. In
                                                          Association for Computational Linguistics.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
  woo Jeong, and Gary Geunbae Lee. 2009. Data-           R. Sutton and A. Barto. 1998. Reinforcement Learn-
  driven user simulation for automated evaluation of        ing. MIT Press.
  spoken dialog systems. Computer, Speech & Lan-
                                                         Kees van Deemter. 2009. What game theory can do
  guage, 23:479–509.
                                                           for NLG: the case of vague language. In 12th Eu-
                                                           ropean Workshop on Natural Language Generation
Alexander Koller and Ronald Petrick. 2008. Experi-
                                                           (ENLG).
  ences with planning for natural language generation.
  In ICAPS.                                              Marilyn A. Walker, Candace A. Kamm, and Diane J.
                                                          Litman. 2000. Towards developing general mod-
Oliver Lemon, Kallirroi Georgila, and James Hender-       els of usability with PARADISE. Natural Language
  son. 2006. Evaluating Effectiveness and Portabil-       Engineering, 6(3).
  ity of Reinforcement Learned Dialogue Strategies
  with real users: the TALK TownInfo Evaluation. In      M. Walker, R. Passonneau, and J. Boland. 2001. Quan-
  IEEE/ACL Spoken Language Technology.                     titative and qualitative evaluation of DARPA Com-
                                                           municator spoken dialogue systems. In Proc. of
Oliver Lemon. 2008. Adaptive Natural Language              the Annual Meeting of the Association for Compu-
  Generation in Dialogue using Reinforcement Learn-        tational Linguistics (ACL).
  ing. In Proceedings of SEMdial.                        Marilyn Walker, Amanda Stent, François Mairesse, and
                                                          Rashmi Prasad. 2007. Individual and domain adap-
Oliver Lemon. 2010. Learning what to say and how to       tation in sentence planning for dialogue. Journal of
  say it: joint optimization of spoken dialogue man-      Artificial Intelligence Research (JAIR), 30:413–456.
  agement and Natural Language Generation. Com-
  puter, Speech & Language, to appear.                   Steve Whittaker, Marilyn Walker, and Johanna Moore.
                                                            2002. Fish or Fowl: A Wizard of Oz evaluation
Xingkun Liu, Verena Rieser, and Oliver Lemon. 2009.         of dialogue strategies in the restaurant domain. In
  A wizard-of-oz interface to study information pre-        Proc. of the International Conference on Language
  sentation strategies for spoken dialogue systems. In      Resources and Evaluation (LREC).
  Proc. of the 1st International Workshop on Spoken
  Dialogue Systems.                                      Andi Winterboer, Jiang Hu, Johanna D. Moore, and
                                                           Clifford Nass. 2007. The influence of user tailoring
Crystal Nakatsu. 2008. Learning contrastive connec-        and cognitive load on user performance in spoken
  tives in sentence realization ranking. In Proc. of       dialogue systems. In Proc. of the 10th International
  SIGdial Workshop on Discourse and Dialogue.              Conference of Spoken Language Processing (Inter-
                                                           speech/ICSLP).
Joseph Polifroni and Marilyn Walker. 2006. Learning      SJ Young, J Schatzmann, K Weilhammer, and H Ye.
   database content for spoken dialogue system design.     2007. The Hidden Information State Approach to
   In Proc. of the IEEE/ACL workshop on Spoken Lan-        Dialog Management. In ICASSP 2007.
   guage Technology (SLT).


                                                     1018

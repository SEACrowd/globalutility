                  Joint Syntactic and Semantic Parsing of Chinese
       Junhui Li and Guodong Zhou                                          Hwee Tou Ng
    School of Computer Science & Technology                       Department of Computer Science
               Soochow University                                 National University of Singapore
              Suzhou, China 215006                             13 Computing Drive, Singapore 117417
    {lijunhui, gdzhou}@suda.edu.cn                                 nght@comp.nus.edu.sg

                                                             and a predicate (either a verb or a noun) in the
                      Abstract                               sentence, SRL recognizes and maps all the con-
                                                             stituents in the sentence into their corresponding
    This paper explores joint syntactic and seman-           semantic arguments (roles) of the predicate. In
    tic parsing of Chinese to further improve the            both English and Chinese PropBank (Palmer et
    performance of both syntactic and semantic               al., 2005; Xue and Palmer, 2003), and English
    parsing, in particular the performance of se-            and Chinese NomBank (Meyers et al., 2004; Xue,
    mantic parsing (in this paper, semantic role
                                                             2006), these semantic arguments include core
    labeling). This is done from two levels. Firstly,
    an integrated parsing approach is proposed to            arguments (e.g., Arg0 for agent and Arg1 for
    integrate semantic parsing into the syntactic            recipient) and adjunct arguments (e.g.,
    parsing process. Secondly, semantic informa-             ArgM-LOC for locative argument and
    tion generated by semantic parsing is incorpo-           ArgM-TMP for temporal argument). According
    rated into the syntactic parsing model to better         to predicate type, SRL can be divided into SRL
    capture semantic information in syntactic                for verbal predicates (verbal SRL, in short) and
    parsing. Evaluation on Chinese TreeBank,                 SRL for nominal predicates (nominal SRL, in
    Chinese PropBank, and Chinese NomBank                    short).
    shows that our integrated parsing approach                  With the availability of large annotated cor-
    outperforms the pipeline parsing approach on
                                                             pora such as FrameNet (Baker et al., 1998),
    n-best parse trees, a natural extension of the
    widely used pipeline parsing approach on the             PropBank, and NomBank in English, data-driven
    top-best parse tree. Moreover, it shows that             techniques, including both feature-based and
    incorporating semantic role-related informa-             kernel-based methods, have been extensively
    tion into the syntactic parsing model signifi-           studied for SRL (Carreras and Màrquez, 2004;
    cantly improves the performance of both syn-             Carreras and Màrquez, 2005; Pradhan et al.,
    tactic parsing and semantic parsing. To our              2005; Liu and Ng, 2007). Nevertheless, for both
    best knowledge, this is the first research on            verbal and nominal SRL, state-of-the-art systems
    exploring syntactic parsing and semantic role            depend heavily on the top-best parse tree and
    labeling for both verbal and nominal predi-              there exists a large performance gap between
    cates in an integrated way.
                                                             SRL based on the gold parse tree and the
                                                             top-best parse tree. For example, Pradhan et al.
1    Introduction                                            (2005) suffered a performance drop of 7.3 in
Semantic parsing maps a natural language sen-                F1-measure on English PropBank when using the
tence into a formal representation of its meaning.           top-best parse tree returned from Charniak’s
Due to the difficulty in deep semantic parsing,              parser (Charniak, 2001). Liu and Ng (2007) re-
most previous work focuses on shallow semantic               ported a performance drop of 4.21 in F1-measure
parsing, which assigns a simple structure (such              on English NomBank.
as WHO did WHAT to WHOM, WHEN,                                  Compared with English SRL, Chinese SRL
WHERE, WHY, HOW) to each predicate in a                      suffers more seriously from syntactic parsing.
sentence. In particular, the well-defined semantic           Xue (2008) evaluated on Chinese PropBank and
role labeling (SRL) task has been drawing in-                showed that the performance of Chinese verbal
creasing attention in recent years due to its im-            SRL drops by about 25 in F1-measure when re-
portance in natural language processing (NLP)                placing gold parse trees with automatic ones.
applications, such as question answering (Nara-              Likewise, Xue (2008) and Li et al. (2009) re-
yanan and Harabagiu, 2004), information extrac-              ported a performance drop of about 12 in
tion (Surdeanu et al., 2003), and co-reference               F1-measure in Chinese NomBank SRL.
resolution (Kong et al., 2009). Given a sentence


                                                        1108
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


   While it may be difficult to further improve         As an alternative to the above pseudo-joint
syntactic parsing, a promising alternative is to     learning methods (strictly speaking, they are still
perform both syntactic and semantic parsing in       pipeline methods), one can augment the syntactic
an integrated way. Given the close interaction       label of a constituent with semantic information,
between the two tasks, joint learning not only       like what function parsing does (Merlo and Mu-
allows uncertainty about syntactic parsing to be     sillo, 2005). Yi and Palmer (2005) observed that
carried forward to semantic parsing but also al-     the distributions of semantic labels could poten-
lows useful information from semantic parsing to     tially interact with the distributions of syntactic
be carried backward to syntactic parsing.            labels and redefined the boundaries of constitu-
   This paper explores joint learning of syntactic   ents. Based on this observation, they incorpo-
and semantic parsing for Chinese texts from two      rated semantic role information into syntactic
levels. Firstly, an integrated parsing approach is   parse trees by extending syntactic constituent
proposed to benefit from the close interaction       labels with their coarse-grained semantic roles
between syntactic and semantic parsing. This is      (core argument or adjunct argument) in the sen-
done by integrating semantic parsing into the        tence, and thus unified semantic parsing and
syntactic parsing process. Secondly, various se-     syntactic parsing. The actual fine-grained seman-
mantic role-related features are directly incorpo-   tic roles are assigned, as in other methods, by an
rated into the syntactic parsing model to better     ensemble classifier. However, the results ob-
capture semantic role-related information in syn-    tained with this method were negative, and they
tactic parsing. Evaluation on Chinese TreeBank,      concluded that semantic parsing on PropBank
Chinese PropBank, and Chinese NomBank                was too difficult due to the differences between
shows that our method significantly improves the     chunk annotation and tree structure. Motivated
performance of both syntactic and semantic           by Yi and Palmer (2005), Merlo and Musillo
parsing. This is promising and encouraging. To       (2008) first extended a statistical parser to pro-
our best knowledge, this is the first research on    duce a richly annotated tree that identifies and
exploring syntactic parsing and SRL for verbal       labels nodes with semantic role labels as well as
and nominal predicates in an integrated way.         syntactic labels. Then, they explored both
   The rest of this paper is organized as follows.   rule-based and machine learning techniques to
Section 2 reviews related work. Section 3 pre-       extract predicate-argument structures from this
sents our baseline systems for syntactic and se-     enriched output. Their experiments showed that
mantic parsing. Section 4 presents our proposed      their method was biased against these roles in
method of joint syntactic and semantic parsing       general, thus lowering recall for them (e.g., pre-
for Chinese texts. Section 5 presents the experi-    cision of 87.6 and recall of 65.8).
mental results. Finally, Section 6 concludes the        There have been other efforts in NLP on joint
paper.                                               learning with various degrees of success. In par-
                                                     ticular, the recent shared tasks of CoNLL 2008
2   Related Work                                     and 2009 (Surdeanu et al., 2008; Hajic et al.,
                                                     2009) tackled joint parsing of syntactic and se-
Compared to the large body of work on either
                                                     mantic dependencies. However, all the top 5 re-
syntactic parsing (Ratnaparkhi, 1999; Collins,
                                                     ported systems decoupled the tasks, rather than
1999; Charniak, 2001; Petrov and Klein, 2007),
                                                     building joint models. Compared with the disap-
or SRL (Carreras and Màrquez, 2004; Carreras
                                                     pointing results of joint learning on syntactic and
and Màrquez, 2005; Jiang and Ng, 2006), there is
                                                     semantic parsing, Miller et al. (2000) and Finkel
relatively less work on their joint learning.
                                                     and Manning (2009) showed the effectiveness of
   Koomen et al. (2005) adopted the outputs of
                                                     joint learning on syntactic parsing and some
multiple SRL systems (each on a single parse
                                                     simple NLP tasks, such as information extraction
tree) and combined them into a coherent predi-
                                                     and name entity recognition. In addition, at-
cate argument output by solving an optimization
                                                     tempts on joint Chinese word segmentation and
problem. Sutton and McCallum (2005) adopted a
                                                     part-of-speech (POS) tagging (Ng and Low,
probabilistic SRL system to re-rank the N-best
                                                     2004; Zhang and Clark, 2008) also illustrate the
results of a probabilistic syntactic parser. How-
                                                     benefits of joint learning.
ever, they reported negative results, which they
blamed on the inaccurate probability estimates
from their locally trained SRL model.



                                                 1109


                                                                TOP

                                                                 IP


                                                                VP
                Arg1/Rel2                                                                           PU
                Arg0/Rel1            Arg0/Rel2                                    VP
                                     Arg2/Rel1                                                      。
                                                                Sup/Rel2               Arg1/Rel1    .
                   NP                   PP
                                                                  Rel1
                                                                                          NP
              NR         NN      P               NP               VV
                                                                            ArgM-MNR/Rel2      Rel2
             中国         政府               NR           NN
            Chinese     govt.   向
                                                                                 NN                NN
                                to      朝鲜      政府               提供
                                      N. Korean govt.           provide
                                                                   人民币                         贷款
                                                                    RMB                        loan
                Chinese government provides RMB loan to North Korean government.
      Figure 1: Two predicates (Rel1 and Rel2) and their arguments in the style of Chinese PropBank and NomBank.


3     Baseline:  Pipeline              Parsing             on         3.2     Semantic Role Labeling
      Top-Best Parse Tree                                             Figure 1 demonstrates an annotation example of
                                                                      Chinese PropBank and NomBank. In the figure,
In this section, we briefly describe our approach
to syntactic parsing and semantic role labeling,                      the verbal predicate “提供/provide” is annotated
as well as the baseline system with pipeline                          with three core arguments (i.e., “NP ( 中 国
parsing on the top-best parse tree.                                   /Chinese 政府/govt.)” as Arg0, “PP (向/to 朝
                                                                      鲜/N. Korean 政府/govt.)” as Arg2, and “NP
3.1    Syntactic Parsing
                                                                      (人民币/RMB 贷款/loan)” as Arg1), while the
Our syntactic parser re-implements Ratnaparkhi                        nominal predicate “贷款/loan” is annotated with
(1999), which adopts the maximum entropy                              two core arguments (i.e., “NP (中国/Chinese 政
principle. The parser recasts a syntactic parse                       府/govt.)” as Arg1 and “PP (向/to 朝鲜/N. Ko-
tree as a sequence of decisions similar to those
                                                                      rean 政府/govt.)” as Arg0), and an adjunct ar-
of a standard shift-reduce parser and the parsing
process is organized into three left-to-right                         gument (i.e., “NN ( 人 民 币 /RMB)” as
passes via four procedures, called TAG,                               ArgM-MNR, denoting the manner of loan). It is
CHUNK, BUILD, and CHECK.                                              worth pointing out that there is a (Chinese)
First pass. The first pass takes a tokenized sen-                     NomBank-specific label in Figure 1, Sup (sup-
tence as input, and uses TAG to assign each                           port verb) (Xue, 2006), to help introduce the
word a part-of-speech.                                                arguments which occur outside the nominal pre-
Second pass. The second pass takes the output                         dicate-headed noun phrase. In (Chinese) Nom-
of the first pass as input, and uses CHUNK to                         Bank, a verb is considered to be a support verb
recognize basic chunks in the sentence.                               only if it shares at least an argument with the
Third pass. The third pass takes the output of                        nominal predicate.
the second pass as input, and always alternates                       3.2.1     Automatic Predicate Recognition
between BUILD and CHECK in structural pars-
ing in a recursive manner. Here, BUILD decides                        Automatic predicate recognition is a prerequisite
whether a subtree will start a new constituent or                     for the application of SRL systems. For verbal
join the incomplete constituent immediately to                        predicates, it is very easy. For example, 99% of
its left. CHECK finds the most recently pro-                          verbs are annotated as predicates in Chinese
posed constituent, and decides if it is complete.                     PropBank. Therefore, we can simply select any
                                                                      word with a part-of-speech (POS) tag of VV,
                                                                      VA, VC, or VE as verbal predicate.
                                                                         Unlike verbal predicate recognition, nominal
                                                                      predicate recognition is quite complicated. For

                                                                1110


 example, only 17.5% of nouns are annotated as               in F1-measure respectively (on golden parse
 predicates in Chinese NomBank. It is quite                  trees and golden predicates), which are compa-
 common that a noun is annotated as a predicate              rable to Xue (2008) and Li et al. (2009). For
 in some cases but not in others. Therefore, au-             more details, please refer to Xue (2008) and Li
 tomatic predicate recognition is vital to nominal           et al. (2009).
 SRL. In principle, automatic predicate recogni-
 tion can be cast as a binary classification (e.g.,          3.3       Pipeline Parsing on Top-best Parse
 Predicate vs. Non-Predicate) problem. For no-                         Tree
 minal predicates, a binary classifier is trained to         Similar to most of the state-of-the-art systems
 predict whether a noun is a nominal predicate or            (Pradhan et al., 2005; Xue, 2008; Li et al., 2009),
 not. In particular, any word POS-tagged as NN               the top-best parse tree is first returned from our
 is considered as a predicate candidate in both              syntactic parser and then fed into the SRL sys-
 training and testing processes. Let the nominal             tem. Specifically, the verbal (nominal) SRL la-
 predicate candidate be w0, and its left and right           beler is in charge of verbal (nominal) predicates,
 neighboring words/POSs be w-1/p-1and w1/p1,                 respectively. For each sentence, since SRL is
 respectively. Table 1 lists the feature set used in         only performed on one parse tree, only con-
 our model. In Table 1, local features present the           stituents in it are candidates for semantic argu-
 candidate’s contextual information while global             ments. Therefore, if no constituent in the parse
 features show its statistical information in the            tree can map the same text span to an argument
 whole training set.                                         in the manual annotation, the system will not get
                                                             a correct annotation.
Type      Description
local     w0, w-1, w1, p-1, p1                               4       Joint Syntactic and Semantic Parsing
features The first and last characters of the candidate
          Whether w0 is ever tagged as a verb in the         In this section, we first explore pipeline parsing
          training data? Yes/No                              on N-best parse trees, as a natural extension of
global    Whether w0 is ever annotated as a nominal          pipeline parsing on the top-best parse tree. Then,
features predicate in the training data? Yes/No              joint syntactic and semantic parsing is explored
          The most likely label for w0 when it occurs        for Chinese texts from two levels. Firstly, an
          together with w-1 and w1.                          integrated parsing approach to joint syntactic
          The most likely label for w0 when it occurs        and semantic parsing is proposed. Secondly,
          together with w-1.
          The most likely label for w0 when it occurs
                                                             various semantic role-related features are di-
          together with w1.                                  rectly incorporated into the syntactic parsing
 Table 1: Feature set for nominal predicate recognition      model for better interaction between the two
                                                             tasks.

 3.2.2    SRL for Chinese Predicates                         4.1       Pipeline Parsing on N-best Parse Trees
 Our Chinese SRL models for both verbal and                  The pipeline parsing approach employed in this
 nominal predicates adopt the widely-used SRL                paper is largely motivated by the general
 framework, which divides the task into three                framework of re-ranking, as proposed in Sutton
 sequential sub-tasks: argument pruning, argu-               and McCallum (2005). The idea behind this ap-
 ment identification, and argument classification.           proach is that it allows uncertainty about syntac-
 In particular, we follow Xue (2008) and Li et al.           tic parsing to be carried forward through an
 (2009) to develop verbal and nominal SRL                    N-best list, and that a reliable SRL system, to a
 models, respectively. Moreover, we have further             certain extent, can reflect qualities of syntactic
 improved the performance of Chinese verbal                  parse trees. Given a sentence x, a joint parsing
 SRL by exploring additional features, e.g., voice           model is defined over a semantic frame F and a
 position that indicates the voice maker (BA, BEI)           parse tree t in a log-linear way:
 is before or after the constituent in focus, the                Score ( F , t | x )
                                                                                                                      (1)
                                                                 = (1 − α ) log P ( F | t , x ) + α log P ( t | x )
 rule that expands the parent of the constituent in
 focus, and the core arguments defined in the
 predicate’s frame file. For nominal SRL, we                 where P(t|x) is returned by a probabilistic syn-
 simply use the final feature set of Li et al. (2009).       tactic parsing model, e.g., our syntactic parser,
 As a result, our Chinese verbal and nominal SRL             and P(F|t, x) is returned by a probabilistic se-
 systems achieve performance of 92.38 and 72.67              mantic parsing model, e.g. our verbal & nominal


                                                          1111


      Algorithm 1. The algorithm integrating syntactic parsing and SRL.
      Assume:
        t: constituent which is complete with “YES” decision of CHECK procedure
        P: number of predicates
        Pi: ith predicate
        S: SRL result, set of predicates and its arguments
      BEGIN
          srl_prob = 0.0;
          FOR i=1 to P DO
               IF t covers Pi THEN
                   T = number of children of t;
                   FOR j=1 to T DO
                        IF t’s jth child Chj does not cover Pi THEN
                              Run SRL given predicate Pi and constituent Chj to get their semantic role
                                   lbl and its probability prob;
                              IF lbl does not indicate non-argument THEN
                                   srl_prob += log( prob );
                                   S = S ∪ {(Pi, Chj, lbl)};
                              END IF
                        END IF
                   END FOR
               END IF
          END FOR
          return srl_prob;
      END

SRL systems. In our pipeline parsing approach,                 Ideally, we should perform SRL on as many
P(t|x) is calculated as the product of all involved         parse trees as possible, so as to enlarge the
decisions’ probabilities in the syntactic parsing           search scope. However, pipeline parsing on all
model, and P(F|t, x) is calculated as the product           possible parse trees is time-consuming and thus
of all the semantic role labels’ probabilities in a         unrealistic. As an alternative, we turn to inte-
sentence (including both verbal and nominal                 grated parsing, which aims to perform syntactic
SRL). That is to say, we only consider those                and semantic parsing synchronously. The key
constituents that are supposed to be arguments.             idea is to construct a parse tree in a bottom-up
Here, the parameter α is a balance factor in-               way so that it is feasible to perform SRL at suit-
dicating the importance of the semantic parsing             able moments, instead of only when the whole
model.                                                      parse tree is built. Integrated parsing is practica-
   In particular, (F*, t*) with maximal Score(F,            ble, mostly due to the following two observa-
t|x) is selected as the final syntactic and seman-          tions: (1) Given a predicate in a parse tree, its
tic parsing results. Given a sentence, N-best               semantic arguments are usually siblings of the
parse trees are generated first using the syntactic         predicate, or siblings of its ancestor. Actually,
parser, and then for each parse tree, we predict            this special observation has been widely em-
the best SRL frame using our verbal and nomi-               ployed in SRL to prune non-arguments for a
nal SRL systems.                                            verbal or nominal predicate (Xue, 2008; Li et al.,
                                                            2009). (2) SRL feature spaces (both in fea-
4.2    Integrated Parsing                                   ture-based method and kernel-based method)
Although pipeline parsing on N-best parse trees             mostly focus on the predicate-argument structure
could relieve severe dependence on the quality              of a given (predicate, argument) pair. That is to
of the top-best parse tree, there is still a potential      say, once a predicate-argument structure is
drawback: this method suffers from the limited              formed (i.e., an argument candidate is connected
scope covered by the N-best parse trees since the           with the given predicate), there is enough con-
items in the parse tree list may be too similar,            textual information to predict their SRL relation.
especially for long sentences. For example,                    As far as our syntactic parser is concerned, we
50-best parse trees can only represent a combi-             invoke the SRL systems once a new constituent
nation of 5 to 6 binary ambiguities since 2^5 <             covering a predicate is complete with a “YES”
50 < 2^6.                                                   decision from the CHECK procedure. Algorithm

                                                         1112


1 illustrates the integration of syntactic and se-           process. In addition, it is found that 11% of pre-
mantic parsing. For the example shown in Fig-                dicates in a sentence are speculatively attached
ure 2, the CHECK procedure predicts a “YES”                  with two or more core arguments with the same
decision, indicating the immediately proposed                label due to semantic parsing errors (partly
constituent “VP (提供/provide 人民币/RMB                          caused by syntactic parsing errors in automatic
贷款/loan)” is complete. So, at this moment, the               parse trees). This is abnormal since a predicate
verbal SRL system is invoked to predict the se-              normally only allows at most one argument of
mantic label of the constituent “NP (人民币                     each core argument role (i.e., Arg0-Arg4).
                                                             Therefore, such syntactic errors should be
/RMB 贷款/loan)”, given the verbal predicate
                                                             avoidable by considering those arguments al-
“VV (提供/provide)”. Similarly, “PP (向/to 朝                    ready obtained in the bottom-up parsing process.
鲜/N. Korean 政府/govt.)” would also be se-                     On the other hand, taking those expected seman-
mantically labeled as soon as “PP (向/to 朝鲜/N.                tic roles into account would help the syntactic
Korean 政府/govt.)” and “VP (提供/provide 人                      parser. In terms of our syntactic parsing model,
民币/RMB 贷款/loan)” are merged into a big-                      this is done by directly incorporating various
ger VP. In this way, both syntactic and semantic             semantic role-related features into the syntactic
parsing are accomplished when the root node                  parsing model (i.e., the BUILD procedure) when
TOP is formed. It is worth pointing out that all             the newly-formed constituent covers one or
features (Xue, 2008; Li et al., 2009) used in our            more predicates.
SRL model can be instantiated and their values                  For the example shown in Figure 2, once the
are same as the ones when the whole tree is                  constituent “VP (提供/provide 人民币/RMB
available. In particular, the probability computed           贷款/loan)”, which covers a verbal predicate
from the SRL model is interpolated with that of              “VV (提供/provide)”, is complete, the verbal
the syntactic parsing model in a log-linear way              SRL model would be triggered first to mark
(with equal weights in our experiments). This is             constituent “NP (人民币/RMB 贷款/loan)” as
due to our hypothesis that the probability re-               ARG1, given predicate “VV (提供/provide)”.
turned from SRL model is helpful to joint syn-
                                                             Then, the BUILD procedure is called to make
tactic and semantic parsing, considering the
                                                             the BUILD decision for the newly-formed con-
close interaction between the two tasks.
                                                             stituent “VP (提供/provide 人民币/RMB 贷款
       Start_VP / NO                   VP YES?
                                                             /loan)”. Table 2 lists various semantic
                                                             role-related features explored in our syntactic
 …           PP               VV              NP      …      parsing model and their instantiations with re-
                                                             gard to the example shown in Figure 2. In Table
      P           NP          提供         NN        NN        2, feature sf4 gives the possible core semantic
                             provide                         roles that the focus predicate may take, accord-
             NR        NN
      向                                人民币         贷款        ing to its frame file; feature sf5 presents the se-
      to                               RMB         loan      mantic roles that the focus predicate has already
             朝鲜      政府
                                                             occupied; feature sf6 indicates the semantic
           N. Korean govt.
                                                             roles that the focus predicate is expecting; and
  Figure 2: An application of CHECK with YES as the          SF1-SF8 are combined features. Specifically, if
  decision. Thus, VV (提供/provide) and NP (人民币                the current constituent covers n predicates, then
  /RMB 贷款/loan) reduce to a big VP.                          14 * n features would be instantiated. Moreover,
                                                             we differentiate whether the focus predicate is
                                                             verbal or nominal, and whether it is the head
4.3    Integrating Semantic Role-related
                                                             word of the current constituent.
       Features into Syntactic Parsing Model
                                                                Feature Selection. Some features proposed
The integrated parsing approach as shown in                  above may not be effective in syntactic parsing.
Section 4.2 performs syntactic and semantic                  Here we adopt the greedy feature selection algo-
parsing synchronously. In contrast to traditional            rithm as described in Jiang and Ng (2006) to
syntactic parsers where no semantic role-related             select useful features empirically and incremen-
information is used, it may be interesting to in-            tally according to their contributions on the de-
vestigate the contribution of such information in            velopment data. The algorithm repeatedly se-
the syntactic parsing model, due to the availabil-           lects one feature each time which contributes the
ity of such information in the syntactic parsing             most, and stops when adding any of the remain-


                                                          1113


ing features fails to improve the syntactic pars-           call, precision, and their F1-measure for evalua-
ing performance.                                            tion of SRL on automatic predicates, combining
                                                            verbal SRL and nominal SRL. An argument is
Feat.  Description                                          correctly labeled if there is an argument in man-
sf1    Path: the syntactic path from C to P. (VP>VV)        ual annotation with the same semantic label that
sf2    Predicate: the predicate itself. (提供/provide)        spans the same words. Moreover, we also report
sf3    Predicate class (Xue, 2008): the class that P        the performance of predicate recognition. To see
       belongs to. (C3b)                                    whether an improvement in F1-measure is statis-
sf4 Possible roles: the core semantic roles P may           tically significant, we also conduct significance
       take. (Arg0, Arg1, Arg2)                             tests using a type of stratified shuffling which in
sf5 Detected roles: the core semantic roles already         turn is a type of compute-intensive randomized
       assigned to P. (Arg1)
                                                            tests. In this paper, ‘>>>’, ‘>>’, and ‘>’ denote
sf6 Expected roles: possible semantic roles P is
       still expecting. (Arg0, Arg2)
                                                            p-values less than or equal to 0.01, in-between
SF1 For each already detected argument, its role            (0.01, 0.05], and bigger than 0.05, respectively.
       label + its path from P. (Arg1+VV<VP>NP)                We are not aware of any SRL system comb-
SF2 sf1 + sf2. (VP>VV+提供/provide)                           ing automatic predicate recognition, verbal SRL
SF3 sf1 + sf3. (VP>VV+C3b)                                  and nominal SRL on Chinese PropBank and
SF4 Combined possible argument roles.                       NomBank. Xue (2008) experimented independ-
       (Arg0+Arg1+Arg2)                                     ently with verbal and nominal SRL and assumed
SF5 Combined detected argument roles. (Arg1)                correct predicates. Li et al. (2009) combined
SF6 Combined expected argument roles.                       nominal predicate recognition and nominal SRL
       (Arg0+Arg2)                                          on Chinese NomBank. The CoNLL-2009 shared
SF7 For each expected semantic role, sf1 + its role         task (Hajic et al., 2009) included both verbal and
       label. (VP>VV+Arg0, VP>VV+Arg2)                      nominal SRL on dependency parsing, instead of
SF8 For each expected semantic role, sf2 + its role         constituent-based syntactic parsing. Thus the
       label.                                               SRL performances of their systems are not di-
         (提供/provide+Arg0, 提供/provide+Arg2)                 rectly comparable to ours.
Table 2: SRL-related features and their instantiations
for syntactic parsing, with “VP (提供/provide 人民              5.2   Results and Discussions
币/RMB 贷款/loan)” as the current constituent C
                                                            Results of pipeline parsing on N-best parse
and “提供/provide” as the focus predicate P, based
                                                            trees. While performing pipeline parsing on
on Figure 2.
                                                            N-best parse trees, 20-best (the same as the heap
5       Experiments and Results                             size in our syntactic parsing) parse trees are ob-
                                                            tained for each sentence using our syntactic
We have evaluated our integrated parsing ap-                parser as described in Section 3.1. The balance
proach on Chinese TreeBank 5.1 and corre-                   factor α is set to 0.5 indicating that the two
sponding Chinese PropBank and NomBank.                      components in formula (1) are equally important.
                                                            Table 3 compares the two pipeline parsing ap-
5.1      Experimental Settings                              proaches on the top-best parse tree and the
This version of Chinese PropBank and Chinese                N-best parse trees. It shows that the approach on
NomBank consists of standoff annotations on                 N-best parse trees outperforms the one on the
the file (chtb 001 to 1151.fid) of Chinese Penn             top-best parse tree by 0.42 (>>>) in F1-measure
TreeBank 5.1. Following the experimental set-               on SRL. In addition, syntactic parsing also bene-
tings in Xue (2008) and Li et al. (2009), 648               fits from the N-best parse trees approach with
files (chtb 081 to 899.fid) are selected as the             improvement of 0.17 (>>>) in F1-measure. This
training data, 72 files (chtb 001 to 040.fid and            suggests that pipeline parsing on N-best parse
chtb 900 to 931.fid) are held out as the test data,         trees can improve both syntactic and semantic
and 40 files (chtb 041 to 080.fid) are selected as          parsing.
the development data. In particular, the training,             It is worth noting that our experimental results
test, and development data contain 31,361                   in applying the re-ranking framework in Chinese
(8,642), 3,599 (1,124), and 2,060 (731) verbal              pipeline parsing on N-best parse trees are very
(nominal) propositions, respectively.                       encouraging, considering the pessimistic results
   For the evaluation measurement on syntactic              of Sutton and McCallum (2005), in which the
parsing, we report labeled recall, labeled preci-           re-ranking framework failed to improve the per-
sion, and their F1-measure. Also, we report re-             formance on English SRL. It may be because,

                                                         1114


unlike Sutton and McCallum (2005), P(F, t|x)              search space, although the integrated parsing
defined in this paper only considers those con-           approach integrates the SRL probability and the
stituents which are identified as arguments. This         syntactic parsing probability in the same manner
can effectively avoid the noises caused by the            as the pipeline parsing approach on 20-best
predominant non-argument constituents. More-              parse trees. However, the syntactic parsing per-
over, the huge performance gap between Chi-               formance gap between the integrated parsing
nese semantic parsing on the gold parse tree and          approach and the pipeline parsing approach on
that on the top-best parse tree leaves much room          20-best parse trees is negligible.
for performance improvement.                                 Results of integrated parsing with semantic
                                                          role-related features. After performing the
Method          Task           R (%)   P (%)   F1         greedy feature selection algorithm on the devel-
                Syntactic      76.68   79.12   77.88      opment data, features {SF3, SF2, sf5, sf6, SF4}
                SRL            62.96   65.04   63.98      as proposed in Section 4.3 are sequentially se-
                Predicate      94.18   92.28   93.22      lected for syntactic parsing. As what we have
Pipeline on top
                 V-SRL         65.33   68.52   66.88      assumed, knowledge about the detected seman-
-best parse tree
                 V-Predicate   89.52   93.12   91.29      tic roles and expected semantic roles is helpful
                 N-SRL         49.58   48.19   48.88      for syntactic parsing. Table 3 also lists the per-
                 N-Predicate   86.83   71.76   78.58      formance achieved with those selected features.
                Syntactic      76.89   79.25   78.05      It shows that the integration of semantic
                SRL            62.99   65.88   64.40      role-related features in integrated parsing sig-
                Predicate      94.07   92.22   93.13      nificantly enhances both the performance of syn-
Pipeline on 20                                            tactic and semantic parsing by 0.44 (>>>) and
-best parse trees V-SRL       65.41 69.09 67.20
                  V-Predicate 89.66 93.02 91.31           0.49 (>>) respectively in F1-measure. In addi-
                  N-SRL       49.24 49.46 49.35           tion, it shows that it outperforms the wide-
                  N-Predicate 86.65 72.15 78.74           ly-used pipeline parsing approach on top-best
                  Syntactic   77.14 79.01 78.07           parse tree by 0.63 (>>>) and 1.58 (>>>) in
                  SRL         62.67 67.67 65.07           F1-measure on syntactic and semantic parsing,
                  Predicate   93.97 92.42 93.19           respectively. Finally, it shows that it outper-
Integrated                                                forms the widely-used pipeline parsing approach
                  V-SRL       65.37 70.27 67.74
parsing
                  V-Predicate 90.08 92.87 91.45           on 20-best parse trees by 0.46 (>>>) and 1.16
                  N-SRL       48.02 52.83 50.31           (>>>) in F1-measure on syntactic and semantic
                  N-Predicate 85.41 73.23 78.85           parsing, respectively. This is very encouraging,
                  Syntactic   77.47 79.58 78.51           considering the notorious difficulty and
Integrated        SRL         63.14 68.17 65.56           complexity of both the syntactic and semantic
parsing with      Predicate   93.97 92.52 93.24           parsing tasks.
semantic          V-SRL       65.74 70.98 68.26              Table 3 also shows that our proposed method
role-related      V-Predicate 89.86 93.17 91.49           works well for both verbal SRL and nominal
features          N-SRL       48.80 52.67 50.66           SRL. In addition, it shows that the performance
                  N-Predicate 85.85 72.78 78.78           of predicate recognition is very stable due to its
Table 3: Syntactic and semantic parsing performance       high dependence on POS tagging results, rather
on test data (using gold standard word boundaries).       than syntactic parsing results. Finally, it is not
“V-” denotes “verbal” while “N-”denotes “nominal”.
                                                          surprising to find out that the performance of
                                                          predicate recognition when mixing verbal and
   Results of integrated parsing. Table 3 also
                                                          nominal predicates is better than the perform-
compares the integrated parsing approach with
                                                          ance of either verbal predicates or nominal
the two pipeline parsing approaches. It shows
                                                          predicates.
that the integrated parsing approach improves
the performance of both syntactic and semantic            5.3 Extending the Word-based Syntactic
parsing by 0.19 (>) and 1.09 (>>>) respectively           Parser to a Character-based Syntactic Parser
in F1-measure over the pipeline parsing ap-
proach on the top-best parse tree. It is also not         The above experimental results on a word-based
surprising to find out that the integrated parsing        syntactic parser (assuming correct word seg-
approach outperforms the pipeline parsing ap-             mentation) show that both syntactic and seman-
proach on 20-best parse trees by 0.67 (>>>) in            tic parsing benefit from our integrated parsing
F1-measure on SRL, due to its exploring a larger          approach. However, observing the great chal-
                                                          lenge of word segmentation in Chinese informa-

                                                       1115


tion processing, it is still unclear whether and       Method                Task     R (%) P (%) F1
how much joint learning benefits charac-               Pipeline on top-best Syntactic 82.23 84.28 83.24
ter-based syntactic and semantic parsing. In this        parse tree          SRL      60.40 62.75 61.55
section, we extended the Ratnaparkhi parser            Pipeline on 20-best Syntactic 82.25 84.29 83.26
(1999) to a character-based parser (with auto-         parse trees           SRL      60.17 63.63 61.85
matic word segmentation), and then examined            Integrated parsing Syntactic 82.51 84.31 83.40
the effectiveness of joint learning.                   with semantic         SRL      60.09 65.35 62.61
                                                       role-related features
   Given the three-pass process in the
                                                       Table 4: Performance with the character-based pars-
word-based syntactic parser, it is easy to extend      er 1 (using automatically recognized word bounda-
it to a character-based parser for Chinese texts.      ries).
This can be done by only replacing the TAG
procedure in the first pass with a POSCHUNK            6    Conclusion
procedure, which integrates Chinese word seg-
mentation and POS tagging in one step, follow-         In this paper, we explore joint syntactic and se-
ing the method described in (Ng and Low 2004).         mantic parsing to improve the performance of
Here, each character is annotated with both a          both syntactic and semantic parsing, in particular
boundary tag and a POS tag. The 4 possible             that of semantic parsing. Evaluation shows that
boundary tags include “B” for a character that         our integrated parsing approach outperforms the
begins a word and is followed by another char-         pipeline parsing approach on N-best parse trees,
acter, “M” for a character that occurs in the          a natural extension of the widely-used pipeline
middle of a word, “E” for a character that ends a      parsing approach on the top-best parse tree. It
word, and “S” for a character that occurs as a         also shows that incorporating semantic informa-
single-character word. For example, “北京市               tion into syntactic parsing significantly improves
/Beijing city/NR” would be decomposed into             the performance of both syntactic and semantic
three units: “ 北 /north/B_NR”, “ 京                     parsing. This is very promising and encouraging,
                                                       considering the complexity of both syntactic and
/capital/M_NR”, and “市/city/E_NR”. Also, “是
                                                       semantic parsing.
/is/VC” would turn into “是/is/S_VC”. Through              To our best knowledge, this is the first suc-
POSCHUNK, all characters in a sentence are             cessful research on exploring syntactic parsing
first assigned with POS chunk labels which must        and semantic role labeling for verbal and nomi-
be compatible with previous ones, and then             nal predicates in an integrated way.
merged into words with their POS tags. For ex-
ample, “北/north/B_NR”, “京/capital/M_NR”,               Acknowledgments
and “市/city/E_NR” will be merged as “北京市
                                                       The first two authors were financially supported
/Beijing/NR”, “是/is/S_VC” will become “是
                                                       by Projects 60683150, 60970056, and 90920004
/is/VC”. Finally the merged results of the PO-         under the National Natural Science Foundation
SCHUNK are fed into the CHUNK procedure of             of China. This research was also partially sup-
the second pass.                                       ported by a research grant R-252-000-225-112
   Using the same data split as the previous ex-       from National University of Singapore Aca-
periments, word segmentation achieves perfor-          demic Research Fund. We also want to thank the
mance of 96.3 in F1-measure on the test data.          reviewers for insightful comments.
Table 4 lists the syntactic and semantic parsing
performance by adopting the character-based
                                                       References
parser.
   Table 4 shows that integrated parsing benefits      Collin F. Baker, Charles J. Fillmore, and John B.
syntactic and semantic parsing when automatic            Lowe. 1998. The Berkeley FrameNet Project. In
word segmentation is considered. However, the            Proceedings of COLING-ACL 1998.
improvements are smaller due to the extra noise        Xavier Carreras and Lluis Màrquez. 2004. Introduc-
caused by automatic word segmentation. For               tion to the CoNLL-2004 Shared Task: Semantic
example, our experiments show that the per-              Role Labeling. In Proceedings of CoNLL 2004.
formance of predicate recognition drops from
93.2 to 90.3 in F1-measure when replacing cor-         1
rect word segmentations with automatic ones.             POS tags are included in evaluating the perform-
                                                       ance of a character-based syntactic parser. Thus it
                                                       cannot be directly compared with the word-based one
                                                       where correct word segmentation is assumed.

                                                    1116


Xavier Carreras and Lluis Màrquez. 2005. Introduc-           All-at-Once? Word-Based or Character-Based? In
  tion to the CoNLL-2005 Shared Task: Semantic               Proceedings of EMNLP 2004.
  Role Labeling. In Proceedings of CoNLL 2005.
                                                         Martha Palmer, Daniel Gildea, and Paul Kingsbury.
Eugene Charniak. 2001. Immediate-Head Parsing for          2005. The Proposition Bank: An Annotated Cor-
  Language Models. In Proceedings of ACL 2001.             pus of Semantic Roles. Computational Linguistics,
                                                           31, 71-106.
Michael Collins. 1999. Head-Driven Statistical Mod-
  els for Natural Language Parsing. Ph.D. thesis,        Slav Petrov and Dan Klein. 2007. Improved Infer-
  University of Pennsylvania.                               ence for Unlexicalized Parsing. In Proceesings of
                                                            NAACL 2007.
Jenny Rose Finkel and Christopher D. Manning.
   2009. Joint Parsing and Named Entity Recognition.     Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
   In Proceedings of NAACL 2009.                           Wayne Ward, James H. Martin, and Daniel Juraf-
                                                           sky. 2005. Support Vector Learning for Semantic
Jan Hajic, Massimiliano Ciaramita, Richard Johans-
                                                           Argument Classification. Machine Learning, 2005,
   son, et al. 2009. The CoNLL-2009 Shared Task:
                                                           60:11-39.
   Syntactic and Semantic Dependencies in Multiple
   Languages. In Proceedings of CoNLL 2009.              Adwait Ratnaparkhi. 1999. Learning to Parse Natural
                                                           Language with Maximum Entropy Models. Ma-
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
                                                           chine Learning, 34, 151-175.
  Role Labeling of NomBank: A Maximum Entropy
  Approach. In Proceedings of EMNLP 2006.                Mihai Surdeanu, Sanda Harabagiu, John Williams
                                                           and Paul Aarseth. 2003. Using Predi-
Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009.
                                                           cate-Argument Structures for Information Extrac-
  Employing the Centering Theory in Pronoun
                                                           tion. In Proceedings of ACL 2003.
  Resolution from the Semantic Perspective. In
  Proceedings of EMNLP 2009.                             Mihai Surdeanu, Richard Johansson, Adam Meyers,
                                                           Lluis Màrquez, and Joakim Nivre. 2008. The
Peter Koomen, Vasin Punyakanok, Dan Roth,
                                                           CoNLL-2008 Shared Task on Joint Parsing of
  Wen-tau Yih. 2005. Generalized Inference with
                                                           Syntactic and Semantic Dependencies. In Pro-
  Multiple Semantic Role Labeling Systems. In
                                                           ceedings of CoNLL 2008.
  Proceedings of CoNLL 2005.
                                                         Charles Sutton and Andrew McCallum. 2005. Joint
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
                                                           Parsing and Semantic Role Labeling. In Proceed-
  and Peide Qian. 2009. Improving Nominal SRL in
                                                           ings of CoNLL2005.
  Chinese Language with Verbal SRL information
  and Automatic Predicate Recognition. In Pro-           Nianwen Xue and Martha Palmer. 2003. Annotating
  ceedings of EMNLP 2009.                                  the Propositions in the Penn Chinese TreeBank. In
                                                           Proceedings of the 2nd SIGHAN Workshop on
Chang Liu and Hwee Tou Ng. 2007. Learning Pre-
                                                           Chinese Language Processing.
  dictive Structures for Semantic Role Labeling of
  NomBank. In Proceedings of ACL 2007.                   Nianwen Xue. 2006. Annotating the Predi-
                                                           cate-Argument Structure of Chinese Nominaliza-
Paola Merlo and Gabriele Mussillo. 2005. Accurate
                                                           tions. In Proceedings of LREC 2006.
  Function Parsing. In Proceedings of EMNLP 2005.
                                                         Nianwen Xue. 2008. Labeling Chinese Predicates
Paola Merlo and Gabriele Musillo. 2008. Semantic
                                                           with Semantic Roles. Computational Linguistics,
  Parsing for High-Precision Semantic Role Label-
                                                           34(2):225-255.
  ling. In Proceedings of CoNLL 2008.
                                                         Szu-ting Yi and Martha Palmer. 2005. The Integra-
Adam Meyers, Ruth Reeves, Catherine Macleod,
                                                           tion of Syntactic Parsing and Semantic Role La-
  Rachel Szekely, Veronika Zielinska, Brian Young,
                                                           beling. In Proceedings of CoNLL 2005.
  and Ralph Grishman. 2004. Annotating Noun Ar-
  gument Structure for NomBank. In Proceedings of        Yue Zhang and Stephen Clark. 2008. Joint Word
  LREC 2004.                                               Segmentation and POS Tagging Using a Single
                                                           Perceptron. In Proceedings of ACL 2008.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
  Weischedel. 2000. A Novel Use of Statistical
  Parsing to Extract Information from Text. In Pro-
  ceedings of ANLP 2000.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
   tion Answering based on Semantic Structures. In
   Proceedings of COLING 2004.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese
  Part-of-Speech Tagging: One-at-a-Time or

                                                      1117

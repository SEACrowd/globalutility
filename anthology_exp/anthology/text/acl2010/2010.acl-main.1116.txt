                  Topic Models for Word Sense Disambiguation and
                           Token-based Idiom Detection

                         Linlin Li, Benjamin Roth, and Caroline Sporleder
                                Saarland University, Postfach 15 11 50
                                      66041 Saarbrücken, Germany
                            {linlin, beroth, csporled}@coli.uni-saarland.de


                      Abstract                                for an overview). Recently, several researchers
                                                              have experimented with topic models (Brody and
    This paper presents a probabilistic model                 Lapata, 2009; Boyd-Graber et al., 2007; Boyd-
    for sense disambiguation which chooses                    Graber and Blei, 2007; Cai et al., 2007) for sense
    the best sense based on the conditional                   disambiguation and induction. Topic models are
    probability of sense paraphrases given a                  generative probabilistic models of text corpora in
    context. We use a topic model to decom-                   which each document is modelled as a mixture
    pose this conditional probability into two                over (latent) topics, which are in turn represented
    conditional probabilities with latent vari-               by a distribution over words.
    ables. We propose three different instanti-                  Previous approaches using topic models for
    ations of the model for solving sense dis-                sense disambiguation either embed topic features
    ambiguation problems with different de-                   in a supervised model (Cai et al., 2007) or rely
    grees of resource availability. The pro-                  heavily on the structure of hierarchical lexicons
    posed models are tested on three different                such as WordNet (Boyd-Graber et al., 2007). In
    tasks: coarse-grained word sense disam-                   this paper, we propose a novel framework which
    biguation, fine-grained word sense disam-                 is fairly resource-poor in that it requires only 1)
    biguation, and detection of literal vs. non-              a large unlabelled corpus from which to estimate
    literal usages of potentially idiomatic ex-               the topics distributions, and 2) paraphrases for the
    pressions. In all three cases, we outper-                 possible target senses. The paraphrases can be
    form state-of-the-art systems either quan-                user-supplied or can be taken from existing re-
    titatively or statistically significantly.                sources.
                                                                 We approach the sense disambiguation task by
1   Introduction
                                                              choosing the best sense based on the conditional
Word sense disambiguation (WSD) is the task of                probability of sense paraphrases given a context.
automatically determining the correct sense for a             We propose three models which are suitable for
target word given the context in which it occurs.             different situations: Model I requires knowledge
WSD is an important problem in NLP and an es-                 of the prior distribution over senses and directly
sential preprocessing step for many applications,             maximizes the conditional probability of a sense
including machine translation, question answering             given the context; Model II maximizes this condi-
and information extraction. However, WSD is a                 tional probability by maximizing the cosine value
difficult task, and despite the fact that it has been         of two topic-document vectors (one for the sense
the focus of much research over the years, state-             and one for the context). We apply these models
of-the-art systems are still often not good enough            to coarse- and fine-grained WSD and find that they
for real-world applications. One major factor that            outperform comparable systems for both tasks.
makes WSD difficult is a relative lack of manu-                  We also test our framework on the related task
ally annotated corpora, which hampers the perfor-             of idiom detection, which involves distinguishing
mance of supervised systems.                                  literal and nonliteral usages of potentially ambigu-
   To address this problem, there has been a                  ous expressions such as rock the boat. For this
significant amount of work on unsupervised                    task, we propose a third model. Model III cal-
WSD that does not require manually sense-                     culates the probability of a sense given a context
disambiguated training data (see McCarthy (2009)              according to the component words of the sense


                                                        1138
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


paraphrase. Specifically, it chooses the sense type   iosyncracies in the hierarchical structure of Word-
which maximizes the probability (given the con-       Net can harm performance. This is a general prob-
text) of the paraphrase component word with the       lem for methods which use hierarchical lexicons
highest likelihood of occurring in that context.      to model semantic distance (Budanitsky and Hirst,
This model also outperforms state-of-the-art sys-     2006). In our approach, we circumvent this prob-
tems.                                                 lem by exploiting paraphrase information for the
                                                      target senses rather than relying on the structure
2   Related Work                                      of WordNet as a whole.
                                                         Topic models have also been applied to the re-
There is a large body of work on WSD, cover-
                                                      lated task of word sense induction. Brody and
ing supervised, unsupervised (word sense induc-
                                                      Lapata (2009) propose a method that integrates a
tion) and knowledge-based approaches (see Mc-
                                                      number of different linguistic features into a single
Carthy (2009) for an overview). While most su-
                                                      generative model.
pervised approaches treat the task as a classifica-
tion task and use hand-labelled corpora as train-        Topic models have been previously consid-
ing data, most unsupervised systems automatically     ered for metaphor extraction and estimating the
group word tokens into similar groups using clus-     frequency of metaphors (Klebanov et al., 2009;
tering algorithms, and then assign labels to each     Bethard et al., 2009). However, we have a differ-
sense cluster. Knowledge-based approaches ex-         ent focus in this paper, which aims to distinguish
ploit information contained in existing resources.    literal and nonliteral usages of potential idiomatic
They can be combined with supervised machine-         expressions. A number of methods have been ap-
learning models to assemble semi-supervised ap-       plied to this task. Katz and Giesbrecht (2006)
proaches.                                             devise a supervised method in which they com-
   Recently, a number of systems have been pro-       pute the meaning vectors for the literal and non-
posed that make use of topic models for sense         literal usages of a given expression in the trainning
disambiguation. Cai et al. (2007), for example,       data. Birke and Sarkar (2006) use a clustering al-
use LDA to capture global context. They com-          gorithm which compares test instances to two au-
pute topic models from a large unlabelled corpus      tomatically constructed seed sets (one literal and
and include them as features in a supervised sys-     one nonliteral), assigning the label of the closest
tem. Boyd-Graber and Blei (2007) propose an un-       set. An unsupervised method that computes co-
supervised approach that integrates McCarthy et       hesive links between the component words of the
al.’s (2004) method for finding predominant word      target expression and its context have been pro-
senses into a topic modelling framework. In ad-       posed (Sporleder and Li, 2009; Li and Sporleder,
dition to generating a topic from the document’s      2009). Their system predicts literal usages when
topic distribution and sampling a word from that      strong links can be found.
topic, the enhanced model also generates a distri-
butional neighbour for the chosen word and then       3     The Sense Disambiguation Model
assigns a sense based on the word, its neighbour
and the topic. Boyd-Graber and Blei (2007) test       3.1    Topic Model
their method on WSD and information retrieval
tasks and find that it can lead to modest improve-    As pointed out by Hofmann (1999), the starting
ments over state-of-the-art results.                  point of topic models is to decompose the con-
   In another unsupervised system, Boyd-Graber        ditional word-document probability distribution
et al. (2007) enhance the basic LDA algorithm by      p(w|d) into two different distributions: the word-
incorporating WordNet senses as an additional la-     topic distribution p(w|z), and the topic-document
tent variable. Instead of generating words directly   distribution p(z|d) (see Equation 1). This allows
from a topic, each topic is associated with a ran-    each semantic topic z to be represented as a multi-
dom walk through the WordNet hierarchy which          nominal distribution of words p(w|z), and each
generates the observed word. Topics and synsets       document d to be represented as a multinominal
are then inferred together. While Boyd-Graber         distribution of semantic topics p(z|d). The model
et al. (2007) show that this method can lead to       introduces a conditional independence assumption
improvements in accuracy, they also find that id-     that document d and word w are independent con-


                                                  1139


ditioned on the hidden variable, topic z.              very well covered in WordNet, such as idioms.
           p(w|d) =
                     X
                         p(z|d)p(w|z)           (1)    For those situations, we propose another model,
                       z
                                                       Model III, in which contexts are treated as docu-
                                                       ments while sense paraphrases are treated as se-
LDA is a Bayesian version of this framework with       quences of independent words.1
Dirichlet hyper-parameters (Blei et al., 2003).          Model I directly maximizes the conditional
   The inference of the two distributions given an     probability of the sense given the context, where
observed corpus can be done through Gibbs Sam-         the sense is modeled as a ‘paraphrase document’
pling (Geman and Geman, 1987; Griffiths and            ds and the context as a ‘context document’ dc.
Steyvers, 2004). For each turn of the sampling,        The conditional probability of sense given context
each word in each document is assigned a seman-        p(ds|dc) can be rewritten as a joint probability di-
tic topic based on the current word-topic distribu-    vided by a normalization factor:
tion and topic-document distribution. The result-
ing topic assignments are then used to re-estimate                                           p(ds, dc)
                                                                          p(ds|dc) =                            (3)
a new word-topic distribution and topic-document                                               p(dc)
distribution for the next turn. This process re-          This joint probability can be rewritten as a gen-
peats until convergence. To avoid statistical co-      erative process by introducing a hidden variable z.
incidence, the final estimation of the distributions   We make the conditional independence assump-
is made by the average of all the turns after con-     tion that, conditioned on the topic z, a paraphrase
vergence.                                              document ds is generated independently of the
3.2   The Sense Disambiguation Model                   specific context document dc:
Assigning the correct sense s to a target word w
occurring in a context c involves finding the sense
                                                                              X
                                                              p(ds, dc) =          p(ds)p(z|ds)p(dc|z)          (4)
which maximizes the conditional probability of                                 z
senses given a context:
                                                         We apply the same process to the conditional
               s = arg max p(si |c)             (2)
                       si                              probability p(dc|z). It can be rewritten as:

 In our model, we represent a sense (si ) as a col-                                  p(dc)p(z|dc)
lection of ‘paraphrases’ that capture (some aspect                       p(dc|z) =                              (5)
                                                                                         p(z)
of) the meaning of the sense. These paraphrases
can be taken from an existing resource such as            Now, the disambiguation model p(ds|dc) can be
WordNet (Miller, 1995) or supplied by the user         rewritten as a prior p(ds) times a topic function
(see Section 4).                                       f (z):
   This conditional probability is decomposed by                                     X p(z|dc)p(z|ds)
incorporating a hidden variable, topic z, intro-              p(ds|dc) = p(ds)                                  (6)
                                                                                         z
                                                                                                    p(z)
duced by the topic model. We propose three varia-
tions of the basic model, depending on how much           As p(z) is a uniform distribution according to
background information is available, i.e., knowl-      the uniform Dirichlet priors assumption, Equation
edge of the prior sense distribution available and     6 can be rewritten as:
type of sense paraphrases used. In Model I and                                  X
Model II, the sense paraphrases are obtained from            p(ds|dc) ∝ p(ds)       p(z|dc)p(z|ds) (7)
WordNet, and both the context and the sense para-                                        z
phrases are treated as documents, c = dc and
                                                          Model I:
s = ds.
   WordNet is a fairly rich resource which pro-                arg max p(dsi )
                                                                                     X
                                                                                             p(z|dc)p(z|dsi )   (8)
vides detailed information about word senses                       dsi               z
(glosses, example sentences, synsets, semantic re-
lations between senses, etc.). Sometimes such de-         Model I has the disadvantage that it requires
tailed information may not be available, for in-       information about the prior distribution of senses
stance for languages for which such a resource             1
                                                             The idea is that these key words capture the meaning of
does not exist or for expressions that are not         the idioms.


                                                   1140


p(ds), which is not always available. We use sense          potentially idiomatic phrase ‘rock the boat’ can be
frequency information from WordNet to estimate              paraphrased as ‘break the norm’ or ‘cause trou-
the prior sense distribution, although it must be           ble’. A similar topic distribution to that of the
kept in mind that, depending on the genre of the            individual words ‘norm’ or ‘trouble’ would be
texts, it is possible that the distribution of senses       strong supporting evidence of the corresponding
in the testing corpus may diverge greatly from the          idiomatic reading.). We propose Model III:
WordNet-based estimation. If there is no means                                   X
for estimating the prior sense distribution of an                 arg max max        p(wi |z)p(z|dc)       (10)
                                                                     qsi   wi ∈qs
                                                                                    z
experimental corpus, generally a uniform distri-
bution must be assumed. However, this assump-                where qs is a collection of words contained in the
tion does not hold, as the true distribution of word        sense paraphrases.
senses is often highly skewed (McCarthy, 2009).             3.3    Inference
   To overcome this problem, we propose Model
II, which indirectly maximizes the sense-context            One possible inference approach is to combine the
probability by maximizing the cosine value of two           context documents and sense paraphrases into a
document vectors that encode the document-topic             corpus and run Gibbs sampling on this corpus.
frequencies from sampling, v(z|dc) and v(z|ds).             The problem with this approach is that the test set
The document vectors are represented by topics,             and sense paraphrase set are relatively small, and
with each dimension representing the number of              topic models running on a small corpus are less
times that the tokens in this document are assigned         likely to capture rich semantic topics. One sim-
to a certain topic.                                         ple explanation for this is that a small corpus usu-
   Model II:                                                ally has a relatively small vocabulary, which is less
                                                            representative of topics, i.e., p(w|z) cannot be es-
          arg max cos(v(z|dc), v(z|dsi ))            (9)    timated reliably.
             dsi                                               In order to overcome this problem, we infer the
                                                            word-topic distribution from a very large corpus
 If the prior distribution of senses is known, Model
I is the best choice. However, Model II has to be           (Wikipedia dump, see Section 4). All the follow-
chosen instead when this knowledge is not avail-            ing inference experiments on the test corpus are
able. In our experiments, we test the performance           based on the assumption that the word-topic dis-
of both models (see Section 5).                             tribution p(w|z) is the same as the one estimated
    If the sense paraphrases are very short, it is diffi-   from the Wikipedia dump. Inference of topic-
cult to reliably estimate p(z|ds). In order to solve        document distributions for context and sense para-
this problem, we treat the sense paraphrase ds as           phrases is done by fixing the word-topic distribu-
a ‘query’, a concept which is used in information           tion as a constant.
retrieval. One model from information retrieval
                                                            4     Experimental Setup
takes the conditional probability of the query given
the document as a product of all the conditional            We evaluate our models on three different tasks:
probabilities of words in the query given the doc-          coarse-grained WSD, fine-grained WSD and lit-
ument. The assumption is that the query is gener-           eral vs. nonliteral sense detection. In this section
ated by a collection of conditionally independent           we discuss our experimental set-up. We start by
words (Song and Croft, 1999).                               describing the three datasets for evaluation and an-
    We make the same assumption here. How-                  other dataset for probability estimation. We also
ever, instead of taking the product of all the condi-       discuss how we choose sense paraphrases and in-
tional probabilities of words given the document,           stance contexts.
we take the maximum. There are two reasons for                 Data We use three datasets for evaluation. The
this: (i) taking the product may penalize longer            coarse-grained task is evaluated on the Semeval-
paraphrases since the product of probabilities de-          2007 Task-07 benchmark dataset released by Nav-
creases as there are more words; (ii) we do not             igli et al. (2009). The dataset consists of 5377
want to model the probability of generating spe-            words of running text from five different articles:
cific paraphrases, but rather the probability of gen-       the first three were obtained from the WSJ cor-
erating a sense, which might only be represented            pus, the fourth was the Wikipedia entry for com-
by one or two words in the paraphrases (e.g., the           puter programming, and the fifth was an excerpt of


                                                        1141


Amy Steedman’s Knights of the Art, biographies                   tions caused by tagging or lemmatization errors,
of Italian painters. The proportion of the non news              we manually corrected any bad tags and lemmas
text, the last two articles, constitutes 51.87% of the           for the target instances.4
whole testing set. It consists of 1108 nouns, 591                   Sense Paraphrases For word sense disam-
verbs, 362 adjectives, and 208 adverbs. The data                 biguation tasks, the paraphrases of the sense keys
were annotated with coarse-grained senses which                  are represented by information from WordNet 2.1.
were obtained by clustering senses from the Word-                (Miller, 1995). To obtain the paraphrases, we use
Net 2.1 sense inventory based on the procedure                   the word forms, glosses and example sentences
proposed by Navigli (2006).                                      of the synset itself and a set of selected reference
    To determine whether our model is also suitable              synsets (i.e., synsets linked to the target synset by
for fine-grained WSD, we test on the data provided               specific semantic relations, see Table 1). We ex-
by Pradhan et al. (2009) for the Semeval-2007                    cluded the ‘hypernym reference synsets’, since in-
Task-17 (English fine-grained all-words task).                   formation common to all of the child synsets may
This dataset is a subset of the set from Task-07. It             confuse the disambiguation process.
comprises the three WSJ articles from Navigli et                    For the literal vs. nonliteral sense detection
al. (2009). A total of 465 lemmas were selected as               task, we selected the paraphrases of the nonlit-
instances from about 3500 words of text. There are               eral meaning from several online idiom dictionar-
10 instances marked as ‘U’ (undecided sense tag).                ies. For the literal senses, we used 2-3 manu-
Of the remaining 455 instances, 159 are nouns and                ally selected words with which we tried to cap-
296 are verbs. The sense inventory is from Word-                 ture (aspects of) the literal meaning of the expres-
Net 2.1.                                                         sion.5 For instance, the literal ‘paraphrases’ that
    Finally, we test our model on the related sense              we chose for ‘break the ice’ were ice, water and
disambiguation task of distinguishing literal and                snow. The paraphrases are shorter for the idiom
nonliteral usages of potentially ambiguous expres-               task than for the WSD task, because the mean-
sions such as break the ice. For this, we use the                ing descriptions from the idiom dictionaries are
dataset from Sporleder and Li (2009) as a test set.              shorter than what we get from WordNet. In the
This dataset consists of 3964 instances of 17 po-                latter case, each sense can be represented by its
tential English idioms which were manually anno-                 synset as well as its reference synsets.
tated as literal or nonliteral.                                     Instance Context We experimented with differ-
    A Wikipedia dump2 is used to estimate the                    ent context sizes for the disambiguation task. The
multinomial word-topic distribution. This dataset,               five different context settings that we used for the
which consists of 320,000 articles,3 is significantly            WSD tasks are: collocations (1w), ±5-word win-
larger than SemCor, which is the dataset used by                 dow (5w), ±10-word window (10w), current sen-
Boyd-Graber et al. (2007). All markup from the                   tence, and whole text. Because the idiom corpus
Wikipedia dump was stripped off using the same                   also includes explicitly marked paragraph bound-
filter as the ESA implementation (Sorg and Cimi-                 aries, we included ‘paragraph’ as a sixth type of
ano, 2008), and stopwords were filtered out using                context size for the idiom sense detection task.
the Snowball (Porter, October 2001) stopword list.
In addition, words with a Wikipedia document fre-                5       Experiments
quency of 1 were filtered out. The lemmatized
                                                                 As mentioned above, we test our proposed sense
version of the corpus consists of 299,825 lexical
                                                                 disambiguation framework on three tasks. We
units.
                                                                 start by describing the sampling experiments for
    The test sets were POS-tagged and lemmatized
using RASP (Briscoe and Carroll, 2006). The in-                      4
                                                                       This was done by comparing the predicted sense keys
ference processes are run on the lemmatized ver-                 and the gold standard sense keys. We only checked instances
                                                                 for which the POS-tags in the predicted sense keys are not
sion of the corpus. For the Semeval-2007 Task 17                 consistent with those in the gold standard. This was the case
English all-words, the organizers do not supply the              for around 20 instances.
                                                                     5
part-of-speech and lemma information of the tar-                       Note that we use the word ‘paraphrase’ in a fairly wide
                                                                 sense in this paper. Sometimes it is not possible to obtain ex-
get instances. In order to avoid the wrong predic-               act paraphrases. This applies especially to the task of distin-
                                                                 guishing literal from nonliteral senses of multi-word expres-
   2
       We use the English snapshot of 2009-07-13                 sions. In this case we take as paraphrases some key words
   3
       All articles of fewer than 100 words were discarded.      which capture salient aspects of the meaning.


                                                              1142


 POS        Paraphrase reference synsets
 N          hyponyms, instance hyponyms, member holonyms, substance holonyms, part holonyms,
            member meronyms, part meronyms, substance meronyms, attributes, topic members,
            region members, usage members, topics, regions, usages
 V          Troponyms, entailments, outcomes, phrases, verb groups, topics, regions, usages, sentence frames
 A          similar, pertainym, attributes, related, topics, regions, usages
 R          pertainyms, topics, regions, usages

Table 1: Selected reference synsets from WordNet that were used for different parts-of-speech to obtain
word sense paraphrase. N(noun), V(verb), A(adj), R(adv).

estimating the word-topic distribution from the             Type II both need extra resources. Type II has
Wikipedia dump. We used the package provided                an advantage over Type I since the prior knowl-
by Wang et al. (2009) with the suggested Dirich-            edge of the sense distribution can be estimated
let hyper-parameters 6 . In order to avoid statistical      from annotated corpora (e.g.: SemCor, Senseval).
instability, the final result is averaged over the last     In contrast, training data in Type I may be sys-
50 iterations. We did four rounds of sampling with          tem specific (e.g.: different input format, different
1000, 500, 250, and 125 topics respectively. The            annotation guidelines). McCarthy (2009) also ad-
final word-topic distribution is a normalized con-          dresses the issue of performance and cost by com-
catenate of the four distributions estimated in each        paring supervised word sense disambiguation sys-
round. In average, the sampling program run on              tems with unsupervised ones.
the Wikipedia dump consumed 20G memory, and                    We exclude the system provided by one of
each round took about one week on a single AMD              the organizers (UoR-SSI) from our categorization.
Dual-Core 1000MHZ processor.                                The reason is that although this system is claimed
                                                            to be unsupervised, and it performs better than
5.1      Coarse-Grained WSD                                 all the participating systems (including the super-
In this section we first describe the landscape of          vised systems) in the SemEval-2007 shared task, it
similar systems against which we compare our                still needs to incorporate a lot of prior knowledge,
models, then present the results of the comparison.         specifically information about co-occurrences be-
The systems that participated in the SemEval-2007           tween different word senses, which was obtained
coarse-grained WSD task (Task-07) can be di-                from a number of resources (SSI+LKB) includ-
vided into three categories, depending on whether           ing: (i) SemCor (manually annotated); (ii) LDC-
training data is needed and whether other types             DSO (partly manually annotated); (iii) collocation
of background knowledge are required: What we               dictionaries which are then disambiguated semi-
call Type I includes all the systems that need an-          automatically. Even though the system is not
notated training data. All the participating sys-           “trained”, it needs a lot of information which is
tems that have the mark TR fall into this cate-             largely dependent on manually annotated data, so
gory (see Navigli et al. (2009) for the evaluation          it does not fit neatly into the categories Type II or
for all the participating systems). Type II con-            Type III either.
sists of systems that do not need training data but            Table 2 lists the best participating systems of
require prior knowledge of the sense distribution           each type in the SemEval-2007 task (Type I:
(estimated sense frequency). All the participating          NUS-PT (Chan et al., 2007); Type II: UPV-WSD
systems that have the mark MFS belong to this cat-          (Buscaldi and Rosso, 2007); Type III: TKB-UO
egory. Systems that need neither training data nor          (Anaya-Sánchez et al., 2007)). Our Model I be-
prior sense distribution knowledge are categorized          longs to Type II, and our Model II belongs to Type
as Type III.                                                III.
                                                               Table 2 compares the performance of our mod-
   We make this distinction based on two princi-
                                                            els with the Semeval-2007 participating systems.
ples: (i) the cost of building a system; (ii) the
                                                            We only compare the F-score, since all the com-
portability of the established resource. Type III
                                                            pared systems have an attempted rate7 of 1.0,
is the cheapest system to build, while Type I and
                                                               7
                                                                 Attempted rate is defined as the total number of disam-
   6                             50
       They were set as: α =   #topics
                                         and β = 0.01.      biguated output instances divided by the total number of input


                                                         1143


which makes both the precision and recall rates the    System      Noun     Verb     Adj      Adv      All
same as the F-score. We focus on comparisons be-       UoR-SSI     84.12    78.34    85.36    88.46    83.21
tween our models and the best SemEval-2007 par-        NUS-PT      82.31    78.51    85.64    89.42    82.50
ticipating systems within the same type. Model I is    UPV-WSD     79.33    72.76    84.53    81.52    78.63∗
compared with UPV-WSD, and Model II is com-            TKB-UO      70.76    62.61    78.73    74.04    70.210
pared with TKB-UO. In addition, we also compare        MII–ref     78.16    70.39    79.56    81.25    76.64
our system with the most frequent sense baseline       MII+ref     80.05    70.73    82.04    82.21    78.140
which was not outperformed by any of the systems       MI+ref      79.96    75.47    83.98    86.06    79.99∗
of Type II and Type III in the SemEval-2007 task.      BLmf s      77.44    75.30    84.25    87.50    78.99∗
   Comparison on Type III is marked with 0 , while
comparison on Type II is marked with ∗. We find        Table 2: Model performance (F-score) on the
that Model II performs statistically significantly     coarse-grained dataset (context=sentence). Para-
better than the best participating system of the       phrases with/without reference synsets (+ref/-ref).
same type TKB-UO (p<<0.01, χ2 test). When
encoded with the prior knowledge of sense distri-           Context    Ate.     Pre.     Rec.     F1
bution, Model I outperforms by 1.36% the best               ±1w        91.67    75.05    68.80    71.79
Type II system UPV-WSD, although the differ-                ±5w        99.29    77.14    76.60    76.87
ence is not statistically significant. Furthermore,         ±10w       100      77.92    77.92    77.92
Model I also quantitatively outperforms the most            text       100      76.86    76.86    76.86
frequent sense baseline BLmf s , which, as men-             sent.      100      78.14    78.14    78.14
tioned above, was not beat by any participating
systems that do not use training data.                 Table 3: Model II performance on different con-
   We also find that our model works best for          text size. attempted rate (Ate.), precision (Pre.),
nouns. The unsupervised Type III model Model           recall (Rec.), F-score (F1).
II achieves better results than the most frequent
sense baseline on nouns, but not on other parts-
of-speech. This is in line with results obtained       ence synsets. As can be seen from the table, in-
by previous systems (Griffiths et al., 2005; Boyd-     cluding all reference synsets in sense paraphrases
Graber and Blei, 2008; Cai et al., 2007). While the    increases performance. Longer paraphrases con-
performance on verbs can be increased to outper-       tain more information, and they are statistically
form the most frequent sense baseline by including     more stable for inference.
the prior sense probability, the performance on ad-       We find that nouns get the greatest perfor-
jectives and adverbs remains below the most fre-       mance boost from including reference synsets, as
quent sense baseline. We think that there are three    they have the largest number of different types of
reasons for this: first, adjectives and adverbs have   synsets. We also find the ‘similar’ reference synset
fewer reference synsets for paraphrases compared       for adjectives to be very useful. Performance on
with nouns and verbs (see Table 1); second, adjec-     adjectives increases by 2.75% when including this
tives and adverbs tend to convey less key semantic     reference synset.
content in the document, so they are more difficult
to capture by the topic model; and third, adjectives      Context analysis In order to study how the con-
and adverbs are a small portion of the test set, so    text influences the performance, we experiment
their performances are statistically unstable. For     with Model II on different context sizes (see Ta-
example, if ‘already’ appears 10 times out of 20       ble 3). We find sentence context is the best size for
adverb instances, a system may get bad result on       this disambiguation task. Using a smaller context
adverbs only because of its failure to disambiguate    not only reduces the precision, but also reduces the
the word ‘already’.                                    recall rate, which is caused by the all-zero topic as-
   Paraphrase analysis Table 2 also shows the          signment by the topic model for documents only
effect of different ways of choosing sense para-       containing words that are not in the vocabulary.
phrases. MII+ref is the result of including the ref-   As a result, the model is unable to disambiguate.
erence synsets, while MII-ref excludes the refer-      The context based on the whole text (article) does
                                                       not perform well either, possibly because using the
instances.                                             full text folds in too much noisy information.


                                                   1144


               System      F-score                            System        Precl     Recl      Fl        Acc.
               RACAI       52.7 ±4.5                          Basemaj       -         -         -         78.25
               BLmf s      55.91±4.5                          co-graph      50.04     69.72     58.26     78.38
               MI+ref      56.99±4.5                          boot.         71.86     66.36     69.00     87.03
                                                              Model III     67.05     81.07     73.40     87.24
Table 4: Model performance (F-score) for the fine-
grained word sense disambiguation task.                   Table 5: Performance on the literal or nonliteral
                                                          sense disambiguation task on idioms. literal pre-
5.2   Fine-grained WSD                                    cision (Precl ), literal recall (Recl ), literal F-score
We saw in the previous section that our frame-            (Fl ), accuracy(Acc.).
work performs well on coarse-grained WSD. Fine-
grained WSD, however, is a more difficult task. To        system by Li and Sporleder (2009), although not
determine whether our framework is also able to           statistically significantly. This shows how a lim-
detect subtler sense distinctions, we tested Model I      ited amount of human knowledge (e.g., para-
on the English all-words subtask of SemEval-2007          phrases) can be added to an unsupervised system
Task-17 (see Table 4).                                    for a strong boost in performance ( Model III com-
   We find that Model I performs better than both         pared with the cohesion-graph and the bootstrap-
the best unsupervised system, RACAI (Ion and              ping approaches).
Tufiş, 2007) and the most frequent sense baseline           For obvious reasons, this approach is sensitive
(BLmf s ), although these differences are not sta-        to the quality of the paraphrases. The paraphrases
tistically significant due to the small size of the       chosen to characterise (aspects of) the meaning of
available test data (465).                                a sense should be non-ambiguous between the lit-
                                                          eral or idiomatic meaning. For instance, ‘fire’ is
5.3   Idiom Sense Disambiguation                          not a good choice for a paraphrase of the literal
In the previous section, we provided the results          reading of ‘play with fire’, since this word can
of applying our framework to coarse- and fine-            be interpreted literally as ‘fire’ or metaphorically
grained word sense disambiguation tasks. For              as ‘something dangerous’. The verb component
both tasks, our models outperform the state-of-           word ‘play’ is a better literal paraphrase.
the-art systems of the same type either quantita-            For the same reason, this approach works well
tively or statistically significantly. In this section,   for expressions where the literal and nonliteral
we apply Model III to another sense disambigua-           readings are well separated (i.e., occur in different
tion task, namely distinguishing literal and nonlit-      contexts), while the performance drops for expres-
eral senses of ambiguous expressions.                     sions whose literal and idiomatic readings can ap-
   WordNet has a relatively low coverage for id-          pear in a similar context. We test the performance
iomatic expressions. In order to represent non-           on individual idioms on the five most frequent id-
literal senses, we replace the paraphrases obtained       ioms in our corpus8 (see Table 6). We find that
automatically from WordNet by words selected              ‘drop the ball’ is a difficult case. The words ‘fault’,
manually from online idiom dictionaries (for the          ‘mistake’, ‘fail’ or ‘miss’ can be used as the nonlit-
nonliteral sense) and by linguistic introspection         eral paraphrases. However, it is also highly likely
(for the literal sense). We then compare the topic        that these words are used to describe a scenario in
distributions of literal and nonliteral senses.           a baseball game, in which ‘drop the ball’ is used
   As the paraphrases obtained from the idiom dic-        literally. In contrast, the performance on ‘rock the
tionary are very short, we treat the paraphrase           boat’ is much better, since the nonliteral reading
as a sequence of independent words instead of             of the phrases ‘break the norm’ or ‘cause trouble’
as a document and apply Model III (see Sec-               are less likely to be linked with the literal reading
tion 3). Table 5 shows the results of our pro-            ‘boat’. This may also be because ‘boat’ is not of-
posed model compared with state-of-the-art sys-           ten used metaphorically in the corpus.
tems. We find that the system significantly out-             As the topic distribution of nouns and verbs
performs the majority baseline (p<<0.01, χ2 test)         exhibit different properties, topic comparisons
and the cohesion-graph based approach proposed            across parts-of-speech do not make sense. We
by Sporleder and Li (2009) (p<<0.01, χ2 test).               8
                                                               We tested only on the most frequent idioms in order to
The system also outperforms the bootstrapping             avoid statistically unreliable observations.


                                                      1145


              Idiom            Acc.                    Acknowledgments
              drop the ball    75.86                   This work was funded by the DFG within the
              play with fire   91.17                   Cluster of Excellence “Multimodal Computing
              break the ice    87.43                   and Interaction”.
              rock the boat    95.82
              set in stone     89.39
                                                       References
    Table 6: Performance on individual idioms.         H. Anaya-Sánchez, A. Pons-Porrata, R. Berlanga-
                                                         Llavori. 2007. TKB-UO: using sense clustering for
make the topic distributions comparable by mak-          WSD. In SemEval ’07: Proceedings of the 4th Inter-
ing sure each type of paraphrase contains the same       national Workshop on Semantic Evaluations, 322–
sets of parts-of-speech. For instance, we do not         325.
permit combinations of literal paraphrases which       S. Bethard, V. T. Lai, J. H. Martin. 2009. Topic model
only consist of nouns and nonliteral paraphrases          analysis of metaphor frequency for psycholinguistic
which only consist of verbs.                              stimuli. In CALC ’09: Proceedings of the Workshop
                                                          on Computational Approaches to Linguistic Creativ-
6   Conclusion                                            ity, 9–16, Morristown, NJ, USA. Association for
                                                          Computational Linguistics.
We propose three models for sense disambigua-          J. Birke, A. Sarkar. 2006. A clustering approach
tion on words and multi-word expressions. The             for the nearly unsupervised recognition of nonliteral
basic idea of these models is to compare the topic        language. In Proceedings of EACL-06.
distribution of a target instance with the candidate   D. M. Blei, A. Y. Ng, M. I. Jordan. 2003. Latent
sense paraphrases and choose the most probable           dirichlet allocation. Journal of Machine Learning
                                                         Reseach, 3:993–1022.
one. While Model I and Model III model the
problem in a probabilistic way, Model II uses a        J. Boyd-Graber, D. Blei. 2007. PUTOP: turning
vector space model by comparing the cosine val-           predominant senses into a topic model for word
                                                          sense disambiguation. In Proceedings of the Fourth
ues of two topic vectors. Model II and Model III          International Workshop on Semantic Evaluations
are completely unsupervised, while Model I needs          (SemEval-2007), 277–281.
the prior sense distribution. Model I and Model        J. Boyd-Graber, D. Blei. 2008. Syntactic topic models.
II treat the sense paraphrases as documents, while        Computational Linguistics.
Model III treats the sense paraphrases as a collec-
                                                       J. Boyd-Graber, D. Blei, X. Zhu. 2007. A topic
tion of independent words.                                model for word sense disambiguation. In Proceed-
   We test the proposed models on three tasks. We         ings of the 2007 Joint Conference on Empirical
apply Model I and Model II to the WSD tasks due           Methods in Natural Language Processing and Com-
to the availability of more paraphrase information.       putational Natural Language Learning (EMNLP-
                                                          CoNLL), 1024–1033.
Model III is applied to the idiom detection task
since the paraphrases from the idiom dictionary        T. Briscoe, J. Carroll. 2006. Evaluating the accuracy
                                                          of an unlexicalized statistical parser on the PARC
are smaller. We find that all models outperform           DepBank. In Proceedings of the COLING/ACL on
comparable state-of-the-art systems either quanti-        Main conference poster sessions, 41–48.
tatively or statistically significantly.               S. Brody, M. Lapata. 2009. Bayesian word sense
   By testing our framework on three different            induction. In Proceedings of the 12th Conference
sense disambiguation tasks, we show that the              of the European Chapter of the ACL (EACL 2009),
framework can be used flexibly in different ap-           103–111.
plication tasks. The system also points out a          A. Budanitsky, G. Hirst. 2006. Evaluating wordnet-
promising way of solving the granularity problem         based measures of lexical semantic relatedness.
of word sense disambiguation, as new application         Computational Linguistics, 32(1):13–47.
tasks which need different sense granularities can     D. Buscaldi, P. Rosso. 2007. UPV-WSD: Combining
utilize this framework when new paraphrases of           different WSD methods by means of Fuzzy Borda
                                                         Voting. In SemEval ’07: Proceedings of the 4th
sense clusters are available. In addition, this sys-     International Workshop on Semantic Evaluations,
tem can also be used in a larger context such as         434–437.
figurative language identification (literal or figu-   J. Cai, W. S. Lee, Y. W. Teh. 2007. Improving word
rative) and sentiment detection (positive or nega-        sense disambiguation using topic features. In Pro-
tive).                                                    ceedings of the 2007 Joint Conference on Empirical


                                                   1146


  Methods in Natural Language Processing and Com-          R. Navigli, K. C. Litkowski, O. Hargraves. 2009.
  putational Natural Language Learning (EMNLP-               SemEval-2007 Task 07: Coarse-grained English all-
  CoNLL), 1015–1023.                                         words task. In Proceedings of the 4th International
                                                             Workshop on Semantic Evaluation (SemEval-2007).
Y. S. Chan, H. T. Ng, Z. Zhong. 2007. NUS-PT: ex-
   ploiting parallel texts for word sense disambiguation   R. Navigli. 2006. Meaningful clustering of senses
   in the English all-words tasks. In SemEval ’07: Pro-      helps boost word sense disambiguation perfor-
   ceedings of the 4th International Workshop on Se-         mance. In Proceedings of the 44th Annual Meeting
   mantic Evaluations, 253–256.                              of the Association for Computational Liguistics joint
                                                             with the 21st International Conference on Computa-
S. Geman, D. Geman. 1987. Stochastic relaxation,             tional Liguistics (COLING-ACL 2006).
   Gibbs distributions, and the Bayesian restoration
   of images. In Readings in computer vision: is-          M. Porter. October 2001.   Snowball: A lan-
   sues, problems, principles, and paradigms, 564–           guage for stemming algorithms.     http:
   584. Morgan Kaufmann Publishers Inc., San Fran-           //snowball.tartarus.org/texts/
   cisco, CA, USA.                                           introduction.html.
                                                           S. S. Pradhan, E. Loper, D. Dligach, M. Palmer. 2009.
T. L. Griffiths, M. Steyvers. 2004. Finding scientific
                                                              SemEval-2007 Task 07: Coarse-grained English all-
   topics. Proceedings of the National Academy of Sci-
                                                              words task. In Proceedings of the 4th International
   ences, 101(Suppl. 1):5228–5235.
                                                              Workshop on Semantic Evaluation (SemEval-2007).
T. L. Griffiths, M. Steyvers, D. M. Blei, J. B. Tenen-     F. Song, W. B. Croft. 1999. A general language model
   baum. 2005. Integrating topics and syntax. In In           for information retrieval (poster abstract). In Re-
   Advances in Neural Information Processing Systems          search and Development in Information Retrieval,
   17, 537–544. MIT Press.                                    279–280.
T. Hofmann. 1999. Probabilistic latent semantic in-        P. Sorg, P. Cimiano. 2008. Cross-lingual information
   dexing. In SIGIR ’99: Proceedings of the 22nd an-          retrieval with explicit semantic analysis. In In Work-
   nual international ACM SIGIR conference on Re-             ing Notes for the CLEF 2008 Workshop.
   search and development in information retrieval,
   50–57.                                                  C. Sporleder, L. Li. 2009. Unsupervised recognition of
                                                              literal and non-literal use of idiomatic expressions.
R. Ion, D. Tufiş. 2007. Racai: meaning affinity mod-         In Proceedings of EACL-09.
   els. In SemEval ’07: Proceedings of the 4th Inter-      Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, E. Y. Chang.
   national Workshop on Semantic Evaluations, 282–            2009. Plda: Parallel latent dirichlet allocation for
   287, Morristown, NJ, USA. Association for Compu-           large-scale applications. In Proc. of 5th Interna-
   tational Linguistics.                                      tional Conference on Algorithmic Aspects in Infor-
G. Katz, E. Giesbrecht. 2006. Automatic identifi-             mation and Management. Software available at
  cation of non-compositional multi-word expressions          http://code.google.com/p/plda.
  using latent semantic analysis. In Proceedings of the
  ACL/COLING-06 Workshop on Multiword Expres-
  sions: Identifying and Exploiting Underlying Prop-
  erties.
B. B. Klebanov, E. Beigman, D. Diermeier. 2009. Dis-
   course topics and metaphors. In CALC ’09: Pro-
   ceedings of the Workshop on Computational Ap-
   proaches to Linguistic Creativity, 1–8, Morristown,
   NJ, USA. Association for Computational Linguis-
   tics.
L. Li, C. Sporleder. 2009. Contextual idiom detection
   without labelled data. In Proceedings of EMNLP-
   09.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004.
  Finding predominant word senses in untagged text.
  In Proceedings of the 42nd Meeting of the Associa-
  tion for Computational Linguistics (ACL’04), Main
  Volume, 279–286.
D. McCarthy. 2009. Word sense disambiguation:
  An overview. Language and Linguistics Compass,
  3(2):537–558.
G. A. Miller. 1995. WordNet: a lexical database for
  english. Commun. ACM, 38(11):39–41.


                                                       1147

      PCFGs, Topic Models, Adaptor Grammars and Learning Topical
             Collocations and the Structure of Proper Names

                                         Mark Johnson
                                     Department of Computing
                                       Macquarie University
                                 mjohnson@science.mq.edu.au


                     Abstract                                so Bayesian inference for PCFGs can be used to
                                                             learn LDA topic models as well. The importance
    This paper establishes a connection be-                  of this observation is primarily theoretical, as cur-
    tween two apparently very different kinds                rent Bayesian inference algorithms for PCFGs are
    of probabilistic models. Latent Dirich-                  less efficient than those for LDA inference. How-
    let Allocation (LDA) models are used                     ever, once this link is established it suggests a vari-
    as “topic models” to produce a low-                      ety of extensions to the LDA topic models, two of
    dimensional representation of documents,                 which we explore in this paper. The first involves
    while Probabilistic Context-Free Gram-                   extending the LDA topic model so that it generates
    mars (PCFGs) define distributions over                   collocations (sequences of words) rather than indi-
    trees. The paper begins by showing that                  vidual words. The second applies this idea to the
    LDA topic models can be viewed as a                      problem of automatically learning internal struc-
    special kind of PCFG, so Bayesian in-                    ture of proper names (NPs), which is useful for
    ference for PCFGs can be used to infer                   definite NP coreference models and other applica-
    Topic Models as well. Adaptor Grammars                   tions.
    (AGs) are a hierarchical, non-parameteric                   The rest of this paper is structured as follows.
    Bayesian extension of PCFGs. Exploit-                    The next section reviews Latent Dirichlet Alloca-
    ing the close relationship between LDA                   tion (LDA) topic models, and the following sec-
    and PCFGs just described, we propose                     tion reviews Probabilistic Context-Free Grammars
    two novel probabilistic models that com-                 (PCFGs). Section 4 shows how an LDA topic
    bine insights from LDA and AG models.                    model can be expressed as a PCFG, which pro-
    The first replaces the unigram component                 vides the fundamental connection between LDA
    of LDA topic models with multi-word se-                  and PCFGs that we exploit in the rest of the
    quences or collocations generated by an                  paper, and shows how it can be used to define
    AG. The second extension builds on the                   a “sticky topic” version of LDA. The follow-
    first one to learn aspects of the internal               ing section reviews Adaptor Grammars (AGs), a
    structure of proper names.                               non-parametric extension of PCFGs introduced by
                                                             Johnson et al. (2007b). Section 6 exploits the con-
1   Introduction                                             nection between LDA and PCFGs to propose an
Over the last few years there has been consider-             AG-based topic model that extends LDA by defin-
able interest in Bayesian inference for complex hi-          ing distributions over collocations rather than indi-
erarchical models both in machine learning and in            vidual words, and section 7 applies this extension
computational linguistics. This paper establishes            to the problem of finding the structure of proper
a theoretical connection between two very differ-            names.
ent kinds of probabilistic models: Probabilistic
                                                             2   Latent Dirichlet Allocation Models
Context-Free Grammars (PCFGs) and a class of
models known as Latent Dirichlet Allocation (Blei            Latent Dirichlet Allocation (LDA) was introduced
et al., 2003; Griffiths and Steyvers, 2004) models           as an explicit probabilistic counterpart to La-
that have been used for a variety of tasks in ma-            tent Semantic Indexing (LSI) (Blei et al., 2003).
chine learning. Specifically, we show that an LDA            Like LSI, LDA is intended to produce a low-
model can be expressed as a certain kind of PCFG,            dimensional characterisation or summary of a doc-


                                                       1148
      Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                              e.g., by Bayesian inference. (The adaptor gram-
                       β         φ     `                      mar software we used in the experiments de-
                                                              scribed below automatically does this kind of
        α      θ       Z         W n                          hyper-parameter inference).
                                     m                           The inference task is to find the topic probabil-
                                                              ity vector θ j of each document Dj given the words
Figure 1: A graphical model “plate” representa-               wj,k of the documents; in general this also requires
tion of an LDA topic model. Here ` is the number              inferring the topic to word distributions φ and the
of topics, m is the number of documents and n is              topic assigned to each word zj,k . Blei et al. (2003)
the number of words per document.                             describe a Variational Bayes inference algorithm
                                                              for LDA models based on a mean-field approx-
                                                              imation, while Griffiths and Steyvers (2004) de-
ument in a collection of documents for informa-               scribe an Markov Chain Monte Carlo inference al-
tion retrieval purposes. Both LSI and LDA do                  gorithm based on Gibbs sampling; both are quite
this by mapping documents to points in a rela-                effective in practice.
tively low-dimensional real-valued vector space;
distance in this space is intended to correspond to           3   Probabilistic Context-Free Grammars
document similarity.                                          Context-Free Grammars are a simple model of hi-
   An LDA model is an explicit generative proba-              erarchical structure often used to describe natu-
bilistic model of a collection of documents. We               ral language syntax. A Context-Free Grammar
describe the “smoothed” LDA model here (see                   (CFG) is a quadruple (N, W, R, S) where N and
page 1006 of Blei et al. (2003)) as it corresponds            W are disjoint finite sets of nonterminal and ter-
precisely to the Bayesian PCFGs described in sec-             minal symbols respectively, R is a finite set of pro-
tion 4. It generates a collection of documents by             ductions or rules of the form A → β where A ∈ N
first generating multinomials φi over the vocab-              and β ∈ (N ∪W )? , and S ∈ N is the start symbol.
ulary V for each topic i ∈ 1, . . . , `, where ` is              In what follows, it will be useful to interpret a
the number of topics and φi,w is the probability              CFG as generating sets of finite, labelled, ordered
of generating word w in topic i. Then it gen-                 trees TA for each X ∈ N ∪ W . Informally, TX
erates each document Dj , j = 1, . . . , m in turn            consists of all trees t rooted in X where for each
by first generating a multinomial θ j over topics,            local tree (B, β) in t (i.e., where B is a parent’s
where θj,i is the probability of topic i appearing            label and β is the sequence of labels of its imme-
in document j. (θ j serves as the low-dimensional             diate children) there is a rule B → β ∈ R.
representation of document Dj ). Finally it gener-               Formally, the sets TX are the smallest sets of
ates each of the n words of document Dj by first              trees that satisfy the following equations.
selecting a topic z for the word according to θ j ,              If X ∈ W (i.e., if X is a terminal) then TX =
and then drawing a word from φz . Dirichlet priors            {X}, i.e., TX consists of a single tree, which in
with parameters β and α respectively are placed               turn only consists of a single node labelled X.
on the φi and the θ j in order to avoid the zeros                If X ∈ N (i.e., if X is a nonterminal) then
that can arise from maximum likelihood estima-                                  [
tion (i.e., sparse data problems).                               TX =                    T REEX (TB1 , . . . , TBn )
   The LDA generative model can be compactly                             X→B1 ...Bn ∈RX
expressed as follows, where “∼” should be read
as “is distributed according to”.                             where RA = {A → β : A → β ∈ R} for each
                                                              A ∈ N , and
  φi    ∼   Dir(β)    i = 1, . . . , `
  θj    ∼   Dir(α)    j = 1, . . . , m                              T REEX (TB1 , . . . , TBn )
                                                                         (                                     )
 zj,k   ∼   θj        j = 1, . . . , m; k = 1, . . . , n                       X              ti ∈ TBi ,
                                                                      =       P   P :
 wj,k   ∼   φzj,k     j = 1, . . . , m; k = 1, . . . , n                                      i = 1, . . . , n
                                                                             
                                                                            t1 . . . tn

  In inference, the parameters α and β of the                 That is, T REEX (TB1 , . . . , TBn ) consists of the set
Dirichlet priors are either fixed (i.e., chosen by            of trees with whose root node is labelled X and
the model designer), or else themselves inferred,             whose ith child is a member of TBi .


                                                           1149


   The set of trees generated by the CFG is TS ,                 4    LDA topic models as PCFGs
where S is the start symbol, and the set of strings
generated by the CFG is the set of yields (i.e., ter-            This section explains how to construct a PCFG
minal strings) of the trees in TS .                              that generates the same distribution over a collec-
   A Probabilistic Context-Free Grammar (PCFG)                   tion of documents as an LDA model, and where
is a pair consisting of a CFG and set of multino-                Bayesian inference for the PCFG’s rule proba-
mial probability vectors θ X indexed by nontermi-                bilities yields the corresponding distributions as
nals X ∈ N , where θ X is a distribution over the                Bayesian inference of the corresponding LDA
rules RX (i.e., the rules expanding X). Informally,              models. (There are several different ways of en-
θX→β is the probability of X expanding to β using                coding LDA models as PCFGs; the one presented
the rule X → β ∈ RX . More formally, a PCFG                      here is not the most succinct — it is possible to
associates each X ∈ N ∪ W with a distribution                    collapse the Doc and Doc0 nonterminals — but it
GX over the trees TX as follows.                                 has the advantage that the LDA distributions map
   If X ∈ W (i.e., if X is a terminal) then GX                   straight-forwardly onto PCFG nonterminals).
is the distribution that puts probability 1 on the                  The terminals W of the CFG consist of the vo-
single-node tree labelled X.                                     cabulary V of the LDA model plus a set of special
   If X ∈ N (i.e., if X is a nonterminal) then:                  “document identifier” terminals “ j ” for each doc-
         X                                                       ument j ∈ 1, . . . , m, where m is the number of
  GX =        θX→B1 ...Bn TDX (GB1 , . . . , GBn ) (1)           documents. In the PCFG encoding strings from
      X→B1 ...Bn ∈RX                                             document j are prefixed with “ j ”; this indicates
                                                                 to the grammar which document the string comes
where:
                                                                 from. The nonterminals consist of the start symbol
                                               n
                                                                 Sentence, Docj and Doc0j for each j ∈ 1, . . . , m,
                                       !
                            X                  Y
TDA (G1 , . . . , Gn )    P   P          =         Gi (ti ).   and Topici for each i ∈ 1, . . . , `, where ` is the
                         t1 . . . tn           i=1
                                                                 number of topics in the LDA model.
That is, TDA (G1 , . . . , Gn ) is a distribution over              The rules of the CFG are all instances of the
TA where each subtree ti is generated indepen-                   following schemata:
dently from Gi . These equations have solutions
(i.e., the PCFG is said to be “consistent”) when                    Sentence → Doc0j       j ∈ 1, . . . , m
the rule probabilities θ A obey certain conditions;                 Doc0j → j              j ∈ 1, . . . , m
see e.g., Wetherell (1980) for details.                             Doc0j → Doc0j Docj     j ∈ 1, . . . , m
   The PCFG generates the distribution over trees                   Docj → Topici          i ∈ 1, . . . , `; j ∈ 1, . . . , m
GS , where S is the start symbol. The distribu-                     Topici → w             i ∈ 1, . . . , `; w ∈ V
tion over the strings it generates is obtained by
marginalising over the trees.                                       Figure 2 depicts a tree generated by such a
   In a Bayesian PCFG one puts Dirichlet priors                  CFG. The relationship between the LDA model
Dir(αX ) on each of the multinomial rule proba-                  and the PCFG can be understood by studying the
bility vectors θ X for each nonterminal X ∈ N .                  trees generated by the CFG. In these trees the left-
This means that there is one Dirichlet parameter                 branching spine of nodes labelled Doc0j propagate
αX→β for each rule X → β ∈ R in the CFG.                         the document identifier throughout the whole tree.
   In the “unsupervised” inference problem for a                 The nodes labelled Topici indicate the topics as-
PCFG one is given a CFG, parameters αX for the                   signed to particular words, and the local trees ex-
Dirichlet priors over the rule probabilities, and a              panding Docj to Topici (one per word in the docu-
corpus of strings. The task is to infer the cor-                 ment) indicate the distribution of topics in the doc-
responding posterior distribution over rule prob-                ument.
abilities θ X . Recently Bayesian inference algo-                   The corresponding Bayesian PCFG associates
rithms for PCFGs have been described. Kurihara                   probabilities with each of the rules in the CFG.
and Sato (2006) describe a Variational Bayes algo-               The probabilities θ Topici associated with the rules
rithm for inferring PCFGs using a mean-field ap-                 expanding the Topici nonterminals indicate how
proximation, while Johnson et al. (2007a) describe               words are distributed across topics; the θ Topici
a Markov Chain Monte Carlo algorithm based on                    probabilities correspond exactly to to the φi prob-
Gibbs sampling.                                                  abilities in the LDA model. The probabilities


                                                             1150


                               Sentence                 ily a good way of estimating LDA topic models.
                                                        Current Bayesian PCFG inference algorithms re-
                                Doc3'
                                                        quire time proportional to the cube of the length of
                        Doc3'             Doc3          the longest string in the training corpus, and since
                                                        these strings correspond to entire documents in our
                Doc3'           Doc3 Topic7             embedding, blindly applying a Bayesian PCFG in-
                                                        ference algorithm is likely to be impractical.
           Doc3'        Doc3    Topic4 faster
                                                           A little reflection shows that the embedding still
        Doc3' Doc3 Topic4 compute                       holds if the strings in the PCFG corpus correspond
                                                        to sentences or even smaller units of the original
         _3 Topic4 circuits                             document collection, so a single document would
             shallow                                    be mapped to multiple strings in the PCFG infer-
                                                        ence task. In this way the cubic time complex-
                                                        ity of PCFG inference can be mitigated. Also, the
Figure 2: A tree generated by the CFG encoding          trees generated by these CFGs have a very spe-
an LDA topic model. The prefix “ 3” indicates           cialized left-branching structure, and it is straight-
that this string belongs to document 3. The tree        forward to modify the general-purpose CFG infer-
also indicates the assignment of words to topics.       ence procedures to avoid the cubic time complex-
                                                        ity for such grammars: thus it may be practical to
                                                        estimate topic models via grammatical inference.
θ Docj associated with rules expanding Docj spec-
                                                           However, we believe that the primary value of
ify the distribution of topics in document j; they
                                                        the embedding of LDA topic models into Bayesian
correspond exactly to the probabilities θ j of the
                                                        PCFGs is theoretical: it suggests a number of
LDA model. (The PCFG also specifies several
                                                        novel extensions of both topic models and gram-
other distributions that are suppressed in the LDA
                                                        mars that may be worth exploring. Our claim here
model. For example θ Sentence specifies the distri-
                                                        is not that these models are the best algorithms for
bution of documents in the corpus. However, it is
                                                        performing these tasks, but that the relationship
easy to see that these distributions do not influence
                                                        we described between LDA models and PCFGs
the topic distributions; indeed, the expansions of
                                                        suggests a variety of interesting novel models.
the Sentence nonterminal are completely deter-
                                                           We end this section with a simple example of
mined by the document distribution in the corpus,
                                                        such a modification to LDA. Inspired by the stan-
and are not affected by θ Sentence ).
                                                        dard embedding of HMMs into PCFGs, we pro-
   A Bayesian PCFG places Dirichlet priors
                                                        pose a “sticky topic” variant of LDA in which ad-
Dir(αA ) on the corresponding rule probabilities
                                                        jacent words are more likely to be assigned the
θ A for each A ∈ N . In the PCFG encoding an
                                                        same topic. Such an LDA extension is easy to
LDA model, the αTopici parameters correspond
                                                        describe as a PCFG (see Fox et al. (2008) for a
exactly to the β parameters of the LDA model, and
                                                        similar model presented as an extended HMM).
the αDocj parameters correspond to the α param-
                                                        The nonterminals Sentence and Topici for i =
eters of the LDA model.
                                                        1, . . . , ` have the same interpretation as before, but
   As suggested above, each document Dj in the          we introduce new nonterminals Docj,i that indi-
LDA model is mapped to a string in the corpus           cate we have just generated a nonterminal in doc-
used to train the corresponding PCFG by prefix-         ument j belonging to topic i. Given a collection of
ing it with a document identifier “ j ”. Given this     m documents and ` topics, the rule schemata are
training data, the posterior distribution over rule     as follows:
probabilities θDocj → Topici is the same as the pos-
terior distribution over topics given documents θj,i       Sentence → Docj,i            i ∈ 1, . . . , `;
in the original LDA model.                                                              j ∈ 1, . . . , m
                                                           Docj,1 → j                   j ∈ 1, . . . , m
   As we will see below, this connection between
                                                           Docj,i → Docj,i0 Topici      i, i0 ∈ 1, . . . , `;
PCFGs and LDA topic models suggests a num-
                                                                                        j ∈ 1, . . . , m
ber of interesting variants of both PCFGs and
                                                           Topici → w                   i ∈ 1, . . . , `; w ∈ V
topic models. Note that we are not suggesting
that Bayesian inference for PCFGs is necessar-             A sample parse generated by a “sticky topic”


                                                    1151


                                Sentence                sic categories”, say NP, VP, etc., and a set of rules
                                                        that use these basic categories, say S → NP VP.
                                  Doc3,7
                                                        The inference task is to learn a set of refined cate-
                         Doc3,4            Topic7       gories and rules (e.g., S7 → NP2 VP5 ) as well as
                                                        their probabilities; this approach can therefore be
                Doc3,4            Topic4 faster         viewed as a Bayesian version of the “split-merge”
                                                        approach to grammar induction (Petrov and Klein,
          Doc3,4        Topic4 compute
                                                        2007).
       Doc3,1 Topic4 circuits                              In the second approach, which we adopt here,
                                                        we regard the set of rules R as potentially un-
         _3   shallow                                   bounded, and try to learn the rules required to
                                                        describe a training corpus as well as their prob-
Figure 3: A tree generated by the “sticky topic”        abilities. Adaptor grammars are an example of
CFG. Here a nonterminal Doc3, 7 indicates we            this approach (Johnson et al., 2007b), where en-
have just generated a word in document 3 belong-        tire subtrees generated by a “base grammar” can
ing to topic 7.                                         be viewed as distinct rules (in that we learn a sep-
                                                        arate probability for each subtree). The inference
                                                        task is non-parametric if there are an unbounded
CFG is shown in Figure 3. The probabilities of
                                                        number of such subtrees.
the rules Docj,i → Docj,i0 Topici in this PCFG
                                                           We review the adaptor grammar generative pro-
encode the probability of shifting from topic i to
                                                        cess below; for an informal introduction see John-
topic i0 (this PCFG can be viewed as generating
                                                        son (2008) and for details of the adaptor grammar
the string from right to left).
                                                        inference procedure see Johnson and Goldwater
   We can use non-uniform sparse Dirichlet pri-
                                                        (2009).
ors on the probabilities of these rules to encour-
                                                           An adaptor grammar (N, W, R, S, θ, A, C) con-
age “topic stickiness”. Specifically, by setting
                                                        sists of a PCFG (N, W, R, S, θ) in which a sub-
the Dirichlet parameters for the “topic shift” rules
                                                        set A ⊆ N of the nonterminals are adapted, and
Docj,i0 → Docj,i Topici where i0 6= i much lower
                                                        where each adapted nonterminal X ∈ A has an
than the parameters for the “topic preservation”
                                                        associated adaptor CX . An adaptor CX for X is a
rules Docj,i → Docj,i Topici , Bayesian inference
                                                        function that maps a distribution over trees TX to
will be biased to find distributions in which adja-
                                                        a distribution over distributions over TX (we give
cent words will tend to have the same topic.
                                                        examples of adaptors below).
5   Adaptor Grammars                                       Just as for a PCFG, an adaptor grammar de-
                                                        fines distributions GX over trees TX for each X ∈
Non-parametric Bayesian inference, where the in-        N ∪ W . If X ∈ W or X 6∈ A then GX is defined
ference task involves learning not just the values      just as for a PCFG above, i.e., using (1). How-
of a finite vector of parameters but which parame-      ever, if X ∈ A then GX is defined in terms of an
ters are relevant, has been the focus of intense re-    additional distribution HX as follows:
search in machine learning recently. In the topic-
                                                           GX ∼ CX (HX )
modelling community this has lead to work on                    X
Dirichlet Processes and Chinese Restaurant Pro-            HX =   θX→Y1 ...Ym TDX (GY1 , . . . , GYm )
cesses, which can be used to estimate the number              X→Y1 ...Ym ∈RX
of topics as well as their distribution across docu-    That is, the distribution GX associated with an
ments (Teh et al., 2006).                               adapted nonterminal X ∈ A is a sample from
   There are two obvious non-parametric exten-          adapting (i.e., applying CX to) its “ordinary”
sions to PCFGs. In the first we regard the set          PCFG distribution HX . In general adaptors are
of nonterminals N as potentially unbounded, and         chosen for the specific properties they have. For
try to learn the set of nonterminals required to de-    example, with the adaptors used here GX typically
scribe the training corpus. This approach goes un-      concentrates mass on a smaller subset of the trees
der the name of the “infinite HMM” or “infinite         TX than HX does.
PCFG” (Beal et al., 2002; Liang et al., 2007; Liang        Just as with the PCFG, an adaptor grammar gen-
et al., 2009). Informally, we are given a set of “ba-   erates the distribution over trees GS , where S ∈ N


                                                    1152


is the start symbol. However, while GS in a PCFG           tables). If aX = 0 then the PYP is equivalent to
is a fixed distribution (given the rule probabili-         a CRP with αX = bX , while if aX = 1 then the
ties θ), in an adaptor grammar the distribution GS         PYP generates samples from HX .
is itself a random variable (because each GX for              Informally, the CRP has a strong preference
X ∈ A is random), i.e., an adaptor grammar gen-            to regenerate trees that have been generated fre-
erates a distribution over distributions over trees        quently before, leading to a “rich-get-richer” dy-
TS . However, the posterior joint distribution Pr(t)       namics. The PYP can mitigate this somewhat by
of a sequence t = (t1 , . . . , tn ) of trees in TS is     reducing the effective count of previously gener-
well-defined:                                              ated trees and redistributing that probability mass
                    Z                                      to new trees generated from HX . As Goldwa-
       Pr(t) =         GS (t1 ) . . . GS (tn ) dG          ter et al. (2006) explain, Bayesian inference for
                                                           HX given samples from GX is effectively per-
where the integral is over all of the random distri-       formed from types if aX = 0 and from tokens
butions GX , X ∈ A. The adaptors we use in this            if aX = 1, so varying aX smoothly interpolates
paper are Dirichlet Processes or two-parameter             between type-based and token-based inference.
Poisson-Dirichlet Processes, for which it is pos-             Adaptor grammars have previously been used
sible to compute this integral. One way to do this         primarily to study grammatical inference in the
uses the predictive distributions:                         context of language acquisition. The word seg-
                                                           mentation task involves segmenting a corpus
Pr(tn+1 | t, HX )
      Z                                                    of unsegmented phonemic utterance representa-
  ∝      GX (t1 ) . . . GX (tn+1 )CX (GX | HX ) dGX        tions into words (Elman, 1990; Bernstein-Ratner,
                                                           1987). For example, the phoneme string corre-
where t = (t1 , . . . , tn ) and each ti ∈ TX . The pre-   sponding to “you want to see the book” (with its
dictive distribution for the Dirichlet Process is the      correct segmentation indicated) is as follows:
(labeled) Chinese Restaurant Process (CRP), and
the predictive distribution for the two-parameter              y Mu Nw Ma Mn Mt Nt Mu Ns Mi ND M6 Nb MU Mk
Poisson-Dirichlet process is the (labeled) Pitman-
Yor Process (PYP).                                         We can represent any possible segmentation of any
   In the context of adaptor grammars, the CRP is:         possible sentence as a tree generated by the fol-
                                                           lowing unigram adaptor grammar.
   CRP(t | t, αX , HX ) ∝ nt (t) + αX HX (t)
                                                                 Sentence → Word
where nt (t) is the number of times t appears in t
                                                                 Sentence → Word Sentence
and αX > 0 is a user-settable “concentration pa-
                                                                 Word → Phonemes
rameter”. In order to generate the next tree tn+1
                                                                 Phonemes → Phoneme
a CRP either reuses a tree t with probability pro-
                                                                 Phonemes → Phoneme Phonemes
portional to number of times t has been previously
generated, or else it “backs off” to the “base distri-
bution” HX and generates a fresh tree t with prob-  The trees generated by this adaptor grammar are
ability proportional to αX HX (t).                  the same as the trees generated by the CFG rules.
  The PYP is a generalization of the CRP:           For example, the following skeletal parse in which
                                                    all but the Word nonterminals are suppressed (the
PYP(t | t, aX , bX , HX )                           others  are deterministically inferrable) shows the
   ∝ max(0, nt (t) − mt aX ) + (maX + bX )HX (t) parse that corresponds to the correct segmentation
                                                    of the string above.
Here aX ∈ [0, 1] and bX > 0 are user-settable            (Word y u) (Word w a n t) (Word t u)
parameters, and mt is the number of times the PYP        (Word s i) (Word d 6) (Word b u k)
           P t in t from the base distribution HX ,
has generated                                       Because the Word nonterminal is adapted (indi-
and m = t∈TX mt is the number of times any          cated here by underlining) the adaptor grammar
tree has been generated from HX . (In the Chinese   learns the probability of the entire Word subtrees
Restaurant metaphor, mt is the number of tables     (e.g., the probability that b u k is a Word); see
labeled with t, and m is the number of occupied     Johnson (2008) for further details.


                                                       1153


6       Topic models with collocations                                 Topic1 → associative memory
                                                                       Topic1 → hamming distance
Here we combine ideas from the unigram word
                                                                       Topic1 → randomly chosen
segmentation adaptor grammar above and the
                                                                       Topic1 → standard deviation
PCFG encoding of LDA topic models to present
                                                                       Topic3 → action potentials
a novel topic model that learns topical colloca-
                                                                       Topic3 → membrane potential
tions. (For a non-grammar-based approach to this
                                                                       Topic3 → primary visual cortex
problem see Wang et al. (2007)). Specifically, we
                                                                       Topic3 → visual system
take the PCFG encoding of the LDA topic model
                                                                       Topic10 → nervous system
described above, but modify it so that the Topici
                                                                       Topic10 → action potential
nodes generate sequences of words rather than sin-
                                                                       Topic10 → ocular dominance
gle words. Then we adapt each of the Topici non-
                                                                       Topic10 → visual field
terminals, which means that we learn the probabil-
ity of each of the sequences of words it can expand                  The following are skeletal sample parses, where
to.                                                               we have elided all but the adapted nonterminals
                                                                  (i.e., all we show are the Topic nonterminals, since
         Sentence → Docj                j ∈ 1, . . . , m          the other structure can be inferred deterministi-
         Docj → j                       j ∈ 1, . . . , m          cally). Note that because Griffiths et al. (2007)
         Docj → Docj Topici             i ∈ 1, . . . , `;         segmented the NIPS abstracts at punctuation sym-
                                        j ∈ 1, . . . , m          bols, the training corpus contains more than one
         Topici → Words                 i ∈ 1, . . . , `          string from each abstract.
                                                                         3 (Topic5 polynomial size)
         Words → Word
                                                                           (Topic15 threshold circuits)
         Words → Words Word
         Word → w           w∈V
                                                                       4 (Topic11 studied)
   In order to demonstrate that this model                               (Topic19 pattern recognition algorithms)
works, we implemented this using the publically-
available adaptor grammar inference software,1                         4 (Topic2 feedforward neural network)
and ran it on the NIPS corpus (composed of pub-                          (Topic1 implementation)
lished NIPS abstracts), which has previously been
used for studying collocation-based topic models                       5 (Topic11 single)
(Griffiths et al., 2007). Because there is no gen-                       (Topic10 ocular dominance stripe)
erally accepted evaluation for collocation-finding,                      (Topic12 low) (Topic3 ocularity)
we merely present some of the sample analyses                            (Topic12 drift rate)
found by our adaptor grammar. We ran our adap-
                                                                  7   Finding the structure of proper names
tor grammar with ` = 20 topics (i.e., 20 distinct
Topici nonterminals). Adaptor grammar inference                   Grammars offer structural and positional sensitiv-
on this corpus is actually relatively efficient be-               ity that is not exploited in the basic LDA topic
cause the corpus provided by Griffiths et al. (2007)              models. Here we explore the potential for us-
is already segmented by punctuation, so the termi-                ing Bayesian inference for learning linear order-
nal strings are generally rather short. Rather than               ing constraints that hold between elements within
set the Dirichlet parameters by hand, we placed                   proper names.
vague priors on them and estimated them as de-                       The Penn WSJ treebank is a widely used re-
scribed in Johnson and Goldwater (2009).                          source within computational linguistics (Marcus
   The following are some examples of colloca-                    et al., 1993), but one of its weaknesses is that
tions found by our adaptor grammar:                               it does not indicate any structure internal to base
                                                                  noun phrases (i.e., it presents “flat” analyses of the
         Topic0 → cost function
                                                                  pre-head NP elements). For many applications it
         Topic0 → fixed point
                                                                  would be extremely useful to have a more elab-
         Topic0 → gradient descent
                                                                  orated analysis of this kind of NP structure. For
         Topic0 → learning rates
                                                                  example, in an NP coreference application, if we
    1
        http://web.science.mq.edu.au/ ˜mjohnson/Software.htm      could determine that Bill and Hillary are both first


                                                               1154


names then we could infer that Bill Clinton and        is intended as a “catch-all” expansion to provide
Hillary Clinton are likely to refer to distinct in-    analyses for proper names that don’t fit either of
dividuals. On the other hand, because Mr in Mr         the first two expansions).
Clinton is not a first name, it is possible that Mr       We extracted all of the proper names (i.e.,
Clinton and Bill Clinton refer to the same individ-    phrases of category NNP and NNPS) in the Penn
ual (Elsner et al., 2009).                             WSJ treebank and used them as the training cor-
   Here we present an adaptor grammar based on         pora for the adaptor grammar just described. The
the insights of the PCFG encoding of LDA topic         adaptor grammar inference procedure found skele-
models that learns some of the structure of proper     tal sample parses such as the following:
names. The key idea is that elements in proper
names typically appear in a fixed order; we expect          (A0 barrett) (A3 smith)
honorifics to appear before first names, which ap-          (A0 albert) (A2 j.) (A3 smith) (A4 jr.)
pear before middle names, which in turn appear              (A0 robert) (A2 b.) (A3 van dover)
before surnames, etc. Similarly, many company               (B0 aim) (B1 prime rate) (B2 plus) (B5
names end in fixed phrases such as Inc. Here                fund) (B6 inc.)
we think of first names as a kind of topic, albeit          (B0 balfour) (B1 maclaine) (B5 interna-
one with a restricted positional location. One of           tional) (B6 ltd.)
the challenges is that some of these structural ele-        (B0 american express) (B1 information
ments can be filled by multiword expressions; e.g.,         services) (B6 co)
de Groot can be a surname. We deal with this by             (U abc) (U sports)
permitting multi-word collocations to fill the cor-         (U sports illustrated)
responding positions, and use the adaptor gram-             (U sports unlimited)
mar machinery to learn these collocations.
                                                       While a full evaluation will have to await further
   Inspired by the grammar presented in Elsner         study, in general it seems to distinguish person
et al. (2009), our adaptor grammar is as follows,      names from company names reasonably reliably,
where adapted nonterminals are indicated by un-        and it seems to have discovered that person names
derlining as before.                                   consist of a first name (A0), a middle name or ini-
                                                       tial (A2), a surname (A3) and an optional suffix
           NP → (A0) (A1) . . . (A6)
                                                       (A4). Similarly, it seems to have uncovered that
           NP → (B0) (B1) . . . (B6)
                                                       company names typically end in a phrase such as
           NP → Unordered+
                                                       inc, ltd or co.
           A0 → Word+
             ...                                       8   Conclusion
           A6 → Word+
           B0 → Word+                                  This paper establishes a connection between two
             ...                                       very different kinds of probabilistic models; LDA
           B6 → Word+                                  models of the kind used for topic modelling, and
           Unordered → Word+                           PCFGs, which are a standard model of hierarchi-
                                                       cal structure in language. The embedding we pre-
   In this grammar parentheses indicate optional-      sented shows how to express an LDA model as a
ity, and the Kleene plus indicates iteration (these    PCFG, and has the property that Bayesian infer-
were manually expanded into ordinary CFG rules         ence of the parameters of that PCFG produces an
in our experiments). The grammar provides three        equivalent model to that produced by Bayesian in-
different expansions for proper names. The first       ference of the LDA model’s parameters.
expansion says that a proper name can consist of           The primary value of this embedding is theoret-
some subset of the six different collocation classes   ical rather than practical; we are not advocating
A0 through A6 in that order, while the second ex-      the use of PCFG estimation procedures to infer
pansion says that a proper name can consist of         LDA models. Instead, we claim that the embed-
some subset of the collocation classes B0 through      ding suggests novel extensions to both the LDA
B6, again in that order. Finally, the third expan-     topic models and PCFG-style grammars. We jus-
sion says that a proper name can consist of an ar-     tified this claim by presenting several hybrid mod-
bitrary sequence of “unordered” collocations (this     els that combine aspects of both topic models and


                                                   1155


grammars. We don’t claim that these are neces-             In Andrew McCallum and Sam Roweis, editors,
sarily the best models for performing any particu-         Proceedings of the 25th Annual International Con-
                                                           ference on Machine Learning (ICML 2008), pages
lar tasks; rather, we present them as examples of
                                                           312–319. Omnipress.
models inspired by a combination of PCFGs and
LDA topic models. We showed how the LDA                 Sharon Goldwater, Tom Griffiths, and Mark John-
to PCFG embedding suggested a “sticky topic”              son. 2006. Interpolating between types and tokens
                                                          by estimating power-law generators. In Y. Weiss,
model extension to LDA. We then discussed adap-           B. Schölkopf, and J. Platt, editors, Advances in Neu-
tor grammars, and inspired by the LDA topic mod-          ral Information Processing Systems 18, pages 459–
els, presented a novel topic model whose prim-            466, Cambridge, MA. MIT Press.
itive elements are multi-word collocations rather       Thomas L. Griffiths and Mark Steyvers. 2004. Find-
than words. We concluded with an adaptor gram-            ing scientific topics. Proceedings of the National
mar that learns aspects of the internal structure of      Academy of Sciences, 101:52285235.
proper names.                                           Thomas L. Griffiths, Mark Steyvers, and Joshua B.
                                                          Tenenbaum. 2007. Topics in semantic representa-
Acknowledgments                                           tion. Psychological Review, 114(2):211244.
This research was funded by US NSF awards               Mark Johnson and Sharon Goldwater. 2009. Im-
0544127 and 0631667, as well as by a start-up            proving nonparameteric Bayesian inference: exper-
award from Macquarie University. I’d like to             iments on unsupervised word segmentation with
                                                         adaptor grammars. In Proceedings of Human Lan-
thank the organisers and audience at the Topic
                                                         guage Technologies: The 2009 Annual Conference
Modeling workshop at NIPS 2009, my former col-           of the North American Chapter of the Associa-
leagues at Brown University (especially Eugene           tion for Computational Linguistics, pages 317–325,
Charniak, Micha Elsner, Sharon Goldwater, Tom            Boulder, Colorado, June. Association for Computa-
Griffiths and Erik Sudderth), my new colleagues          tional Linguistics.
at Macquarie University and the ACL reviewers           Mark Johnson, Thomas Griffiths, and Sharon Gold-
for their excellent suggestions and comments on          water. 2007a. Bayesian inference for PCFGs via
this work. Naturally all errors remain my own.           Markov chain Monte Carlo. In Human Language
                                                         Technologies 2007: The Conference of the North
                                                         American Chapter of the Association for Computa-
                                                         tional Linguistics; Proceedings of the Main Confer-
References                                               ence, pages 139–146, Rochester, New York, April.
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002.      Association for Computational Linguistics.
  The infinite Hidden Markov Model. In T. Dietterich,
                                                        Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
  S. Becker, and Z. Ghahramani, editors, Advances in
                                                         water. 2007b. Adaptor Grammars: A framework for
  Neural Information Processing Systems, volume 14,
                                                         specifying compositional nonparametric Bayesian
  pages 577–584. The MIT Press.
                                                         models. In B. Schölkopf, J. Platt, and T. Hoffman,
N. Bernstein-Ratner. 1987. The phonology of parent-      editors, Advances in Neural Information Processing
  child speech. In K. Nelson and A. van Kleeck,          Systems 19, pages 641–648. MIT Press, Cambridge,
  editors, Children’s Language, volume 6. Erlbaum,       MA.
  Hillsdale, NJ.
                                                        Mark Johnson. 2008. Using adaptor grammars to iden-
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.      tifying synergies in the unsupervised acquisition of
  2003. Latent Dirichlet allocation. Journal of Ma-      linguistic structure. In Proceedings of the 46th An-
  chine Learning Research, 3:993–1022.                   nual Meeting of the Association of Computational
                                                         Linguistics, Columbus, Ohio. Association for Com-
Jeffrey Elman. 1990. Finding structure in time. Cog-     putational Linguistics.
   nitive Science, 14:197–211.
                                                        Kenichi Kurihara and Taisuke Sato. 2006. Variational
Micha Elsner, Eugene Charniak, and Mark Johnson.          Bayesian grammar induction for natural language.
  2009. Structured generative models for unsuper-         In 8th International Colloquium on Grammatical In-
  vised named-entity clustering. In Proceedings of        ference.
  Human Language Technologies: The 2009 Annual
  Conference of the North American Chapter of the       Percy Liang, Slav Petrov, Michael Jordan, and Dan
  Association for Computational Linguistics, pages        Klein. 2007. The infinite PCFG using hierarchi-
  164–172, Boulder, Colorado, June. Association for       cal Dirichlet processes. In Proceedings of the 2007
  Computational Linguistics.                              Joint Conference on Empirical Methods in Natural
                                                          Language Processing and Computational Natural
E. Fox, E. Sudderth, M. Jordan, and A. Willsky. 2008.     Language Learning (EMNLP-CoNLL), pages 688–
   An HDP-HMM for systems with state persistence.         697.


                                                    1156


Percy Liang, Michael Jordan, and Dan Klein. 2009.
  Probabilistic grammars and hierarchical Dirichlet
  processes. In The Oxford Handbook of Applied
  Bayesian Analysis. Oxford University Press.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
  Marcinkiewicz. 1993. Building a large annotated
  corpus of English: The Penn Treebank. Computa-
  tional Linguistics, 19(2):313–330.
Slav Petrov and Dan Klein. 2007. Improved infer-
   ence for unlexicalized parsing. In Human Language
   Technologies 2007: The Conference of the North
   American Chapter of the Association for Computa-
   tional Linguistics; Proceedings of the Main Confer-
   ence, pages 404–411, Rochester, New York. Associ-
   ation for Computational Linguistics.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
   erarchical Dirichlet processes. Journal of the Amer-
   ican Statistical Association, 101:1566–1581.
Xuerui Wang, Andrew McCallum, and Xing Wei.
  2007. Topical n-grams: Phrase and topic discovery,
  with an application to information retrieval. In Pro-
  ceedings of the 7th IEEE International Conference
  on Data Mining (ICDM), pages 697–702.
C.S. Wetherell. 1980. Probabilistic languages: A re-
  view and some open questions. Computing Surveys,
  12:361–379.




                                                      1157

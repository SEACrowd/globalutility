     A Cognitive Cost Model of Annotations Based on Eye-Tracking Data

    Katrin Tomanek                   Udo Hahn                    Steffen Lohmann                  Jürgen Ziegler
Language & Information          Language & Information        Dept. of Computer Science &     Dept. of Computer Science &
Engineering (JULIE) Lab         Engineering (JULIE) Lab          Applied Cognitive Science      Applied Cognitive Science
     Universität Jena               Universität Jena         Universität Duisburg-Essen     Universität Duisburg-Essen
      Jena, Germany                  Jena, Germany                  Duisburg, Germany              Duisburg, Germany




                         Abstract                              structure phenomena, Since the NLP community
                                                               is further extending their work into these more and
     We report on an experiment to track com-                  more sophisticated semantic and pragmatic analyt-
     plex decision points in linguistic meta-                  ics, there seems to be no end in sight for increas-
     data annotation where the decision behav-                 ingly complex and diverse annotation tasks.
     ior of annotators is observed with an eye-                   Yet, producing annotations is pretty expensive.
     tracking device. As experimental con-                     So the question comes up, how we can rationally
     ditions we investigate different forms of                 manage these investments so that annotation cam-
     textual context and linguistic complexity                 paigns are economically doable without loss in an-
     classes relative to syntax and semantics.                 notation quality. The economics of annotations are
     Our data renders evidence that annotation                 at the core of Active Learning (AL) where those
     performance depends on the semantic and                   linguistic samples are focused on in the entire doc-
     syntactic complexity of the decision points               ument collection, which are estimated as being
     and, more interestingly, indicates that full-             most informative to learn an effective classifica-
     scale context is mostly negligible – with                 tion model (Cohn et al., 1996). This intentional
     the exception of semantic high-complexity                 selection bias stands in stark contrast to prevailing
     cases. We then induce from this obser-                    sampling approaches where annotation examples
     vational data a cognitively grounded cost                 are randomly chosen.
     model of linguistic meta-data annotations                    When different approaches to AL are compared
     and compare it with existing non-cognitive                with each other, or with standard random sam-
     models. Our data reveals that the cogni-                  pling, in terms of annotation efficiency, up until
     tively founded model explains annotation                  now, the AL community assumed uniform annota-
     costs (expressed in annotation time) more                 tion costs for each linguistic unit, e.g. words. This
     adequately than non-cognitive ones.                       claim, however, has been shown to be invalid in
                                                               several studies (Hachey et al., 2005; Settles et al.,
1    Introduction
                                                               2008; Tomanek and Hahn, 2010). If uniformity
Today’s NLP systems, in particular those rely-                 does not hold and, hence, the number of annotated
ing on supervised ML approaches, are meta-data                 units does not indicate the true annotation efforts
greedy. Accordingly, in the past years, we have                required for a specific sample, empirically more
witnessed a massive quantitative growth of anno-               adequate cost models are needed.
tated corpora. They differ in terms of the nat-                   Building predictive models for annotation costs
ural languages and domains being covered, the                  has only been addressed in few studies for now
types of linguistic meta-data being solicited, and             (Ringger et al., 2008; Settles et al., 2008; Arora
the text genres being served. We have seen large-              et al., 2009). The proposed models are based
scale efforts in syntactic and semantic annotations            on easy-to-determine, yet not so explanatory vari-
in the past related to POS tagging and parsing,                ables (such as the number of words to be anno-
on the one hand, and named entities and rela-                  tated), indicating that accurate models of anno-
tions (propositions), on the other hand. More re-              tation costs remain a desideratum. We here, al-
cently, we are dealing with even more challeng-                ternatively, consider different classes of syntac-
ing issues such as subjective language, a large                tic and semantic complexity that might affect the
variety of co-reference and (e.g., RST-style) text             cognitive load during the annotation process, with


                                                          1158
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1158–1167,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


the overall goal to find additional and empirically      models outperform simpler ones relative to cost
more adequate variables for cost modeling.               prediction using annotation time as a cost mea-
   The complexity of linguistic utterances can be        sure. Based on this finding, we suggest that cog-
judged either by structural or by behavioral crite-      nitive criteria are helpful for uncovering the real
ria. Structural complexity emerges, e.g., from the       costs of corpus annotation.
static topology of phrase structure trees and pro-
cedural graph traversals exploiting the topology         2 Experimental Design
of parse trees (see Szmrecsányi (2004) or Cheung
                                                         In our study, we applied, for the first time ever to
and Kemper (1992) for a survey of metrics of this
                                                         the best of our knowledge, eye-tracking to study
type). However, structural complexity criteria do
                                                         the cognitive processes underlying the annotation
not translate directly into empirically justified cost
                                                         of linguistic meta-data, named entities in particu-
measures and thus have to be taken with care.
                                                         lar. In this task, a human annotator has to decide
   The behavioral approach accounts for this prob-       for each word whether or not it belongs to one of
lem as it renders observational data of the an-          the entity types of interest.
notators’ eye movements. The technical vehicle              We used the English part of the M UC 7 corpus
to gather such data are eye-trackers which have          (Linguistic Data Consortium, 2001) for our study.
already been used in psycholinguistics (Rayner,          It contains New York Times articles from 1996 re-
1998). Eye-trackers were able to reveal, e.g.,           porting on plane crashes. These articles come al-
how subjects deal with ambiguities (Frazier and          ready annotated with three types of named entities
Rayner, 1987; Rayner et al., 2006; Traxler and           considered important in the newspaper domain,
Frazier, 2008) or with sentences which require           viz. “persons”, “locations”, and “organizations”.
re-analysis, so-called garden path sentences (Alt-          Annotation of these entity types in newspaper
mann et al., 2007; Sturt, 2007).                         articles is admittedly fairly easy. We chose this
   The rationale behind the use of eye-tracking de-      rather simple setting because the participants in
vices for the observation of annotation behavior is      the experiment had no previous experience with
that the length of gaze durations and behavioral         document annotation and no serious linguistic
patterns underlying gaze movements are consid-           background. Moreover, the limited number of
ered to be indicative of the hardness of the lin-        entity types reduced the amount of participants’
guistic analysis and the expenditures for the search     training prior to the actual experiment, and posi-
of clarifying linguistic evidence (anchor words) to      tively affected the design and handling of the ex-
resolve hard decision tasks such as phrase attach-       perimental apparatus (see below).
ments or word sense disambiguation. Gaze dura-              We triggered the annotation processes by giving
tion and search time are then taken as empirical         our participants specific annotation examples. An
correlates of linguistic complexity and, hence, un-      example consists of a text document having one
cover the real costs. We therefore consider eye-         single annotation phrase highlighted which then
tracking as a promising means to get a better un-        had to be semantically annotated with respect to
derstanding of the nature of the linguistic annota-      named entity mentions. The annotation task was
tion processes with the ultimate goal of identifying     defined such that the correct entity type had to be
predictive factors for annotation cost models.           assigned to each word in the annotation phrase. If
   In this paper, we first describe an empirical         a word belongs to none of the three entity types a
study where we observed the annotators’ reading          fourth class called “no entity” had to be assigned.
behavior while annotating a corpus. Section 2               The phrases highlighted for annotation were
deals with the design of the study, Section 3 dis-       complex noun phrases (CNPs), each a sequence of
cusses its results. In Section 4 we then focus on        words where a noun (or an equivalent nominal ex-
the implications this study has on building cost         pression) constitutes the syntactic head and thus
models and compare a simple cost model mainly            dominates dependent words such as determin-
relying on word and character counts and addi-           ers, adjectives, or other nouns or nominal expres-
tional simple descriptive characteristics with one       sions (including noun phrases and prepositional
that can be derived from experimental data as pro-       phrases). CNPs with even more elaborate inter-
vided from eye-tracking. We conclude with ex-            nal syntactic structures, such as coordinations, ap-
periments which reveal that cognitively grounded         positions, or relative clauses, were isolated from


                                                     1159


their syntactic host structure and the intervening              Two experimental groups were formed to study
linguistic material containing these structures was          different contexts. In the document context con-
deleted to simplify overly long sentences. We also           dition the whole newspaper article was shown as
discarded all CNPs that did not contain at least             annotation example, while in the sentence context
one entity-critical word, i.e., one which might be a         condition only the sentence containing the annota-
named entity according to its orthographic appear-           tion phrase was presented. The participants3 were
ance (e.g., starting with an upper-case letter). It          randomly assigned to one of these groups. We de-
should be noted that such orthographic signals are           cided for this between-subjects design to avoid any
by no means a sufficient condition for the presence          irritation of the participants caused by constantly
of a named entity mention within a CNP.                      changing contexts. Accordingly, the participants
   The choice of CNPs as stimulus phrases is mo-             were assigned to one of the experimental groups
tivated by the fact that named entities are usually          and corresponding context condition already in the
fully encoded by this kind of linguistic structure.          second training phase that took place shortly be-
The chosen stimulus – an annotation example with             fore the experiment started (see below).
one phrase highlighted for annotation – allows for
                                                             2.2 Hypotheses and Dependent Variables
an exact localization of the cognitive processes
and annotation actions performed relative to that            We tested the following two hypotheses:
specific phrase.                                             Hypothesis H1: Annotators perform differently
                                                                in the two context conditions.
2.1 Independent Variables
                                                                    H1 is based on the linguistically plausible
We defined two measures for the complexity of                       assumption that annotators are expected to
the annotation examples: The syntactic complex-                     make heavy use of the surrounding context
ity was given by the number of nodes in the con-                    because such context could be helpful for the
stituent parse tree which are dominated by the an-                  correct disambiguation of entity classes. Ac-
notation phrase (Szmrecsányi, 2004).1 According                    cordingly, lacking context, an annotator is ex-
to a threshold on the number of nodes in such a                     pected to annotate worse than under the con-
parse tree, we classified CNPs as having either                     dition of full context. However, the availabil-
high or low syntactic complexity.                                   ity of (too much) context might overload and
   The semantic complexity of an annotation ex-                     distract annotators, with a presumably nega-
ample is based on the inverse document frequency                    tive effect on annotation performance.
df of the words in the annotation phrase according
to a reference corpus.2 We calculated the seman-             Hypothesis H2: The complexity of the annota-
tic complexity score of an annotation phrase as                 tion phrases determines the annotation per-
         1
max i df (w    , where wi is the i-th word of the anno-         formance.
            i)
tation phrase. Again, we empirically determined a                   The assumption is that high syntactic or se-
threshold classifying annotation phrases as having                  mantic complexity significantly lowers the
either high or low semantic complexity. Addition-                   annotation performance.
ally, this automatically generated classification            In order to test these hypotheses we collected data
was manually checked and, if necessary, revised              for the following dependent variables: (a) the an-
by two annotation experts. For instance, if an an-           notation accuracy – we identified erroneous enti-
notation phrase contained a strong trigger (e.g., a          ties by comparison with the original gold annota-
social role or job title, as with “spokeswoman” in           tions in the M UC 7 corpus, (b) the time needed per
the annotation phrase “spokeswoman Arlene”), it              annotation example, and (c) the distribution and
was classified as a low-semantic-complexity item             duration of the participants’ eye gazes.
even though it might have been assigned a high
                                                                3
inverse document frequency (due to the infrequent                 20 subjects (12 female) with an average age of 24 years
                                                             (mean = 24, standard deviation (SD) = 2.8) and normal or
word “Arlene”).                                              corrected-to-normal vision capabilities took part in the study.
                                                             All participants were students with a computing-related study
   1
      Constituency parse structure was obtained from the     background, with good to very good English language skills
O PEN NLP parser (http://opennlp.sourceforge.                (mean = 7.9, SD = 1.2, on a ten-point scale with 1 = “poor”
net/) trained on PennTreeBank data.                          and 10 = “excellent”, self-assessed), but without any prior
    2
      We chose the English part of the Reuters RCV2 corpus   experience in annotation and without previous exposure to
as the reference corpus for our experiments.                 linguistic training.


                                                         1160


2.3 Stimulus Material                                  ger coordination (three keys for the positive en-
 According to the above definition of complex-         tity classes, one for the negative “no entity” class,
 ity, we automatically preselected annotation ex-      and one to confirm the annotation). Pre-tests had
 amples characterized by either a low or a high de-    shown that the participants could easily issue the
 gree of semantic and syntactic complexity. After      annotations without looking down at the keyboard.
 manual fine-tuning of the example set assuring an        We recorded the participant’s eye movements
 even distribution of entity types and syntactic cor-  on  a Tobii T60 eye-tracking device which is in-
 rectness of the automatically derived annotation      visibly embedded in a 17” TFT monitor and com-
 phrases, we finally selected 80 annotation exam-      paratively tolerant to head movements. The partic-
 ples for the experiment. These were divided into      ipants were seated in a comfortable position with
 four subsets of 20 examples each falling into one     their head in a distance of 60-70 cm from the mon-
 of the following complexity classes:                  itor. Screen resolution was set to 1280 x 1024 px
                                                       and the annotation examples were presented in the
sem-syn: low semantic/low syntactic complexity         middle of the screen in a font size of 16 px and a
SEM-syn: high semantic/low syntactic complexity        line spacing of 5 px. The presentation area had no
sem-SYN: low semantic/high syntactic complexity fixed height and varied depending on the context
SEM-SYN: high semantic/high syntactic complexity condition and length of the newspaper article. The
                                                       text was always vertically centered on the screen.
 2.4 Experimental Apparatus and Procedure
                                                          All participants were familiarized with the
 The annotation examples were presented in a           annotation task and the guidelines in a pre-
 custom-built tool and its user interface was kept     experimental workshop where they practiced an-
 as simple as possible not to distract the eye move-   notations on various exercise examples (about 60
 ments of the participants. It merely contained one    minutes). During the next two days, one after the
 frame showing the text of the annotation example,     other participated in the actual experiment which
 with the annotation phrase being highlighted. A       took between 15 and 30 minutes, including cali-
 blank screen was shown after each annotation ex-      bration of the eye-tracking device. Another 20-30
 ample to reset the eyes and to allow a break, if      minutes of training time directly preceded the ex-
 needed. The time the blank screen was shown was       periment. After the experiment, participants were
 not counted as annotation time. The 80 annotation     interviewed and asked to fill out a questionnaire.
 examples were presented to all participants in the    Overall, the experiment took about two hours for
 same randomized order, with a balanced distribu-      each participant for which they were financially
 tion of the complexity classes. A variation of the    compensated. Participants were instructed to fo-
 order was hardly possible for technical and ana-      cus more on annotation accuracy than on annota-
 lytical reasons but is not considered critical due to tion time as we wanted to avoid random guess-
 extensive, pre-experimental training (see below).     ing. Accordingly, as an extra incentive, we re-
 The limitation on 80 annotation examples reduces      warded the three participants with the highest an-
 the chances of errors due to fatigue or lack of at-   notation accuracy with cinema vouchers. None of
 tention that can be observed in long-lasting anno-    the participants reported serious difficulties with
 tation activities.                                    the newspaper articles or annotation tool and all
    Five introductory examples (not considered in      understood the annotation task very well.
 the final evaluation) were given to get the subjects
 used to the experimental environment. All anno-       3 Results
 tation examples were chosen in a way that they
 completely fitted on the screen (i.e., text length    We used a mixed-design analysis of variance
 was limited) to avoid the need for scrolling (and     (ANOVA) model to test the hypotheses, with the
 eye distraction). The position of the CNP within      context condition as between-subjects factor and
 the respective context was randomly distributed,      the two complexity classes as within-subject fac-
 excluding the first and last sentence.                tors.
    The participants used a standard keyboard to as-
 sign the entity types for each word of the annota-    3.1 Testing Context Conditions
 tion example. All but 5 keys were removed from        To test hypothesis H1 we compared the number
 the keyboard to avoid extra eye movements for fin-    of annotation errors on entity-critical words made


                                                   1161


                                                                    above     before     anno phrase      after    below
 percentage of participants looking at a sub-area                   35%        32%          100%          34%       16%
 average number of fixations per sub-area                            2.2                    14.1                     1.3

          Table 1: Distribution of annotators’ attention among sub-areas per annotation example.


by the annotators in the two contextual conditions               participants looked in the textual context above the
(complete document vs. sentence). Surprisingly,                  annotation phrase embedding sentence, and even
on the total of 174 entity-critical words within                 less perceived the context below (16%). The sen-
the 80 annotation examples, we found exactly the                 tence parts before and after the annotation phrase
same mean value of 30.8 errors per participant in                were, on the average, visited by one third (32%
both conditions. There were also no significant                  and 34%, respectively) of the participants. The
differences in the average time needed to annotate               uneven distribution of the annotators’ attention be-
an example in both conditions (means of 9.2 and                  comes even more apparent in a comparison of the
8.6 seconds, respectively, with F (1, 18) = 0.116,               total number of fixations on the different text parts:
p = 0.74).4 These results seem to suggest that it                14 out of an average of 18 fixations per example
makes no difference (neither for annotation accu-                were directed at the annotation phrase and the sur-
racy nor for time) whether or not annotators are                 rounding sentence, the text context above the an-
shown textual context beyond the sentence that                   notation chunk received only 2.2 fixations on the
contains the annotation phrase.                                  average and the text context below only 1.3.
   To further investigate this finding we analyzed                  Thus, the eye-tracking data indicates that the
eye-tracking data of the participants gathered for               textual context is not as important as might have
the document context condition. We divided the                   been expected for quick and accurate annotation.
whole text area into five sub-areas as schemat-                  This result can be explained by the fact that par-
ically shown in Figure 1. We then determined                     ticipants of the document-context condition used
the average proportion of participants that directed             the context whenever they thought it might help,
their gaze at least once at these sub-areas. We con-             whereas participants of the sentence-context con-
sidered all fixations with a minimum duration of                 dition spent more time thinking about a correct an-
100 ms, using a fixation radius (i.e., the smallest              swer, overall with the same result.
distance that separates fixations) of 30 px and ex-
cluded the first second (mainly used for orientation             3.2 Testing Complexity Classes
and identification of the annotation phrase).                    To test hypothesis H2 we also compared the av-
                                                                 erage annotation time and the number of errors
                                                                 on entity-critical words for the complexity subsets
                                                                 (see Table 2). The ANOVA results show highly
                                                                 significant differences for both annotation time
                                                                 and errors.5 A pairwise comparison of all sub-
                                                                 sets in both conditions with a t-test showed non-
                                                                 significant results only between the SEM-syn and
                                                                 syn-SEM subsets.6
                                                                    Thus, the empirical data generally supports hy-
Figure 1: Schematic visualization of the sub-areas               pothesis H2 in that the annotation performance
of an annotation example.                                        seems to correlate with the complexity of the an-
                                                                 notation phrase, on the average.
   Table 1 reveals that on average only 35% of the                   5
                                                                       Annotation time results: F (1, 18) = 25, p < 0.01 for
                                                                 the semantic complexity and F (1, 18) = 76.5, p < 0.01
    4                                                            for the syntactic complexity; Annotation complexity results:
      In general, we observed a high variance in the number of
errors and time values between the subjects. While, e.g., the    F (1, 18) = 48.7, p < 0.01 for the semantic complexity and
fastest participant handled an example in 3.6 seconds on the     F (1, 18) = 184, p < 0.01 for the syntactic complexity.
                                                                     6
average, the slowest one needed 18.9 seconds; concerning               t(9) = 0.27, p = 0.79 for the annotation time in the
the annotation errors on the 174 entity-critical words, these    document context condition, and t(9) = 1.97, p = 0.08 for
ranged between 21 and 46 errors.                                 the annotation errors in the sentence context condition.


                                                             1162


                  experimental      complexity       e.-c.        time             errors
                    condition           class       words      mean SD      mean     SD      rate
                                      sem-syn         36       4.0s 2.0      2.7     2.1    .075
                    document         SEM-syn          25       9.2s 6.7      5.1     1.4    .204
                    condition        sem-SYN          51       9.6s 4.0      9.1     2.9    .178
                                    SEM-SYN           62       14.2s 9.5    13.9     4.5    .224
                                      sem-syn         36       3.9s 1.3      1.1     1.4    .031
                    sentence         SEM-syn          25       7.5s 2.8      6.2     1.9    .248
                    condition        sem-SYN          51       9.6s 2.8      9.0     3.9    .176
                                    SEM-SYN           62       13.5s 5.0    14.5     3.4    .234

Table 2: Average performance values for the 10 subjects of each experimental condition and 20 anno-
tation examples of each complexity class: number of entity-critical words, mean annotation time and
standard deviations (SD), mean annotation errors, standard deviations, and error rates (number of errors
divided by number of entity-critical words).


3.3 Context and Complexity
We also examined whether the need for inspect-
ing the context increases with the complexity of
the annotation phrase. Therefore, we analyzed the
eye-tracking data in terms of the average num-
ber of fixations on the annotation phrase and on
its embedding contexts for each complexity class
(see Table 3). The values illustrate that while the
number of fixations on the annotation phrase rises
generally with both the semantic and the syntactic               phrase      antecedent
complexity, the number of fixations on the context
rises only with semantic complexity. The num-                Figure 2: Annotation example with annotation
ber of fixations on the context is nearly the same           phrase and the antecedent for “Roselawn” in the
for the two subsets with low semantic complexity             text (left), and gaze plot of one participant show-
(sem-syn and sem-SYN, with 1.0 and 1.5), while               ing a scanning-for-coreference behavior (right).
it is significantly higher for the two subsets with             This finding is also qualitatively supported by
high semantic complexity (5.6 and 5.0), indepen-             the gaze plots we generated from the eye-tracking
dent of the syntactic complexity.7                           data. Figure 2 shows a gaze plot for one partici-
  complexity      fix. on phrase     fix. on context         pant that illustrates a scanning-for-coreference be-
      class       mean      SD       mean      SD            havior we observed for several annotation phrases
                                                             with high semantic complexity. In this case, words
    sem-syn          4.9    4.0        1.0     2.9
                                                             were searched in the upper context, which accord-
   SEM-syn           8.1    5.4        5.6     5.6
                                                             ing to their orthographic signals might refer to a
   sem-SYN         18.1     7.7        1.5     2.0
                                                             named entity but which could not completely be
  SEM-SYN          25.4     9.3        5.0     4.1
                                                             resolved only relying on the information given by
Table 3: Average number of fixations on the anno-            the annotation phrase itself and its embedding sen-
tation phrase and context for the document condi-            tence. This is the case for “Roselawn” in the an-
tion and 20 annotation examples of each complex-             notation phrase “Roselawn accident”. The con-
ity class.                                                   text reveals that Roselawn, which also occurs in
                                                             the first sentence, is a location. A similar proce-
  These results suggest that the need for context            dure is performed for acronyms and abbreviations
mainly depends on the semantic complexity of the             which cannot be resolved from the immediate lo-
annotation phrase, while it is less influenced by its        cal context – searches mainly visit the upper con-
syntactic complexity.                                        text. As indicated by the gaze movements, it also
    7
      ANOVA result of F (1, 19) = 19.7, p < 0.01 and sig-    became apparent that texts were rather scanned for
nificant differences also in all pairwise comparisons.       hints instead of being deeply read.


                                                        1163


4    Cognitively Grounded Cost Modeling                           tance values as another metric for syntactic com-
                                                                  plexity. Lin (1996) has already shown that human
We now discuss whether the findings on dependent
                                                                  performance on sentence processing tasks can be
variables from our eye-tracking study are fruitful
                                                                  predicted using such a measure. Our third syn-
for actually modeling annotation costs. There-
                                                                  tactic complexity measure is based on the prob-
fore, we learn a linear regression model with time
                                                                  ability of part-of-speech (POS) 2-grams. Given
(an operationalization of annotation costs) as the
                                                                  a POS 2-gram model, which we learned from
dependent variable. We compare our ‘cognitive’
                                                                  the automatically POS-tagged M UC 7 corpus, the
model against a baseline model which relies on
                                                                  complexity of an annotation phrase is defined by
some simple formal text features only, and test                   Pn
                                                                     i=2 P (POSi |POSi−1 ) where POSi refers to the
whether the newly introduced features help predict
                                                                  POS-tag of the i-th word of the annotation phrase.
annotation costs more accurately.
                                                                  A similar measure has been used by Roark et al.
4.1 Features                                                      (2007) who claim that complex syntactic struc-
                                                                  tures correlate with infrequent or surprising com-
The features for the baseline model, character- and               binations of POS tags.
word-based, are similar to the ones used by Ring-
                                                                     As far as the quantification of semantic com-
ger et al. (2008) and Settles et al. (2008).8 Our
                                                                  plexity is concerned, we use (a) the inverse docu-
cognitive model, however, makes additional use
                                                                  ment frequency df (wi ) of each word wi (cf. Sec-
of features based on linguistic complexity, and in-
                                                                  tion 2.1), and a measure based on the semantic
cludes syntactic and semantic criteria related to the
                                                                  ambiguity of each word, i.e., the number of mean-
annotation phrases. These features were inspired
                                                                  ings contained in W ORD N ET,9 within an annota-
by the insights provided by our eye-tracking ex-
                                                                  tion phrase. We consider the maximum ambigu-
periments. All features are designed such that they
                                                                  ity of the words within the annotation phrase as
can automatically be derived from unlabeled data,
                                                                  the overall ambiguity of the respective annotation
a necessary condition for such features to be prac-
                                                                  phrase. This measure is based on the assumption
tically applicable.
                                                                  that annotation phrases with higher semantic am-
   To account for our findings that syntactic and                 biguity are harder to annotate than low-ambiguity
semantic complexity correlates with annotation                    ones. Finally, we add the Flesch-Kincaid Read-
performance, we added three features based on                     ability Score (Klare, 1963), a well-known metric
syntactic, and two based on semantic complex-                     for estimating the comprehensibility and reading
ity measures. We decided for the use of multiple                  complexity of texts.
measures because there is no single agreed-upon
                                                                     As already indicated, some of the hardness of
metric for either syntactic or semantic complex-
                                                                  annotations is due to tracking co-references and
ity. This decision is further motivated by find-
                                                                  abbreviations. Both often cannot be resolved lo-
ings which reveal that different measures are often
                                                                  cally so that annotators need to consult the con-
complementary to each other so that their combi-
                                                                  text of an annotation chunk (cf. Section 3.3).
nation better approximates the inherent degrees of
                                                                  Thus, we also added features providing informa-
complexity (Roark et al., 2007).
                                                                  tion whether the annotation phrases contain entity-
   As for syntactic complexity, we use two mea-
                                                                  critical words which may denote the referent of an
sures based on structural complexity including (a)
                                                                  antecedent of an anaphoric relation. In the same
the number of nodes of a constituency parse tree
                                                                  vein, we checked whether an annotation phrase
which are dominated by the annotation phrase
                                                                  contains expressions which can function as an ab-
(cf. Section 2.1), and (b) given the dependency
                                                                  breviation by virtue of their orthographical appear-
graph of the sentence embedding the annotation
                                                                  ance, e.g., consist of at least two upper-case letters.
phrase, we consider the distance between words
                                                                     Since our participants were sometimes scanning
for each dependency link within the annotation
                                                                  for entity-critical words, we also added features
phrase and consider the maximum over such dis-
                                                                  providing information on the number of entity-
    8
      In preliminary experiments our set of basic features com-   critical words within the annotation phrase. Ta-
prised additional features providing information on the usage     ble 4 enumerates all feature classes and single fea-
of stop words in the annotation phrase and on the number
of paragraphs, sentences, and words in the respective annota-     tures used for determining our cost model.
tion example. However, since we found these features did not
                                                                     9
have any significant impact on the model, we removed them.               http://wordnet.princeton.edu/


                                                              1164


Feature Group        # Features   Feature Description
characters (basic)       6        number of characters and words per annotation phrase; test whether
                                  words in a phrase start with capital letters, consist of capital letters only,
                                  have alphanumeric characters, or are punctuation symbols
words                    2        number of entity-critical words and percentage of entity-critical words
                                  in the annotation phrase
complexity               6        syntactic complexity: number of dominated nodes, POS n-gram proba-
                                  bility, maximum dependency distance;
                                  semantic complexity: inverse document frequency, max. ambiguity;
                                  general linguistic complexity: Flesch-Kincaid Readability Score
semantics                3        test whether entity-critical word in annotation phrase is used in docu-
                                  ment (preceding or following current phrase); test whether phrase con-
                                  tains an abbreviation

                                    Table 4: Features for cost modeling.


 4.2 Evaluation                                             For both annotators, the baseline model is sig-
                                                         nificantly outperformed in terms of R2 by our
 To test how well annotation costs can be mod-           ‘cognitive’ model (p < 0.05). Considering the
 eled by the features described above, we used the       features that were inspired from the eye-tracking
 M UC 7T corpus, a re-annotation of the M UC 7 cor-      study, R2 is increased from 0.4695 to 0.6263 on
 pus (Tomanek and Hahn, 2010). M UC 7T has time          the timing data of annotator A, and from 0.464 to
 tags attached to the sentences and CNPs. These          0.6185 on the data of annotator B. These numbers
 time tags indicate the time it took to annotate the     clearly demonstrate that annotation costs are more
 respective phrase for named entity mentions of the      adequately modelled by the additional features we
 types person, location, and organization. We here       identified through our eye-tracking study.
 made use of the time tags of the 15,203 CNPs in
                                                            Our ‘cognitive’ model now consists of 21 co-
 M UC 7T . M UC 7T has been annotated by two an-
                                                         efficients. We tested for the significance of this
 notators (henceforth called A and B) and so we
                                                         model’s regression terms. For annotator A we
 evaluated the cost models for both annotators. We
                                                         found all coefficients to be significant with respect
 learned a simple linear regression model with the
                                                         to the model (p < 0.05), for annotator B all coeffi-
 annotation time as dependent variable and the fea-
                                                         cients except one were significant. Figure 6 shows
 tures described above as independent variables.
                                                         the coefficients of annotator A’s ‘cognitive’ model
 The baseline model only includes the basic feature
                                                         along with the standard errors and t-values.
 set, whereas the ‘cognitive’ model incorporates all
 features described above.
                                                         5 Summary and Conclusions
    Table 5 depicts the performance of both mod-
 els induced from the data of annotator A and B.         In this paper, we explored the use of eye-tracking
 The coefficient of determination (R2 ) describes        technology to investigate the behavior of human
 the proportion of the variance of the dependent         annotators during the assignment of three types of
 variable that can be described by the given model.      named entities – persons, organizations and loca-
 We report adjusted R2 to account for the different      tions – based on the eye-mind assumption. We
 numbers of features used in both models.                tested two main hypotheses – one relating to the
                                                         amount of contextual information being used for
   model        R2 on A’s data    R2 on B’s data         annotation decisions, the other relating to differ-
   baseline          0.4695           0.4640             ent degrees of syntactic and semantic complex-
   cognitive         0.6263           0.6185             ity of expressions that had to be annotated. We
                                                         found experimental evidence that the textual con-
 Table 5: Adjusted R2 values on both models and          text is searched for decision making on assigning
 for annotators A and B.                                 semantic meta-data at a surprisingly low rate (with


                                                    1165


Feature Group        Feature Name/Coefficient                       Estimate    Std. Error   t value   Pr(>|t|)
                     (Intercept)                                   855.0817      33.3614      25.63     0.0000
characters (basic)   token number                                 -304.3241      29.6378     -10.27     0.0000
                     char number                                     7.1365       2.2622       3.15     0.0016
                     has token initcaps                            244.4335      36.1489       6.76     0.0000
                     has token allcaps                            -342.0463      62.3226      -5.49     0.0000
                     has token alphanumeric                       -197.7383      39.0354      -5.07     0.0000
                     has token punctuation                        -303.7960      50.3570      -6.03     0.0000
words                number tokens entity like                     934.3953      13.3058      70.22     0.0000
                     percentage tokens entity like                -729.3439      43.7252     -16.68     0.0000
complexity           sem compl inverse document freq               392.8855      35.7576      10.99     0.0000
                     sem compl maximum ambiguity                   -13.1344       1.8352      -7.16     0.0000
                     synt compl number dominated nodes              87.8573       7.9094      11.11     0.0000
                     synt compl pos ngram probability              287.8137      28.2793      10.18     0.0000
                     syn complexity max dependency distance         28.7994       9.2174       3.12     0.0018
                     flesch kincaid readability                     -0.4117       0.1577      -2.61     0.0090
semantics            has entity critical token used above           73.5095      24.1225       3.05     0.0023
                     has entity critical token used below         -178.0314      24.3139      -7.32     0.0000
                     has abbreviation                              763.8605      73.5328      10.39     0.0000

                                   Table 6: ‘Cognitive’ model of annotator A.


  the exception of tackling high-complexity seman-       of complex cognitive tasks (such as gaze dura-
  tic cases and resolving co-references) and that an-    tion and gaze movements for named entity anno-
  notation performance correlates with semantic and      tation) to explanatory models (in our case, a time-
  syntactic complexity.                                  based cost model for annotation) follows a much
     The results of these experiments were taken as      warranted avenue in research in NLP where fea-
  a heuristic clue to focus on cognitively plausi-       ture farming becomes a theory-driven, explanatory
  ble features of learning empirically rooted cost       process rather than a much deplored theory-blind
  models for annotation. We compared a simple            engineering activity (cf. ACL-WS-2005 (2005)).
  cost model (basically taking the number of words          In this spirit, our focus has not been on fine-
  and characters into account) with a cognitively        tuning this cognitive cost model to achieve even
  grounded model and got a much higher fit for the       higher fits with the time data. Instead, we aimed at
  cognitive model when we compared cost predic-          testing whether the findings from our eye-tracking
  tions of both model classes on the recently re-        study can be exploited to model annotation costs
  leased time-stamped version of the M UC 7 corpus.      more accurately.
     We here want to stress the role of cognitive evi-      Still, future work will be required to optimize
  dence from eye-tracking to determine empirically       a cost model for eventual application where even
  relevant features for the cost model. The alterna-     more accurate cost models may be required. This
  tive, more or less mechanical feature engineering,     optimization may include both exploration of ad-
  suffers from the shortcoming that is has to deal       ditional features (such as domain-specific ones)
  with large amounts of (mostly irrelevant) features     as well as experimentation with other, presum-
  – a procedure which not only requires increased        ably non-linear, regression models. Moreover,
  amounts of training data but also is often compu-      the impact of improved cost models on the effi-
  tationally very expensive.                             ciency of (cost-sensitive) selective sampling ap-
     Instead, our approach introduces empirical,         proaches, such as Active Learning (Tomanek and
  theory-driven relevance criteria into the feature      Hahn, 2009), should be studied.
  selection process. Trying to relate observables


                                                     1166


References                                                 Brian Roark, Margaret Mitchell, and Kristy Holling-
                                                             shead. 2007. Syntactic complexity measures for
ACL-WS-2005. 2005. Proceedings of the ACL Work-              detecting mild cognitive impairment. In Proceed-
  shop on Feature Engineering for Machine Learn-             ings of the Workshop on BioNLP 2007: Biological,
  ing in Natural Language Processing. accessible             Translational, and Clinical Language Processing,
  via http://www.aclweb.org/anthology/                       pages 1–8.
  W/W05/W05-0400.pdf.
Gerry Altmann, Alan Garnham, and Yvette Dennis.            Burr Settles, Mark Craven, and Lewis Friedland. 2008.
  2007. Avoiding the garden path: Eye movements              Active learning with real annotation costs. In
  in context. Journal of Memory and Language,                Proceedings of the NIPS 2008 Workshop on Cost-
  31(2):685–712.                                             Sensitive Machine Learning, pages 1–10.

Shilpa Arora, Eric Nyberg, and Carolyn Rosé. 2009.        Patrick Sturt. 2007. Semantic re-interpretation and
  Estimating annotation cost for active learning in a        garden path recovery. Cognition, 105:477–488.
  multi-annotator environment. In Proceedings of the
                                                           Benedikt M. Szmrecsányi. 2004. On operationalizing
  NAACL HLT 2009 Workshop on Active Learning for
                                                             syntactic complexity. In Proceedings of the 7th In-
  Natural Language Processing, pages 18–26.
                                                             ternational Conference on Textual Data Statistical
Hintat Cheung and Susan Kemper. 1992. Competing              Analysis. Vol. II, pages 1032–1039.
  complexity metrics and adults’ production of com-
  plex sentences. Applied Psycholinguistics, 13:53–        Katrin Tomanek and Udo Hahn.          2009.     Semi-
  76.                                                        supervised active learning for sequence labeling. In
                                                             ACL 2009 – Proceedings of the 47th Annual Meet-
David Cohn, Zoubin Ghahramani, and Michael Jordan.           ing of the ACL and the 4th IJCNLP of the AFNLP,
  1996. Active learning with statistical models. Jour-       pages 1039–1047.
  nal of Artificial Intelligence Research, 4:129–145.
                                                           Katrin Tomanek and Udo Hahn. 2010. Annotation
Lyn Frazier and Keith Rayner. 1987. Resolution of            time stamps: Temporal metadata from the linguistic
  syntactic category ambiguities: Eye movements in           annotation process. In LREC 2010 – Proceedings of
  parsing lexically ambiguous sentences. Journal of          the 7th International Conference on Language Re-
  Memory and Language, 26:505–526.                           sources and Evaluation.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.        Matthew Traxler and Lyn Frazier. 2008. The role of
  Investigating the effects of selective sampling on the    pragmatic principles in resolving attachment ambi-
  annotation task. In CoNLL 2005 – Proceedings of           guities: Evidence from eye movements. Memory &
  the 9th Conference on Computational Natural Lan-          Cognition, 36:314–328.
  guage Learning, pages 144–151.
George Klare. 1963. The Measurement of Readability.
  Ames: Iowa State University Press.
Dekang Lin. 1996. On the structural complexity of
  natural language sentences. In COLING 1996 – Pro-
  ceedings of the 16th International Conference on
  Computational Linguistics, pages 729–733.
Linguistic Data Consortium. 2001. Message Under-
  standing Conference (MUC) 7. Philadelphia: Lin-
  guistic Data Consortium.
Keith Rayner, Anne Cook, Barbara Juhasz, and Lyn
  Frazier. 2006. Immediate disambiguation of lex-
  ically ambiguous words during reading: Evidence
  from eye movements. British Journal of Psychol-
  ogy, 97:467–482.
Keith Rayner. 1998. Eye movements in reading and
  information processing: 20 years of research. Psy-
  chological Bulletin, 126:372–422.
Eric Ringger, Marc Carmen, Robbie Haertel, Kevin
   Seppi, Deryle Lonsdale, Peter McClanahan, James
   Carroll, and Noel Ellison. 2008. Assessing the
   costs of machine-assisted corpus annotation through
   a user study. In LREC 2008 – Proceedings of the 6th
   International Conference on Language Resources
   and Evaluation, pages 3318–3324.


                                                       1167

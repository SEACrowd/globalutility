              A Rational Model of Eye Movement Control in Reading

                                 Klinton Bicknell and Roger Levy
                                     Department of Linguistics
                                 University of California, San Diego
                              9500 Gilman Dr, La Jolla, CA 92093-0108
                             {kbicknell,rlevy}@ling.ucsd.edu


                      Abstract                                2003; Keller, 2004; Levy & Jaeger, 2007; Jaeger,
    A number of results in the study of real-                 2010). To the extent that the behavior of these
    time sentence comprehension have been                     models looks like human behavior, it suggests that
    explained by computational models as re-                  humans are making rational use of all the infor-
    sulting from the rational use of probabilis-              mation available to them in language processing.
    tic linguistic information. Many times,                   In the domain of incremental language compre-
    these hypotheses have been tested in read-                hension, especially, there is a substantial amount
    ing by linking predictions about relative                 of computational work suggesting that humans be-
    word difficulty to word-aggregated eye                    have rationally (e.g., Jurafsky, 1996; Narayanan &
    tracking measures such as go-past time. In                Jurafsky, 2001; Levy, 2008; Levy, Reali, & Grif-
    this paper, we extend these results by ask-               fiths, 2009). Most of this work has taken as its
    ing to what extent reading is well-modeled                task predicting the difficulty of each word in a sen-
    as rational behavior at a finer level of anal-            tence, a major result being that a large component
    ysis, predicting not aggregate measures,                  of the difficulty of a word appears to be a function
    but the duration and location of each fix-                of its probability in context (Hale, 2001; Smith &
    ation. We present a new rational model of                 Levy, 2008). Much of the empirical basis for this
    eye movement control in reading, the cen-                 work comes from studying reading, where word
    tral assumption of which is that eye move-                difficulty can be related to the amount of time
    ment decisions are made to obtain noisy                   that a reader spends on a particular word. To re-
    visual information as the reader performs                 late these predictions about word difficulty to the
    Bayesian inference on the identities of the               data obtained in eye tracking experiments, the eye
    words in the sentence. As a case study,                   movement record has been summarized through
    we present two simulations demonstrating                  word aggregate measures, such as the average du-
    that the model gives a rational explanation               ration of the first fixation on a word, or the amount
    for between-word regressions.                             of time between when a word is first fixated and
                                                              when the eyes move to its right (‘go-past time’).
1   Introduction
                                                                 It is important to note that this notion of word
The language processing tasks of reading, listen-             difficulty is an abstraction over the actual task of
ing, and even speaking are remarkably difficult.              reading, which is made up of more fine-grained
Good performance at each one requires integrat-               decisions about how long to leave the eyes in
ing a range of types of probabilistic information             their current position, and where to move them
and making incremental predictions on the ba-                 next, producing the series of relatively stable pe-
sis of noisy, incomplete input. Despite these re-             riods (fixations) and movements (saccades) that
quirements, empirical work has shown that hu-                 characterize the eye tracking record. While there
mans perform very well (e.g., Tanenhaus, Spivey-              has been much empirical work on reading at
Knowlton, Eberhard, & Sedivy, 1995). Sophisti-                this fine-grained scale (see Rayner, 1998 for an
cated models have been developed that explain                 overview), and there are a number of successful
many of these effects using the tools of com-                 models (Reichle, Pollatsek, & Rayner, 2006; En-
putational linguistics and large-scale corpora to             gbert, Nuthmann, Richter, & Kliegl, 2005), little
make normative predictions for optimal perfor-                is known about the extent to which human read-
mance in these tasks (Genzel & Charniak, 2002,                ing behavior appears to be rational at this finer


                                                        1168
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1168–1178,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


grained scale. In this paper, we present a new ratio-   based on a principle of minimizing the expected
nal model of eye movement control in reading, the       entropy in the distribution over identities of the
central assumption of which is that eye movement        current word. Unfortunately, however, the Mr.
decisions are made to obtain noisy visual informa-      Chips model simplifies the problem of reading in
tion, which the reader uses in Bayesian inference       a number of ways: First, it uses a unigram model
about the form and structure of the sentence. As a      as its language model, and thus fails to use any
case study, we show that this model gives a ratio-      information in the linguistic context to help with
nal explanation for between-word regressions.           word identification. Second, it only moves on to
   In Section 2, we briefly describe the leading        the next word after unambiguous identification of
models of eye movements in reading, and in Sec-         the current word, whereas there is experimental
tion 3, we describe how these models account for        evidence that comprehenders maintain some un-
between-word regressions and the intuition behind       certainty about the word identities. In other work,
our model’s account of them. Section 4 describes        we have extended the Mr. Chips model to remove
the model and its implementation and Sections 5–        these two limitations, and show that the result-
6 describe two simulations we performed with the        ing model more closely matches human perfor-
model comparing behavioral policies that make re-       mance (Bicknell & Levy, 2010). The larger prob-
gressions to those that do not. In Simulation 1, we     lem, however, is that each of these models uses
show that specific regressive policies outperform       an unrealistic model of visual input, which obtains
specific non-regressive policies, and in Simulation     absolute knowledge of the characters in its visual
2, we use optimization to directly find optimal         window. Thus, there is no reason for the model to
policies for three performance measures. The re-        spend longer on one fixation than another, and the
sults show that the regressive policies outperform      model only makes predictions for where saccades
non-regressive policies across a wide range of per-     are targeted, and not how long fixations last.
formance measures, demonstrating that our model            Reichle and Laurent (2006) presented a rational
predicts that making between-word regressions is        model that overcame the limitations of Mr. Chips
a rational strategy for reading.                        to produce predictions for both fixation durations
                                                        and locations, focusing on the ways in which eye
2   Models of eye movements in reading                  movement behavior is an adaptive response to the
                                                        particular constraints of the task of reading. Given
The two most successful models of eye move-             this focus, Reichle and Laurent used a very simple
ments in reading are E-Z Reader (Reichle, Pollat-       word identification function, for which the time re-
sek, Fisher, & Rayner, 1998; Reichle et al., 2006)      quired to identify a word was a function only of its
and SWIFT (Engbert, Longtin, & Kliegl, 2002;            length and the relative position of the eyes. In this
Engbert et al., 2005). Both of these models charac-     paper, we present another rational model of eye
terize the problem of reading as one of word iden-      movement control in reading that, like Reichle and
tification. In E-Z Reader, for example, the system      Laurent, makes predictions for fixation durations
identifies each word in the sentence serially, mov-     and locations, but which focuses instead on the
ing attention to the next word in the sentence only     dynamics of word identification at the core of the
after processing the current word is complete, and      task of reading. Specifically, our model identifies
(to slightly oversimplify), the eyes then follow the    the words in a sentence by performing Bayesian
attentional shifts at some lag. SWIFT works simi-       inference combining noisy input from a realistic
larly, but with the main difference being that pro-     visual model with a language model that takes
cessing and attention are distributed over multiple     context into account.
words, such that adjacent words can be identified
in parallel. While both of these models provide a       3   Explaining between-word regressions
good fit to eye tracking data from reading, neither
model asks the higher level question of what a ra-      In this paper, we use our model to provide a
tional solution to the problem would look like.         novel explanation for between-word regressive
    The first model to ask this question, Mr. Chips     saccades. In reading, about 10–15% of saccades
(Legge, Klitz, & Tjan, 1997; Legge, Hooven,             are regressive – movements from right-to-left (or
Klitz, Mansfield, & Tjan, 2002), predicts the op-       to previous lines). To understand how models
timal sequence of saccade targets to read a text        such as E-Z Reader or SWIFT account for re-


                                                    1169


gressive saccades to previous words, recall that       ous regions to fall. In these cases, a rational way to
the system identifies words in the sentence (gen-      respond might be to make a between-word regres-
erally) left to right, and that identification of a    sive saccade to get more visual information about
word in these models takes a certain amount of         the (now) low confidence previous region.
time and then is completed. In such a setup, why          To illustrate this idea, consider the case of a lan-
should the eyes ever move backwards? Three ma-         guage composed of just two strings, AB and BA,
jor answers have been put forward. One possibil-       and assume that the eyes can only get noisy in-
ity given by E-Z Reader is as a response to over-      formation about the identity of one character at a
shoot; i.e., the eyes move backwards to a previ-       time. After obtaining a little information about the
ous word because they accidentally landed fur-         identity of the first character, the reader may be
ther forward than intended due to motor error.         reasonably confident that its identity is A and move
Such an explanation could only account for small       on to obtaining visual input about the second char-
between-word regressions, of about the magni-          acter. If the first noisy input about the second char-
tude of motor error. The most recent version,          acter also indicates that it is probably A, then the
E-Z Reader 10 (Reichle, Warren, & McConnell,           normative probability that the first character is A
2009), has a new component that can produce            (and thus a rational reader’s confidence in its iden-
longer between-word regressions. Specifically, the     tity) will fall. This simple example just illustrates
model includes a flag for postlexical integration      the point that if a reader is combining noisy vi-
failure, that – when triggered – will instruct the     sual information with a language model, then con-
model to produce a between-word regression to          fidence in previous regions will sometimes fall.
the site of the failure. That is, between-word re-        There are two ways that a rational agent might
gressions in E-Z Reader 10 can arise because of        deal with this problem. The first option would be
postlexical processes external to the model’s main     to reach a higher level of confidence in the iden-
task of word identification. A final explanation for   tity of each word before moving on to the right,
between-word regressions, which arises as a result     i.e., slowing down reading left-to-right to prevent
of normal processes of word identification, comes      having to make right-to-left regressions. The sec-
from the SWIFT model. In the SWIFT model, the          ond option is to read left-to-right relatively more
reader can fail to identify a word but move past       quickly, and then make occasional right-to-left re-
it and continue reading. In these cases, there is      gressions in the cases where probability in pre-
a chance that the eyes will at some point move         vious regions falls. In this paper, we present two
back to this unidentified word to identify it. From    simulations suggesting that when using a rational
the present perspective, however, it is unclear how    model to read natural language, the best strate-
it could be rational to move past an unidentified      gies for coping with the problem of confidence
word and decide to revisit it only much later.         about previous regions dropping – for any trade-
                                                       off between speed and accuracy – involve making
   Here, we suggest a new explanation for              between-word regressions. In the next section, we
between-word regressions that arises as a result       present the details of our model of reading and its
of word identification processes (unlike that of       implementation, and then we present our two sim-
E-Z Reader) and can be understood as rational          ulations in the sections following.
(unlike that of SWIFT). Whereas in SWIFT and
E-Z Reader, word recognition is a process that         4   Reading as Bayesian inference
takes some amount of time and is then ‘com-
pleted’, some experimental evidence suggests that      At its core, the framework we are proposing is one
word recognition may be best thought of as a           of reading as Bayesian inference. Specifically, the
process that is never ‘completed’, as comprehen-       model begins reading with a prior distribution over
ders appear to both maintain uncertainty about the     possible identities of a sentence given by its lan-
identity of previous input and to update that uncer-   guage model. On the basis of that distribution, the
tainty as more information is gained about the rest    model decides whether or not to move its eyes (and
of the sentence (Connine, Blasko, & Hall, 1991;        if so where to move them to) and obtains noisy
Levy, Bicknell, Slattery, & Rayner, 2009). Thus, it    visual input about the sentence at the eyes’ posi-
is possible that later parts of a sentence can cause   tion. That noisy visual input then gives the likeli-
a reader’s confidence in the identity of the previ-    hood term in a Bayesian belief update, where the


                                                   1170


model’s prior distribution over the identity of the      obtains visual input around the current location
sentence given the language model is updated to a        of the eyes, and then chooses between three ac-
posterior distribution taking into account both the      tions: (a) continuing to fixate the currently fixated
language model and the visual input obtained thus        position, (b) initiating a saccade to a new posi-
far. On the basis of that new distribution, the model    tion, or (c) stopping reading of the sentence. If
again selects an action and the cycle repeats.           on the ith timestep, the model chooses option (a),
   This framework is unique among models of eye          the timestep advances to i + 1 and another sam-
movement control in reading (except Mr. Chips)           ple of visual input is obtained around the current
in having a fully explicit model of how visual in-       position. If the model chooses option (c), the read-
put is used to discriminate word identity. This ap-      ing immediately ends. If a saccade is initiated (b),
proach stands in sharp contrast to other models,         there is a lag of two timesteps, roughly represent-
which treat the time course of word identifica-          ing the time required to plan and execute a sac-
tion as an exogenous function of other influenc-         cade, during which the model again obtains visual
ing factors (such as word length, frequency, and         input around the current position and then the eyes
predictability). The hope in our approach is that        move – with some motor error – toward the in-
the influence of these key factors on the eye move-      tended target ti , landing on position `i . On the next
ment record will fall out as a natural consequence       time step, visual input is obtained around `i and
of rational behavior itself. For example, it is well     another decision is made. The motor error for sac-
known that the higher the conditional probabil-          cades follows the form of random error used by all
ity of a word given preceding material, the more         major models of eye movements in reading: the
rapidly that word is read (Boston, Hale, Kliegl,         landing position `i is normally distributed around
Patil, & Vasishth, 2008; Demberg & Keller, 2008;         the intended target ti with standard deviation given
Ehrlich & Rayner, 1981; Smith & Levy, 2008).             by a linear function of the intended distance1
E-Z Reader and SWIFT incorporate this finding by                                                          
specifying a dependency on word predictability in                  `i ∼ N ti , (δ0 + δ1 |ti − `i−1 |)2             (1)
the exogenous function determining word process-
ing time. In our framework, in contrast, we would        for some linear coefficients δ0 and δ1 . In the ex-
expect such an effect to emerge as a byproduct of        periments reported in this paper, we follow the
Bayesian inference: words with high prior proba-         SWIFT model in using δ0 = 0.87, δ1 = 0.084.
bility (conditional on preceding fixations) will re-
                                                         4.2   Noisy visual input
quire less visual input to be reliably identified.
   An implemented model in this framework must           As stated earlier, the role of noisy visual input in
formalize a number of pieces of the reading prob-        our model is as the likelihood term in a Bayesian
lem, including the possible actions available to the     inference about sentence form and identity. There-
reader and their consequences, the nature of vi-         fore, if we denote the input obtained thus far from
sual input, a means of combining visual input with       a sentence as I, all the information pertinent to
prior expectations about sentence form and struc-        the reader’s inferences can be encapsulated in the
ture, and a control policy determining how the           form p(I|w) for possible sentences w. We assume
model will choose actions on the basis of its poste-     that the inputs deriving from each character posi-
rior distribution over the identities of the sentence.   tion are conditionally independent given sentence
In the remainder of this section, we present these       identity, so that if w j denotes letter j of the sen-
details of the formalization of the reading problem      tence and I( j) denotes the component of visual
we used for the simulations reported in this paper:      input associated with that letter, then we can de-
actions (4.1), visual input (4.2), formalization of      compose p(I|w) as ∏ j p(I( j)|w j ). For simplicity,
the Bayesian inference problem (4.3), control pol-       we assume that each character is either a lowercase
icy (4.4), and finally, implementation of the model      letter or a space. The visual input obtained from
using weighted finite state automata (4.5).              an individual fixation can thus be summarized as
                                                         a vector of likelihoods p(I( j)|w j ), as shown in
4.1   Formal problem of reading: Actions                     1 In the terminology of the literature, the model has only
                                                         random motor error (variance), not systematic error (bias).
For our model, we assume a series of discrete            Following Engbert and Krügel (2010), systematic error may
timesteps, and on each time step, the model first        arise from Bayesian estimation of the best saccade distance.


                                                     1171


                                                                *
      ...          a       s             a             c        a       t               s        a       t             a       t             a             t      ...
a         0   .04   .08   0   .15   0   .02    .07   .05    0   .003    .04   .06   0   .05   .10   0   .04   0   .04
c         0   .04   .02   0   .07   0   .25    .01   .01    0   .005    .01   .01   0   .05   .08   0   .04   0   .04
                                                                                                                   
.         .    .     .    .    .    .    .     .   .        .    .       .     .    .    .     .    .    .    .    . 
.         .    .     .    .    .    .    .     .   .        .    .       .     .    .    .     .    .    .    .    . 
.         .    .     .    .    .    .    .     .   .        .    .       .     .    .    .     .    .    .    .    . 
                                                                                                                   
s         0   .04   .04   0   .01   0   .03    .03  .002    0    .21    .03   .02   0   .07   .02   0   .04   0   .04
                                                                                                                   
                                                                                                                   
t         0   .04   .03   0   .01   0   .01   .003  .05     0    .02    .07   .12   0   .05   .05   0   .04   0   .04
.         .    .     .    .    .    .    .     .   .        .    .       .     .    .    .     .    .    .    .    . 
                                                                                                                   
.         .    .     .    .    .    .    .     .   .        .    .       .     .    .    .     .    .    .    .    . 
 .           .      .       .      .      .      .      .        .       .       .       .        .       .      .      .       .      .      .      .      .
             1      0       0      1      0      1      0        0       0       1       0        0       0      1      0       0      1      0      1      0



Figure 1: Peripheral and foveal visual input in the model. The asymmetric Gaussian curve indicates
declining perceptual acuity centered around the fixation point (marked by ∗). The vector underneath each
                      √ the likelihood p(I( j)|w j ) for each possible letter w j , taken from a single input
letter position denotes
sample with Λ = 1/ 3 (see vector at the left edge of the figure for key, and Section 4.2). In peripheral
vision, the letter/whitespace distinction is veridical, but no information about letter identity is obtained.
Note in this particular sample, input from the fixated character and the following one is rather inaccurate.


Figure 1. As in the real visual system, our vi-                                       This roughly corresponds to empirical estimates
sual acuity function decreases with retinal eccen-                                    that humans obtain useful information in reading
tricity; we follow the SWIFT model in assuming                                        from about 19 characters, more from the right of
that the spatial distribution of visual processing                                    fixation than the left (Rayner, 1998). Hence in Fig-
rate follows an asymmetric Gaussian with σL =                                         ure 1, for example, left-peripheral visual input can
2.41, σR = 3.74, which we discretize into process-                                    be represented as veridical knowledge of the initial
ing rates for each character position. If ε denotes a                                 whitespace (denoted ), and a uniform distribution
character’s eccentricity in characters from the cen-                                  over the 26 letters of English for the letter a.
ter of fixation, then the proportion of the total pro-
cessing rate at that eccentricity λ (ε) is given by                                   4.2.2          Foveal visual input
integrating the asymmetric Gaussian over a char-                                      In addition, for those eccentricities with a process-
acter width centered on that position,                                                ing rate proportion λ (ε) that is at least 1% of the
                                           (                                          total processing rate (ε ∈ [−5, 8]) the model re-
          Z ε+.5              
                 1          x2               σL , x < 0                               ceives foveal visual input, defined only for letters2
λ (ε) =            exp − 2 dx, σ =                                                    to give noisy information about the letter’s iden-
           ε−.5  Z         2σ                σR , x ≥ 0
                                                                                      tity. This threshold of 1% roughly corresponds to
where the normalization constant Z is given by                                        estimates that readers get information useful for
                  r                                                                   letter identification from about 4 characters to the
                      π                                                               left and 8 to the right of fixation (Rayner, 1998).
              Z=        (σL + σR ).
                      2                                                                  In our model, each letter is equally confusable
                                                                                      with all others, following Norris (2006, 2009),
From this distribution, we derive two types of vi-
                                                                                      but ignoring work on letter confusability (which
sual input, peripheral input giving word boundary
                                                                                      could be added to future model revisions; Engel,
information and foveal input giving information
                                                                                      Dougherty, & Jones, 1973; Geyer, 1977). Visual
about letter identity.
                                                                                      information about each character is obtained by
4.2.1 Peripheral visual input                                                         sampling. Specifically, we represent each letter as
In our model, any eccentricity with a processing                                      a 26-dimensional vector, where a single element
rate proportion λ (ε) at least 0.5% of the rate pro-                                  is 1 and the other 25 are zeros, and given this rep-
portion for the centrally fixated character (ε ∈                                      resentation, foveal input for a letter is given as a
[−7, 12]), yields peripheral visual input, defined                                    sample from a 26-dimensional Gaussian with a
as veridical word boundary information indicat-                                           2 For white space, the model is already certain of the iden-
ing whether each character is a letter or a space.                                    tity because of peripheral input.


                                                                                1172


mean equal to the letter’s true identity and a di-           (a)      m = [.6, .7, .6, .4, .3, .6]:       Keep fixating (3)
agonal covariance matrix Σ(ε) = λ (ε)−1/2 I. It is           (b)      m = [.6, .4, .9, .4, .3, .6]:       Move back (to 2)
relatively straightforward to show that under these          (c)      m = [.6, .7, .9, .4, .3, .6]:       Move forward (to 6)
conditions, if we take the processing rate to be the         (d)      m = [.6, .7, .9, .8, .7, .7]:       Stop reading
expected change in log-odds of the true letter iden-
                                                          Figure 2: Values of m for a 6 character sentence
tity relative to any other that a single sample brings
                                                          under which a model fixating position 3 would
about, then the rate equals λ (ε). We scale the over-
                                                          take each of its four actions, if α = .7 and β = .5.
all processing rate by multiplying each rate by Λ.
For the experiments in this paper, we set Λ = 4.
For each fixation, we sample independently from           most likely character c in position j,
the appropriate distribution for each character po-
sition and then compute the likelihood given each                         m( j) = max p(wn = c|I1i )
                                                                                      c
possible letter, as illustrated in the non-peripheral
                                                                                 = max       ∑         p(w0 |I1i ).    (4)
region of Figure 1.                                                                   c
                                                                                          w0 :w0n =c

4.3   Inference about sentence identity                   Intuitively, a high value of m means that the model
Given the visual input and a language model, in-          is relatively confident about the character’s iden-
ferences about the identity of the sentence w can         tity, and a low value that it is relatively uncertain.
be made by standard Bayesian inference, where                Given the values of this statistic, our model de-
the prior is given by the language model and the          cides between four possible actions, as illustrated
likelihood is a function of the total visual input ob-    in Figure 2. If the value of this statistic for the cur-
tained from the first to the ith timestep I1i ,           rent position of the eyes m(`i ) is less than a pa-
                                                          rameter α, the model chooses to continue fixating
                           p(w)p(I1i |w)                  the current position (2a). Otherwise, if the value
            p(w|I1i ) =                     .      (2)    of m( j) is less than β for some leftward position
                          ∑(w0 )p(I1i |w0 )                j < `i , the model initiates a saccade to the closest
                          w0
                                                          such position (2b). If m( j) ≥ β for all j < `i , then
If we let I( j) denote the input received about char-     the model initiates a saccade to n characters past
acter position j and let w j denote the jth character     the closest position to the right j > `i for which
in sentence identity w, then the likelihood can be        m( j) < α (2c).3 Finally, if no such positions exist
broken down by character position as                      to the right, the model stops reading the sentence
                              n                           (2d). Intuitively, then, the model reads by making
            p(I1i |w) = ∏ p(I1i ( j)|w j )                a rightward sweep to bring its confidence in each
                              j=1                         character up to α, but pauses to move left if confi-
                                                          dence in a previous character falls below β .
where n is the final character about which there is
any visual input. Similarly, we can decompose this        4.5      Implementation with wFSAs
into the product of the likelihoods of each sample        This model can be efficiently and simply im-
                          n       i
                                                          plemented using weighted finite-state automata
           p(I1i |w) = ∏ ∏ p(It ( j)|w j ).        (3)    (wFSAs; Mohri, 1997) as follows: First, we be-
                       j=1 t=1                            gin with a wFSA representation of the language
                                                          model, where each arc emits a single character (or
If the eccentricity of the jth character on the tth       is an epsilon-transition emitting nothing). To per-
timestep εtj is outside of foveal input or the char-      form belief update given a new visual input, we
acter is a space, the inner term is 0 or 1. If the sam-   create a new wFSA to represent the likelihood of
ple was from a letter in foveal input εtj ∈ [−5, 8], it   each character from the sample. Specifically, this
is the probability of sampling It ( j) from the mul-      wFSA has only a single chain of states, where,
tivariate Gaussian N (w j , ΛΣ(εtj )).                    e.g., the first and second state in the chain are con-
                                                          nected by 27 (or fewer) arcs, which emit each of
4.4   Control policy
                                                              3 The  role of n is to ensure that the model does not cen-
The model uses a simple policy to decide between          ter its visual field on the first uncertain character. We did not
actions based on the marginal probability m of the        attempt to optimize this parameter, but fixed n at 2.


                                                      1173


the possible characters for w1 along with their re-               sisting of the 500 most frequent words in the
spective likelihoods given the visual input (as in                British National Corpus (BNC) as well as all the
the inner term of Equation 3). Next, these two                    words in our test corpus. From this vocabulary, we
wFSAs may simply be composed and then nor-                        constructed a bigram model using the counts from
malized, which completes the belief update, re-                   every bigram in the BNC for which both words
sulting in a new wFSA giving the posterior dis-                   were in vocabulary (about 222,000 bigrams).
tribution over sentences. To calculate the statistic
m, while it is possible to calculate it in closed form            5.1.3    wFSA implementation
from such a wFSA relatively straightforwardly, for                We implemented our model with wFSAs using
efficiency we use Monte Carlo estimation based                    the OpenFST library (Allauzen, Riley, Schalk-
on samples from the wFSA.                                         wyk, Skut, & Mohri, 2007). Specifically, we
                                                                  constructed the model’s initial belief state (i.e.,
5     Simulation 1                                                the distribution over sentences given by its lan-
                                                                  guage model) by directly translating the bigram
With the description of our model in place, we
                                                                  model into a wFSA in the log semiring. We
next proceed to describe the first simulation in
                                                                  then composed this wFSA with a weighted finite-
which we used the model to test the hypothesis
                                                                  state transducer (wFST) breaking words down
that making regressions is a rational way to cope
                                                                  into characters. This was done in order to facili-
with confidence in previous regions falling. Be-
                                                                  tate simple composition with the visual likelihood
cause there is in general no single rational trade-
                                                                  wFSA defined over characters. In the Monte Carlo
off between speed and accuracy, our hypothesis
                                                                  estimation of m, we used 5000 samples from the
is that, for any given level of speed and accu-
                                                                  wFSA. Finally, to speed performance, we bounded
racy achieved by a non-regressive policy, there is a
                                                                  the wFSA to have exactly the number of char-
faster and more accurate policy that makes a faster
                                                                  acters present in the actual sentence and then re-
left-to-right pass but occasionally does make re-
                                                                  normalized.
gressions. In the terms of our model’s policy pa-
rameters α and β described above, non-regressive                  5.1.4    Test corpus
policies are exactly those with β = 0, and a pol-
                                                                  We tested our model’s performance by simulating
icy that is faster on the left-to-right pass but does
                                                                  reading of the Schilling corpus (Schilling, Rayner,
make regressions is one with a lower value of α
                                                                  & Chumbley, 1998). To ensure that our results
but a non-zero β . Thus, we tested the performance
                                                                  did not depend on smoothing, we only tested the
of our model on the reading of a corpus of text typ-
                                                                  model on sentences in which every bigram oc-
ical of that used in reading experiments at a range
                                                                  curred in the BNC. Unfortunately, only 8 of the 48
of reasonable non-regressive policies, as well as a
                                                                  sentences in the corpus met this criterion. Thus,
set of regressive policies with lower α and posi-
                                                                  we made single-word changes to 25 more of the
tive β . Our prediction is that the former set will
                                                                  sentences (mostly changing proper names and rare
be strictly dominated in terms of both speed and
                                                                  nouns) to produce a total of 33 sentences to read,
accuracy by the latter.
                                                                  for which every bigram did occur in the BNC.
5.1    Methods
                                                                  5.2     Results and discussion
5.1.1 Policy parameters
                                                                  For each policy we tested, we measured the aver-
We test 4 non-regressive policies (i.e., those with               age number of timesteps it took to read the sen-
β = 0) with values of α ∈ {.90, .95, .97, .99}, and               tences, as well as the average (natural) log prob-
in addition, test regressive policies with a lower                ability of the correct sentence identity under the
range of α ∈ {.85, .90, .95, .97} and β ∈ {.4, .7}.4              model’s beliefs after reading ended ‘Accuracy’.
5.1.2 Language model                                              The results are plotted in Figure 3. As shown in
Our reader’s language model was an unsmoothed                     the graph, for each non-regressive policy (the cir-
bigram model created using a vocabulary set con-                  cles), there is a regressive policy that outperforms
                                                                  it, both in terms of average number of timesteps
   4 We tested all combinations of these values of α and β
                                                                  taken to read (further to the left) and the average
except for [α, β ] = [.97, .4], because we did not believe that
a value of β so low in relation to α would be very different      log probability of the sentence identity (higher).
from a non-regressive policy.                                     Thus, for a range of policies, these results suggest


                                                              1174


                                                                            6.1.2    Optimization of policy parameters
           −0.6                                                      ●
                                                                            Searching directly for optimal values of α and β
           −0.8
                                                                            for our stochastic reading model is difficult be-
                        ●                    ●                              cause each evaluation of the model with a partic-
Accuracy




           −1.0
                                                                            ular set of parameters produces a different result.
                                        Beta                                We use the P EGASUS method (Ng & Jordan, 2000)
           −1.2
                                         ●   non−regressive (beta=0)        to transform this stochastic optimization problem
                                                 regressive (beta=0.4)
                                                                            into a deterministic one on which we can use stan-
                                                 regressive (beta=0.7)
                  ●                                                         dard optimization algorithms.5 Then, we evaluate
                  50        55          60             65           70      the model’s performance at each value of α and β
                                 Timesteps
                                                                            by reading the full test corpus and averaging per-
                                                                            formance. We then simply use coordinate ascent
Figure 3: Mean number of timesteps taken to read
                                                                            (in logit space) to find the optimal values of α and
a sentence and (natural) log probability of the true
                                                                            β for each performance measure.
identity of the sentence ‘Accuracy’ for a range of
values of α and β . Values of α are not labeled,                            6.1.3    Language model
but increase with the number of timesteps for a                             The language model used in this simulation be-
constant value of β . For each non-regressive pol-                          gins with the same vocabulary set as in Sim. 1,
icy (β = 0), there is a policy with a lower α and                           i.e., the 500 most frequent words in the BNC and
higher β that achieves better accuracy in less time.                        every word that occurs in our test corpus. Because
                                                                            the search algorithm demands that we evaluate the
that making regressions when confidence about                               performance of our model at a number of param-
previous regions falls is a rational reader strategy,                       eter values, however, it is too slow to optimize α
in that it appears to lead to better performance,                           and β using the full language model that we used
both in terms of speed and accuracy.                                        for Sim. 1. Instead, we begin with the same set of
                                                                            bigrams used in Sim. 1 – i.e., those that contain
6           Simulation 2                                                    two in-vocabulary words – and trim this set by re-
In Simulation 2, we perform a more direct test of                           moving rare bigrams that occur less than 200 times
the idea that making regressions is a rational re-                          in the BNC (except that we do not trim any bi-
sponse to the problem of confidence falling about                           grams that occur in our test corpus). This reduces
previous regions using optimization techniques.                             our set of bigrams to about 19,000.
Specifically, we search for optimal policy param-                           6.1.4    wFSA implementation
eter values (α, β ) for three different measures of
                                                                            The implementation was the same as in Sim. 1.
performance, each representing a different trade-
off between the importance of accuracy and speed.                           6.1.5    Test corpus
6.1           Methods                                                       The test corpus was the same as in Sim. 1.
6.1.1 Performance measures                                                  6.2     Results and discussion
We examine performance measures interpolating                               The optimal values of α and β for each γ ∈
between speed and accuracy of the form                                      {.025, .1, .4} are given in Table 1 along with the
                        L(1 − γ) − T γ                               (5)    mean values for L and T found at those parameter
                                                                            values. As the table shows, the optimization proce-
where L is the log probability of the true identity                         dure successfully found values of α and β , which
of the sentence under the model’s beliefs at the end                        go up (slower reading) as γ goes down (valuing
of reading, and T is the total number of timesteps                          accuracy more than time). In addition, we see that
before the model decided to stop reading. Thus,                             the average results of reading at these parameter
each different performance measure is determined                            values are also as we would expect, with T and L
by the weighting for time γ. We test three values of                        going up as γ goes down. As predicted, the optimal
γ ∈ {.025, .1, .4}. The first of these weights accu-                            5 Specifically, this involves fixing the random number gen-
racy highly, while the final one weights 1 timestep                         erator for each run to produce the same values, resulting in
almost as much as 1 unit of log probability.                                minimizing the variance in performance across evaluations.


                                                                         1175


    γ      α     β     Timesteps   Log probability       support a novel explanation for between-word re-
    .025   .90   .99   41.2        -0.02                 gressive saccades in reading: that they are used to
    .1     .36   .80   25.8        -0.90                 gather visual input about previous regions when
    .4     .18   .38   16.4        -4.59                 confidence about them falls. Simulation 1 showed
                                                         that a range of policies making regressions in these
Table 1: Optimal values of α and β found for each        cases outperforms a range of non-regressive poli-
performance measure γ tested and mean perfor-            cies. In Simulation 2, we directly searched for op-
mance at those values, measured in timesteps T           timal values for the policy parameters for three dif-
and (natural) log probability L.                         ferent performance measures, representing differ-
                                                         ent speed-accuracy trade-offs, and found that the
values of β found are non-zero across the range of       optimal policies in each case make substantial use
policies, which include policies that value speed        of between-word regressions when confidence in
over accuracy much more than in Sim. 1. This             previous regions falls. In addition to supporting
provides more evidence that whatever the partic-         a novel motivation for between-word regressions,
ular performance measure used, policies making           these simulations demonstrate the possibility for
regressive saccades when confidence in previous          testing a range of questions that were impossi-
regions falls perform better than those that do not.     ble with previous models of reading related to the
    There is one interesting difference between the      goals of a reader, such as how should reading be-
results of this simulation and those of Sim. 1,          havior change as accuracy is valued more.
which is that here, the optimal policies all have a         There are a number of obvious ways for the
value of β > α. That may at first seem surprising,       model to move forward. One natural next step is
since the model’s policy is to fixate a region un-       to make the model more realistic by using letter
til its confidence becomes greater than α and then       confusability matrices. In addition, the link to pre-
return if it falls below β . It would seem, then, that   vious work in sentence processing can be made
the only reasonable values of β are those that are       tighter by incorporating syntax-based language
strictly below α. In fact, this is not the case be-      models. It also remains to compare this model’s
cause of the two time step delay between the de-         predictions to human data more broadly on stan-
cision to move the eyes and the execution of that        dard benchmark measures for models of read-
saccade. Because of this delay, the model’s confi-       ing. The most important future development, how-
dence when it leaves a region (relevant to β ) will      ever, will be moving toward richer policy families,
generally be higher than when it decided to leave        which enable more intelligent decisions about eye
(determined by α). In Simulation 2, because of the       movement control, based not just on simple confi-
smaller grammar that was used, the model’s confi-        dence statistics calculated independently for each
dence in a region’s identity rises more quickly and      character position, but rather which utilize the rich
this difference is exaggerated.                          structure of the model’s posterior beliefs about the
                                                         sentence identity (and of language itself) to make
7     Conclusion                                         more informed decisions about the best time to
                                                         move the eyes and the best location to direct them
In this paper, we presented a model that performs        next.
Bayesian inference on the identity of a sentence,
combining a language model with noisy informa-           Acknowledgments
tion about letter identities from a realistic visual
input model. On the basis of these inferences, it        The authors thank Jeff Elman, Tom Griffiths,
uses a simple policy to determine how long to            Andy Kehler, Keith Rayner, and Angela Yu for
continue fixating the current position and where         useful discussion about this work. This work bene-
to fixate next, on the basis of information about        fited from feedback from the audiences at the 2010
where the model is uncertain about the sentence’s        LSA and CUNY conferences. The research was
identity. As such, it constitutes a rational model       partially supported by NIH Training Grant T32-
of eye movement control in reading, extending the        DC000041 from the Center for Research in Lan-
insights from previous results about rationality in      guage at UC San Diego to K.B., by a research
language comprehension.                                  grant from the UC San Diego Academic Senate
   The results of two simulations using this model       to R.L., and by NSF grant 0953870 to R.L.


                                                     1176


References                                              Genzel, D., & Charniak, E. (2003). Variation of
                                                          entropy and parse trees of sentences as a func-
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W.,         tion of the sentence number. In M. Collins &
  & Mohri, M. (2007). OpenFst: A general and              M. Steedman (Eds.), Proceedings of the 2003
  efficient weighted finite-state transducer library.     Conference on Empirical Methods in Natural
  In Proceedings of the Ninth International Con-          Language Processing (pp. 65–72). Sapporo,
  ference on Implementation and Application of            Japan: Association for Computational Linguis-
  Automata, (CIAA 2007) (Vol. 4783, p. 11-23).            tics.
  Springer.
                                                        Geyer, L. H. (1977). Recognition and confusion
Bicknell, K., & Levy, R. (2010). Rational eye
                                                          of the lowercase alphabet. Perception & Psy-
  movements in reading combining uncertainty
                                                          chophysics, 22, 487–490.
  about previous words with contextual probabil-
                                                        Hale, J. (2001). A probabilistic Earley parser as
  ity. In Proceedings of the 32nd Annual Confer-
                                                          a psycholinguistic model. In Proceedings of the
  ence of the Cognitive Science Society. Austin,
                                                          Second Meeting of the North American Chapter
  TX: Cognitive Science Society.
                                                          of the Association for Computational Linguistics
Boston, M. F., Hale, J. T., Kliegl, R., Patil, U., &
                                                          (Vol. 2, pp. 159–166). New Brunswick, NJ: As-
  Vasishth, S. (2008). Parsing costs as predic-
                                                          sociation for Computational Linguistics.
  tors of reading difficulty: An evaluation using
                                                        Jaeger, T. F. (2010). Redundancy and re-
  the potsdam sentence corpus. Journal of Eye
                                                          duction: Speakers manage syntactic in-
  Movement Research, 2(1), 1–12.
                                                          formation density.       Cognitive Psychology.
Connine, C. M., Blasko, D. G., & Hall, M. (1991).
                                                          doi:10.1016/j.cogpsych.2010.02.002.
  Effects of subsequent sentence context in audi-
  tory word recognition: Temporal and linguistic        Jurafsky, D. (1996). A probabilistic model of
  constraints. Journal of Memory and Language,            lexical and syntactic access and disambiguation.
  30, 234–250.                                            Cognitive Science, 20, 137–194.
Demberg, V., & Keller, F. (2008). Data from eye-        Keller, F. (2004). The entropy rate principle as
  tracking corpora as evidence for theories of syn-       a predictor of processing effort: An evaluation
  tactic processing complexity. Cognition, 109,           against eye-tracking data. In D. Lin & D. Wu
  193–210.                                                (Eds.), Proceedings of the 2004 Conference on
Ehrlich, S. F., & Rayner, K. (1981). Contextual           Empirical Methods in Natural Language Pro-
  effects on word perception and eye movements            cessing (pp. 317–324). Barcelona, Spain: As-
  during reading. Journal of Verbal Learning and          sociation for Computational Linguistics.
  Verbal Behavior, 20, 641–655.                         Legge, G. E., Hooven, T. A., Klitz, T. S., Mans-
Engbert, R., & Krügel, A. (2010). Readers use             field, J. S., & Tjan, B. S. (2002). Mr.
  Bayesian estimation for eye movement control.           Chips 2002: new insights from an ideal-observer
  Psychological Science, 21, 366–371.                     model of reading. Vision Research, 42, 2219–
Engbert, R., Longtin, A., & Kliegl, R. (2002). A          2234.
  dynamical model of saccade generation in read-        Legge, G. E., Klitz, T. S., & Tjan, B. S. (1997).
  ing based on spatially distributed lexical pro-         Mr. Chips: an Ideal-Observer model of reading.
  cessing. Vision Research, 42, 621–636.                  Psychological Review, 104, 524–553.
Engbert, R., Nuthmann, A., Richter, E. M., &            Levy, R. (2008). A noisy-channel model of ra-
  Kliegl, R. (2005). SWIFT: A dynamical model             tional human sentence comprehension under un-
  of saccade generation during reading. Psycho-           certain input. In Proceedings of the 2008 Con-
  logical Review, 112, 777–813.                           ference on Empirical Methods in Natural Lan-
Engel, G. R., Dougherty, W. G., & Jones, B. G.            guage Processing (pp. 234–243). Honolulu,
  (1973).     Correlation and letter recognition.         Hawaii: Association for Computational Linguis-
  Canadian Journal of Psychology, 27, 317–326.            tics.
Genzel, D., & Charniak, E. (2002, July). Entropy        Levy, R., Bicknell, K., Slattery, T., & Rayner,
  rate constancy in text. In Proceedings of the 40th      K. (2009). Eye movement evidence that read-
  annual meeting of the Association for Computa-          ers maintain and act on uncertainty about past
  tional Linguistics (pp. 199–206). Philadelphia:         linguistic input. Proceedings of the National
  Association for Computational Linguistics.              Academy of Sciences, 106, 21086–21090.


                                                    1177


Levy, R., & Jaeger, T. F. (2007). Speakers op-            22.
  timize information density through syntactic re-      Reichle, E. D., Warren, T., & McConnell, K.
  duction. In B. Schölkopf, J. Platt, & T. Hoffman        (2009). Using E-Z Reader to model the ef-
  (Eds.), Advances in Neural Information Pro-             fects of higher level language processing on eye
  cessing Systems 19 (pp. 849–856). Cambridge,            movements during reading. Psychonomic Bul-
  MA: MIT Press.                                          letin & Review, 16, 1–21.
Levy, R., Reali, F., & Griffiths, T. L. (2009).         Schilling, H. E. H., Rayner, K., & Chumbley, J. I.
  Modeling the effects of memory on human on-             (1998). Comparing naming, lexical decision,
  line sentence processing with particle filters. In      and eye fixation times: Word frequency effects
  D. Koller, D. Schuurmans, Y. Bengio, & L. Bot-          and individual differences. Memory & Cogni-
  tou (Eds.), Advances in Neural Information Pro-         tion, 26, 1270–1281.
  cessing Systems 21 (pp. 937–944).                     Smith, N. J., & Levy, R. (2008). Optimal process-
Mohri, M. (1997). Finite-state transducers in lan-        ing times in reading: a formal model and empir-
  guage and speech processing. Computational              ical investigation. In B. C. Love, K. McRae, &
  Linguistics, 23, 269–311.                               V. M. Sloutsky (Eds.), Proceedings of the 30th
Narayanan, S., & Jurafsky, D. (2001). A Bayesian          Annual Conference of the Cognitive Science So-
  model predicts human parse preference and               ciety (pp. 595–600). Austin, TX: Cognitive Sci-
  reading time in sentence processing. In T. Diet-        ence Society.
  terich, S. Becker, & Z. Ghahramani (Eds.), Ad-        Tanenhaus, M. K., Spivey-Knowlton, M. J., Eber-
  vances in Neural Information Processing Sys-            hard, K. M., & Sedivy, J. C. (1995). Integration
  tems 14 (pp. 59–65). Cambridge, MA: MIT                 of visual and linguistic information in spoken
  Press.                                                  language comprehension. Science, 268, 1632–
Ng, A. Y., & Jordan, M. (2000). PEGASUS:                  1634.
  A policy search method for large MDPs and
  POMDPs. In Uncertainty in Artificial Intelli-
  gence, Proceedings of the Sixteenth Conference
  (pp. 406–415).
Norris, D. (2006). The Bayesian reader: Explain-
  ing word recognition as an optimal Bayesian de-
  cision process. Psychological Review, 113, 327–
  357.
Norris, D. (2009). Putting it all together: A unified
  account of word recognition and reaction-time
  distributions. Psychological Review, 116, 207–
  219.
Rayner, K. (1998). Eye movements in reading and
  information processing: 20 years of research.
  Psychological Bulletin, 124, 372–422.
Reichle, E. D., & Laurent, P. A. (2006). Using
  reinforcement learning to understand the emer-
  gence of “intelligent” eye-movement behavior
  during reading. Psychological Review, 113,
  390–408.
Reichle, E. D., Pollatsek, A., Fisher, D. L., &
  Rayner, K. (1998). Toward a model of eye
  movement control in reading. Psychological Re-
  view, 105, 125–157.
Reichle, E. D., Pollatsek, A., & Rayner, K.
  (2006). E-Z Reader: A cognitive-control, serial-
  attention model of eye-movement behavior dur-
  ing reading. Cognitive Systems Research, 7, 4–


                                                    1178

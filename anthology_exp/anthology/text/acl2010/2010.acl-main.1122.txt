                “Ask not what Textual Entailment can do for You...”

                      Mark Sammons V.G.Vinod Vydiswaran Dan Roth
                           University of Illinois at Urbana-Champaign
                       {mssammon|vgvinodv|danr}@illinois.edu




                      Abstract                                selves to solve tasks requiring more complex rea-
                                                              soning and synthesis of information; many other
    We challenge the NLP community to par-                    tasks must be solved to achieve human-like perfor-
    ticipate in a large-scale, distributed effort             mance on tasks such as Question Answering. But
    to design and build resources for devel-                  there is no clear process for identifying potential
    oping and evaluating solutions to new and                 tasks (other than consensus by a sufficient num-
    existing NLP tasks in the context of Rec-                 ber of researchers), nor for quantifying their po-
    ognizing Textual Entailment. We argue                     tential contribution to existing NLP tasks, let alone
    that the single global label with which                   to Natural Language Understanding.
    RTE examples are annotated is insufficient                   Recent “grand challenges” such as Learning by
    to effectively evaluate RTE system perfor-                Reading, Learning To Read, and Machine Reading
    mance; to promote research on smaller, re-                are prompting more careful thought about the way
    lated NLP tasks, we believe more detailed                 these tasks relate, and what tasks must be solved
    annotation and evaluation are needed, and                 in order to understand text sufficiently well to re-
    that this effort will benefit not just RTE                liably reason with it. This is an appropriate time
    researchers, but the NLP community as                     to consider a systematic process for identifying
    a whole. We use insights from success-                    semantic analysis tasks relevant to natural lan-
    ful RTE systems to propose a model for                    guage understanding, and for assessing their
    identifying and annotating textual infer-                 potential impact on NLU system performance.
    ence phenomena in textual entailment ex-                     Research on Recognizing Textual Entailment
    amples, and we present the results of a pi-               (RTE), largely motivated by a “grand challenge”
    lot annotation study that show this model                 now in its sixth year, has already begun to address
    is feasible and the results immediately use-              some of the problems identified above. Tech-
    ful.                                                      niques developed for RTE have now been suc-
                                                              cessfully applied in the domains of Question An-
1   Introduction
                                                              swering (Harabagiu and Hickl, 2006) and Ma-
Much of the work in the field of Natural Lan-                 chine Translation (Pado et al., 2009), (Mirkin
guage Processing is founded on an assumption                  et al., 2009). The RTE challenge examples are
of semantic compositionality: that there are iden-            drawn from multiple domains, providing a rel-
tifiable, separable components of an unspecified              atively task-neutral setting in which to evaluate
inference process that will develop as research               contributions of different component solutions,
in NLP progresses. Tasks such as Named En-                    and RTE researchers have already made incremen-
tity and coreference resolution, syntactic and shal-          tal progress by identifying sub-problems of entail-
low semantic parsing, and information and rela-               ment, and developing ad-hoc solutions for them.
tion extraction have been identified as worthwhile               In this paper we challenge the NLP community
tasks and pursued by numerous researchers. While              to contribute to a joint, long-term effort to iden-
many have (nearly) immediate application to real              tify, formalize, and solve textual inference prob-
world tasks like search, many are also motivated              lems motivated by the Recognizing Textual Entail-
by their potential contribution to more ambitious             ment setting, in the following ways:
Natural Language tasks. It is clear that the compo-           (a) Making the Recognizing Textual Entailment
nents/tasks identified so far do not suffice in them-         setting a central component of evaluation for


                                                        1199
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1199–1208,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


relevant NLP tasks such as NER, Coreference,                    through reuse of successful solutions and focus on
parsing, data acquisition and application, and oth-             unresolved problems.
ers. While many “component” tasks are consid-                      In this paper we demonstrate that Textual En-
ered (almost) solved in terms of expected improve-              tailment systems are already “interesting”, in that
ments in performance on task-specific corpora, it               they have made significant progress beyond a
is not clear that this translates to strong perfor-             “smart” lexical baseline that is surprisingly hard
mance in the RTE domain, due either to prob-                    to beat (section 2). We argue that Textual Entail-
lems arising from unrelated, unsolved entailment                ment, as an application that clearly requires so-
phenomena that co-occur in the same examples,                   phisticated textual inference to perform well, re-
or to domain change effects. The RTE task of-                   quires the solution of a range of sub-problems,
fers an application-driven setting for evaluating a             some familiar and some not yet known. We there-
broad range of NLP solutions, and will reinforce                fore propose RTE as a promising and worthwhile
good practices by NLP researchers. The RTE                      task for large-scale community involvement, as it
task has been designed specifically to exercise tex-            motivates the study of many other NLP problems
tual inference capabilities, in a format that would             in the context of general textual inference.
make RTE systems potentially useful components                     We outline the limitations of the present model
in other “deep” NLP tasks such as Question An-                  of evaluation of RTE performance, and identify
swering and Machine Translation. 1                              kinds of evaluation that would promote under-
(b) Identifying relevant linguistic phenomena,                  standing of the way individual components can
interactions between phenomena, and their                       impact Textual Entailment system performance,
likely impact on RTE/textual inference. Deter-                  and allow better objective evaluation of RTE sys-
mining the correct label for a single textual en-               tem behavior without imposing additional burdens
tailment example requires human analysts to make                on RTE participants. We use this to motivate a
many smaller, localized decisions which may de-                 large-scale annotation effort to provide data with
pend on each other. A broad, carefully conducted                the mark-up sufficient to support these goals.
effort to identify and annotate such local phenom-                 To stimulate discussion of suitable annotation
ena in RTE corpora would allow their distributions              and evaluation models, we propose a candidate
in RTE examples to be quantified, and allow eval-               model, and provide results from a pilot annota-
uation of NLP solutions in the context of RTE. It               tion effort (section 3). This pilot study establishes
would also allow assessment of the potential im-                the feasibility of an inference-motivated annota-
pact of a solution to a specific sub-problem on the             tion effort, and its results offer a quantitative in-
RTE task, and of interactions between phenomena.                sight into the difficulty of the TE task, and the dis-
Such phenomena will almost certainly correspond                 tribution of a number of entailment-relevant lin-
to elements of linguistic theory; but this approach             guistic phenomena over a representative sample
brings a data-driven approach to focus attention on             from the NIST TAC RTE 5 challenge corpus. We
those phenomena that are well-represented in the                argue that such an evaluation and annotation ef-
RTE corpora, and which can be identified with suf-              fort can identify relevant subproblems whose so-
ficiently close agreement.                                      lution will benefit not only Textual Entailment but
(c) Developing resources and approaches that                    a range of other long-standing NLP tasks, and can
allow more detailed assessment of RTE sys-                      stimulate development of new ones. We also show
tems. At present, it is hard to know what spe-                  how this data can be used to investigate the behav-
cific capabilities different RTE systems have, and              ior of some of the highest-scoring RTE systems
hence, which aspects of successful systems are                  from the most recent challenge (section 4).
worth emulating or reusing. An evaluation frame-
work that could offer insights into the kinds of                2   NLP Insights from Textual Entailment
sub-problems a given system can reliably solve
would make it easier to identify significant ad-                The task of Recognizing Textual Entailment
vances, and thereby promote more rapid advances                 (RTE), as formulated by (Dagan et al., 2006), re-
                                                                quires automated systems to identify when a hu-
    1
      The Parser Training and Evaluation using Textual En-      man reader would judge that given one span of text
tailment track of SemEval 2 takes this idea one step further,
by evaluating performance of an isolated NLP task using the     (the Text) and some unspecified (but restricted)
RTE methodology.                                                world knowledge, a second span of text (the Hy-


                                                            1200


 Text:  The purchase of LexCorp by BMI for $2Bn                                     Rank        System id     Accuracy
        prompted widespread sell-offs by traders as they                             1              I          0.735
        sought to minimize exposure.                                                 2              E          0.685
                                                                                     3              H          0.670
 Hyp 1: BMI acquired another company.                                                4              J          0.667
 Hyp 2: BMI bought LexCorp for $3.4Bn.                                               5              G          0.662
                                                                                     6              B          0.638
                                                                                     7              D          0.633
Figure 1: Some representative RTE examples.                                          8              F          0.632
                                                                                     9              A          0.615
                                                                                     9              C          0.615
                                                                                     9              K          0.615
                                                                                      -            Lex         0.612
pothesis) is true. The task was extended in (Gi-
ampiccolo et al., 2007) to include the additional             Table 1: Top performing systems in the RTE 5 2-
requirement that systems identify when the Hy-                way task.
pothesis contradicts the Text. In the example
                                                                           Lex          E           G           H           I           J
shown in figure 1, this means recognizing that the                Lex     1.000       0.667       0.693       0.678       0.660       0.778
                                                                        (184,183)   (157,132)   (168,122)   (152,136)   (165,137)   (165,135)
Text entails Hypothesis 1, while Hypothesis 2 con-                E                   1.000       0.667       0.675       0.673       0.702
                                                                                    (224,187)   (192,112)   (178,131)   (201,127)   (186,131)
tradicts the Text. This operational definition of                 G                               1.000       0.688       0.713       0.745
                                                                                                (247,150)   (186,120)   (218,115)   (198,125)
Textual Entailment avoids commitment to any spe-                  H                                           1.000       0.705       0.707
                                                                                                            (219,183)   (194,139)   (178,136)
cific knowledge representation, inference method,                   I                                                     1.000       0.705
                                                                                                                        (260,181)   (198,135)
or learning approach, thus encouraging applica-                    J                                                                  1.000
tion of a wide range of techniques to the problem.                                                                                  (224,178)


                                                              Table 2: In each cell, top row shows observed
2.1   An Illustrative Example                                 agreement and bottom row shows the number of
                                                              correct (positive, negative) examples on which the
The simple RTE examples in figure 1 (most RTE                 pair of systems agree.
examples have much longer Texts) illustrate some
typical inference capabilities demonstrated by hu-
                                                              2.2       The State of the Art in RTE 5
man readers in determining whether one span of
text contains the meaning of another.                         The outputs for all systems that participated in the
   To recognize that Hypothesis 1 is entailed by the          RTE 5 challenge were made available to partici-
text, a human reader must recognize that “another             pants. We compared these to each other and to
company” in the Hypothesis can match “Lex-                    a smart lexical baseline (Do et al., 2010) (lexical
Corp”. She must also identify the nominalized                 match augmented with a WordNet similarity mea-
relation “purchase”, and determine that “A pur-               sure, stemming, and a large set of low-semantic-
chased by B” implies “B acquires A”.                          content stopwords) to assess the diversity of the
                                                              approaches of different research groups. To get
   To recognize that Hypothesis 2 contradicts the             the fullest range of participants, we used results
Text, similar steps are required, together with the           from the two-way RTE task. We have anonymized
inference that because the stated purchase price is           the system names.
different in the Text and Hypothesis, but with high              Table 1 shows that many participating systems
probability refers to the same transaction, Hypoth-           significantly outperform our smart lexical base-
esis 2 contradicts the Text.                                  line. Table 2 reports the observed agreement be-
   It could be argued that this particular example            tween systems and the lexical baseline in terms of
might be resolved by simple lexical matching; but             the percentage of examples on which a pair of sys-
it should be evident that the Text can be made                tems gave the same label. The agreement between
lexically very dissimilar to Hypothesis 1 while               most systems and the baseline is about 67%, which
maintaining the Entailment relation, and that con-            suggests that systems are not simply augmented
versely, the lexical overlap between the Text and             versions of the lexical baseline, and are also dis-
Hypothesis 2 can be made very high, while main-               tinct from each other in their behaviors.2
taining the Contradiction relation. This intuition               Common characteristics of RTE systems re-
is borne out by the results of the RTE challenges,
                                                                 2
which show that lexical similarity-based systems                   Note that the expected agreement between two random
                                                              RTE decision-makers is 0.5, so the agreement scores accord-
are outperformed by systems that use other, more              ing to Cohen’s Kappa measure (Cohen, 1960) are between
structured analysis, as shown in the next section.            0.3 and 0.4.


                                                           1201


ported by their designers were the use of struc-         of the NLP community. While there is widespread
tured representations of shallow semantic content        belief that there are many relevant entailment phe-
(such as augmented dependency parse trees and            nomena, though each individually may be rele-
semantic role labels); the application of NLP re-        vant to relatively few RTE examples (the Sparse-
sources such as Named Entity recognizers, syn-           ness problem), we know of no systematic analysis
tactic and dependency parsers, and coreference           to determine what those phenomena are, and how
resolvers; and the use of special-purpose ad-hoc         sparsely represented they are in existing RTE data.
modules designed to address specific entailment             If it were even known what phenomena were
phenomena the researchers had identified, such as        relevant to specific entailment examples, it might
the need for numeric reasoning. However, it is           be possible to more accurately distinguish system
not possible to objectively assess the role these ca-    capabilities, and promote adoption of successful
pabilities play in each system’s performance from        solutions to sub-problems. An annotation-side
the system outputs alone.                                solution also maintains the desirable agnosticism
                                                         of the RTE problem formulation, by not imposing
2.3   The Need for Detailed Evaluation                   the requirement on system developers of generat-
                                                         ing an explanation for each answer. Of course, if
An ablation study that formed part of the of-            examples were also annotated with explanations
ficial RTE 5 evaluation attempted to evaluate            in a consistent format, this could form the basis of
the contribution of publicly available knowledge         a new evaluation of the kind essayed in the pilot
resources such as WordNet (Fellbaum, 1998),              study in (Giampiccolo et al., 2007).
VerbOcean (Chklovski and Pantel, 2004), and
DIRT (Lin and Pantel, 2001) used by many of              3   Annotation Proposal and Pilot Study
the systems. The observed contribution was in
most cases limited or non-existent. It is premature,     As part of our challenge to the NLP commu-
however, to conclude that these resources have lit-      nity, we propose a distributed OntoNotes-style ap-
tle potential impact on RTE system performance:          proach (Hovy et al., 2006) to this annotation ef-
most RTE researchers agree that the real contribu-       fort: distributed, because it should be undertaken
tion of individual resources is difficult to assess.     by a diverse range of researchers with interests
As the example in figure 1 illustrates, most RTE         in different semantic phenomena; and similar to
examples require a number of phenomena to be             the OntoNotes annotation effort because it should
correctly resolved in order to reliably determine        not presuppose a fixed, closed ontology of entail-
the correct label (the Interaction problem); a per-      ment phenomena, but rather, iteratively hypoth-
fect coreference resolver might as a result yield lit-   esize and refine such an ontology using inter-
tle improvement on the standard RTE evaluation,          annotator agreement as a guiding principle. Such
even though coreference resolution is clearly re-        an effort would require a steady output of RTE ex-
quired by human readers in a significant percent-        amples to form the underpinning of these annota-
age of RTE examples.                                     tions; and in order to get sufficient data to repre-
   Various efforts have been made by individ-            sent less common, but nonetheless important, phe-
ual research teams to address specific capabili-         nomena, a large body of data is ultimately needed.
ties that are intuitively required for good RTE             A research team interested in annotating a new
performance, such as (de Marneffe et al., 2008),         phenomenon should use examples drawn from the
and the formal treatment of entailment phenomena         common corpus. Aside from any task-specific
in (MacCartney and Manning, 2009) depends on             gold standard annotation they add to the entail-
and formalizes a divide-and-conquer approach to          ment pairs, they should augment existing explana-
entailment resolution. But the phenomena-specific        tions by indicating in which examples their phe-
capabilities described in these approaches are far       nomenon occurs, and at which point in the exist-
from complete, and many are not yet invented. To         ing explanation for each example. In fact, this
devote real effort to identify and develop such ca-      latter effort – identifying phenomena relevant to
pabilities, researchers must be confident that the       textual inference, marking relevant RTE examples,
resources (and the will!) exist to create and eval-      and generating explanations – itself enables other
uate their solutions, and that the resource can be       researchers to select from known problems, assess
shown to be relevant to a sufficiently large subset      their likely impact, and automatically generate rel-


                                                     1202


evant corpora.                                         would label this as a 5) Numerical Quantity Mis-
   To assess the feasibility of annotating RTE-        match and 6) Excluding Argument (it can’t be the
oriented local entailment phenomena, we devel-         case that in the same transaction, the same com-
oped an inference model that could be followed by      pany was sold for two different prices).
annotators, and conducted a pilot annotation study.       Note that neither explanation mentions
We based our initial effort on observations about      the anaphora resolution connecting “they” to
RTE data we made while participating in RTE            “traders”, because it is not strictly required to
challenges, together with intuitive conceptions of     determine the entailment label.
the kinds of knowledge that might be available in         As our example illustrates, this process makes
semi-structured or structured form. In this sec-       sense for both positive and negative examples. It
tion, we present our annotation inference model,       also reflects common approaches in RTE systems,
and the results of our pilot annotation effort.        many of which have explicit alignment compo-
                                                       nents that map parts of the Hypothesis to parts of
3.1   Inference Process                                the Text prior to a final decision stage.
To identify and annotate RTE sub-phenomena in
RTE examples, we need a defensible model for the       3.2   Annotation Labels
entailment process that will lead to consistent an-    We sought to identify roles for background knowl-
notation by different researchers, and to an exten-    edge in terms of domains and general inference
sible framework that can accommodate new phe-          steps, and the types of linguistic phenomena that
nomena as they are identified.                         are involved in representing the same information
   We modeled the entailment process as one of         in different ways, or in detecting key differences
manipulating the text and hypothesis to be as sim-     in two similar spans of text that indicate a differ-
ilar as possible, by first identifying parts of the    ence in meaning. We annotated examples with do-
text that matched parts of the hypothesis, and then    mains (such as “Work”) for two reasons: to estab-
identifying connecting structure. Our inherent as-     lish whether some phenomena are correlated with
sumption was that the meanings of the Text and         particular domains; and to identify domains that
Hypothesis could be represented as sets of n-ary       are sufficiently well-represented that a knowledge
relations, where relations could be connected to       engineering study might be possible.
other relations (i.e., could take other relations as      While we did not generate an explicit repre-
arguments). As we followed this procedure for a        sentation of our entailment process, i.e. explana-
given example, we marked which entailment phe-         tions, we tracked which phenomena were strictly
nomena were required for the inference. We illus-      required for inference. The annotated corpora and
trate the process using the example in figure 1.       simple CGI scripts for annotation are available at
   First, we would identify the arguments “BMI”        http://cogcomp.cs.illinois.edu/Data/ACL2010 RTE.php.
and “another company” in the Hypothesis as                The phenomena that we considered during an-
matching “BMI” and “LexCorp” respectively, re-         notation are presented in Tables 3, 4, 5, and 6. We
quiring 1) Parent-Sibling to recognize that “Lex-      tried to define each phenomenon so that it would
Corp” can match “company”. We would tag the            apply to both positive and negative examples, but
example as requiring 2) Nominalization Resolu-         ran into a problem: often, negative examples can
tion to make “purchase” the active relation and        be identified principally by structural differences:
3) Passivization to move “BMI” to the subject po-      the components of the Hypothesis all match com-
sition. We would then tag it with 4) Simple Verb       ponents in the Text, but they are not connected
Rule to map “A purchase B” to “A acquire B”.           by the appropriate structure in the Text. In the
These operations make the relevant portion of the      case of contradictions, it is often the case that a
Text identical to the Hypothesis, so we are done.      key relation in the Hypothesis must be matched to
   For the same Text, but with Hypothesis 2 (a neg-    an incompatible relation in the Text. We selected
ative example), we follow the same steps 1-3. We       names for these structural behaviors, and tagged
would then use 4) Lexical Relation to map “pur-        them when we observed them, but the counterpart
chase” to “buy”. We would then observe that the        for positive examples must always hold: it must
only possible match for the hypothesis argument        necessarily be the case that the structure in the
“for $3.4Bn” is the text argument “for $2Bn”. We       Text linking the arguments that match those in the


                                                   1203


                                                                     Phenomenon            Occurrence      Agreement
Hypothesis must be comparable to the Hypothesis
                                                                     coreference            35.00%           0.698
structure. We therefore did not tag this for positive                simple rewrite rule    32.62%           0.580
                                                                     lexical relation       25.00%           0.738
examples.                                                            implicit relation      23.33%           0.633
   We selected a subset of 210 examples from the                     factoid                15.00%           0.412
                                                                     parent-sibling         11.67%           0.500
NIST TAC RTE 5 (Bentivogli et al., 2009) Test                        genetive relation       9.29%           0.608
                                                                     nominalization          8.33%           0.514
set drawn equally from the three sub-tasks (IE, IR                   event chain             6.67%           0.589
                                                                     coerced relation        6.43%           0.540
and QA). Each example was tagged by both an-                         passive-active          5.24%           0.583
notators. Two passes were made over the data: the                    numeric reasoning       4.05%           0.847
                                                                     spatial reasoning       3.57%           0.720
first covered 50 examples from each RTE sub-task,
while the second covered an additional 20 exam-              Table 5: Occurrence statistics for entailment phe-
ples from each sub-task. Between the two passes,             nomena and knowledge resources
concepts the annotators identified as difficult to
annotate were discussed and more carefully spec-                   Phenomenon                 Occurrence     Agreement
                                                                   missing argument            16.19%          0.763
ified, and several new concepts were introduced                    missing relation            14.76%          0.708
                                                                   excluding argument          10.48%          0.952
based on annotator observations.                                   Named Entity mismatch        9.29%          0.921
   Tables 3, 4, 5, and 6 present information                       excluding relation           5.00%          0.870
                                                                   disconnected relation        4.52%          0.580
about the distribution of the phenomena we                         missing modifier             3.81%          0.465
                                                                   disconnected argument        3.33%          0.764
tagged, and the inter-annotator agreement (Co-                     Numeric Quant. mismatch      3.33%          0.882
hen’s Kappa (Cohen, 1960)) for each. “Occur-
rence” lists the average percentage of examples la-          Table 6: Occurrences of negative-only phenomena
beled with a phenomenon by the two annotators.
          Domain             Occurrence     Agreement        the frequency of most of the domains we selected,
          work                16.90%          0.918          that knowledge engineering efforts also have a key
          name                12.38%          0.833
          die kill injure     12.14%          0.979          role in improving RTE performance.
          group                9.52%          0.794
          be in                8.57%          0.888
          kinship              7.14%          1.000          3.3   Discussion
          create               6.19%          1.000
          cause                6.19%          0.854
          come from            5.48%          0.879          Perhaps surprisingly, given the difficulty of the
          win compete          3.10%          0.813          task, inter-annotator agreement was consistently
          Others              29.52%          0.864
                                                             good to excellent (above 0.6 and 0.8, respec-
Table 3: Occurrence statistics for domains in the            tively), with few exceptions, indicating that for
annotated data.                                              most targeted phenomena, the concepts were well-
                                                             specified. The results confirmed our initial intu-
        Phenomenon             Occurrence     Agreement      ition about some phenomena: for example, that
        Named Entity            91.67%          0.856        coreference resolution is central to RTE, and that
        locative                17.62%          0.623
        Numerical Quantity      14.05%          0.905        detecting the connecting structure is crucial in dis-
        temporal                 5.48%          0.960
        nominalization           4.05%          0.245        cerning negative from positive examples. We also
        implicit relation        1.90%          0.651        found strong evidence that the difference between
Table 4: Occurrence statistics for hypothesis struc-         contradiction and unknown entailment examples
ture features.                                               is often due to the behavior of certain relations that
                                                             either preclude certain other relations holding be-
   From the tables it is apparent that good perfor-          tween the same arguments (for example, winning
mance on a range of phenomena in our inference               a contest vs. losing a contest), or which can only
model are likely to have a significant effect on             hold for a single referent in one argument position
RTE results, with coreference being deemed es-               (for example, “work” relations such as job title are
sential to the inference process for 35% of exam-            typically constrained so that a single person holds
ples, and a number of other phenomena are suffi-             one position).
ciently well represented to merit near-future atten-            We found that for some examples, there was
tion (assuming that RTE systems do not already               more than one way to infer the hypothesis from the
handle these phenomena, a question we address in             text. Typically, for positive examples this involved
section 4). It is also clear from the predominance           overlap between phenomena; for example, Coref-
of Simple Rewrite Rule instances, together with              erence might be expected to resolve implicit rela-


                                                          1204


tions induced from appositive structures. In such       have a disconnected or exclusion component (ar-
cases we annotated every way we could find.             gument/relation). System J got 81% of cases with
   In future efforts, annotators should record the      a disconnected component wrong.
entailment steps they used to reach their decision.     (d) Some phenomena are handled well by certain
This will make disagreement resolution simpler,         systems, but not by others. For example, failing
and could also form a possible basis for generating     to recognize a parent-sibling relation between
gold standard explanations. At a minimum, each          entities/concepts seems to be one of the top-5
inference step must identify the spans of the Text      phenomena active in systems E and H. System
and Hypothesis that are involved and the name of        H also fails to correctly label over 53% of the
the entailment phenomenon represented; in addi-         examples having kinship relation.
tion, a partial order over steps must be specified
when one inference step requires that another has       2. Which phenomena have strong correlations
been completed.                                         to the entailment labels among hard examples?
   Future annotation efforts should also add a          We called an example hard if at least 4 of the top 5
category “Other”, to indicate for each example          systems got the example wrong. In our annotation
whether the annotator considers the listed entail-      dataset, there were 41 hard examples. Some of
ment phenomena sufficient to identify the label. It     the phenomena that strongly correlate with the
might also be useful to assess the difficulty of each   TE labels on hard examples are: deeper lexical
example based on the time required by the anno-         relation between words (ρ = 0.542), and need
tator to determine an explanation, for comparison       for external knowledge (ρ = 0.345). Further, we
with RTE system errors.                                 find that the top-5 systems tend to make mistakes
   These, together with specifications that mini-       in cases where the lexical approach also makes
mize the likely disagreements between different         mistakes (ρ = 0.355).
groups of annotators, are processes that must be
refined as part of the broad community effort we
                                                        3. What more can be said about individual
seek to stimulate.
                                                        systems? In order to better understand the system
4   Pilot RTE System Analysis                           behavior, we wanted to check if we could predict
                                                        the system behavior based on the phenomena
In this section, we sketch out ways in which            we identified as important in the examples.
the proposed analysis can be applied to learn           We learned SVM classifiers over the identified
something about RTE system behavior, even               phenomena and the lexical similarity score to
when those systems do not provide anything              predict both the labels and errors systems make
beyond the output label. We present the analysis        for each of the top-5 systems. We could predict all
in terms of sample questions we hope to answer          10 system behaviors with over 70% accuracy, and
with such an analysis.                                  could predict labels and mistakes made by two of
                                                        the top-5 systems with over 77% accuracy. This
1. If a system needs to improve its performance,        indicates that although the identified phenomena
which features should it concentrate on? To an-         are indicative of the system performance, it is
swer this question, we looked at the top-5 systems      probably too simplistic to assume that system
and tried to find which phenomena are active in         behavior can be easily reproduced solely as a
the mistakes they make.                                 disjunction of phenomena present in the examples.
(a) Most systems seem to fail on examples that
need numeric reasoning to get the entailment de-        4. Does identifying the phenomena correctly
cision right. For example, system H got all 10 ex-      help learn a better TE system? We tried to
amples with numeric reasoning wrong.                    learn an entailment classifier over the phenomenon
(b) All top-5 systems make consistent errors in         identified and the top 5 system outputs. The results
cases where identifying a mismatch in named en-         are summarized in Table 7. All reported num-
tities (NE) or numerical quantities (NQ) is impor-      bers are 20-fold cross-validation accuracy from
tant to make the right decision. System G got 69%       an SVM classifier learned over the features men-
of cases with NE/NQ mismatches wrong.                   tioned. The results show that correctly identify-
(c) Most systems make errors in examples that           ing the named-entity and numeric quantity mis-


                                                    1205


         No.                 Feature description                  No. of Accuracy over which features
                                                                  feats phenomena pheno. + sys. labels
         (0)   Only system labels                                   5       —             0.714
         (1)   Domain and hypothesis features (Tables 3, 4)        16     0.510           0.705
         (2)   (1) + NE + NQ                                       18     0.619           0.762
         (3)   (1) + Knowledge resources (subset of Table 5)       22     0.662           0.762
         (4)   (3) + NE + NQ                                       24     0.738           0.805
         (5)   (1) + Entailment and Knowledge resources (Table 5)  29     0.748           0.791
         (6)   (5) + negative-only phenomena (Table 6)             38     0.971           0.943

      Table 7: Accuracy in predicting the label based on the phenomena and top-5 system labels.


matches improves the overall accuracy signifi-              The entailment setting introduces a potentially
cantly. If we further recognize the need for knowl-      broader context to resource development and as-
edge resources correctly, we can correctly explain       sessment, as the hypothesis and text provide con-
the label for 80% of the examples. Adding the            text for each other in a way different than local
entailment and negation features helps us explain        context from, say, the same paragraph in a docu-
the label for 97% of the examples in the annotated       ment: in RTE’s positive examples, the Hypothe-
corpus.                                                  sis either restates some part of the Text, or makes
   It must be clarified that the results do not show     statements inferable from the statements in the
the textual entailment problem itself is solved with     Text. This is not generally true of neighboring sen-
97% accuracy. However, we believe that if a              tences in a document. This distinction opens the
system could recognize key negation phenomena            door to “purposeful”, or goal-directed, inference
such as Named Entity mismatch, presence of Ex-           in a way that may not be relevant to a task studied
cluding arguments, etc. correctly and consistently,      in isolation.
it could model them as a Contradiction features             The RTE community seems mainly convinced
in the final inference process to significantly im-      that incremental advances in local entailment phe-
prove its overall accuracy. Similarly, identifying       nomena (including application of world knowl-
and resolving the key entailment phenomena in            edge) are needed to make significant progress.
the examples, would boost the inference process          They need ways to identify sub-problems of tex-
in positive examples. However, significant effort        tual inference, and to evaluate those solutions both
is still required to obtain near-accurate knowledge      in isolation and in the context of RTE. RTE system
and linguistic resources.                                developers are likely to reward well-engineered
                                                         solutions by adopting them and citing their au-
5   Discussion                                           thors, because such solutions are easier to incor-
                                                         porate into RTE systems. They are also more
NLP researchers in the broader community contin-         likely to adopt solutions with established perfor-
ually seek new problems to solve, and pose more          mance levels. These characteristics promote pub-
ambitious tasks to develop NLP and NLU capabil-          lication of software developed to solve NLP tasks,
ities, yet recognize that even solutions to problems     attention to its usability, and publication of mate-
which are considered “solved” may not perform as         rials supporting reproduction of results presented
well on domains different from the resources used        in technical papers.
to train and develop them. Solutions to such NLP            For these reasons, we assert that RTE is a nat-
tasks could benefit from evaluation and further de-      ural motivator of new NLP tasks, as researchers
velopment on corpora drawn from a range of do-           look for components capable of improving perfor-
mains, like those used in RTE evaluations.               mance; and that RTE is a natural setting for evalu-
   It is also worthwhile to consider each task as        ating solutions to a broad range of NLP problems,
part of a larger inference process, and therefore        though not in its present formulation: we must
motivated not just by performance statistics on          solve the problem of credit assignment, to recog-
special-purpose corpora, but as part of an inter-        nize component contributions. We have therefore
connected web of resources; and the task of Rec-         proposed a suitable annotation effort, to provide
ognizing Textual Entailment has been designed to         the resources necessary for more detailed evalua-
exercise a wide range of linguistic and reasoning        tion of RTE systems.
capabilities.                                               We have presented a linguistically-motivated


                                                     1206


analysis of entailment data based on a step-wise          respectfully suggest that you “ask not what RTE
procedure to resolve entailment decisions, in-            can do for you, but what you can do for RTE...”
tended to allow independent annotators to reach
consistent decisions, and conducted a pilot anno-         Acknowledgments
tation effort to assess the feasibility of such a task.   We thank the anonymous reviewers for their help-
   We do not claim that our set of domains or phe-        ful comments and suggestions. This research was
nomena are complete: for example, our illustra-           partly sponsored by Air Force Research Labora-
tive example could be tagged with a domain Merg-          tory (AFRL) under prime contract no. FA8750-
ers and Acquisitions, and a different team of re-         09-C-0181, by a grant from Boeing and by MIAS,
searchers might consider Nominalization Resolu-           the Multimodal Information Access and Synthesis
tion to be a subset of Simple Verb Rules. This kind       center at UIUC, part of CCICADA, a DHS Center
of disagreement in coverage is inevitable, but we         of Excellence. Any opinions, findings, and con-
believe that in many cases it suffices to introduce       clusion or recommendations expressed in this ma-
a new domain or phenomenon, and indicate its re-          terial are those of the author(s) and do not neces-
lation (if any) to existing domains or phenomena.         sarily reflect the view of the sponsors.
In the case of introducing a non-overlapping cate-
gory, no additional information is needed. In other
cases, the annotators can simply indicate the phe-        References
nomena being merged or split (or even replaced).          Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
This information will allow other researchers to            Giampiccolo, and Bernando Magnini. 2009. The
integrate different annotation sources and main-            fifth pascal recognizing textual entailment chal-
tain a consistent set of annotations.                       lenge. In Notebook papers and Results, Text Analy-
                                                            sis Conference (TAC), pages 14–24.
6   Conclusions                                           Timothy Chklovski and Patrick Pantel. 2004. VerbO-
                                                            cean: Mining the Web for Fine-Grained Semantic
In this paper, we have presented a case for a broad,        Verb Relations. In Proceedings of Conference on
long-term effort by the NLP community to coordi-            Empirical Methods in Natural Language Processing
nate annotation efforts around RTE corpora, and to          (EMNLP-04), pages 33–40.
evaluate solutions to NLP tasks relating to textual       Jacob Cohen. 1960. A coefficient of agreement
inference in the context of RTE. We have iden-               for nominal scales. Educational and Psychological
tified limitations in the existing RTE evaluation            Measurement, 20(1):37–46.
scheme, proposed a more detailed evaluation to            I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
address these limitations, and sketched a process            The PASCAL Recognising Textual Entailment Chal-
for generating this annotation. We have proposed             lenge., volume 3944. Springer-Verlag, Berlin.
an initial annotation scheme to prompt discussion,
                                                          Marie-Catherine de Marneffe, Anna N. Rafferty, and
and through a pilot study, demonstrated that such          Christopher D. Manning. 2008. Finding contradic-
annotation is both feasible and useful.                    tions in text. In Proceedings of ACL-08: HLT, pages
    We ask that researchers not only contribute            1039–1047, Columbus, Ohio, June. Association for
                                                           Computational Linguistics.
task specific annotation to the general pool, and
indicate how their task relates to those already          Quang Do, Dan Roth, Mark Sammons, Yuancheng
added to the annotated RTE corpora, but also in-            Tu, and V.G.Vinod Vydiswaran. 2010. Robust,
vest the additional effort required to augment the          Light-weight Approaches to compute Lexi-
                                                            cal Similarity.      Computer Science Research
cross-domain annotation: marking the examples               and Technical Reports, University of Illinois.
in which their phenomenon occurs, and augment-              http://L2R.cs.uiuc.edu/∼danr/Papers/DRSTV10.pdf.
ing the annotator-generated explanations with the
relevant inference steps.                                 C. Fellbaum. 1998. WordNet: An Electronic Lexical
                                                             Database. MIT Press.
    These efforts will allow a more meaningful
evaluation of RTE systems, and of the compo-              Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
nent NLP technologies they depend on. We see                and Bill Dolan. 2007. The third pascal recognizing
                                                            textual entailment challenge. In Proceedings of the
the potential for great synergy between different           ACL-PASCAL Workshop on Textual Entailment and
NLP subfields, and believe that all parties stand to        Paraphrasing, pages 1–9, Prague, June. Association
gain from this collaborative effort. We therefore           for Computational Linguistics.


                                                      1207


Sanda Harabagiu and Andrew Hickl. 2006. Meth-
  ods for Using Textual Entailment in Open-Domain
  Question Answering. In Proceedings of the 21st In-
  ternational Conference on Computational Linguis-
  tics and 44th Annual Meeting of the Association for
  Computational Linguistics, pages 905–912, Sydney,
  Australia, July. Association for Computational Lin-
  guistics.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
  Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
  The 90% solution. In Proceedings of HLT/NAACL,
  New York.

D. Lin and P. Pantel. 2001. DIRT: discovery of in-
  ference rules from text. In Proc. of ACM SIGKDD
  Conference on Knowledge Discovery and Data Min-
  ing 2001, pages 323–328.

Bill MacCartney and Christopher D. Manning. 2009.
   An extended model of natural logic. In The Eighth
   International Conference on Computational Seman-
   tics (IWCS-8), Tilburg, Netherlands.

Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
  Dagan, Marc Dymetman, and Idan Szpektor. 2009.
  Source-language entailment modeling for translat-
  ing unknown terms. In ACL/AFNLP, pages 791–
  799, Suntec, Singapore, August. Association for
  Computational Linguistics.

Sebastian Pado, Michel Galley, Dan Jurafsky, and
  Christopher D. Manning. 2009. Robust machine
  translation evaluation with entailment features. In
  Proceedings of the Joint Conference of the 47th An-
  nual Meeting of the ACL and the 4th International
  Joint Conference on Natural Language Processing
  of the AFNLP, pages 297–305, Suntec, Singapore,
  August. Association for Computational Linguistics.




                                                    1208

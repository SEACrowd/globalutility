    Improved Unsupervised POS Induction through Prototype Discovery


                            Omri Abend1∗ Roi Reichart2                 Ari Rappoport1
                                    1
                                        Institute of Computer Science, 2 ICNC
                                          Hebrew University of Jerusalem
                               {omria01|roiri|arir}@cs.huji.ac.il




                       Abstract                                central members. Our algorithm first clusters
                                                               words based on a fine morphological representa-
    We present a novel fully unsupervised al-                  tion. It then clusters the most frequent words,
    gorithm for POS induction from plain text,                 defining landmark clusters which constitute the
    motivated by the cognitive notion of proto-                cores of the categories. Finally, it maps the rest
    types. The algorithm first identifies land-                of the words to these categories. The last two
    mark clusters of words, serving as the                     stages utilize a distributional representation that
    cores of the induced POS categories. The                   has been shown to be effective for unsupervised
    rest of the words are subsequently mapped                  parsing (Seginer, 2007).
    to these clusters. We utilize morpho-                         We evaluated the algorithm in both English and
    logical and distributional representations                 German, using four different mapping-based and
    computed in a fully unsupervised manner.                   information theoretic clustering evaluation mea-
    We evaluate our algorithm on English and                   sures. The results obtained are generally better
    German, achieving the best reported re-                    than all existing POS induction algorithms.
    sults for this task.                                          Section 2 reviews related work. Sections 3 and
                                                               4 detail the algorithm. Sections 5, 6 and 7 describe
1   Introduction                                               the evaluation, experimental setup and results.
Part-of-speech (POS) tagging is a fundamental
                                                               2 Related Work
NLP task, used by a wide variety of applications.
However, there is no single standard POS tag-                  Unsupervised and semi-supervised POS tagging
ging scheme, even for English. Schemes vary                    have been tackled using a variety of methods.
significantly across corpora and even more so                  Schütze (1995) applied latent semantic analysis.
across languages, creating difficulties in using               The best reported results (when taking into ac-
POS tags across domains and for multi-lingual                  count all evaluation measures, see Section 5) are
systems (Jiang et al., 2009). Automatic induction              given by (Clark, 2003), which combines dis-
of POS tags from plain text can greatly alleviate              tributional and morphological information with
this problem, as well as eliminate the efforts in-             the likelihood function of the Brown algorithm
curred by manual annotations. It is also a problem             (Brown et al., 1992). Clark’s tagger is very sen-
of great theoretical interest. Consequently, POS               sitive to its initialization. Reichart et al. (2010b)
induction is a vibrant research area (see Section 2).          propose a method to identify the high quality runs
   In this paper we present an algorithm based                 of this algorithm. In this paper, we show that
on the theory of prototypes (Taylor, 2003), which              our algorithm outperforms not only Clark’s mean
posits that some members in cognitive categories               performance, but often its best among 100 runs.
are more central than others. These practically de-            Most research views the task as a sequential la-
fine the category, while the membership of other               beling problem, using HMMs (Merialdo, 1994;
elements is based on their association with the                Banko and Moore, 2004; Wang and Schuurmans,
     ∗
       Omri Abend is grateful to the Azrieli Foundation for    2005) and discriminative models (Smith and Eis-
the award of an Azrieli Fellowship.                            ner, 2005; Haghighi and Klein, 2006). Several


                                                          1298
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


techniques were proposed to improve the HMM            use a morphological representation based on sig-
model. A Bayesian approach was employed by             natures, which are sets of affixes that represent a
(Goldwater and Griffiths, 2007; Johnson, 2007;         family of words sharing an inflectional or deriva-
Gao and Johnson, 2008). Van Gael et al. (2009)         tional morphology (Goldsmith, 2001).
used the infinite HMM with non-parametric pri-
ors. Graça et al. (2009) biased the model to induce   3 Distributional Algorithm
a small number of possible tags for each word.
                                                       Our algorithm is given a plain text corpus and op-
   The idea of utilizing seeds and expanding them
                                                       tionally a desired number of clusters k. Its output
to less reliable data has been used in several pa-
                                                       is a partitioning of words into clusters. The al-
pers. Haghighi and Klein (2006) use POS ‘pro-
                                                       gorithm utilizes two representations, distributional
totypes’ that are manually provided and tailored
                                                       and morphological. Although eventually the latter
to a particular POS tag set of a corpus. Fre-
                                                       is used before the former, for clarity of presenta-
itag (2004) and Biemann (2006) induce an ini-
                                                       tion we begin by detailing the base distributional
tial clustering and use it to train an HMM model.
                                                       algorithm. In the next section we describe the mor-
Dasgupta and Ng (2007) generate morphological
                                                       phological representation and its integration into
clusters and use them to bootstrap a distributional
                                                       the base algorithm.
model. Goldberg et al. (2008) use linguistic con-
siderations for choosing a good starting point for     Overview. The algorithm consists of two main
the EM algorithm. Zhao and Marcus (2009) ex-           stages: landmark clusters discovery, and word
pand a partial dictionary and use it to learn dis-     mapping. For the former, we first compute a dis-
ambiguation rules. Their evaluation is only at the     tributional representation for each word. We then
type level and only for half of the words. Ravi        cluster the coordinates corresponding to high fre-
and Knight (2009) use a dictionary and an MDL-         quency words. Finally, we define landmark clus-
inspired modification to the EM algorithm.             ters. In the word mapping stage we map each word
   Many of these works use a dictionary provid-        to the most similar landmark cluster.
ing allowable tags for each or some of the words.         The rationale behind using only the high fre-
While this scenario might reduce human annota-         quency words in the first stage is twofold. First,
tion efforts, it does not induce a tagging scheme      prototypical members of a category are frequent
but remains tied to an existing one. It is further     (Taylor, 2003), and therefore we can expect the
criticized in (Goldwater and Griffiths, 2007).         salient POS tags to be represented in this small
                                                       subset. Second, higher frequency implies more re-
Morphological representation. Many POS in-             liable statistics. Since this stage determines the
duction models utilize morphology to some ex-          cores of all resulting clusters, it should be as accu-
tent. Some use simplistic representations of termi-    rate as possible.
nal letter sequences (e.g., (Smith and Eisner, 2005;
Haghighi and Klein, 2006)). Clark (2003) models        Distributional representation. We use a sim-
the entire letter sequence as an HMM and uses it       plified form of the elegant representation of lexi-
to define a morphological prior. Dasgupta and Ng       cal entries used by the Seginer unsupervised parser
(2007) use the output of the Morfessor segmenta-       (Seginer, 2007). Since a POS tag reflects the
tion algorithm for their morphological representa-     grammatical role of the word and since this rep-
tion. Morfessor (Creutz and Lagus, 2005), which        resentation is effective to parsing, we were moti-
we use here as well, is an unsupervised algorithm      vated to apply it to the present task.
that segments words and classifies each segment           Let W be the set of word types in the corpus.
as being a stem or an affix. It has been tested on     The right context entry of a word x ∈ W is a pair
several languages with strong results.                 of mappings r intx : W → [0, 1] and r adjx :
   Our work has several unique aspects. First,         W → [0, 1]. For each w ∈ W , r adjx (w) is an
our clustering method discovers prototypes in a        adjacency score of w to x, reflecting w’s tendency
fully unsupervised manner, mapping the rest of         to appear on the right hand side of x.
the words according to their association with the         For each w ∈ W , r intx (w) is an interchange-
prototypes. Second, we use a distributional repre-     ability score of x with w, reflecting the tendency
sentation which has been shown to be effective for     of w to appear to the left of words that tend to ap-
unsupervised parsing (Seginer, 2007). Third, we        pear to the right of x. This can be viewed as a


                                                   1299


similarity measure between words with respect to        representation vectors.
their right context. The higher the scores the more
the words tend to be adjacent/interchangeable.          Coordinate clustering. Each of our landmark
   Left context parameters l intx and l adjx are        clusters will correspond to a set of high frequency
defined analogously.                                    words (HFWs). The number of HFWs is much
   There are important subtleties in these defini-      larger than the number of expected POS tags.
tions. First, for two words x, w ∈ W , r adjx (w)       Hence we should cluster HFWs. Our algorithm
is generally different from l adjw (x). For exam-       does that by unifying some of the non-zero coordi-
ple, if w is a high frequency word and x is a low       nates corresponding to HFWs in the distributional
frequency word, it is likely that w appears many        representation defined above.
times to the right of x, yielding a high r adjx (w),       We extract the words that appear more than N
but that x appears only a few times to the left of w    times per million1 and apply the following proce-
yielding a low l adjw (x). Second, from the defi-       dure I times (5 in our experiments).
nition of r intx (w) and r intw (x), it is clear that      We run average link clustering with a threshold
they need not be equal.                                 α (AVGLINKα , (Jain et al., 1999)) on these words,
   These functions are computed incrementally by        in each iteration initializing every HFW to have
a bootstrapping process. We initialize all map-         its own cluster. AVGLINKα means running the av-
pings to be identically 0. We iterate over the words    erage link algorithm until the two closest clusters
in the training corpus. For every word instance x,      have a distance larger than α. We then use the in-
we take the word immediately to its right y and         duced clustering to update the distributional rep-
update x’s right context using y’s left context:        resentation, by collapsing all coordinates corre-
                                    l adjy (w)          sponding to words appearing in the same cluster
      ∀w ∈ W : r intx (w) +=                            into a single coordinate whose value is the sum
                                       N (y)
                               (
                                 1           w=y        of the collapsed coordinates’ values. In order to
  ∀w ∈ W : r adjx (w) += l inty (w)                     produce a conservative (fine) clustering, we used a
                                             w 6= y
                                   N (y)                relatively low α value of 0.25.
   The division by N (y) (the number of times y            Note that the AVGLINKα initialization in each
appears in the corpus before the update) is done in     of the I iterations assigns each HFW to a sepa-
order not to give a disproportional weight to high      rate cluster. The iterations differ in the distribu-
frequency words. Also, r intx (w) and r adjx (w)        tional representation of the HFWs, resulting from
might become larger than 1. We therefore nor-           the previous iterations.
malize them after all updates are performed by the         In our English experiments, this process re-
number of occurrences of x in the corpus.               duced the dimension of the HFWs set (the num-
   We update l intx and l adjx analogously using        ber of coordinates that are non-zero in at least one
the word z immediately to the left of x. The up-        of the HFWs) from 14365 to 10722. The aver-
dates of the left and right functions are done in       age number of non-zero coordinates per word de-
parallel.                                               creased from 102 to 55.
   We define the distributional representation of a        Since all eventual POS categories correspond to
word type x to be a 4|W | + 2 dimensional vector        clusters produced at this stage, to reduce noise we
vx . Each word w yields four coordinates, one for       delete clusters of less than five elements.
each direction (left/right) and one for each map-
ping type (int/adj). Two additional coordinates         Landmark detection. We define landmark clus-
represent the frequency in which the word appears       ters using the clustering obtained in the final iter-
to the left and to the right of a stopping punc-        ation of the coordinate clustering stage. However,
tuation. Of the 4|W | coordinates corresponding         the number of clusters might be greater than the
to words, we allow only 2n to be non-zero: the          desired number k, which is an optional parame-
n top scoring among the right side coordinates          ter of the algorithm. In this case we select a sub-
(those of r intx and r adjx ), and the n top scoring    set of k clusters that best covers the HFW space.
among the left side coordinates (those of l intx        We use the following heuristic. We start from the
and l adjx ). We used n = 50.                           most frequent cluster, and greedily select the clus-
   The distance between two words is defined to            1
                                                             We used N = 100, yielding 1242 words for English and
be one minus the cosine of the angle between their      613 words for German.


                                                    1300


ter farthest from the clusters already selected. The                 Types      join   joins joined joining
                                                                     Stem       join    join     join    join
distance between two clusters is defined to be the                  Affixes      φ        s       ed      ing
average distance between their members. A clus-                    Signature             {φ, ed, s, ing}
ter’s distance from a set of clusters is defined to
be its minimal distance from the clusters in the           Figure 1: An example for a morphological representation,
                                                           defined to be the conjunction of its affix(es) with the stem’s
set. The final set of clusters {L1 , ..., Lk } and their   signature.
members are referred to as landmark clusters and
prototypes, respectively.
                                                              In addition, we incorporate capitalization infor-
Mapping all words. Each word w ∈ W is as-                  mation into the model, by constraining all words
signed the cluster Li that contains its nearest pro-       that appear capitalized in more than half of their
totype:                                                    instances to belong to a separate cluster, regard-
     d(w, Li ) = minx∈Li {1 − cos(vw , vx )}               less of their morphological representation. The
             M ap(w) = argminLi {d(w, Li )}                motivation for doing so is practical: capitalization
                                                           is used in many languages to mark grammatical
   Words that appear less than 5 times are consid-         categories. For instance, in English capitalization
ered as unknown words. We consider two schemes             marks the category of proper names and in Ger-
for handling unknown words. One randomly maps              man it marks the noun category . We report En-
each such word to a cluster, using a probabil-             glish results both with and without this modifica-
ity proportional to the number of unique known             tion.
words already assigned to that cluster. However,              Words that contain non-alphanumeric charac-
when the number k of landmark clusters is rela-            ters are represented as the sequence of the non-
tively large, it is beneficial to assign all unknown       alphanumeric characters they include, e.g., ‘vis-à-
words to a separate new cluster (after running the         vis’ is represented as (“-”, “-”). We do not as-
algorithm with k − 1). In our experiments, we use          sign a morphological representation to words in-
the first option when k is below some threshold            cluding more than one stem (like weatherman), to
(we used 15), otherwise we use the second.                 words that have a null affix (i.e., where the word
4       Morphological Model                                is identical to its stem) and to words whose stem
                                                           is not shared by any other word (signature of size
The morphological model generates another word             1). Words that were not assigned a morphologi-
clustering, based on the notion of a signature.            cal representation are included as singletons in the
This clustering is integrated with the distributional      morphological clustering.
model as described below.
                                                           4.2 Distributional-Morphological Algorithm
4.1 Morphological Representation
                                                           We detail the modifications made to our base
We use the Morfessor (Creutz and Lagus, 2005)              distributional algorithm given the morphological
word segmentation algorithm. First, all words in           clustering defined above.
the corpus are segmented. Then, for each stem,
the set of all affixes with which it appears (its sig-     Coordinate clustering and landmarks. We
nature, (Goldsmith, 2001)) is collected. The mor-          constrain AVGLINKα to begin by forming links be-
phological representation of a word type is then           tween words appearing in the same morphologi-
defined to be its stem’s signature in conjunction          cal cluster. Only when the distance between the
with its specific affixes2 (See Figure 1).                 two closest clusters gets above α we remove this
   We now collect all words having the same rep-           constraint and proceed as before. This is equiv-
resentation. For instance, if the words joined and         alent to performing AVGLINKα separately within
painted are found to have the same signature, they         each morphological cluster and then using the re-
would share the same cluster since both have the           sult as an initial condition for an AVGLINKα coor-
affix ‘ ed’. The word joins does not share the same        dinate clustering. The modified algorithm in this
cluster with them since it has a different affix, ‘ s’.    stage is otherwise identical to the distributional al-
This results in coarse-grained clusters exclusively        gorithm.
defined according to morphology.                           Word mapping. In this stage words that are not
    2
        A word may contain more than a single affix.       prototypes are mapped to one of the landmark


                                                       1301


clusters. A reasonable strategy would be to map        2007) and NVI (Reichart and Rappoport, 2009),
all words sharing a morphological cluster as a sin-    VI’s (Meila, 2007) normalized version.
gle unit. However, these clusters are too coarse-
grained. We therefore begin by partitioning the        6 Experimental Setup
morphological clusters into sub-clusters according
to their distributional behavior. We do so by apply-   Since a goal of unsupervised POS tagging is in-
ing AVGLINKβ (the same as AVGLINKα but with a          ducing an annotation scheme, comparison to an
different parameter) to each morphological clus-       existing scheme is problematic. To address this
ter. Since our goal is cluster refinement, we use a    problem we compare to three different schemes
β that is considerably higher than α (0.9).            in two languages. In addition, the two English
   We then find the closest prototype to each such     schemes we compare with were designed to tag
sub-cluster (averaging the distance across all of      corpora contained in our training set, and have
the latter’s members) and map it as a single unit      been widely and successfully used with these cor-
to the cluster containing that prototype.              pora by a large number of applications.
                                                          Our algorithm was run with the exact same pa-
5   Clustering Evaluation                              rameters on both languages: N = 100 (high fre-
                                                       quency threshold), n = 50 (the parameter that
We evaluate the clustering produced by our algo-       determines the effective number of coordinates),
rithm using an external quality measure: we take       α = 0.25 (cluster separation during landmark
a corpus tagged by gold standard tags, tag it using    cluster generation), β = 0.9 (cluster separation
the induced tags, and compare the two taggings.        during refinement of morphological clusters).
There is no single accepted measure quantifying           The algorithm we compare with in most detail
the similarity between two taggings. In order to       is (Clark, 2003), which reports the best current
be as thorough as possible, we report results using    results for this problem (see Section 7). Since
four known measures, two mapping-based mea-            Clark’s algorithm is sensitive to its initialization,
sures and two information theoretic ones.              we ran it a 100 times and report its average and
                                                       standard deviation in each of the four measures.
Mapping-based measures. The induced clus-
                                                       In addition, we report the percentile in which our
ters have arbitrary names. We define two map-
                                                       result falls with respect to these 100 runs.
ping schemes between them and the gold clus-
ters. After the induced clusters are mapped, we           Punctuation marks are very frequent in corpora
can compute a derived accuracy. The Many-to-1          and are easy to cluster. As a result, including them
measure finds the mapping between the gold stan-       in the evaluation greatly inflates the scores. For
dard clusters and the induced clusters which max-      this reason we do not assign a cluster to punctua-
imizes accuracy, allowing several induced clusters     tion marks and we report results using this policy,
to be mapped to the same gold standard cluster.        which we recommend for future work. However,
The 1-to-1 measure finds the mapping between           to be able to directly compare with previous work,
the induced and gold standard clusters which max-      we also report results for the full POS tag set.
imizes accuracy such that no two induced clus-         We do so by assigning a singleton cluster to each
ters are mapped to the same gold cluster. Com-         punctuation mark (in addition to the k required
puting this mapping is equivalent to finding the       clusters). This simple heuristic yields very high
maximal weighted matching in a bipartite graph,        performance on punctuation, scoring (when all
whose weights are given by the intersection sizes      other words are assumed perfect tagging) 99.6%
between matched classes/clusters. As in (Reichart      (99.1%) 1-to-1 accuracy when evaluated against
and Rappoport, 2008), we use the Kuhn-Munkres          the English fine (coarse) POS tag sets, and 97.2%
algorithm (Kuhn, 1955; Munkres, 1957) to solve         when evaluated against the German POS tag set.
this problem.                                             For English, we trained our model on the
                                                       39832 sentences which constitute sections 2-21 of
Information theoretic measures. These are              the PTB-WSJ and on the 500K sentences from
based on the observation that a good clustering re-    the NYT section of the NANC newswire corpus
duces the uncertainty of the gold tag given the in-    (Graff, 1995). We report results on the WSJ part
duced cluster, and vice-versa. Several such mea-       of our data, which includes 950028 words tokens
sures exist; we use V (Rosenberg and Hirschberg,       in 44389 types. Of the tokens, 832629 (87.6%)


                                                   1302


   English                  Fine k=13                              Coarse k=13                            Fine k=34
                Prototype         Clark                Prototype         Clark                Prototype         Clark
                 Tagger        µ         σ       %      Tagger        µ         σ      %       Tagger        µ         σ       %
 Many–to–1         61.0       55.1      1.6     100      70.0        66.9      2.1     94        71.6       69.8      1.5      90
                   55.5       48.8      1.8     100      66.1        62.6      2.3     94        67.5       65.5      1.7      90
    1–to–1         60.0       52.2      1.9     100      58.1        49.4      2.9    100        63.5       54.5      1.6     100
                   54.9       46.0      2.2     100      53.7        43.8      3.3    100        58.8       48.5      1.8     100
     NVI          0.652      0.773 0.027        100      0.841      0.972 0.036       100       0.663      0.725 0.018        100
                  0.795      0.943 0.033        100      1.052      1.221 0.046       100       0.809      0.885 0.022        100
      V           0.636      0.581 0.015        100      0.590      0.543 0.018       100       0.677      0.659 0.008        100
                  0.542      0.478 0.019        100      0.484      0.429 0.023       100       0.608      0.588 0.010         98
                     German                      k=17                                  k=26
                                  Prototype         Clark               Prototype         Clark
                                   Tagger         µ        σ       %     Tagger        µ         σ       %
                    Many–to-1        64.6       64.7      1.2      41     68.2       67.8       1.0      60
                                     58.9       59.1      1.4      40     63.2       62.8       1.2      60
                      1–to–1         53.7       52.0      1.8      77     56.0       52.0       2.1      99
                                     48.0       46.0      2.3      78     50.7       45.9       2.6      99
                       NVI          0.667       0.675 0.019        66     0.640      0.682 0.019        100
                                    0.819       0.829 0.025        66     0.785      0.839 0.025        100
                        V           0.646       0.645 0.010        50     0.675      0.657 0.008        100
                                    0.552       0.553 0.013        48     0.596      0.574 0.010        100

Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark’s average score (µ),
Clark’s standard deviation (σ) and the fraction of Clark’s results that scored worse than our model (%). For the mapping based
measures, results are accuracy percentage. For V ∈ [0, 1], higher is better. For high quality output, N V I ∈ [0, 1] as well, and
lower is better. In each entry, the top number indicates the score when including punctuation and the bottom number the score
when excluding it. In English, our results are always better than Clark’s. In German, they are almost always better.


are not punctuation. The percentage of unknown                     corpus (Baroni et al., 2009). DeWAC is a cor-
words (those appearing less than five times) is                    pus extracted by web crawling and is therefore
1.6%. There are 45 clusters in this annotation                     out of domain. We report results on the NEGRA
scheme, 34 of which are not punctuation.                           part, which includes 346320 word tokens of 49402
   We ran each algorithm both with k=13 and                        types. Of the tokens, 289268 (83.5%) are not
k=34 (the number of desired clusters). We com-                     punctuation. The percentage of unknown words
pare the output to two annotation schemes: the fine                (those appearing less than five times) is 8.1%.
grained PTB WSJ scheme, and the coarse grained                     There are 62 clusters in this annotation scheme,
tags defined in (Smith and Eisner, 2005). The                      51 of which are not punctuation.
output of the k=13 run is evaluated both against                      We ran the algorithms with k=17 and k=26.
the coarse POS tag annotation (the ‘Coarse k=13’                   k=26 was chosen since it is the number of clus-
scenario) and against the full PTB-WSJ annotation                  ters that cover each more than 0.5% of the NE-
scheme (the ‘Fine k=13’ scenario). The k=34 run                    GRA tokens, and in total cover 96% of the (non-
is evaluated against the full PTB-WSJ annotation                   punctuation) tokens. In order to test our algo-
scheme (the ‘Fine k=34’ scenario).                                 rithm in another scenario, we conducted experi-
   The POS cluster frequency distribution tends to                 ments with k=17 as well, which covers 89.9% of
be skewed: each of the 13 most frequent clusters                   the tokens. All outputs are compared against NE-
in the PTB-WSJ cover more than 2.5% of the to-                     GRA’s gold standard scheme.
kens (excluding punctuation) and together 86.3%                       We do not report results for k=51 (where the
of them. We therefore chose k=13, since it is both                 number of gold clusters is the same as the number
the number of coarse POS tags (excluding punctu-                   of induced clusters), since our algorithm produced
ation) as well as the number of frequent POS tags                  only 42 clusters in the landmark detection stage.
in the PTB-WSJ annotation scheme. We chose                         We could of course have modified the parame-
k=34 in order to evaluate against the full 34 tags                 ters to allow our algorithm to produce 51 clusters.
PTB-WSJ annotation scheme (excluding punctua-                      However, we wanted to use the exact same param-
tion) using the same number of clusters.                           eters as those used for the English experiments to
   For German, we trained our model on the 20296                   minimize the issue of parameter tuning.
sentences of the NEGRA corpus (Brants, 1997)                          In addition to the comparisons described above,
and on the first 450K sentences of the DeWAC                       we present results of experiments (in the ‘Fine


                                                             1303


                B       B+M     B+C       F(I=1)      F                         1

    M-to-1     53.3      54.8    58.2      57.3     61.0                       0.8
    1-to-1     50.2      51.7    55.1      54.8     60.0
     NVI      0.782     0.720   0.710     0.742     0.652                      0.6
                                                                                                             Gold Standard

      V       0.569     0.598   0.615     0.597     0.636                      0.4                           Induced


                                                                               0.2
Table 2: A comparison of partial versions of the model in
the ‘Fine k=13’ WSJ scenario. M-to-1 and 1-to-1 results are                     0
                                                                                 0   5   10   15   20   25   30   35    40   45

reported in accuracy percentage. Lower NVI is better. B is the
strictly distributional algorithm, B+M adds the morphologi-
cal model, B+C adds capitalization to B, F(I=1) consists of       Figure 2: POS class frequency distribution for our model
all components, where only one iteration of coordinate clus-      and the gold standard, in the ‘Fine k=34’ scenario. The dis-
tering is performed, and F is the full model.                     tributions are similar.

               M-to-1     1-to-1         V             VI
 Prototype      71.6       63.5        0.677          2.00        by maximizing a non-convex function. These
   Clark        69.8       54.5        0.659          2.18        functions have many local maxima and the specific
    HK           –         41.3          –             –
                                                                  solution to which algorithms that maximize them
     J         43–62      37–47          –         4.23–5.74
    GG           –          –            –            2.8         converge strongly depends on their (random) ini-
    GJ           –       40–49.9         –         4.03–4.47      tialization. Therefore, their output’s quality often
    VG           –          –        0.54-0.59      2.5–2.9       significantly diverges from the average. This issue
 GGTP-45        65.4       44.5          –             –
 GGTP-17        70.2       49.5          –             –          is discussed in depth in (Reichart et al., 2010b).
                                                                  Our algorithm is deterministic3 .
Table 4: Comparison of our algorithms with the recent fully          For German, in the k=26 scenario our algorithm
unsupervised POS taggers for which results are reported. The
models differ in the annotation scheme, the corpus size and       outperforms Clark’s, often outperforming even its
the number of induced clusters (k) that they used. HK:            maximum in 100 runs. In the k=17 scenario, our
(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.         algorithm obtains a higher score than Clark with
GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse
tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags,         probability 0.4 to 0.78, depending on the measure
k=25–50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine          and scenario. Clark’s average score is slightly bet-
tags, k=50. VG: (Van Gael et al., 2009), 1.17M tokens, fine
tags, k=47–192. GGTP-45: (Graça et al., 2009), 1.17M to-
                                                                  ter in the Many-to-1 measure, while our algorithm
kens, fine tags, k=45. GGTP-17: (Graça et al., 2009), 1.17M      performs somewhat better than Clark’s average in
tokens, coarse tags, k=17. Lower VI values indicate better        the 1-to-1 and NVI measures.
clustering. VI is computed using e as the base of the loga-
rithm. Our algorithm gives the best results.                         The DeWAC corpus from which we extracted
                                                                  statistics for the German experiments is out of do-
                                                                  main with respect to NEGRA. The correspond-
k=13’ scenario) that quantify the contribution of                 ing corpus in English, NANC, is a newswire cor-
each component of the algorithm. We ran the base                  pus and therefore clearly in-domain with respect
distributional algorithm, a variant which uses only               to WSJ. This is reflected by the percentage of un-
capitalization information (i.e., has only one non-               known words, which was much higher in German
singleton morphological class, that of words ap-                  than in English (8.1% and 1.6%), lowering results.
pearing capitalized in most of their instances) and                  Table 2 shows the effect of each of our algo-
a variant which uses no capitalization information,               rithm’s components. Each component provides
defining the morphological clusters according to                  an improvement over the base distributional algo-
the morphological representation alone.                           rithm. The full coordinate clustering stage (sev-
                                                                  eral iterations, F) considerably improves the score
7    Results                                                      over a single iteration (F(I=1)). Capitalization in-
                                                                  formation increases the score more than the mor-
Table 1 presents results for the English and Ger-
                                                                  phological information, which might stem from
man experiments. For English, our algorithm ob-
                                                                  the granularity of the POS tag set with respect to
tains better results than Clark’s in all measures and
                                                                  names. This analysis is supported by similar ex-
scenarios. It is without exception better than the
                                                                  periments we made in the ‘Coarse k=13’ scenario
average score of Clark’s and in most cases better
                                                                  (not shown in tables here). There, the decrease in
than the maximal Clark score obtained in 100 runs.
                                                                  performance was only of 1%–2% in the mapping
   A significant difference between our algorithm
and Clark’s is that the latter, like most algorithms                3
                                                                      The fluctuations inflicted on our algorithm by the random
which addressed the task, induces the clustering                  mapping of unknown words are of less than 0.1% .


                                                               1304


                      Excluding Punctuation                   Including Punctuation                 Perfect Punctuation
                M-to-1 1-to-1      NVI      V           M-to-1 1-to-1      NVI      V      M-to-1      1-to-1    NVI    V
    Van Gael     59.1      48.4   0.999 0.530            62.3      51.3    0.861 0.591      64.0        54.6    0.820 0.610
    Prototype    67.5      58.8   0.809 0.608            71.6      63.5    0.663 0.677      71.6        63.9    0.659 0.679

Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
ment schemes. Left section: punctuation tokens are excluded. Middle section: punctuation tokens are included. Right section:
perfect assignment of punctuation is assumed.


based measures and 3.5% in the V measure.                               We have also performed a manual error anal-
   Finally, Table 4 presents reported results for all                ysis, which showed that our algorithm performs
recent algorithms we are aware of that tackled the                   much better on closed classes than on open
task of unsupervised POS induction from plain                        classes. In order to asses this quantitatively, let
text. Results for our algorithm’s and Clark’s are                    us define a random variable for each of the gold
reported for the ‘Fine, k=34’ scenario. The set-                     clusters, which receives a value corresponding to
tings of the various experiments vary in terms of                    each induced cluster with probability proportional
the exact annotation scheme used (coarse or fine                     to their intersection size. For each gold cluster,
grained) and the size of the test set. However, the                  we compute the entropy of this variable. In ad-
score differences are sufficiently large to justify                  dition, we greedily map each induced cluster to a
the claim that our algorithm is currently the best                   gold cluster and compute the ratio between their
performing algorithm on the PTB-WSJ corpus for                       intersection size and the size of the gold cluster
POS induction from plain text4 .                                     (mapping accuracy).
   Since previous works provided results only for                       We experimented in the ‘Fine k=34’ scenario.
the scenario in which punctuation is included, the                   The clusters that obtained the best scores were
reported results are not directly comparable. In                     (brackets indicate mapping accuracy and entropy
order to quantify the effect various punctuation                     for each of these clusters) coordinating conjunc-
schemes have on the results, we evaluated the                        tions (95%, 0.32), prepositions (94%, 0.32), de-
‘iHMM: PY-fixed’ model (Van Gael et al., 2009)                       terminers (94%, 0.44) and modals (93%, 0.45).
and ours when punctuation is excluded, included                      These are all closed classes.
or perfectly tagged5 . The results (Table 3) indi-                      The classes on which our algorithm performed
cate that most probably even after an appropriate                    worst consist of open classes, mostly verb types:
correction for punctuation, our model remains the                    past tense verbs (47%, 2.2), past participle verbs
best performing one.                                                 (44%, 2.32) and the morphologically unmarked
                                                                     non-3rd person singular present verbs (32%, 2.86).
8        Discussion                                                  Another class with low performance is the proper
                                                                     nouns (37%, 2.9). The errors there are mostly
In this work we presented a novel unsupervised al-                   of three types: confusions between common and
gorithm for POS induction from plain text. The al-                   proper nouns (sometimes due to ambiguity), un-
gorithm first generates relatively accurate clusters                 known words which were put in the unknown
of high frequency words, which are subsequently                      words cluster, and abbreviations which were given
used to bootstrap the entire clustering. The dis-                    a separate class by our algorithm. Finally, the al-
tributional and morphological representations that                   gorithm’s performance on the heterogeneous ad-
we use are novel for this task.                                      verbs class (19%, 3.73) is the lowest.
   We experimented on two languages with map-                           Clark’s algorithm exhibits6 a similar pattern
ping and information theoretic clustering evalua-                    with respect to open and closed classes. While
tion measures. Our algorithm obtains the best re-                    his algorithm performs considerably better on ad-
ported results on the English PTB-WSJ corpus. In                     verbs (15% mapping accuracy difference and 0.71
addition, our results are almost always better than                  entropy difference), our algorithm scores consid-
Clark’s on the German NEGRA corpus.                                  erably better on prepositions (17%, 0.77), su-
     4
      Graça et al. (2009) report very good results for 17 tags in   perlative adjectives (38%, 1.37) and plural proper
the M-1 measure. However, their 1-1 results are quite poor,          names (45%, 1.26).
and results for the common IT measures were not reported.
Their results for 45 tags are considerably lower.                       6
                                                                          Using average mapping accuracy and entropy over the
    5
      We thank the authors for sending us their data.                100 runs.


                                                                 1305


   Naturally, this analysis might reflect the arbi-    monosemous algorithms in the future in both type
trary nature of a manually design POS tag set          level and token level evaluations.
rather than deficiencies in automatic POS induc-          The skewed (Zipfian) distribution of POS class
tion algorithms. In future work we intend to ana-      frequencies in corpora is a problem for many POS
lyze the output of such algorithms in order to im-     induction algorithms, which by default tend to in-
prove POS tag sets.                                    duce a clustering having a balanced distribution.
                                                       Explicit modifications to these algorithms were in-
   Our algorithm and Clark’s are monosemous
                                                       troduced in order to bias their model to produce
(i.e., they assign each word exactly one tag), while
                                                       such a distribution (see (Clark, 2003; Johnson,
most other algorithms are polysemous. In order to
                                                       2007; Reichart et al., 2010b)). An appealing prop-
assess the performance loss caused by the monose-
                                                       erty of our model is its ability to induce a skewed
mous nature of our algorithm, we took the M-1
                                                       distribution without being explicitly tuned to do
greedy mapping computed for the entire dataset
                                                       so, as seen in Figure 2.
and used it to compute accuracy over the monose-
mous and polysemous words separately. Results          Acknowledgements. We would like to thank
are reported for the English ‘Fine k=34’ scenario      Yoav Seginer for his help with his parser.
(without punctuation). We define a word to be
monosemous if more than 95% of its tokens are
assigned the same gold standard tag. For English,      References
there are approximately 255K polysemous tokens         Michele Banko and Robert C. Moore, 2004. Part of
and 578K monosemous ones. As expected, our               Speech Tagging in Context. COLING ’04.
algorithm is much more accurate on the monose-         Marco Baroni, Silvia Bernardini, Adriano Ferraresi and
mous tokens, achieving 76.6% accuracy, com-             Eros Zanchetta, 2009. The WaCky Wide Web: A
pared to 47.1% on the polysemous tokens.                Collection of Very Large Linguistically Processed
                                                        Web-Crawled Corpora. Language Resources and
   The evaluation in this paper is done at the token    Evaluation.
level. Type level evaluation, reflecting the algo-     Chris Biemann,    2006.     Unsupervised Part-of-
rithm’s ability to detect the set of possible POS        Speech Tagging Employing Efficient Graph Cluster-
tags for each word type, is important as well. It        ing. COLING-ACL ’06 Student Research Work-
could be expected that a monosemous algorithm            shop.
such as ours would perform poorly in a type level      Thorsten Brants, 1997. The NEGRA Export Format.
evaluation. In (Reichart et al., 2010a) we discuss       CLAUS Report, Saarland University.
type level evaluation at depth and propose type        Peter F. Brown, Vincent J. Della Pietra, Peter V. de
level evaluation measures applicable to the POS          Souze, Jenifer C. Lai and Robert Mercer, 1992.
induction problem. In that paper we compare the          Class-Based N-Gram Models of Natural Language.
performance of our Prototype Tagger with lead-           Computational Linguistics, 18(4):467–479.
ing unsupervised POS tagging algorithms (Clark,        Alexander Clark, 2003. Combining Distributional and
2003; Goldwater and Griffiths, 2007; Gao and             Morphological Information for Part of Speech In-
Johnson, 2008; Van Gael et al., 2009). Our al-           duction. EACL ’03.
gorithm obtained the best results in 4 of the 6        Mathias Creutz and Krista Lagus, 2005. Inducing the
measures in a margin of 4–6%, and was second            Morphological Lexicon of a Natural Language from
best in the other two measures. Our results were        Unannotated Text. AKRR ’05.
better than Clark’s (the only other monosemous         Sajib Dasgupta and Vincent Ng,       2007. Unsu-
algorithm evaluated there) on all measures in a          pervised Part-of-Speech Acquisition for Resource-
margin of 5–21%. The fact that our monose-               Scarce Languages. EMNLP-CoNLL ’07.
mous algorithm was better than good polysemous         Dayne Freitag, 2004. Toward Unsupervised Whole-
algorithms in a type level evaluation can be ex-         Corpus Tagging. COLING ’04.
plained by the prototypical nature of the POS phe-     Jianfeng Gao and Mark Johnson, 2008. A Compar-
nomenon (a longer discussion is given in (Reichart        ison of Bayesian Estimators for Unsupervised Hid-
et al., 2010a)). However, the quality upper bound         den Markov Model POS Taggers. EMNLP ’08.
for monosemous algorithms is obviously much            Yoav Goldberg, Meni Adler and Michael Elhadad,
lower than that for polysemous algorithms, and           2008. EM Can Find Pretty Good HMM POS-
we expect polysemous algorithms to outperform            Taggers (When Given a Good Start). ACL ’08.


                                                   1306


John Goldsmith, 2001. Unsupervised Learning of the       Andrew Rosenberg and Julia Hirschberg, 2007. V-
  Morphology of a Natural Language. Computational          Measure: A Conditional Entropy-Based External
  Linguistics, 27(2):153–198.                              Cluster Evaluation Measure. EMNLP ’07.

Sharon Goldwater and Tom Griffiths, 2007. Fully          Hinrich Schütze, 1995. Distributional part-of-speech
  Bayesian Approach to Unsupervised Part-of-Speech         tagging. EACL ’95.
  Tagging. ACL ’07.
                                                         Yoav Seginer, 2007. Fast Unsupervised Incremental
João Graça, Kuzman Ganchev, Ben Taskar and Fre-          Parsing. ACL ’07.
   nando Pereira, 2009. Posterior vs. Parameter Spar-
   sity in Latent Variable Models. NIPS ’09.             Noah A. Smith and Jason Eisner, 2005. Contrastive
                                                           Estimation: Training Log-Linear Models on Unla-
David Graff, 1995. North American News Text Cor-           beled Data. ACL ’05.
  pus. Linguistic Data Consortium. LDC95T21.
                                                         John R. Taylor, 2003. Linguistic Categorization: Pro-
Aria Haghighi and Dan Klein, 2006. Prototype-driven        totypes in Linguistic Theory, Third Edition. Oxford
  Learning for Sequence Labeling. HLT–NAACL ’06.           University Press.

                                                         Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,      mani, 2009. The Infinite HMM for Unsupervised
  1999. Data Clustering: A Review. ACM Computing            POS Tagging. EMNLP ’09.
  Surveys 31(3):264–323.
                                                         Qin Iris Wang and Dale Schuurmans, 2005. Im-
Wenbin Jiang, Liang Huang and Qun Liu, 2009. Au-           proved Estimation for Unsupervised Part-of-Speech
  tomatic Adaptation of Annotation Standards: Chi-         Tagging. IEEE NLP–KE ’05.
  nese Word Segmentation and POS Tagging – A Case
  Study. ACL ’09.                                        Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
                                                           supervised Learner for POS Disambiguation Rules
Mark Johnson, 2007. Why Doesnt EM Find Good                Given Only a Minimal Lexicon. EMNLP ’09.
 HMM POS-Taggers? EMNLP-CoNLL ’07.

Harold W. Kuhn, 1955. The Hungarian method for
  the Assignment Problem. Naval Research Logistics
  Quarterly, 2:83-97.

Marina Meila, 2007. Comparing Clustering – an In-
 formation Based Distance. Journal of Multivariate
 Analysis, 98:873–895.

Bernard Merialdo, 1994. Tagging English Text with
  a Probabilistic Model. Computational Linguistics,
  20(2):155–172.

James Munkres, 1957. Algorithms for the Assignment
  and Transportation Problems. Journal of the SIAM,
  5(1):32–38.

Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
  els for Unsupervised Part-of-Speech Tagging. ACL
  ’09.

Roi Reichart and Ari Rappoport, 2008. Unsupervised
  Induction of Labeled Parse Trees by Clustering with
  Syntactic Features. COLING ’08.

Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
  tering Evaluation Measure. CoNLL ’09.

Roi Reichart, Omri Abend and Ari Rappoport, 2010a.
  Type Level Clustering Evaluation: New Measures
  and a POS Induction Case Study. CoNLL ’10.

Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.
  Improved Unsupervised POS Induction Using In-
  trinsic Clustering Quality and a Zipfian Constraint.
  CoNLL ’10.


                                                     1307

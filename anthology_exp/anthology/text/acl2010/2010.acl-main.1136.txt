       Understanding the Semantic Structure of Noun Phrase Queries


                                                Xiao Li
                                           Microsoft Research
                                           One Microsoft Way
                                        Redmond, WA 98052 USA
                                        xiaol@microsoft.com




                     Abstract                                semantically annotated web documents. Search-
                                                             ing over such data sources, in many cases, can
   Determining the semantic intent of web                    offer more relevant and essential results com-
   queries not only involves identifying their               pared with merely returning web pages that con-
   semantic class, which is a primary focus                  tain query keywords. Table 1 shows a simplified
   of previous works, but also understanding                 view of a structured data source, where each row
   their semantic structure. In this work, we                represents a movie object. Consider the query
   formally define the semantic structure of                 “johnny depp movies 2010”. It is possible to re-
   noun phrase queries as comprised of intent                trieve a set of movie objects from Table 1 that
   heads and intent modifiers. We present                    satisfy the constraints Year = 2010 and Cast 3
   methods that automatically identify these                 Johnny Depp. This would deliver direct answers to
   constituents as well as their semantic roles              the query rather than having the user sort through
   based on Markov and semi-Markov con-                      list of keyword results.
   ditional random fields. We show that the                     In no small part, the success of such an ap-
   use of semantic features and syntactic fea-               proach relies on robust understanding of query in-
   tures significantly contribute to improving               tent. Most previous works in this area focus on
   the understanding performance.                            query intent classification (Shen et al., 2006; Li
                                                             et al., 2008b; Arguello et al., 2009). Indeed, the
1 Introduction                                               intent class information is crucial in determining
                                                             if a query can be answered by any structured data
Web queries can be considered as implicit ques-              sources and, if so, by which one. In this work, we
tions or commands, in that they are performed ei-            go one step further and study the semantic struc-
ther to find information on the web or to initiate           ture of a query, i.e., individual constituents of a
interaction with web services. Web users, how-               query and their semantic roles. In particular, we
ever, rarely express their intent in full language.          focus on noun phrase queries. A key contribution
For example, to find out “what are the movies of             of this work is that we formally define query se-
2010 in which johnny depp stars”, a user may sim-            mantic structure as comprised of intent heads (IH)
ply query “johnny depp movies 2010”. Today’s                 and intent modifiers (IM), e.g.,
search engines, generally speaking, are based on
matching such keywords against web documents                  [IM:Title alice in wonderland] [IM:Year 2010] [IH cast]
and ranking relevant results using sophisticated
features and algorithms.                                     It is determined that “cast” is an IH of the above
   As search engine technologies evolve, it is in-           query, representing the essential information the
creasingly believed that search will be shifting             user intends to obtain. Furthermore, there are two
away from “ten blue links” toward understanding              IMs, “alice in wonderland” and “2010”, serving as
intent and serving objects. This trend has been              filters of the information the user receives.
largely driven by an increasing amount of struc-                 Identifying the semantic structure of queries can
tured and semi-structured data made available to             be beneficial to information retrieval. Knowing
search engines, such as relational databases and             the semantic role of each query constituent, we


                                                       1337
      Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1337–1345,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


         Title         Year        Genre              Director                    Cast                    Review
       Precious        2009        Drama             Lee Daniels       Gabby Sidibe, Mo’Nique,. . .
         2012          2009    Action, Sci Fi     Roland Emmerich   John Cusack, Chiwetel Ejiofor,. . .
        Avatar         2009    Action, Sci Fi      James Cameron    Sam Worthington, Zoe Saldana,. . .
   The Rum Diary       2010   Adventure, Drama     Bruce Robinson    Johnny Depp,Giovanni Ribisi,. . .
 Alice in Wonderland   2010   Adventure, Family      Tim Burton     Mia Wasikowska, Johnny Depp,. . .

              Table 1: A simplified view of a structured data source for the Movie domain.


can reformulate the query into a structured form         rive training data for learning a CRF-based tagger.
or reweight different query constituents for struc-      Manshadi and Li developed a hybrid, generative
tured data retrieval (Robertson et al., 2004; Kim        grammar model for a similar task. Both works are
et al., 2009; Paparizos et al., 2009). Alternatively,    closely related to one aspect of our work, which
the knowledge of IHs, IMs and semantic labels of         is to assign semantic labels to IMs. A key differ-
IMs may be used as additional evidence in a learn-       ence is that they do not conceptually distinguish
ing to rank framework (Burges et al., 2005).             between IHs and IMs.
   A second contribution of this work is to present         On the other hand, there have been a series of
methods that automatically extract the semantic          research studies related to IH identification (Pasca
structure of noun phrase queries, i.e., IHs, IMs         and Durme, 2007; Pasca and Durme, 2008). Their
and the semantic labels of IMs. In particular, we        methods aim at extracting attribute names, such
investigate the use of transition, lexical, semantic     as cost and side effect for the concept Drug, from
and syntactic features. The semantic features can        documents and query logs in a weakly-supervised
be constructed from structured data sources or by        learning framework. When used in the context
mining query logs, while the syntactic features can      of web queries, attribute names usually serve as
be obtained by readily-available syntactic analy-        IHs. In fact, one immediate application of their
sis tools. We compare the roles of these features        research is to understand web queries that request
in two discriminative models, Markov and semi-           factual information of some concepts, e.g. “asiprin
Markov conditional random fields. The second             cost” and “aspirin side effect”. Their framework,
model is especially interesting to us since in our       however, does not consider the identification and
task it is beneficial to use features that measure       categorization of IMs (attribute values).
segment-level characteristics. Finally, we evaluate
                                                         2.2 Question answering
our proposed models and features on manually-
annotated query sets from three domains, while           Query intent understanding is analogous to ques-
our techniques are general enough to be applied          tion understanding for question answering (QA)
to many other domains.                                   systems. Many web queries can be viewed as the
                                                         keyword-based counterparts of natural language
2 Related Works                                          questions. For example, the query “california na-
                                                         tional” and “national parks califorina” both imply
2.1   Query intent understanding                         the question “What are the national parks in Cali-
As mentioned in the introduction, previous works         fornia?”. In particular, a number of works investi-
on query intent understanding have largely fo-           gated the importance of head noun extraction in
cused on classification, i.e., automatically map-        understanding what-type questions (Metzler and
ping queries into semantic classes (Shen et al.,         Croft, 2005; Li et al., 2008a). To extract head
2006; Li et al., 2008b; Arguello et al., 2009).          nouns, they applied syntax-based rules using the
There are relatively few published works on un-          information obtained from part-of-speech (POS)
derstanding the semantic structure of web queries.       tagging and deep parsing. As questions posed
The most relevant ones are on the problem of             in natural language tend to have strong syntactic
query tagging, i.e., assigning semantic labels to        structures, such an approach was demonstrated to
query terms (Li et al., 2009; Manshadi and Li,           be accurate in identifying head nouns.
2009). For example, in “canon powershot sd850               In identifying IHs in noun phrase queries, how-
camera silver”, the word “canon” should be tagged        ever, direct syntactic analysis is unlikely to be as
as Brand. In particular, Li et al. leveraged click-      effective. This is because syntactic structures are
through data and a database to automatically de-         in general less pronounced in web queries. In this


                                                    1338


work, we propose to use POS tagging and parsing          Intent modifier
outputs as features, in addition to other features, in   In contrast, an intent modifier (IM) is a query seg-
extracting the semantic structure of web queries.        ment that corresponds to an attribute value (of
                                                         some attribute name). The role of IMs is to impos-
2.3   Information extraction
                                                         ing constraints on the attributes of an intent class.
Finally, there exist large bodies of work on infor-      For example, there are two constraints implied in
mation extraction using models based on Markov           the query “alice in wonderland 2010 cast”: (1) the
and semi-Markov CRFs (Lafferty et al., 2001;             Title of the movie is “alice in wonderland”; and
Sarawagi and Cohen, 2004), and in particular for         (2) the Year of the movie is “2010”. Interestingly,
the task of named entity recognition (McCallum           the user does not explicitly specify the attribute
and Li, 2003).                                           names, i.e., Title and Year, in this query. Such
   The problem studied in this work is concerned         information, however, can be inferred given do-
with identifying more generic “semantic roles” of        main knowledge. In fact, one important goal of
the constituents in noun phrase queries. While           this work is to identify the semantic labels of IMs,
some IM categories belong to named entities such         i.e., the attribute names they implicitly refer to. We
as IM:Director for the intent class Movie, there         use Ac to denote the set of IM semantic labels for
can be semantic labels that are not named entities       the intent class c.
such as IH and IM:Genre (again for Movie).
                                                         Other
3 Query Semantic Structure                               Additionally, there can be query segments that do
Unlike database query languages such as SQL,             not play any semantic roles, which we refer to as
web queries are usually formulated as sequences          Other.
of words without explicit structures. This makes         3.2 Syntactic analysis
web queries difficult to interpret by computers.
For example, should the query “aspirin side effect”      The notion of IHs and IMs in this work is closely
be interpreted as “the side effect of aspirin” or “the   related to that of linguistic head nouns and modi-
aspirin of side effect”? Before trying to build mod-     fiers for noun phrases. In many cases, the IHs of
els that can automatically makes such decisions,         noun phrase queries are exactly the head nouns in
we first need to understand what constitute the se-      the linguistic sense. Exceptions mostly occur in
mantic structure of a noun phrase query.                 queries without explicit IHs, e.g., “movie avatar”
                                                         in which the head noun “avatar” serves as an IM
3.1   Definition                                         instead. Due to the strong resemblance, it is inter-
We let C denote a set of query intent classes that       esting to see if IHs can be identified by extracting
represent semantic concepts such as Movie, Prod-         linguistic head nouns from queries based on syn-
uct and Drug. The query constituents introduced          tactic analysis. To this end, we apply the follow-
below are all defined w.r.t. the intent class of a       ing heuristics for head noun extraction. We first
query, c ∈ C, which is assumed to be known.              run a POS-tagger and a chunker jointly on each
                                                         query, where the POS-tagger/chunker is based on
Intent head                                              an HMM system trained on English Penn Tree-
An intent head (IH) is a query segment that cor-         bank (Gao et al., 2001). We then mark the right
responds to an attribute name of an intent class.        most NP chunk before any prepositional phrase
For example, the IH of the query “alice in won-          or adjective clause, and apply the NP head rules
derland 2010 cast” is “cast”, which is an attribute      (Collins, 1999) to the marked NP chunk.
name of Movie. By issuing the query, the user in-           The main problem with this approach, however,
tends to find out the values of the IH (i.e., cast). A   is that a readily-available POS tagger or chunker is
query can have multiple IHs, e.g., “movie avatar         usually trained on natural language sentences and
director and cast”. More importantly, there can          thus is unlikely to produce accurate results on web
be queries without an explicit IH. For example,          queries. As shown in (Barr et al., 2008), the lexi-
“movie avatar” does not contain any segment that         cal category distribution of web queries is dramat-
corresponds to an attribute name of Movie. Such a        ically different from that of natural languages. For
query, however, does have an implicit intent which       example, prepositions and subordinating conjunc-
is to obtain general information about the movie.        tions, which are strong indicators of the syntactic


                                                     1339


structure in natural languages, are often missing in     4.1 CRFs
web queries. Moreover, unlike most natural lan-          One natural approach to extracting the semantic
guages that follow the linear-order principle, web       structure of queries is to use linear-chain CRFs
queries can have relatively free word orders (al-        (Lafferty et al., 2001). They model the con-
though some orders may occur more often than             ditional probability of a label sequence given
others statistically). These factors make it diffi-      the input, where the labels, denoted as y =
cult to produce reliable syntactic analysis outputs.     (y1 , y2 , . . . , yM ), yi ∈ Y, have a one-to-one cor-
Consequently, the head nouns and hence the IHs           respondence with the word tokens in the input.
extracted therefrom are likely to be error-prone, as        Using linear-chain CRFs, we aim to find the la-
will be shown by our experiments in Section 6.3.         bel sequence that maximizes
   Although a POS tagger and a chunker may not
work well on queries, their output can be used as
                                                                                       (M +1                         )
                                                                              1         X
features for learning statistical models for seman-      pλ (y|x) =                exp       λ · f (yi−1 , yi , x, i) .
                                                                           Zλ (x)
tic structure extraction, which we introduce next.                                      i=1
                                                                                                                   (2)
4 Models                                                 The partition function Zλ (x) is a normalization
                                                         factor. λ is a weight vector and f (yi−1 , yi , x) is
This section presents two statistical models for se-     a vector of feature functions referred to as a fea-
mantic understanding of noun phrase queries. As-         ture vector. The features used in CRFs will be de-
suming that the intent class c ∈ C of a query is         scribed in Section 5.
known, we cast the problem of extracting the se-            Given manually-labeled queries, we estimate λ
mantic structure of the query into a joint segmen-       that maximizes the conditional likelihood of train-
tation/classification problem. At a high level, we       ing data while regularizing model parameters. The
would like to identify query segments that corre-        learned model is then used to predict the label se-
spond to IHs, IMs and Others. Furthermore, for           quence y for future input sequences x. To obtain s
each IM segment, we would like to assign a se-           in Equation (1), we simply concatenate the maxi-
mantic label, denoted by IM:a, a ∈ Ac , indicating       mum number of consecutive word tokens that have
which attribute name it refers to. In other words,       the same label and treat the resulting sequence as a
our label set consists of Y = {IH, {IM:a}a∈Ac ,          segment. By doing this, we implicitly assume that
Other}.                                                  there are no two adjacent segments with the same
   Formally, we let x = (x1 , x2 , . . . , xM ) denote   label in the true segment sequence. Although this
an input query of length M . To avoid confusion,         assumption is not always correct in practice, we
we use i to represent the index of a word token          consider it a reasonable approximation given what
and j to represent the index of a segment in the         we empirically observed in our training data.
following text. Our goal is to obtain
                                                         4.2 Semi-Markov CRFs
                ∗
              s = argmax p(s|c, x)                (1)    In contrast to standard CRFs, semi-Markov CRFs
                        s
                                                         directly model the segmentation of an input se-
where s = (s1 , s2 , . . . , sN ) denotes a query seg-   quence as well as a classification of the segments
mentation as well as a classification of all seg-        (Sarawagi and Cohen, 2004), i.e.,
ments. Each segment sj is represented by a tu-                                      N +1
ple (uj , vj , yj ). Here uj and vj are the indices of                   1        X
                                                            p(s|x) =          exp   λ · f (sj−1 , sj , x) (3)
the starting and ending word tokens respectively;                      Zλ (x)
                                                                                     j=1
yj ∈ Y is a label indicating the semantic role of
s. We further augment the segment sequence with          In this case, the features f (sj−1 , sj , x) are de-
two special segments: Start and End, represented         fined on segments instead of on word tokens.
by s0 and sN +1 respectively. For notional simplic-      More precisely, they are of the function form
ity, we assume that the intent class is given and        f (yj−1 , yj , x, uj , vj ). It is easy to see that by
use p(s|x) as a shorthand for p(s|c, x), but keep in     imposing a constraint ui = vi , the model is
mind that the label space and hence the parameter        reduced to standard linear-chain CRFs. Semi-
space is class-dependent. Now we introduce two           Markov CRFs make Markov assumptions at the
methods of modeling p(s|x).                              segment level, thereby naturally offering means to


                                                     1340


      CRF features
      A1: Transition δ(yi−1 = a)δ(yi = b)               transiting from state a to b
      A2: Lexical    δ(xi = w)δ(yi = b)                 current word is w
      A3: Semantic   δ(xi ∈ WL )δ(yi = b)               current word occurs in lexicon L
      A4: Semantic   δ(xi−1:i ∈ WL )δ(yi = b)           current bigram occurs in lexicon L
      A5: Syntactic  δ(POS(xi ) = z)δ(yi = b)           POS tag of the current word is z
      Semi-Markov CRF features
      B1: Transition δ(yj−1 = a)δ(yj = b)               Transiting from state a to b
      B2: Lexical    δ(xuj :vj = w)δ(yj = b)            Current segment is w
      B3: Lexical    δ(xuj :vj 3 w)δ(yj = b)            Current segment contains word w
      B4: Semantic   δ(xuj :vj ∈ L)δ(yj = b)            Current segment is an element in lexicon L
      B5: Semantic   max s(xuj :vj , l)δ(yj = b)        The max similarity between the segment and elements in L
                       l∈L
      B6: Syntactic   δ(POS(xuj :vj ) = z)δ(yj = b)     Current segment’s POS sequence is z
      B7: Syntactic   δ(Chunk(xuj :vj ) = c)δ(yj = b)   Current segment is a chunk with phrase type c

Table 2: A summary of feature types in CRFs and segmental CRFs for query understanding. We assume
that the state label is b in all features and omit this in the feature descriptions.


incorporate segment-level features, as will be pre-         sized query segment. The use of B3 would favor
sented in Section 5.                                        unseen words being included in adjacent segments
                                                            rather than to be isolated as separate segments.
5 Features
In this work, we explore the use of transition, lexi-       5.3 Semantic features
cal, semantic and syntactic features in Markov and
semi-Markov CRFs. The mathematical expression               Models relying on lexical features may require
of these features are summarized in Table 2 with            very large amounts of training data to produce
details described as follows.                               accurate prediction performance, as the feature
                                                            space is in general large and sparse. To make our
5.1   Transition features                                   model generalize better, we create semantic fea-
Transition features, i.e., A1 and B1 in Table 2,            tures based on what we call lexicons. A lexicon,
capture state transition patterns between adjacent          denoted as L, is a cluster of semantically-related
word tokens in CRFs, and between adjacent seg-              words/phrases. For example, a cluster of movie
ments in semi-Markov CRFs. We only use first-               titles or director names can be such a lexicon. Be-
order transition features in this work.                     fore describing how such lexicons are generated
                                                            for our task, we first introduce the forms of the
5.2   Lexical features                                      semantic features assuming the availability of the
                                                            lexicons.
In CRFs, a lexical feature (A2) is implemented as
a binary function that indicates whether a specific            We let L denote a lexicon, and WL denote the
word co-occurs with a state label. The set of words         set of n-grams extracted from L. For CRFs, we
to be considered in this work are those observed            create a binary function that indicates whether any
in the training data. We can also generalize this           n-gram in WL co-occurs with a state label, with
type of features from words to n-grams. In other            n = 1, 2 for A3, A4 respectively. For both A3
words, instead of inspecting the word identity at           and A4, the number of such semantic features is
the current position, we inspect the n-gram iden-           equal to the number of lexicons multiplied by the
tity by applying a window of length n centered at           number of state labels.
the current position.                                          The same source of semantic knowledge can be
   Since feature functions are defined on segments          conveniently incorporated in semi-Markov CRFs.
in semi-Markov CRFs, we create B2 that indicates            One set of semantic features (B4) inspect whether
whether the phrase in a hypothesized query seg-             the phrase of a hypothesized query segment
ment co-occurs with a state label. Here the set of          matches any element in a given lexicon. A sec-
phrase identities are extracted from the query seg-         ond set of semantic features (B5) relax the exact
ments in the training data. Furthermore, we create          match constraints made by B4, and take as the fea-
another type of lexical feature, B3, which is acti-         ture value the maximum “similarity” between the
vated when a specific word occurs in a hypothe-             query segment and all lexicon elements. The fol-


                                                        1341


lowing similarity function is used in this work ,        used in expanding the semantic lexicons for IMs
                                                         when database resources are not available. But we
       s(xuj :vj , l) = 1 − Lev(xuj :vj , l)/|l|   (4)
                                                         do not use such techniques in our work since the
where Lev represents the Levenshtein distance.           lexicons extracted from databases in general have
Notice that we normalize the Levenshtein distance        good precision and coverage.
by the length of the lexicon element, as we em-
pirically found it performing better compared with       5.4 Syntactic features
normalizing by the length of the segment. In com-        As mentioned in Section 3.2, web queries often
puting the maximum similarity, we first retrieve a       lack syntactic cues and do not necessarily follow
set of lexicon elements with a positive tf-idf co-       the linear order principle. Consequently, applying
sine distance with the segment; we then evaluate         syntactic analysis such as POS tagging or chunk-
Equation (4) for each retrieved element and find         ing using models trained on natural language cor-
the one with the maximum similarity score.               pora is unlikely to give accurate results on web
Lexicon generation                                       queries, as supported by our experimental evi-
                                                         dence in Section 6.3. It may be beneficial, how-
To create the semantic features described above,
                                                         ever, to use syntactic analysis results as additional
we generate two types of lexicons leveraging
                                                         evidence in learning.
databases and query logs for each intent class.
   The first type of lexicon is an IH lexicon com-          To this end, we generate a sequence of POS tags
prised of a list of attribute names for the intent       for a given query, and use the co-occurrence of
class, e.g., “box office” and “review” for the intent    POS tag identities and state labels as syntactic fea-
class Movie. One easy way of composing such a            tures (A5) for CRFs.
list is by aggregating the column names in the cor-         For semi-Markov CRFs, we instead examine
responding database such as Table 1. However,            the POS tag sequence of the corresponding phrase
this approach may result in low coverage on IHs          in a query segment. Again their identities are com-
for some domains. Moreover, many database col-           bined with state labels to create syntactic features
umn names, such as Title, are unlikely to appear as      B6. Furthermore, since it is natural to incorporate
IHs in queries. Inspired by Pasca and Van Durme          segment-level features in semi-Markov CRFs, we
(2007), we apply a bootstrapping algorithm that          can directly use the output of a syntactic chunker.
automatically learns attribute names for an intent       To be precise, if a query segment is determined by
class from query logs. The key difference from           the chunker to be a chunk, we use the indicator of
their work is that we create templates that consist      the phrase type of the chunk (e.g., NP, PP) com-
of semantic labels at the segment level from train-      bined with a state label as the feature, denoted by
ing data. For example, “alice in wonderland 2010         B7 in the Table. Such features are not activated if
cast” is labeled as “IM:Title IM:Year IH”, and thus      a query segment is determined not to be a chunk.
“IM:Title + IM:Year + #” is used as a template. We
select the most frequent templates (top 2 in this        6 Evaluation
work) from training data and use them to discover
                                                         6.1 Data
new IH phrases from the query log.
   Secondly, we have a set IM lexicons, each com-        To evaluate our proposed models and features, we
prised of a list of attribute values of an attribute     collected queries from three domains, Movie, Job
name in Ac . We exploit internal resources to gen-       and National Park, and had them manually anno-
erate such lexicons. For example, the lexicon for        tated. The annotation was given on both segmen-
IM:Title (in Movie) is a list of movie titles gener-     tation of the queries and classification of the seg-
ated by aggregating the values in the Title column       ments according to the label sets defined in Ta-
of a movie database. Similarly, the lexicon for          ble 3. There are 1000/496 samples in the train-
IM:Employee (in Job) is a list of employee names         ing/test set for the Movie domain, 600/366 for the
extracted from a job listing database. Note that         Job domain and 491/185 for the National Park do-
a substantial amount of research effort has been         main. In evaluation, we report the test-set perfor-
dedicated to automatic lexicon acquisition from          mance in each domain as well as the average per-
the Web (Pantel and Pennacchiotti, 2006; Pennac-         formance (weighted by their respectively test-set
chiotti and Pantel, 2009). These techniques can be       size) over all domains.


                                                     1342


                        Movie                               Job                           National Park
         IH               trailer, box office   IH             listing, salary   IH             lodging, calendar
         IM:Award         oscar best picture    IM:Category    engineering       IM:Category national forest
         IM:Cast          johnny depp           IM:City        las vegas         IM:City        page
         IM:Character     michael corleone      IM:County      orange            IM:Country     us
         IM:Category      tv series             IM:Employer walmart              IM:Name        yosemite
         IM:Country       american              IM:Level       entry level       IM:POI         volcano
         IM:Director      steven spielberg      IM:Salary      high-paying       IM:Rating      best
         IM:Genre         action                IM:State       florida           IM:State       flordia
         IM:Rating        best                  IM:Type        full time
         IM:Title         the godfather
         Other            the, in, that         Other           the, in, that    Other          the, in, that

Table 3: Label sets and their respective query segment examples for the intent class Movie, Job and
National Park.


6.2   Metrics                                                   B3, we also tried extending the features based on
There are two evaluation metrics used in our work:              word IDs to those based on n-gram IDs, where
segment F1 and sentence accuracy (Acc). The                     n = 1, 2, 3. This greatly increased the number of
first metric is computed based on precision and re-             lexical features but did not improve learning per-
call at the segment level. Specifically, let us as-             formance, most likely due to the limited amounts
sume that the true segment sequence of a query                  of training data coupled with the sparsity of such
is s = (s1 , s2 , . . . , sN ), and the decoded segment         features. In general, lexical features do not gener-
sequence is s0 = (s01 , s02 , . . . , s0K ). We say that        alize well to the test data, which accounts for the
s0k is a true positive if s0k ∈ s. The precision                relatively poor performance of both models.
and recall, then, are measured as the total num-                Semantic features
ber of true positives divided by the total num-                 We created IM lexicons from three in-house
ber of decoded and true segments respectively.                  databases on Movie, Job and National Parks.
We report the F1-measure which is computed as                   Some lexicons, e.g., IM:State, are shared across
2 · prec · recall/(prec + recall).                              domains. Regarding IH lexicons, we applied the
   Secondly, a sentence is correct if all decoded               bootstrapping algorithm described in Section 5.3
segments are true positives. Sentence accuracy is               to a 1-month query log of Bing. We selected the
measured by the total number of correct sentences               most frequent 57 and 131 phrases to form the IH
divided by the total number of sentences.                       lexicons for Movie and National Park respectively.
                                                                We do not have an IH lexicon for Job as the at-
6.3   Results
                                                                tribute names in that domain are much fewer and
We start with models that incorporate first-order               are well covered by training set examples.
transition features which are standard for both                    We implemented A3 and A4 for CRFs, which
Markov and semi-Markov CRFs. We then exper-                     are based on the n-gram sets created from lex-
iment with lexical features, semantic features and              icons; and B4 and B5 for semi-Markov CRFs,
syntactic features for both models. Table 4 and                 which are based on exact and fuzzy match with
Table 5 give a summarization of all experimental                lexicon items. As shown in Table 4 and 5, drastic
results.                                                        increases in sentence accuracies and F1-measures
                                                                were observed for both models.
Lexical features
The first experiment we did is to evaluate the per-             Syntactic features
formance of lexical features (combined with tran-               As shown in the row A1-A5 in Table 4, combined
sition features). This involves the use of A2 in Ta-            with all other features, the syntactic features (A5)
ble 2 for CRFs, and B2 and B3 for semi-Markov                   built upon POS tags boosted the CRF model per-
CRFs. Note that adding B3, i.e., indicators of                  formance. Table 6 listed the most dominant pos-
whether a query segment contains a word iden-                   itive and negative features based on POS tags for
tity, gave an absolute 7.0%/3.2% gain in sentence               Movie (features for the other two domains are not
accuracy and segment F1 on average, as shown                    reported due to space limit). We can see that
in the row B1-B3 in Table 5. For both A2 and                    many of these features make intuitive sense. For


                                                           1343


                                                  Movie          Job       National Park      Average
             Features                          Acc    F1     Acc     F1    Acc      F1      Acc     F1
             A1,A2: Tran + Lex                 59.9 75.8     65.6 84.7     61.6    75.6     62.1 78.9
             A1-A3: Tran + Lex + Sem           67.9 80.2     70.8 87.4     70.5    80.8     69.4 82.8
             A1-A4: Tran + Lex + Sem           72.4 83.5     72.4 89.7     71.1    82.3     72.2 85.0
             A1-A5: Tran + Lex + Sem + Syn     74.4 84.8     75.1 89.4     75.1    85.4     74.8 86.5
             A2-A5: Lex + Sem + Syn            64.9 78.8     68.1 81.1     64.8    83.7     65.4 81.0

       Table 4: Sentence accuracy (Acc) and segment F1 (F1) using CRFs with different features.
                                                   Movie          Job       National Park      Average
           Features                             Acc    F1     Acc     F1    Acc      F1      Acc     F1
           B1,B2: Tran + Lex                    53.4 71.6     59.6 83.8     60.0    77.3     56.7 76.9
           B1-B3: Tran + Lex                    61.3 77.7     65.9 85.9     66.0    80.7     63.7 80.1
           B1-B4: Tran + Lex + Sem              73.8 83.6     76.0 89.7     74.6    85.3     74.7 86.1
           B1-B5: Tran + Lex + Sem              75.0 84.3     76.5 89.7     76.8    86.8     75.8 86.6
           B1-B6: Tran + Lex + Sem + Syn        75.8 84.3     76.2 89.7     76.8    87.2     76.1 86.7
           B1-B5,B7: Tran + Lex + Sem + Syn     75.6 84.1     76.0 89.3     76.8    86.8     75.9 86.4
           B2-B6:Lex + Sem + Syn                72.0 82.0     73.2 87.9     76.5    89.3     73.8 85.6

Table 5: Sentence accuracy (Acc) and segment F1 (F1) using semi-Markov CRFs with different features.


example, IN (preposition or subordinating con-              and 5. This resulted in substantial degradations
junction) is a strong indicator of Other, while TO          in performance. One intuitive explanation is that
and IM:Date usually do not co-occur. Some fea-              although web queries are relatively “order-free”,
tures, however, may appear less “correct”. This             statistically speaking, some orders are much more
is largely due to the inaccurate output of the POS          likely to occur than others. This makes it benefi-
tagger. For example, a large number of actor                cial to use transition features.
names were mis-tagged as RB, resulting in a high
positive weight of the feature (RB, IM:Cast).               Comparison to syntactic analysis
                                                            Finally, we conduct a simple experiment by using
          Positive        Negative                          the heuristics described in Section 3.2 in extract-
          (IN, Other),    (TO, IM:Date)
          (VBD, Other)    (IN, IM:Cast)                     ing IHs from queries. The precision and recall of
          (CD, IM:Date)   (CD, IH)                          IHs averaged over all 3 domains are 50.4% and
          (RB, IM:Cast)   (IN, IM:Character)                32.8% respectively. The precision and recall num-
Table 6: Syntactic features with the largest posi-          bers from our best model-based system, i.e., B1-
tive/negative weights in the CRF model for Movie            B6 in Table 5, are 89.9% and 84.6% respectively,
                                                            which are significantly better than those based on
                                                            pure syntactic analysis.
   Similarly, we added segment-level POS tag fea-
tures (B6) to semi-Markov CRFs, which lead to               7 Conclusions
the best overall results as shown by the highlighted
numbers in Table 5. Again many of the dominant              In this work, we make the first attempt to define
features are consistent with our intuition. For ex-         the semantic structure of noun phrase queries. We
ample, the most positive feature for Movie is (CD           propose statistical methods to automatically ex-
JJS, IM:Rating) (e.g. 100 best). When syntactic             tract IHs, IMs and the semantic labels of IMs us-
features based on chunking results (B7) are used            ing a variety of features. Experiments show the ef-
instead of B6, the performance is not as good.              fectiveness of semantic features and syntactic fea-
                                                            tures in both Markov and semi-Markov CRF mod-
Transition features                                         els. In the future, it would be useful to explore
In addition, it is interesting to see the importance        other approaches to automatic lexicon discovery
of transition features in both models. Since web            to improve the quality or to increase the coverage
queries do not generally follow the linear order            of both IH and IM lexicons, and to systematically
principle, is it helpful to incorporate transition fea-     evaluate their impact on query understanding per-
tures in learning? To answer this question, we              formance.
dropped the transition features from the best sys-             The author would like to thank Hisami Suzuki
tems, corresponding to the last rows in Table 4             and Jianfeng Gao for useful discussions.


                                                      1344


References                                                Andrew McCallum and Wei Li. 2003. Early results for
                                                            named entity recognition with conditional random
Jaime Arguello, Fernando Diaz, Jamie Callan, and            fields, feature induction and web-enhanced lexicons.
   Jean-Francois Crespo. 2009. Sources of evidence          In Proceedings of the seventh conference on Natural
   for vertical selection. In SIGIR’09: Proceedings of      language learning at HLT-NAACL 2003, pages 188–
   the 32st Annual International ACM SIGIR confer-          191.
   ence on Research and Development in Information
   Retrieval.                                             Donald Metzler and Bruce Croft. 2005. Analysis of
                                                            statistical question classification for fact-based ques-
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
                                                            tions. Jounral of Information Retrieval, 8(3).
  The linguistic structure of English web-search
  queries. In Proceedings of the 2008 Conference on       Patrick Pantel and Marco Pennacchiotti.          2006.
  Empirical Methods in Natural Language Process-            Espresso: Leveraging generic patterns for automati-
  ing, pages 1021–1030.                                     cally har-vesting semantic relations. In Proceedings
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,         of the 21st International Conference on Computa-
  Matt Deeds, Nicole Hamilton, and Greg Hullender.          tional Linguis-tics and the 44th annual meeting of
  2005. Learning to rank using gradient descent. In         the ACL, pages 113–120.
  ICML’05: Proceedings of the 22nd international          Stelios Paparizos, Alexandros Ntoulas, John Shafer,
  conference on Machine learning, pages 89–96.               and Rakesh Agrawal. 2009. Answering web queries
Michael Collins. 1999. Head-Driven Statistical Mod-          using structured data sources. In Proceedings of the
  els for Natural Language Parsing. Ph.D. thesis,            35th SIGMOD international conference on Manage-
  University of Pennsylvania.                                ment of data.

Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun,       Marius Pasca and Benjamin Van Durme. 2007. What
   Ming Zhou, and Chang-Ning Huang. 2001. Im-              you seek is what you get: Extraction of class at-
   proving query translation for CLIR using statistical    tributes from query logs. In IJCAI’07: Proceedings
   models. In SIGIR’01: Proceedings of the 24th An-        of the 20th International Joint Conference on Artifi-
   nual International ACM SIGIR conference on Re-          cial Intelligence.
   search and Development in Information Retrieval.
                                                          Marius Pasca and Benjamin Van Durme.          2008.
Jinyoung Kim, Xiaobing Xue, and Bruce Croft. 2009.         Weakly-supervised acquisition of open-domain
   A probabilistic retrieval model for semistructured      classes and class attributes from web documents and
   data. In ECIR’09: Proceedings of the 31st Euro-         query logs. In Proceedings of ACL-08: HLT.
   pean Conference on Information Retrieval, pages
   228–239.                                               Marco Pennacchiotti and Patrick Pantel. 2009. Entity
                                                           extraction via ensemble semantics. In EMNLP’09:
John Lafferty, Andrew McCallum, and Ferdando               Proceedings of Conference on Empirical Methods in
  Pereira. 2001. Conditional random fields: Prob-          Natural Language Processing, pages 238–247.
  abilistic models for segmenting and labeling se-
  quence data. In Proceedings of the International        Stephen Robertson, Hugo Zaragoza, and Michael Tay-
  Conference on Machine Learning, pages 282–289.             lor. 2004. Simple BM25 extension to multiple
                                                             weighted fields. In CIKM’04: Proceedings of the
Fangtao Li, Xian Zhang, Jinhui Yuan, and Xiaoyan             thirteenth ACM international conference on Infor-
  Zhu. 2008a. Classifying what-type questions by             mation and knowledge management, pages 42–49.
  head noun tagging. In COLING’08: Proceedings
  of the 22nd International Conference on Computa-        Sunita Sarawagi and William W. Cohen. 2004. Semi-
  tional Linguistics, pages 481–488.                        Markov conditional random fields for information
                                                            extraction. In Advances in Neural Information Pro-
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008b. Learn-          cessing Systems (NIPS’04).
  ing query intent from regularized click graph. In
  SIGIR’08: Proceedings of the 31st Annual Interna-       Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.
  tional ACM SIGIR conference on Research and De-           2006. Building bridges for web query classification.
  velopment in Information Retrieval, July.                 In SIGIR’06: Proceedings of the 29th Annual Inter-
                                                            national ACM SIGIR conference on research and de-
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-         velopment in information retrieval, pages 131–138.
  ing structured information from user queries with
  semi-supervised conditional random fields. In SI-
  GIR’09: Proceedings of the 32st Annual Interna-
  tional ACM SIGIR conference on Research and De-
  velopment in Information Retrieva.
Mehdi Manshadi and Xiao Li. 2009. Semantic tagging
 of web search queries. In Proceedings of the 47th
 Annual Meeting of the ACL and the 4th IJCNLP of
 the AFNLP.


                                                      1345

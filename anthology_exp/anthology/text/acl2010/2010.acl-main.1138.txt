Wikipedia as Sense Inventory to Improve Diversity in Web Search Results

                       Celina Santamarı́a, Julio Gonzalo and Javier Artiles
                                            nlp.uned.es
                         UNED, c/Juan del Rosal, 16, 28040 Madrid, Spain
             celina.santamaria@gmail.com julio@lsi.uned.es javart@bec.uned.es




                      Abstract                                may not be possible or even desirable. This is
                                                              often the case with one word and even two word
    Is it possible to use sense inventories to                queries in Web search engines.
    improve Web search results diversity for                     In Web search, there are at least three ways of
    one word queries? To answer this ques-                    coping with ambiguity:
    tion, we focus on two broad-coverage lex-
    ical resources of a different nature: Word-                  • Promoting diversity in the search results
    Net, as a de-facto standard used in Word                       (Clarke et al., 2008): given the query ”oa-
    Sense Disambiguation experiments; and                          sis”, the search engine may try to include rep-
    Wikipedia, as a large coverage, updated                        resentatives for different senses of the word
    encyclopaedic resource which may have a                        (such as the Oasis band, the Organization
    better coverage of relevant senses in Web                      for the Advancement of Structured Informa-
    pages.                                                         tion Standards, the online fashion store, etc.)
                                                                   among the top results. Search engines are
    Our results indicate that (i) Wikipedia has
                                                                   supposed to handle diversity as one of the
    a much better coverage of search results,
                                                                   multiple factors that influence the ranking.
    (ii) the distribution of senses in search re-
    sults can be estimated using the internal                    • Presenting the results as a set of (labelled)
    graph structure of the Wikipedia and the                       clusters rather than as a ranked list (Carpineto
    relative number of visits received by each                     et al., 2009).
    sense in Wikipedia, and (iii) associating
    Web pages to Wikipedia senses with sim-                      • Complementing search results with search
    ple and efficient algorithms, we can pro-                      suggestions (e.g. ”oasis band”, ”oasis fash-
    duce modified rankings that cover 70%                          ion store”) that serve to refine the query in the
    more Wikipedia senses than the original                        intended way (Anick, 2003).
    search engine rankings.                                      All of them rely on the ability of the search en-
                                                              gine to cluster search results, detecting topic simi-
1   Motivation
                                                              larities. In all of them, disambiguation is implicit,
The application of Word Sense Disambiguation                  a side effect of the process but not its explicit tar-
(WSD) to Information Retrieval (IR) has been sub-             get. Clustering may detect that documents about
ject of a significant research effort in the recent           the Oasis band and the Oasis fashion store deal
past. The essential idea is that, by indexing and             with unrelated topics, but it may as well detect
matching word senses (or even meanings) , the re-             a group of documents discussing why one of the
trieval process could better handle polysemy and              Oasis band members is leaving the band, and an-
synonymy problems (Sanderson, 2000). In prac-                 other group of documents about Oasis band lyrics;
tice, however, there are two main difficulties: (i)           both are different aspects of the broad topic Oa-
for long queries, IR models implicitly perform                sis band. A perfect hierarchical clustering should
disambiguation, and thus there is little room for             distinguish between the different Oasis senses at a
improvement. This is the case with most stan-                 first level, and then discover different topics within
dard IR benchmarks, such as TREC (trec.nist.gov)              each of the senses.
or CLEF (www.clef-campaign.org) ad-hoc collec-                   Is it possible to use sense inventories to improve
tions; (ii) for very short queries, disambiguation            search results for one word queries? To answer


                                                        1357
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1357–1366,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


this question, we will focus on two broad-coverage     finding that the use of Wikipedia could substan-
lexical resources of a different nature: WordNet       tially improve diversity in the top results. In Sec-
(Miller et al., 1990), as a de-facto standard used     tion 5 we use the Wikipedia internal link structure
in Word Sense Disambiguation experiments and           and the number of visits per page to estimate rel-
many other Natural Language Processing research        ative frequencies for Wikipedia senses, obtaining
fields; and Wikipedia (www.wikipedia.org), as a        an estimation which is highly correlated with ac-
large coverage and updated encyclopedic resource       tual data in our testbed. Finally, in Section 6 we
which may have a better coverage of relevant           discuss a few strategies to classify Web pages into
senses in Web pages.                                   word senses, and apply the best classifier to en-
   Our hypothesis is that, under appropriate con-      hance diversity in search results. The paper con-
ditions, any of the above mechanisms (clustering,      cludes with a discussion of related work (Section
search suggestions, diversity) might benefit from      7) and an overall discussion of our results in Sec-
an explicit disambiguation (classification of pages    tion 8.
in the top search results) using a wide-coverage
sense inventory. Our research is focused on four       2       Test Set
relevant aspects of the problem:                       2.1       Set of Words
  1. Coverage: Are Wikipedia/Wordnet senses            The most crucial step in building our test set is
     representative of search results? Otherwise,      choosing the set of words to be considered. We
     trying to make a disambiguation in terms of a     are looking for words which are susceptible to
     fixed sense inventory would be meaningless.       form a one-word query for a Web search engine,
                                                       and therefore we should focus on nouns which
  2. If the answer to (1) is positive, the reverse     are used to denote one or more named entities.
     question is also interesting: can we estimate     At the same time we want to have some degree
     search results diversity using our sense inven-   of comparability with previous research on Word
     tories?                                           Sense Disambiguation, which points to noun sets
                                                       used in Senseval/SemEval evaluation campaigns1 .
  3. Sense frequencies: knowing sense frequen-         Our budget for corpus annotation was enough for
     cies in (search results) Web pages is crucial     two persons-month, which limited us to handle
     to have a usable sense inventory. Is it possi-    40 nouns (usually enough to establish statistically
     ble to estimate Web sense frequencies from        significant differences between WSD algorithms,
     currently available information?                  although obviously limited to reach solid figures
                                                       about the general behaviour of words in the Web).
  4. Classification: The association of Web pages
                                                          With these arguments in mind, we decided to
     to word senses must be done with some unsu-
                                                       choose: (i) 15 nouns from the Senseval-3 lexi-
     pervised algorithm, because it is not possible
                                                       cal sample dataset, which have been previously
     to hand-tag training material for every pos-
                                                       employed by (Mihalcea, 2007) in a related ex-
     sible query word. Can this classification be
                                                       periment (see Section 7); (ii) 25 additional words
     done accurately? Can it be effective to pro-
                                                       which satisfy two conditions: they are all am-
     mote diversity in search results?
                                                       biguous, and they are all names for music bands
   In order to provide an initial answer to these      in one of their senses (not necessarily the most
questions, we have built a corpus consisting of 40     salient). The Senseval set is: {argument, arm,
nouns and 100 Google search results per noun,          atmosphere, bank, degree, difference, disc, im-
manually annotated with the most appropriate           age, paper, party, performance, plan, shelter,
Wordnet and Wikipedia senses. Section 2 de-            sort, source}. The bands set is {amazon, apple,
scribes how this corpus has been created, and in       camel, cell, columbia, cream, foreigner, fox, gen-
Section 3 we discuss WordNet and Wikipedia cov-        esis, jaguar, oasis, pioneer, police, puma, rain-
erage of search results according to our testbed.      bow, shell, skin, sun, tesla, thunder, total, traffic,
As this initial results clearly discard Wordnet as     trapeze, triumph, yes}.
a sense inventory for the task, the rest of the pa-       For each noun, we looked up all its possible
per mainly focuses on Wikipedia. In Section 4 we       senses in WordNet 3.0 and in Wikipedia (using
                                                           1
estimate search results diversity from our testbed,            http://senseval.org


                                                   1358


                      Table 1: Coverage of Search Results: Wikipedia vs. WordNet
                                                 Wikipedia                                   WordNet
                                   # senses             # documents            # senses            # documents
                                available/used     assigned to some sense   available/used    assigned to some sense
                 Senseval set      242/100               877 (59%)               92/52              696 (46%)
                  Bands set        640/174              1358 (54%)              78/39               599 (24%)
                    Total          882/274              2235 (56%)              170/91             1295 (32%)




Wikipedia disambiguation pages). Wikipedia has                        an URL in the list was corrupt or not available,
an average of 22 senses per noun (25.2 in the                         it had to be discarded. We provided 150 docu-
Bands set and 16.1 in the Senseval set), and Word-                    ments per name to ensure that the figure of 100 us-
net a much smaller figure, 4.5 (3.12 for the Bands                    able documents per name could be reached with-
set and 6.13 for the Senseval set). For a conven-                     out problems.
tional dictionary, a higher ambiguity might indi-                        Each judge provided annotations for the 4,000
cate an excess of granularity; for an encyclopaedic                   documents in the final data set. In a second round,
resource such as Wikipedia, however, it is just                       they met and discussed their independent annota-
an indication of larger coverage. Wikipedia en-                       tions together, reaching a consensus judgement for
tries for camel which are not in WordNet, for in-                     every document.
stance, include the Apache Camel routing and me-
diation engine, the British rock band, the brand                      3     Coverage of Web Search Results:
of cigarettes, the river in Cornwall, and the World                         Wikipedia vs Wordnet
World War I fighter biplane.
                                                                      Table 1 shows how Wikipedia and Wordnet cover
2.2   Set of Documents                                                the senses in search results. We report each noun
We retrieved the 150 first ranked documents for                       subset separately (Senseval and bands subsets) as
each noun, by submitting the nouns as queries to a                    well as aggregated figures.
Web search engine (Google). Then, for each doc-                          The most relevant fact is that, unsurprisingly,
ument, we stored both the snippet (small descrip-                     Wikipedia senses cover much more search results
tion of the contents of retrieved document) and the                   (56%) than Wordnet (32%). If we focus on the
whole HTML document. This collection of docu-                         top ten results, in the bands subset (which should
ments contain an implicit new inventory of senses,                    be more representative of plausible web queries)
based on Web search, as documents retrieved by                        Wikipedia covers 68% of the top ten documents.
a noun query are associated with some sense of                        This is an indication that it can indeed be useful
the noun. Given that every document in the top                        for promoting diversity or help clustering search
Web search results is supposed to be highly rele-                     results: even if 32% of the top ten documents are
vant for the query word, we assume a ”one sense                       not covered by Wikipedia, it is still a representa-
per document” scenario, although we allow an-                         tive source of senses in the top search results.
notators to assign more than one sense per doc-                          We have manually examined all documents
ument. In general this assumption turned out to be                    in the top ten results that are not covered by
correct except in a few exceptional cases (such as                    Wikipedia: a majority of the missing senses con-
Wikipedia disambiguation pages): only nine docu-                      sists of names of (generally not well-known) com-
ments received more than one WordNet sense, and                       panies (45%) and products or services (26%); the
44 (1.1% of all annotated pages) received more                        other frequent type (12%) of non annotated doc-
than one Wikipedia sense.                                             ument is disambiguation pages (from Wikipedia
                                                                      and also from other dictionaries).
2.3   Manual Annotation                                                  It is also interesting to examine the degree of
We implemented an annotation interface which                          overlap between Wikipedia and Wordnet senses.
stored all documents and a short description for                      Being two different types of lexical resource,
every Wordnet and Wikipedia sense. The annota-                        they might have some degree of complementar-
tors had to decide, for every document, whether                       ity. Table 2 shows, however, that this is not the
there was one or more appropriate senses in each                      case: most of the (annotated) documents either fit
of the dictionaries. They were instructed to pro-                     Wikipedia senses (26%) or both Wikipedia and
vide annotations for 100 documents per name; if                       Wordnet (29%), and just 3% fit Wordnet only.


                                                               1359


                   Table 2: Overlap between Wikipedia and Wordnet in Search Results
                                                           # documents annotated with
                                     Wikipedia & Wordnet      Wikipedia only   Wordnet only      none
                      Senseval set        607 (40%)             270 (18%)         89 (6%)      534 (36%)
                       Bands set          572 (23%)             786 (31%)         27 (1%)     1115 (45%)
                         Total           1179 (29%)             1056 (26%)       116 (3%)     1649 (41%)




Therefore, Wikipedia seems to extend the cover-                    5    Sense Frequency Estimators for
age of Wordnet rather than providing complemen-                         Wikipedia
tary sense information. If we wanted to extend the
coverage of Wikipedia, the best strategy seems to                  Wikipedia disambiguation pages contain no sys-
be to consider lists of companies, products and ser-               tematic information about the relative importance
vices, rather than complementing Wikipedia with                    of senses for a given word. Such information,
additional sense inventories.                                      however, is crucial in a lexicon, because sense dis-
                                                                   tributions tend to be skewed, and knowing them
                                                                   can help disambiguation algorithms.
4   Diversity in Google Search Results                                We have attempted to use two estimators of ex-
                                                                   pected sense distribution:
Once we know that Wikipedia senses are a rep-
resentative subset of actual Web senses (covering                      • Internal relevance of a word sense, measured
more than half of the documents retrieved by the                         as incoming links for the URL of a given
search engine), we can test how well search results                      sense in Wikipedia.
respect diversity in terms of this subset of senses.
                                                                       • External relevance of a word sense, measured
   Table 3 displays the number of different senses
                                                                         as the number of visits for the URL of a given
found at different depths in the search results rank,
                                                                         sense (as reported in http://stats.grok.se).
and the average proportion of total senses that they
represent. These results suggest that diversity is                    The number of internal incoming links is ex-
not a major priority for ranking results: the top                  pected to be relatively stable for Wikipedia arti-
ten results only cover, in average, 3 Wikipedia                    cles. As for the number of visits, we performed
senses (while the average number of senses listed                  a comparison of the number of visits received by
in Wikipedia is 22). When considering the first                    the bands noun subset in May, June and July 2009,
100 documents, this number grows up to 6.85                        finding a stable-enough scenario with one notori-
senses per noun.                                                   ous exception: the number of visits to the noun
  Another relevant figure is the frequency of the                  Tesla raised dramatically in July, because July 10
most frequent sense for each word: in average,                     was the anniversary of the birth of Nicola Tesla,
63% of the pages in search results belong to the                   and a special Google logo directed users to the
most frequent sense of the query word. This is                     Wikipedia page for the scientist.
roughly comparable with most frequent sense fig-                      We have measured correlation between the rela-
ures in standard annotated corpora such as Sem-                    tive frequencies derived from these two indicators
cor (Miller et al., 1993) and the Senseval/Semeval                 and the actual relative frequencies in our testbed.
data sets, which suggests that diversity may not                   Therefore, for each noun w and for each sense wi ,
play a major role in the current Google ranking al-                we consider three values: (i) proportion of doc-
gorithm.                                                           uments retrieved for w which are manually as-
   Of course this result must be taken with care,                  signed to each sense wi ; (ii) inlinks(wi ): rela-
because variability between words is high and un-                  tive amount of incoming links to each sense wi ;
predictable, and we are using only 40 nouns for                    and (iii) visits(wi ): relative number of visits to the
our experiment. But what we have is a positive                     URL for each sense wi .
indication that Wikipedia could be used to im-                        We have measured the correlation between
prove diversity or cluster search results: poten-                  these three values using a linear regression corre-
tially the first top ten results could cover 6.15 dif-             lation coefficient, which gives a correlation value
ferent senses in average (see Section 6.5), which                  of .54 for the number of visits and of .71 for the
would be a substantial growth.                                     number of incoming links. Both estimators seem


                                                             1360


                       Table 3: Diversity in Search Results according to Wikipedia
                                      average # senses in search results     average coverage of Wikipedia senses
                                     Bands set     Senseval set    Total     Bands set    Senseval set    Total
                     First 10 docs      2.88            3.2         3.00        .21           .21           .21
                     First 25           4.44            4.8         4.58        .28           .33           .30
                     First 50           5.56           5.47         5.53        .33           .36           .34
                     First 75           6.56           6.33         6.48        .37           .43           .39
                     First 100          6.96           6.67         6.85        .38           .45           .41




to be positively correlated with real relative fre-                        We also compute two baselines:
quencies in our testbed, with a strong preference
for the number of links.                                                   • A random assignment of senses (precision is
   We have experimented with weighted combina-                               computed as the inverse of the number of
tions of both indicators, using weights of the form                          senses, for every test case).
(k, 1 − k), k ∈ {0, 0.1, 0.2 . . . 1}, reaching a max-
                                                                           • A most frequent sense heuristic which uses
imal correlation of .73 for the following weights:
                                                                             our estimation of sense frequencies and as-
                                                                             signs the same sense (the most frequent) to
 freq(wi ) = 0.9∗inlinks(wi )+0.1∗visits(wi ) (1)                            all documents.

   This weighted estimator provides a slight ad-                       Both are naive baselines, but it must be noted
vantage over the use of incoming links only (.73                     that the most frequent sense heuristic is usually
vs .71). Overall, we have an estimator which has                     hard to beat for unsupervised WSD algorithms in
a strong correlation with the distribution of senses                 most standard data sets.
in our testbed. In the next section we will test its                   We now describe each of the two main ap-
utility for disambiguation purposes.                                 proaches in detail.

6    Association of Wikipedia Senses to                              6.1       VSM Approach
     Web Pages                                                       For each word sense, we represent its Wikipedia
                                                                     page in a (unigram) vector space model, assigning
We want to test whether the information provided                     standard tf*idf weights to the words in the docu-
by Wikipedia can be used to classify search results                  ment. idf weights are computed in two different
accurately. Note that we do not want to consider                     ways:
approaches that involve a manual creation of train-
ing material, because they can’t be used in prac-                          1. Experiment VSM computes inverse docu-
tice.                                                                         ment frequencies in the collection of re-
   Given a Web page p returned by the search                                  trieved documents (for the word being con-
engine for the query w, and the set of senses                                 sidered).
w1 . . . wn listed in Wikipedia, the task is to assign
the best candidate sense to p. We consider two                             2. Experiment VSM-GT uses the statistics pro-
different techniques:                                                         vided by the Google Terabyte collection
                                                                              (Brants and Franz, 2006), i.e. it replaces the
    • A basic Information Retrieval approach,                                 collection of documents with statistics from a
      where the documents and the Wikipedia                                   representative snapshot of the Web.
      pages are represented using a Vector Space
      Model (VSM) and compared with a standard                             3. Experiment VSM-mixed combines statistics
      cosine measure. This is a basic approach                                from the collection and from the Google
      which, if successful, can be used efficiently                           Terabyte collection, following (Chen et al.,
      to classify search results.                                             2009).

    • An approach based on a state-of-the-art su-                       The document p is represented in the same vec-
      pervised WSD system, extracting training ex-                   tor space as the Wikipedia senses, and it is com-
      amples automatically from Wikipedia con-                       pared with each of the candidate senses wi via the
      tent.                                                          cosine similarity metric (we have experimented


                                                               1361


with other similarity metrics such as χ2 , but dif-      pick up the first sense listed in the Wikipedia dis-
ferences are irrelevant). The sense with the high-       ambiguation page.
est similarity to p is assigned to the document. In         We have also experimented with a variant of
case of ties (which are rare), we pick the first sense   the approach that uses our estimation of sense fre-
in the Wikipedia disambiguation page (which in           quencies, similarly to what we did with the VSM
practice is like a random decision, because senses       approach. In this case, (i) when there is a tie be-
in disambiguation pages do not seem to be ordered        tween two or more senses (which is much more
according to any clear criteria).                        likely than in the VSM approach), we pick up the
   We have also tested a variant of this approach        sense with the highest frequency according to our
which uses the estimation of sense frequencies           estimator; and (ii) when no sense reaches 30% of
presented above: once the similarities are com-          the cases in the page to be disambiguated, we also
puted, we consider those cases where two or more         resort to the most frequent sense heuristic (among
senses have a similar score (in particular, all senses   the candidates for the page). This experiment is
with a score greater or equal than 80% of the high-      called TiMBL-core+freq (we discarded ”inlinks”
est score). In that cases, instead of using the small    and ”all” versions because they were clearly worse
similarity differences to select a sense, we pick up     than ”core”).
the one which has the largest frequency according
to our estimator. We have applied this strategy to       6.3   Classification Results
the best performing system, VSM-GT, resulting in         Table 4 shows classification results. The accuracy
experiment VSM-GT+freq.                                  of systems is reported as precision, i.e. the number
                                                         of pages correctly classified divided by the total
6.2   WSD Approach
                                                         number of predictions. This is approximately the
We have used TiMBL (Daelemans et al., 2001),             same as recall (correctly classified pages divided
a state-of-the-art supervised WSD system which           by total number of pages) for our systems, because
uses Memory-Based Learning. The key, in this             the algorithms provide an answer for every page
case, is how to extract learning examples from the       containing text (actual coverage is 94% because
Wikipedia automatically. For each word sense, we         some pages only contain text as part of an image
basically have three sources of examples: (i) oc-        file such as photographs and logotypes).
currences of the word in the Wikipedia page for
the word sense; (ii) occurrences of the word in
                                                                   Table 4: Classification Results
Wikipedia pages pointing to the page for the word
sense; (iii) occurrences of the word in external            Experiment                          Precision
pages linked in the Wikipedia page for the word
                                                            random                                    .19
sense.
                                                            most frequent sense (estimation)          .46
   After an initial manual inspection, we decided
to discard external pages for being too noisy, and          TiMBL-core                                .60
we focused on the first two options. We tried three         TiMBL-inlinks                             .50
alternatives:                                               TiMBL-all                                 .58
                                                            TiMBL-core+freq                           .67
  • TiMBL-core uses only the examples found
                                                            VSM                                       .67
    in the page for the sense being trained.
                                                            VSM-GT                                    .68
  • TiMBL-inlinks uses the examples found in                VSM-mixed                                 .67
    Wikipedia pages pointing to the sense being             VSM-GT+freq                               .69
    trained.

  • TiMBL-all uses both sources of examples.                All systems are significantly better than the
                                                         random and most frequent sense baselines (using
  In order to classify a page p with respect to the      p < 0.05 for a standard t-test). Overall, both ap-
senses for a word w, we first disambiguate all oc-       proaches (using TiMBL WSD machinery and us-
currences of w in the page p. Then we choose the         ing VSM) lead to similar results (.67 vs. .69),
sense which appears most frequently in the page          which would make VSM preferable because it is
according to TiMBL results. In case of ties we           a simpler and more efficient approach. Taking a


                                                     1362


            Figure 1: Precision/Coverage curves for VSM-GT+freq classification algorithm


closer look at the results with TiMBL, there are a     improvement (.69). Comparing the baseline VSM
couple of interesting facts:                           with the optimal setting (VSM-GT+freq), the dif-
                                                       ference is small (.67 vs .69) but relatively robust
  • There is a substantial difference between us-      (p = 0.066 according to the t-test).
    ing only examples taken from the Wikipedia            Remarkably, the use of frequency estimations
    Web page for the sense being trained               is very helpful for the WSD approach but not for
    (TiMBL-core, .60) and using examples from          the SVM one, and they both end up with similar
    the Wikipedia pages pointing to that page          performance figures; this might indicate that using
    (TiMBL-inlinks, .50). Examples taken from          frequency estimations is only helpful up to certain
    related pages (even if the relationship is close   precision ceiling.
    as in this case) seem to be too noisy for the
    task. This result is compatible with findings      6.4   Precision/Coverage Trade-off
    in (Santamarı́a et al., 2003) using the Open       All the above experiments are done at maximal
    Directory Project to extract examples auto-        coverage, i.e., all systems assign a sense for every
    matically.                                         document in the test collection (at least for every
  • Our estimation of sense frequencies turns          document with textual content). But it is possible
    out to be very helpful for cases where our         to enhance search results diversity without anno-
    TiMBL-based algorithm cannot provide an            tating every document (in fact, not every document
    answer: precision rises from .60 (TiMBL-           can be assigned to a Wikipedia sense, as we have
    core) to .67 (TiMBL-core+freq). The differ-        discussed in Section 3). Thus, it is useful to inves-
    ence is statistically significant (p < 0.05) ac-   tigate which is the precision/coverage trade-off in
    cording to the t-test.                             our dataset. We have experimented with the best
                                                       performing system (VSM-GT+freq), introducing
   As for the experiments with VSM, the varia-         a similarity threshold: assignment of a document
tions tested do not provide substantial improve-       to a sense is only done if the similarity of the doc-
ments to the baseline (which is .67). Using idf fre-   ument to the Wikipedia page for the sense exceeds
quencies obtained from the Google Terabyte cor-        the similarity threshold.
pus (instead of frequencies obtained from the set         We have computed precision and coverage for
of retrieved documents) provides only a small im-      every threshold in the range [0.00 − 0.90] (beyond
provement (VSM-GT, .68), and adding the esti-          0.90 coverage was null) and represented the results
mation of sense frequencies gives another small        in Figure 1 (solid line). The graph shows that we


                                                   1363


can classify around 20% of the documents with a
                                                       Table 5: Enhancement of Search Results Diversity
precision above .90, and around 60% of the docu-
ments with a precision of .80.                            rank@10                   # senses   coverage
   Note that we are reporting disambiguation re-          Original rank               2.80       49%
sults using a conventional WSD test set, i.e., one        Wikipedia                   4.75       77%
in which every test case (every document) has             clustering (centroids)      2.50       42%
been manually assigned to some Wikipedia sense.           clustering (top ranked)     2.80       46%
But in our Web Search scenario, 44% of the                random                      2.45       43%
documents were not assigned to any Wikipedia              upper bound                 6.15       97%
sense: in practice, our classification algorithm
would have to cope with all this noise as well.
Figure 1 (dotted line) shows how the preci-                  as one of the top ten documents in the new
sion/coverage curve is affected when the algo-               rank.
rithm attempts to disambiguate all documents re-
trieved by Google, whether they can in fact be as-        • clustering (top ranked): Applies the same
signed to a Wikipedia sense or not. At a coverage           clustering algorithm, but this time the top
of 20%, precision drops approximately from .90 to           ranked document (in the original Google
.70, and at a coverage of 60% it drops from .80 to          rank) of each cluster is selected.
.50. We now address the question of whether this
                                                          • random: Randomly selects ten documents
performance is good enough to improve search re-
                                                            from the set of retrieved results.
sults diversity in practice.
                                                          • upper bound: This is the maximal diversity
6.5   Using Classification to Promote Diversity
                                                            that can be obtained in our testbed. Note that
We now want to estimate how the reported clas-              coverage is not 100%, because some words
sification accuracy may perform in practice to en-          have more than ten meanings in Wikipedia
hance diversity in search results. In order to pro-         and we are only considering the top ten doc-
vide an initial answer to this question, we have            uments.
re-ranked the documents for the 40 nouns in our
testbed, using our best classifier (VSM-GT+freq)          All experiments have been applied on the full
and making a list of the top-ten documents with        set of documents in the testbed, including those
the primary criterion of maximising the number         which could not be annotated with any Wikipedia
of senses represented in the set, and the secondary    sense. Coverage is computed as the ratio of senses
criterion of maximising the similarity scores of the   that appear in the top ten results compared to the
documents to their assigned senses. The algorithm      number of senses that appear in all search results.
proceeds as follows: we fill each position in the         Results are presented in Table 5. Note that di-
rank (starting at rank 1), with the document which     versity in the top ten documents increases from
has the highest similarity to some of the senses       an average of 2.80 Wikipedia senses represented
which are not yet represented in the rank; once all    in the original search engine rank, to 4.75 in the
senses are represented, we start choosing a second     modified rank (being 6.15 the upper bound), with
representative for each sense, following the same      the coverage of senses going from 49% to 77%.
criterion. The process goes on until the first ten     With a simple VSM algorithm, the coverage of
documents are selected.                                Wikipedia senses in the top ten results is 70%
   We have also produced a number of alternative       larger than in the original ranking.
rankings for comparison purposes:                         Using Wikipedia to enhance diversity seems to
                                                       work much better than clustering: both strategies
  • clustering (centroids): this method ap-            to select a representative from each cluster are un-
    plies Hierarchical Agglomerative Clustering        able to improve the diversity of the original rank-
    – which proved to be the most competitive          ing. Note, however, that our evaluation has a bias
    clustering algorithm in a similar task (Artiles    towards using Wikipedia, because only Wikipedia
    et al., 2009) – to the set of search results,      senses are considered to estimate diversity.
    forcing the algorithm to create ten clusters.         Of course our results do not imply that the
    The centroid of each cluster is then selected      Wikipedia modified rank is better than the original


                                                   1364


Google rank: there are many other factors that in-      automatic cluster labeling, finding that Wikipedia
fluence the final ranking provided by a search en-      labels agree with manual labels associated by hu-
gine. What our results indicate is that, with simple    mans to a cluster, much more than with signif-
and efficient algorithms, Wikipedia can be used as      icant terms that are extracted directly from the
a reference to improve search results diversity for     text. In a similar line, both (Gabrilovich and
one-word queries.                                       Markovitch, 2007) and (Syed et al., 2008) provide
                                                        evidence suggesting that categories of Wikipedia
7   Related Work                                        articles can successfully describe common con-
                                                        cepts in documents.
Web search results clustering and diversity in             In the field of Natural Language Processing,
search results are topics that receive an increas-      there has been successful attempts to connect
ing attention from the research community. Diver-       Wikipedia entries to Wordnet senses: (Ruiz-
sity is used both to represent sub-themes in a broad    Casado et al., 2005) reports an algorithm that
topic, or to consider alternative interpretations for   provides an accuracy of 84%. (Mihalcea, 2007)
ambiguous queries (Agrawal et al., 2009), which         uses internal Wikipedia hyperlinks to derive sense-
is our interest here. Standard IR test collections do   tagged examples. But instead of using Wikipedia
not usually consider ambiguous queries, and are         directly as sense inventory, Mihalcea then manu-
thus inappropriate to test systems that promote di-     ally maps Wikipedia senses into Wordnet senses
versity (Sanderson, 2008); it is only recently that     (claiming that, at the time of writing the paper,
appropriate test collections are being built, such as   Wikipedia did not consistently report ambiguity
(Paramita et al., 2009) for image search and (Ar-       in disambiguation pages) and shows that a WSD
tiles et al., 2009) for person name search. We see      system based on acquired sense-tagged examples
our testbed as complementary to these ones, and         reaches an accuracy well beyond an (informed)
expect that it can contribute to foster research on     most frequent sense heuristic.
search results diversity.
   To our knowledge, Wikipedia has not explicitly       8   Conclusions
been used before to promote diversity in search
results; but in (Gollapudi and Sharma, 2009), it        We have investigated whether generic lexical re-
is used as a gold standard to evaluate diversifica-     sources can be used to promote diversity in Web
tion algorithms: given a query with a Wikipedia         search results for one-word, ambiguous queries.
disambiguation page, an algorithm is evaluated as       We have compared WordNet and Wikipedia and
promoting diversity when different documents in         arrived to a number of conclusions: (i) unsurpris-
the search results are semantically similar to dif-     ingly, Wikipedia has a much better coverage of
ferent Wikipedia pages (describing the alternative      senses in search results, and is therefore more ap-
senses of the query). Although semantic similarity      propriate for the task; (ii) the distribution of senses
is measured automatically in this work, our results     in search results can be estimated using the in-
confirm that this evaluation strategy is sound, be-     ternal graph structure of the Wikipedia and the
cause Wikipedia senses are indeed representative        relative number of visits received by each sense
of search results.                                      in Wikipedia, and (iii) associating Web pages to
   (Clough et al., 2009) analyses query diversity in    Wikipedia senses with simple and efficient algo-
a Microsoft Live Search, using click entropy and        rithms, we can produce modified rankings that
query reformulation as diversity indicators. It was     cover 70% more Wikipedia senses than the orig-
found that at least 9.5% - 16.2% of queries could       inal search engine rankings.
benefit from diversification, although no correla-         We expect that the testbed created for this re-
tion was found between the number of senses of a        search will complement the - currently short - set
word in Wikipedia and the indicators used to dis-       of benchmarking test sets to explore search re-
cover diverse queries. This result does not discard,    sults diversity and query ambiguity. Our testbed
however, that queries where applying diversity is       is publicly available for research purposes at
useful cannot benefit from Wikipedia as a sense         http://nlp.uned.es.
inventory.                                                 Our results endorse further investigation on the
   In the context of clustering, (Carmel et al.,        use of Wikipedia to organize search results. Some
2009) successfully employ Wikipedia to enhance          limitations of our research, however, must be


                                                    1365


noted: (i) the nature of our testbed (with every        C. Clarke, M. Kolla, G. Cormack, O. Vechtomova,
search result manually annotated in terms of two          A. Ashkan, S. Büttcher, and I. MacKinnon. 2008.
                                                          Novelty and Diversity in Information Retrieval Eval-
sense inventories) makes it too small to extract
                                                          uation. In Proc. SIGIR’08, pages 659–666. ACM.
solid conclusions on Web searches (ii) our work
does not involve any study of diversity from the        P. Clough, M. Sanderson, M. Abouammoh, S. Navarro,
point of view of Web users (i.e. when a Web                and M. Paramita. 2009. Multiple Approaches to
                                                           Analysing Query Diversity. In Proc. of SIGIR 2009.
query addresses many different use needs in prac-          ACM.
tice); research in (Clough et al., 2009) suggests
that word ambiguity in Wikipedia might not be re-       W. Daelemans, J. Zavrel, K. van der Sloot, and
                                                          A. van den Bosch. 2001. TiMBL: Tilburg Memory
lated with diversity of search needs; (iii) we have
                                                          Based Learner, version 4.0, Reference Guide. Tech-
tested our classifiers with a simple re-ordering of       nical report, University of Antwerp.
search results to test how much diversity can be
improved, but a search results ranking depends on       E. Gabrilovich and S. Markovitch. 2007. Computing
                                                           Semantic Relatedness using Wikipedia-based Ex-
many other factors, some of them more crucial              plicit Semantic Analysis. In Proceedings of The
than diversity; it remains to be tested how can we         20th International Joint Conference on Artificial In-
use document/Wikipedia associations to improve             telligence (IJCAI), Hyderabad, India.
search results clustering (for instance, providing      S. Gollapudi and A. Sharma. 2009. An Axiomatic Ap-
seeds for the clustering process) and to provide           proach for Result Diversification. In Proc. WWW
search suggestions.                                        2009, pages 381–390. ACM New York, NY, USA.

Acknowledgments                                         R. Mihalcea. 2007. Using Wikipedia for Automatic
                                                          Word Sense Disambiguation. In Proceedings of
This work has been partially funded by the Span-          NAACL HLT, volume 2007.
ish Government (project INES/Text-Mess) and the         G. Miller, C. R. Beckwith, D. Fellbaum, Gross, and
Xunta de Galicia.                                         K. Miller. 1990. Wordnet: An on-line lexical
                                                          database. International Journal of Lexicograph,
References                                                3(4).
R. Agrawal, S. Gollapudi, A. Halverson, and S. Leong.   G.A Miller, C. Leacock, R. Tengi, and Bunker R. T.
   2009. Diversifying Search Results. In Proc. of         1993. A Semantic Concordance. In Proceedings of
   WSDM’09. ACM.                                          the ARPA WorkShop on Human Language Technol-
                                                          ogy. San Francisco, Morgan Kaufman.
P. Anick. 2003. Using Terminological Feedback for
   Web Search Refinement : a Log-based Study. In        M. Paramita, M. Sanderson, and P. Clough. 2009. Di-
   Proc. ACM SIGIR 2003, pages 88–95. ACM New             versity in Photo Retrieval: Overview of the Image-
   York, NY, USA.                                         CLEFPhoto task 2009. CLEF working notes, 2009.

J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS       M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
   2 Evaluation Campaign: overview of the Web Peo-        Automatic Assignment of Wikipedia Encyclopaedic
   ple Search Clustering Task. In 2nd Web People          Entries to Wordnet Synsets. Advances in Web Intel-
   Search Evaluation Workshop (WePS 2009), 18th           ligence, 3528:380–386.
   WWW Conference. 2009.
                                                        M. Sanderson. 2000. Retrieving with Good Sense. In-
T. Brants and A. Franz. 2006. Web 1T 5-gram, version      formation Retrieval, 2(1):49–69.
   1. Philadelphia: Linguistic Data Consortium.
                                                        M. Sanderson. 2008. Ambiguous Queries: Test Col-
D. Carmel, H. Roitman, and N. Zwerdling. 2009. En-        lections Need More Sense. In Proceedings of the
  hancing Cluster Labeling using Wikipedia. In Pro-       31st annual international ACM SIGIR conference on
  ceedings of the 32nd international ACM SIGIR con-       Research and development in information retrieval,
  ference on Research and development in information      pages 499–506. ACM New York, NY, USA.
  retrieval, pages 139–146. ACM.
                                                        C. Santamarı́a, J. Gonzalo, and F. Verdejo. 2003.
C. Carpineto, S. Osinski, G. Romano, and Dawid            Automatic Association of Web Directories to Word
  Weiss. 2009. A Survey of Web Clustering Engines.        Senses. Computational Linguistics, 29(3):485–502.
  ACM Computing Surveys, 41(3).
                                                        Z. S. Syed, T. Finin, and Joshi. A. 2008. Wikipedia
Y. Chen, S. Yat Mei Lee, and C. Huang. 2009.               as an Ontology for Describing Documents. In Proc.
  PolyUHK: A Robust Information Extraction System          ICWSM’08.
  for Web Personal Names. In Proc. WWW’09 (WePS-
  2 Workshop). ACM.


                                                    1366

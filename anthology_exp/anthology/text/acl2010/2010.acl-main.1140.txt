      Generating Fine-Grained Reviews of Songs From Album Reviews

                                 Swati Tata and Barbara Di Eugenio
                                     Computer Science Department
                                 University of Illinois, Chicago, IL, USA
                                 {stata2 | bdieugen}@uic.edu



                      Abstract                                reviews available for each individual product in-
                                                              creases, RSs may overwhelm the user if they make
    Music Recommendation Systems often                        all those reviews available. Additionally, in some
    recommend individual songs, as opposed                    reviews only few sentences actually describe the
    to entire albums. The challenge is to gen-                recommended product, hence, the interest in opin-
    erate reviews for each song, since only full              ion mining and in summarizing those reviews.
    album reviews are available on-line. We                      A Music RS could be developed along the lines
    developed a summarizer that combines in-                  of Product RSs. However, Music RSs recom-
    formation extraction and generation tech-                 mend individual tracks, not full albums, e.g. see
    niques to produce summaries of reviews of                 www.itunes.com. Summarizing reviews be-
    individual songs. We present an intrinsic                 comes more complex: available data consists of
    evaluation of the extraction components,                  album reviews, not individual song reviews (www.
    and of the informativeness of the sum-                    amazon.com, www.epinions.com). Com-
    maries; and a user study of the impact of                 ments about a given song are fragmented all over
    the song review summaries on users’ de-                   an album review. Though some web-sites like
    cision making processes. Users were able                  www.last.fm allow users to comment on indi-
    to make quicker and more informed deci-                   vidual songs, the comments are too short (a few
    sions when presented with the summary as                  words such as “awesome song”) to be counted as
    compared to the full album review.                        a full review.
                                                                 In this paper, after presenting related work and
1   Introduction                                              contrasting it to our goals in Section 2, we discuss
In recent years, the personal music collection of             our prototype Music RS in Section 3. We devote
many individuals has significantly grown due to               Section 4 to our summarizer, that extracts com-
the availability of portable devices like MP3 play-           ments on individual tracks from album reviews
ers and of internet services. Music listeners are             and produces a summary of those comments for
now looking for techniques to help them man-                  each individual track recommended to the user.
age their music collections and explore songs they            In Section 5, we report two types of evaluation: an
may not even know they have (Clema, 2006).                    intrinsic evaluation of the extraction components,
Currently, most of those electronic devices follow            and of the coverage of the summary; an extrinsic
a Universal Plug and Play (UPNP) protocol (UPN,               evaluation via a between-subject study. We found
2008), and can be used in a simple network, on                that users make quicker and more informed deci-
which the songs listened to can be monitored. Our             sions when presented with the song review sum-
interest is in developing a Music Recommendation              maries as opposed to the full album review.
System (Music RS) for such a network.
                                                              2   Related Work
   Commercial web-sites such as Amazon (www.
amazon.com) and Barnes and Nobles (www.                       Over the last decade, summarization has become
bnn.com) have deployed Product Recommen-                      a hot topic for research. Quite a few systems were
dation Systems (Product RS) to help customers                 developed for different tasks, including multi-
choose from large catalogues of products. Most                document summarization (Barzilay and McKe-
Product RSs include reviews from customers who                own, 2005; Soubbotin and Soubbotin, 2005; Nas-
bought or tried the product. As the number of                 tase, 2008).


                                                        1376
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1376–1385,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                              is referred to in the review. Titles are often ab-
  What’s not to get? Yes, Maxwell, and Octopus are a
  bit silly! ...                                              breviated, and in different ways, even in the same
  .....                                                       review – e.g. see Octopus for Octopus’s Garden
  “Something” and “Here Comes The Sun” are two of             in Figure 1. Additionally, song titles need not be
  George’s best songs ever (and “Something” may be
  the single greatest love song ever). “Oh Darling” is        noun phrases and hence NP extraction algorithms
  a bluesy masterpiece with Paul screaming.....               miss many occurrences, as was shown by prelimi-
  .......
  “Come Together” contains a great riff, but he ended up
                                                              nary experiments we ran.
  getting sued over the lyrics by Chuck Berry......           c) Reviewers focus on both inherent features such
                                                              as lyrics, genre and instruments, but also on people
                                                              (artist, lyricist, producer etc.), unlike in product
 Figure 1: A sample review for the album “Abbey Road”         reviews where manufacturer/designer are rarely
                                                              mentioned. This variety of features makes it
                                                              harder to generate a coherent summary.
   Whereas summarizing customer reviews can
be seen as multi-document summarization, an
                                                              3   SongRecommend: Prototype Music RS
added necessary step is to first extract the most
important features customers focus on. Hence,                 Figure 2 shows the interface of our prototype Mu-
summarizing customer reviews has mostly been                  sic RS. It is a simple interface dictated by our fo-
studied as a combination of machine learning                  cus on the summarization process (but it was in-
and NLP techniques (Hu and Liu, 2004; Ga-                     formed by a small pilot study). Moving from win-
mon et al., 2005). For example, (Hu and Liu,                  dow to window and from top to bottom:
2004) use associative mining techniques to iden-              a) The top leftmost window shows different de-
tify features that frequently occur in reviews                vices on which the user listens to songs. These
taken from www.epinions.com and www.                          devices are monitored with a UPNP control point.
amazon.com. Then, features are paired to the                  Based on the messages received by the control
nearest words that express some opinion on that               point, the user activities, including the metadata
feature. Most work on product reviews focuses                 of the song, are logged.
on identifying sentences and polarity of opinion              b) Once the user chooses a certain song on one of
terms, not on generating a coherent summary from              the devices (see second window on top), we dis-
the extracted features, which is the main goal                play more information about the song (third top
of our research. Exceptions are (Carenini et al.,             window); we also identify related songs from the
2006; Higashinaka et al., 2006), whose focus was              internet, including: other songs from the same al-
on extracting domain specific ontologies in order             bum, popular songs of the artist and popular songs
to structure summarization of customer reviews.               of related artists, as obtained from Yahoo Music.
   Summarizing reviews on objects different from              c) The top 25 recommendations are shown in the
products, such as restaurants (Nguyen et al.,                 fourth top window. We use the SimpleKMeans
2007), or movies (Zhuang et al., 2006), has also              Clustering (Mitchell, 1997) to identify and rank
been tackled, although not as extensively. We                 the top twenty-five songs which belong to the
are aware of only one piece of work that focuses              same cluster and are closest to the given song.
on music reviews (Downie and Hu, 2006). This                  Closeness between two songs in a cluster is mea-
study is mainly concerned with identifying de-                sured as the number of attributes (album, artist etc)
scriptive patterns in positive or negative reviews            of the songs that match.
but not on summarizing the reviews.                           d) When the user clicks on More Info for one of
                                                              the recommended songs, the pop-up, bottom win-
2.1   Summarizing song reviews is different                   dow is displayed, which contains the summary of
                                                              the reviews for the specific song.
As mentioned earlier, using album reviews for
song summarization poses new challenges:                      4   Extraction and Summarization
a) Comments on features of a song are embed-
ded and fragmented within the album reviews, as               Our summarization framework consists of the five
shown in Figure 1. It is necessary to correctly map           tasks illustrated in Figure 3. The first two tasks
features to songs.                                            pertain to information extraction, the last three to
b) Each song needs to be identified each time it              repackaging the information and generating a co-


                                                           1377


                       Figure 2: SongRecommend Interface


                                         herent summary. Whereas the techniques we use
                                         for each individual step are state-of-the-art, our ap-
                                         proach is innovative in that it integrates them into
                                         an effective end-to-end system. Its effectiveness is
                                         shown by the promising results obtained both via
                                         the intrinsic evaluation, and the user study. Our
                                         framework can be applied to any domain where
                                         reviews of individual components need to be sum-
                                         marized from reviews of collections, such as re-
                                         views of different hotels and restaurants in a city.
                                            Our corpus was opportunistically col-
                                         lected      from      www.amazon.com              and
                                         www.epinions.com.              It consists of 1350
                                         album reviews across 27 albums (50 reviews
                                         per album). 50 randomly chosen reviews were
                                         used for development. Reviews have noise, since
                                         the writing is informal. We did not clean it, for
                                         example we did not correct spelling mistakes.
                                         This corpus was annotated for song titles and song
                                         features. Feature annotation consists of marking
                                         a phrase as a feature and matching it with the song
                                         to which the feature is attributed. Note that we
                                         have no a priori inventory of features; what counts
Figure 3: Summarization Pipeline         as features of songs emerged from the annotation,
                                         since annotators were asked to annotate for noun
                                         phrases which contain “any song related term or
                                         terms spoken in the context of a song”. Further,
                                         they were given about 5 positive and 5 negative


                                     1378


                                                         bum. Given the reviews for an album and the list
  What’s     not   to   get?          Yes,    <song
  id=3>Maxwell</song>,             and        <song      of songs in that album, first, we build a lexicon of
  id=5>Octopus</song> are a bit silly! ...               all the words in the song titles. We also segment
  .........                                              the reviews into sentences via sentence boundary
  .........
  <song id=2>“Something”</song> and <song                detection. All 1,2,3,4-grams for each sentence (the
  id=7>“Here Comes The Sun”</song> are two of            upper-bound 4 was determined experimentally) in
  <feature id=(2,7)>George’s</feature> best songs
  ever (and <song id=2>“Something”</song> may be
                                                         the review are generated. First, n-grams that con-
  ......                                                 tain at least one word with an edit distance greater
  <song id=4>“Oh Darling”</song> is a <feature           than one from a word in the lexicon are filtered
  id=4>bluesy masterpiece</feature> with <feature
  id=4>Paul</feature> screaming......                    out. Second, if higher and lower order n-grams
  .....                                                  overlap at the same position in the same sentence,
  <song id=1>“Come Together”</song> contains a           lower order n-grams are filtered out. Third, the
  great <feature id=1>riff</feature>, but ...
                                                         n-grams are merged if they occur sequentially in
                                                         a sentence. Fourth, the n-grams are further fil-
         Figure 4: A sample annotated review             tered to include only those where (i) the n-gram is
                                                         within quotation marks; and/or (ii) the first char-
                                                         acter of each word in the n-gram is upper case.
examples of features. Figure 4 shows annotations         This filters n-grams such as those shown in sen-
for the excerpt in Figure 1. For example in              tence (b) above. All the n-grams remaining at this
Figure 4, George, Paul, bluesy masterpiece and           point are potential song titles. Finally, for each
riff have been marked as features. Ten randomly          n-gram, we retrieve the set of IDs for each of its
chosen reviews were doubly annotated for song            words and intersect those sets. This intersection
titles and features. The Kappa co-efficient of           always resulted in one single song ID, since song
agreement on both was excellent (0.9), hence the         titles in each album differ by at least one content
rest of the corpus was annotated by one annotator        word. Recall that the algorithm is run on reviews
only. The two annotators were considered to be in        for each album separately.
agreement on a feature if they marked the same
head of phrase and attributed it to the same song.       4.2   Feature Extraction
    We will now turn to describing the component
tasks. The algorithms are described in full in (Tata,    Once the song titles are identified in the album re-
2010).                                                   view, sentences with song titles are used as an-
                                                         chors to (1) identify segments of texts that talk
4.1   Title Extraction                                   about a specific song, and then (2) extract the fea-
Song identification is the first step towards sum-       ture(s) that the pertinent text segment discusses.
marization of reviews. We identify a string of              The first step roughly corresponds to identify-
words as the title of a song to be extracted from        ing the flow of topics in a review. The second step
an album review if it (1) includes some or all the       corresponds to identifying the properties of each
words in the title of a track of that album, and (2)     song. Both steps would greatly benefit from ref-
this string occurs in the right context. Constraint      erence resolution, but current algorithms still have
(2) is necessary because the string of words cor-        a low accuracy. We devised an approach that com-
responding to the title may appear in the lyrics of      bines text tiling (Hearst, 1994) and domain heuris-
the song or anywhere else in the review. The string      tics. The text tiling algorithm divides the text into
Maxwell’s Silver Hammer counts as a title only in        coherent discourse units, to describe the sub-topic
sentence (a) below; the second sentence is a verse       structure of the given text. We found the relatively
in the lyrics:                                           coarse segments the text tiling algorithm provides
a. Then, the wild and weird “Maxwell’s Silver            sufficient to identify different topics.
Hammer.”                                                    An album review is first divided into seg-
b. Bang, Bang, maxwell’s silver hammer cam               ments using the text tiling algorithm.            Let
down on her head.                                        [seg1 , seg2 , ..., segk ] be the segments obtained.
   Similar to Named Entity Recognition (Schedl et        The segments that contain potential features of a
al., 2007), our approach to song title extraction        song are identified using the following heuristics:
is based on n-grams. We proceed album by al-             Step 1: Include segi if it contains a song title.


                                                      1379


These segments are more likely to contain features
                                                          1.   “Maxwell” is a bit silly.
of songs as they are composed of the sentences            2.   “Octopus” is a bit silly.
surrounding the song title.                               3.   “Something” is George’s best song.
Step 2: Include segi+1 if segi is included and            4.   “Here Comes The Sun” is George’s best song.
                                                          5.   “Something” may be the single greatest love song.
segi+1 contains one or more feature terms.                6.   “Oh! Darling” is a bluesy masterpiece.
   Since we have no a priori inventory of features        7.   “Come Together” contains a great riff.
(the feature annotation will be used for evalua-
tion, not for development), we use WordNet (Fell-
                                                               Figure 5: f -sentences corresponding to Figure 1
baum, 1998) to identify feature terms: i.e., those
nouns whose synonyms, direct hypernym or di-
rect hyponym, or the definitions of any of those,      become the components of the new f -sentence.
contain the terms “music” or “song”, or any form       Next, we need to adjust their number and forms.
of these words like “musical”, “songs” etc, for at     This is a natural language generation task, specifi-
least one sense of the noun. Feature terms exclude     cally, sentence realization.
the words “music”, “song”, the artist/band/album          We use YAG (McRoy et al., 2003), a template
name as they are likely to occur across album re-      based sentence realizer. clause is the main tem-
views. All feature terms in the final set of seg-      plate used to generate a sentence. Slots in a tem-
ments selected by the heuristics are taken to be       plate can in turn be templates.     The grammati-
features of the song described by that segment.        cal relationships obtained from the Typed Depen-
                                                       dency Parser such as subject and object identify
4.3   Sentence Partitioning and Regeneration           the slots and the template the slots follows; the
                                                       words in the relationship fill the slot. We use a
After extracting the sentences containing the fea-     morphological tool (Minnen et al., 2000) to ob-
tures, the next step is to divide the sentences into   tain the base form from the original verb or noun,
two or more “sub-sentences”, if necessary. For         so that YAG can generate grammatical sentences.
example, “McCartney’s bouncy bass-line is espe-        Figure 5 shows the regenerated review from Fig-
cially wonderful, and George comes in with an ex-      ure 1.
cellent, minimal guitar solo.” discusses both fea-        YAG regenerates as many f -sentences from the
tures bass and guitar. Only a portion of the sen-      original sentence, as many features were contained
tence describes the guitar.      This sentence can     in it. By the end of this step, for each feature f
thus be divided into two individual sentences. Re-     of a certain song si , we have generated a set of
moving parts of sentences that describe another        f -sentences. This set also contains every original
feature, will have no effect on the summary as         sentence that only covered the single feature f .
a whole as the portions that are removed will be
present in the group of sentences that describe the    4.4      Grouping
other feature.                                         f -sentences are further grouped, by sub-feature
   To derive n sentences, each concerning a single     and by polarity. As concerns sub-feature group-
feature f , from the original sentence that covered    ing, consider the following f -sentences for the
n features, we need to:                                feature guitar:
1. Identify portions of sentences relevant to each
feature f (partitioning)                                  a. George comes in with an excellent, minimal
2. Regenerate each portion as an independent sen-            guitar solo.
tence, which we call f -sentence.
To identify portions of the sentence relevant to the      b. McCartney laid down the guitar lead for this
single feature f , we use the Stanford Typed De-             track.
pendency Parser (Klein and Manning, 2002; de              c. Identical lead guitar provide the rhythmic
Marnee and Manning, 2008). Typed Dependen-                   basis for this song.
cies describe grammatical relationships between
pairs of words in a sentence. Starting from the fea-   The first sentence talks about the guitar solo, the
ture term f in question, we collect all the nouns,     second and the third about the lead guitar. This
adjectives and verbs that are directly related to it   step will create two subgroups, with sentence a in
in the sentence. These nouns, adjectives and verbs     one group and sentences b and c in another. We


                                                   1380


  Let [fx -s1 , fx -s2 , ...fx -sn ] be the set of sentences for            Example: The lyrics are the best
  feature fx and song Sy                                                    Adjectives in the sentence: best

  Step 1: Find the longest common n-gram (LCN) be-                          Senti-wordnet Scores of best:
  tween fx -si and fx -sj for all i 6= j: LCN(fx -si , fx -sj )             Sense 1 (frequency=2):
                                                                            positive = 0.625, negative =0 , objective = 0.375

  Step 2: If LCN(fx -si , fx -sj ) contains the feature term                Sense 2 (frequency=1):
  and is not the feature term alone, fx -si and fx -sj are                  positive = 0.75, negative = 0, objective = 0.25
  in the same group.
                                                                            Polarity Scores Calculation:
  Step 3: For any fx -si , if LCN(fx -si , fx -sj ) for all i and           positive(best) = 2∗0.625+1∗0.75
                                                                                                  (2+1)
                                                                                                            = 0.67
  j, is the feature term, then fx -si belongs to the default                negative(best) = 2∗0+1∗0  =  0
                                                                                               (2+1)
  group for the feature.
                                                                            objective(best) = 2∗0.375+1∗0.25
                                                                                                   (2+1)
                                                                                                             = 0.33

                                                                            Since the sentence contains only the adjective best, its
  Figure 6: Grouping sentences by sub-features                              polarity is positive, from:
                                                                            Max (positive(best), negative(best), objective(best))


identify subgroups via common n-grams between                                        Figure 7: Polarity Calculation
f -sentences, and make sure that only n-grams that
are related to feature f are identified at this stage,
as detailed in Figure 6. When the procedure de-                        jective, which are computed analogously:
scribed in Figure 6 is applied to the three sentences                                f req1 ∗ pos1 + ... + f reqn ∗ posn
above, it identifies guitar as the longest pertinent                       pos(ai ) =
                                                                                            (f req1 + .... + f reqn )
LCN between a and b, and between a and c; and                                                                                (1)
guitar lead between b and c (we do not take into                                   th
                                                                       ai is the i adjective, f reqj is the frequency of
account linear order within n-grams, hence gui-                        the j th sense of ai as given by Wordnet, and posj
tar lead and lead guitar are considered identical).                    is the positive score of the j th sense of ai , as given
Step 2 in Figure 6 will group b and c together since                   by SentiWordnet. Figure 7 shows an example of
guitar lead properly contains the feature term gui-                    calculating the polarity of a sentence.
tar. In Step 3, sentence a is sentence fx -si such                        For an f -sentence, three scores will be com-
that its LCN with all other sentences (b and c) con-                   puted, as the sum of the corresponding scores
tains only the feature term; hence, sentence a is                      (positive, negative, objective) of all the adjectives
left on its own. Note that Steps 2 and 3 ensure                        in the sentence. The polarity of the sentence is de-
that, among all the possible LNCs between pair of                      termined by the maximum of these three scores.
sentences, we only consider the ones containing
the feature in question.                                               4.5      Selection and Ordering

   As concerns polarity grouping, different re-                        Finally, the generation of a coherent summary in-
views may express different opinions regarding a                       volves selection of the sentences to be included,
particular feature. To generate a coherent sum-                        and ordering them in a coherent fashion. This step
mary that mentions conflicting opinions, we need                       has in input groups of f -sentences, where each
to subdivide f -sentences according to polarity.                       group pertains to the feature f , one of its subfea-
                                                                       tures, and one polarity type (positive, negative, ob-
   We use SentiWordNet (Esuli and Sebastiani,                          jective). We need to select one sentence from each
2006), an extension of WordNet where each sense                        subgroup to make sure that all essential concepts
of a word is augmented with the probability of                         are included in the summary. Note that if there are
that sense being positive, negative or neutral. The                    contrasting opinions on one feature or subfeatures,
overall sentence score is based on the scores of the                   one sentence per polarity will be extracted, result-
adjectives contained in the sentence.                                  ing in potentially inconsistent opinions on that fea-
  Since there are a number of senses for each                          ture to be included in the review (we did not ob-
word, an adjective ai in a sentence is scored as the                   serve this happening frequently, and even if it did,
normalized weighted scores of each sense of the                        it did not appear to confuse our users).
adjective. For each ai , we compute three scores,                         Recall that at this point, most f -sentences have
positive, as shown in Formula 1, negative and ob-                      been regenerated from portions of original sen-


                                                                    1381


tences (see Section 4.3). Each f -sentence in a         5     Evaluation
subgroup is assigned a score which is equivalent
to the number of features in the original sentence      In this section we report three evaluations, two
from which the f -sentence was obtained. The sen-       intrinsic and one extrinsic: evaluation of the song
tence which has the lowest score in each subgroup       title and feature extraction steps; evaluation of the
is chosen as the representative for that subgroup.      informativeness of summaries; and a user study to
If multiple sentences have the lowest score, one        judge how summaries affect decision making.
sentence is selected randomly. Our assumption is        5.1    Song Title and Feature Extraction
that among the original sentences, a sentence that
talks about one feature only is likely to express a     The song title extraction and feature extraction al-
stronger opinion about that feature than a sentence     gorithms (Sections 4.1 and 4.2) were manually
in which other features are present.                    evaluated on 100 reviews randomly taken from the
   We order the sentences by exploiting a music         corpus (2 or 3 from each album). This relatively
ontology (Giasson and Raimond, 2007). We have           small number is due to the need to conduct the
extended this ontology to include few additional        evaluation manually. The 100 reviews contained
concepts that correspond to features identified in      1304 occurrences of song titles and 898 occur-
our corpus. Also, we extended each of the classes       rences of song features, as previously annotated.
by adding the domain to which it belongs. We               1294 occurrences of song titles were correctly
identified a total of 20 different domains for all      identified; additionally, 123 spurious occurrences
the features. For example, [saxophone,drums] be-        were also identified. This results in a precision of
longs to the domain Instrument, and [tone, vocals]      91.3%, and recall of 98%. The 10 occurrences that
belong to the domain Sound. We also identified          were not identified contained either abbreviations
the priority order in which each of these domains       like Dr. for Doctor or spelling mistakes (recall that
should appear in the final summary. The order-          we don’t clean up mistakes).
ing of the domains is such that first we present the       Of the 898 occurrences of song features, 853
general features of the song (e.g. Song) domain,        were correctly identified by our feature extraction
then present more specific domains (e.g. Sound,         algorithm, with an additional 41 spurious occur-
Instrument). f −sentences of a single domain form       rences. This results in a precision of 95.4% and a
one paragraph in the final summary. However, fea-       recall of 94.9%. Note that a feature (NP) is con-
tures domains that are considered as sub-domains        sidered as correctly identified, if its head noun is
of another domain are included in the same para-        annotated in a review for the song with correct ID.
graph, but are ordered next to the features of the         As a baseline comparison, we implemented the
parent domain. The complete list of domains is de-      feature extraction algorithm from (Hu and Liu,
scribed in (Tata, 2010). f -sentences are grouped       2004). We compared their algorithm to ours on 10
and ordered according to the domain of the fea-         randomly chosen reviews from our corpus, for a
tures. Figure 8 shows a sample summary when the         total of about 500 sentences. Its accuracy (40.8%
extracted sentences are ordered via this method.        precision, and 64.5% recall) is much lower than
                                                        ours, and than their original results on product re-
 “The Song That Jane Likes” is cute. The song           views (72% precision, and 80% recall).
 has some nice riffs by Leroi Moore. “The Song
                                                        5.2    Informativeness of the summaries
 That Jane Likes” is also amazing funk number.
     The lyrics are sweet and loving.                   To evaluate the information captured in the sum-
     The song carries a light-hearted tone. It has      mary, we randomly selected 5 or 6 songs from 10
 a catchy tune. The song features some nice ac-         albums, and generated the corresponding 52 sum-
 cents.                                                 maries, one per song – this corresponds to a test set
     “The Song That Jane Likes” is beautiful            of about 500 album reviews (each album has about
 song with great rhythm. The funky beat will            50 reviews). Most summary evaluation schemes,
 surely make a move.                                    for example the Pyramid method (Harnly et al.,
     It is a heavily acoustic guitar-based song.        2005), make use of reference summaries writ-
                                                        ten by humans. We approximate those gold-
           Figure 8: Sample summary                     standard reference summaries with 2 or 3 critic re-
                                                        views per album taken from www.pitchfork.


                                                     1382


com, www.rollingstone.com and www.                        cession, with 3 recommendations each (only the
allmusic.com.                                             top 3 recommendations were presented among the
   First, we manually annotated both critic reviews       available 25, see Section 3). Users were asked to
and the automatically generated summaries for             select at least one recommendation for each song,
song titles and song features. 302, i.e., 91.2%           namely, to click on the url where they can listen to
of the features identified in the critic reviews are      the song. They were also asked to base their selec-
also identified in the summaries (recall that a fea-      tion on the information provided by the interface.
ture is considered as identified, if the head-noun of     The first song was a test song for users to get ac-
the NP is identified by both the critic review and        quainted with the system. We collected compre-
the summary, and attributed to the same song). 64         hensive timed logs of the user actions, including
additional features were identified, for a recall of      clicks, when windows are open and closed, etc.
82%. It is not surprising that additional features        After using the system, users were administered a
may appear in the summaries: even if only one of          brief questionnaire which included questions on a
the 50 album reviews talks about that feature, it is      5-point Likert Scale. 18 users interacted with the
included in the summary. Potentially, a threshold         baseline version and 21 users with the experimen-
on frequency of feature mention could increase re-        tal version (five additional subjects were run but
call, but we found out that even a threshold of two       their log data was not properly saved). All users
significantly affects precision.                          were students at our University, and most of them,
   In a second evaluation, we used our Feature            graduate students (no differences were found due
Extraction algorithm to extract features from the         to gender, previous knowledge of music, or educa-
critic reviews, for each song whose summary               tion level).
needs to be evaluated. This is an indirect evalu-            Our main measure is time on task, the total time
ation of that algorithm, in that it shows it is not af-   taken to select the recommendations from song 2
fected by somewhat different data, since the critic       to song 5 – this excludes the time spent listen-
reviews are more formally written. 375, or 95%            ing to the songs. A t-test showed that users in
of the features identified in the critic reviews are      the experimental version take less time to make
also identified in the summaries. 55 additional           their decision when compared to baseline subjects
features were additionally identified, for a recall       (p = 0.019, t = 2.510). This is a positive result,
of 87.5%. These values are comparable, even if            because decreasing time to selection is important,
slightly higher, to the precision and recall of the       given that music collections can include millions
manual annotation described above.                        of songs. However, time-on-task basically repre-
                                                          sents the time it takes users to peruse the review
5.3   Between-Subject User Study                          or summary, and the number of words in the sum-
Our intrinsic evaluation gives satisfactory results.      maries is significantly lower than the number of
However, we believe the ultimate measure of such          words in the reviews (p < 0.001, t = 16.517).
a summarization algorithm is an end-to-end eval-             Hence, we also analyzed the influence of sum-
uation to ascertain whether it affects user behav-        maries on decision making, to see if they have
ior, and how. We conducted a between-subject              any effects beyond cutting down on the number
user study, where users were presented with two           of words to read. Our assumption is that the de-
different versions of our Music RS. For each of           fault choice is to choose the first recommenda-
the recommended songs, the baseline version pro-          tion. Users in the baseline condition picked the
vides only whole album reviews, the experimental          first recommendation as often as the other two rec-
version provides the automatically generated song         ommendations combined; users in the experimen-
feature summary, as shown in Figure 2. The in-            tal condition picked the second and third recom-
terface for the baseline version is similar, but the      mendations more often than the first, and the dif-
summary in the bottom window is replaced by the           ference between the two conditions is significant
corresponding album review. The presented re-             (χ2 = 8.74, df = 1, p = 0.003). If we examine
view is the one among the 50 reviews for that al-         behavior song by song, this holds true especially
bum whose length is closest to the average length         for song 3 (χ2 = 12.3, df = 1, p < 0.001) and
of album reviews in the corpus (478 words).               song 4 (χ2 = 5.08, df = 1, p = 0.024). We
   Each user was presented with 5 songs in suc-           speculate that users in the experimental condition


                                                      1383


are more discriminatory in their choices, because        their choice of recommendations was more varied
important features of the recommended songs are          when presented with song review summaries than
evident in the summaries, but are buried in the al-      with album reviews. Our framework can be ap-
bum reviews. For example, for Song 3, only one           plied to any domain where reviews of individual
of the 20 sentences in the album review is about         components need to be summarized from reviews
the first recommended song, and is not very posi-        of collections, such as travel reviews that cover
tive. Negative opinions are much more evident in         many cities in a country, or different restaurants
the review summaries.                                    in a city.
   The questionnaires included three common
questions between the two conditions. The ex-
perimental subjects gave a more positive assess-         References
ment of the length of the summary than the base-         Regina Barzilay and Kathleen McKeown. 2005. Sen-
line subjects (p = 0.003, t = −3.248, df =                 tence fusion for multidocument news summariza-
                                                           tion. Computational Linguistics, 31(3):297–328.
31.928). There were no significant differences
on the other two questions, feeling overwhelmed          Giuseppe Carenini, Raymond Ng, and Adam Pauls.
by the information provided; and whether the re-           2006. Multi-document summarization of evaluative
view/summary helped them to quickly make their             text. In Proceedings of EACL.
selection.                                               Oscar Clema. 2006. Interaction Design for Recom-
   A multiple Linear Regression with, as predic-           mender Systems. Ph.D. thesis, Universitat Pompeu
tors, the number of words the user read before             Fabra, Barcelona, July.
making the selection and the questions, and time         Marie-Catherine de Marnee and Christopher D. Man-
on task as dependent variable, revealed only one,         ning. 2008. Stanford Typed Dependencies Manual.
not surprising, correlation: the number of words          http://nlp.stanford.edu/software/dependencies manual.pdf.
the user read correlates with time on task (R2 =         J. Stephen Downie and Xiao Hu. 2006. Review min-
0.277, β = 0.509, p = 0.004).                               ing for music digital libraries: Phase ii. In Proceed-
   Users in the experimental version were also              ings of the 6th ACM/IEEE-CS Joint Conference on
                                                            Digital Libraries, pages 196–197, Chapel Hill, NC,
asked to rate the grammaticality and coherence of
                                                            USA.
the summary. The average rating was 3.33 for
grammaticality, and 3.14 for coherence. Whereas          Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
these numbers in isolation are not too telling, they       WordNet: A publicly available lexical resource for
                                                           opinion mining. In Proceedings of LREC-06, the 5th
are at least suggestive that users did not find these      Conference on Language Resources and Evaluation,
summaries badly written. We found no signifi-              Genova, IT.
cant correlations between grammaticality and co-
                                                         Christiane Fellbaum, editor. 1998. WordNet: an elec-
herence of summaries, and time on task.
                                                           tronic lexical database. MIT Press.

6   Discussion and Conclusions                           Michael Gamon, Anthony Aue, Simon Corston-Oliver,
                                                           and Eric Ringger. 2005. Pulse: Mining customer
Most summarization research on customer reviews            opinions from free text. In Advances in Intelli-
focuses on obtaining features of the products, but         gent Data Analysis VI, volume 3646/2005 of Lec-
                                                           ture Notes in Computer Science, pages 121–132.
not much work has been done on presenting them             Springer Berlin / Heidelberg.
as a coherent summary. In this paper, we described
a system that uses information extraction and sum-       Frederick Giasson and Yves Raimond. 2007. Mu-
                                                           sic ontology specification. Working draft, February.
marization techniques in order to generate sum-            http://pingthesemanticweb.com/ontology/mo/.
maries of individual songs from multiple album
reviews. Whereas the techniques we have used             Aaron Harnly, Ani Nenkova, Rebecca Passonneau, and
are state-of-the-art, the contribution of our work is      Owen Rambow. 2005. Automation of summary
                                                           evaluation by the Pyramid method. In Proceedings
integrating them in an effective end-to-end system.        of the Conference on Recent Advances in Natural
We first evaluated it intrinsically as concerns infor-     Language Processing.
mation extraction, and the informativeness of the
                                                         Marti A. Hearst. 1994. Multi-paragraph segmentation
summaries. Perhaps more importantly, we also ran          of expository text. In Proceedings of the 32nd Meet-
an extrinsic evaluation in the context of our proto-      ing of the Association for Computational Linguis-
type Music RS. Users made quicker decisions and           tics, Las Cruces, NM, June.


                                                     1384


Ryuichiro Higashinaka, Rashmi Prasad, and Marilyn
  Walker. 2006. Learning to Generate Naturalistic
  Utterances Using Reviews in Spoken Dialogue Sys-
  tems. In COLING-ACL06, Sidney, Australia.
Minqing Hu and Bing Liu. 2004. Mining and summa-
  rizing customer reviews. In Proceedings of KDD,
  Seattle, Washington, USA, August.
Dan Klein and Christopher D. Manning. 2002. Fast
  exact inference with a factored model for natural
  language parsing. In Advances in Neural Informa-
  tion Processing Systems 15, pages 3–10.
Susan McRoy, Songsak Ukul, and Syed Ali. 2003. An
  augmented template-based approach to text realiza-
  tion. In Natural Language Engineering, pages 381–
  420. Cambridge Press.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
  Robust, applied morphological generation. In Pro-
  ceedings of the 1st International Natural Language
  Generation Conference.
Tom Mitchell. 1997. Machine Learning. McGraw
  Hill.
Vivi Nastase. 2008. Topic-driven multi-document
  summarization with encyclopedic knowledge and
  spreading activation. In Proceedings of the Con-
  ference on Empirical Methods in Natural Language
  Processing.
Patrick Nguyen, Milind Mahajan, and Geoffrey Zweig.
  2007. Summarization of multiple user reviews in
  the restaurant domain. Technical Report MSR-TR-
  2007-126, Microsoft, September.
Markus Schedl, Gerhard Widmer, Tim Pohle, and
 Klaus Seyerlehner. 2007. Web-based detection of
 music band members and line-up. In Proceedings of
 the Australian Computer Society.
M. Soubbotin and S. Soubbotin. 2005. Trade-Off Be-
  tween Factors Influencing Quality of the Summary.
  In Document Understanding Workshop (DUC), Van-
  couver, BC, Canada.
Swati Tata. 2010. SongRecommend: a Music Recom-
  mendation System with Fine-Grained Song Reviews.
  Ph.D. thesis, University of Illinois, Chicago, IL.
2008.   UPnP Device Architecture Version 1.0.
  (www.upnp.org).
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
   review mining and summarization. In Conference
   on Information and Knowledge Management, Ar-
   lington, Virginia, USA.




                                                   1385

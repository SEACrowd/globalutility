A study of Information Retrieval weighting schemes for sentiment analysis

                 Georgios Paltoglou                                     Mike Thelwall
             University of Wolverhampton                         University of Wolverhampton
           Wolverhampton, United Kingdom                        Wolverhampton, United Kingdom
            g.paltoglou@wlv.ac.uk                                m.thelwall@wlv.ac.uk



                      Abstract                                   Most of the work in sentiment analysis has fo-
    Most sentiment analysis approaches use as                 cused on supervised learning techniques (Sebas-
    baseline a support vector machines (SVM)                  tiani, 2002), although there are some notable ex-
    classifier with binary unigram weights.                   ceptions (Turney, 2002; Lin and He, 2009). Pre-
    In this paper, we explore whether more                    vious research has shown that in general the per-
    sophisticated feature weighting schemes                   formance of the former tend to be superior to that
    from Information Retrieval can enhance                    of the latter (Mullen and Collier, 2004; Lin and
    classification accuracy. We show that vari-               He, 2009). One of the main issues for supervised
    ants of the classic tf.idf scheme adapted                 approaches has been the representation of docu-
    to sentiment analysis provide significant                 ments. Usually a bag of words representation is
    increases in accuracy, especially when us-                adopted, according to which a document is mod-
    ing a sublinear function for term frequency               eled as an unordered collection of the words that
    weights and document frequency smooth-                    it contains. Early research by Pang et al. (2002) in
    ing. The techniques are tested on a wide                  sentiment analysis showed that a binary unigram-
    selection of data sets and produce the best               based representation of documents, according to
    accuracy to our knowledge.                                which a document is modeled only by the pres-
                                                              ence or absence of words, provides the best base-
1 Introduction                                                line classification accuracy in sentiment analysis
The increase of user-generated content on the web             in comparison to other more intricate representa-
in the form of reviews, blogs, social networks,               tions using bigrams, adjectives, etc.
tweets, fora, etc. has resulted in an environ-                   Later research has focused on extending the
ment where everyone can publicly express their                document representation with more complex fea-
opinion about events, products or people. This                tures such as structural or syntactic informa-
wealth of information is potentially of vital im-             tion (Wilson et al., 2005), favorability mea-
portance to institutions and companies, providing             sures from diverse sources (Mullen and Collier,
them with ways to research their consumers, man-              2004), implicit syntactic indicators (Greene and
age their reputations and identify new opportuni-             Resnik, 2009), stylistic and syntactic feature selec-
ties. Wright (2009) claims that “for many busi-               tion (Abbasi et al., 2008), “annotator rationales”
nesses, online opinion has turned into a kind of              (Zaidan et al., 2007) and others, but no systematic
virtual currency that can make or break a product             study has been presented exploring the benefits of
in the marketplace”.                                          employing more sophisticated models for assign-
   Sentiment analysis, also known as opinion min-             ing weights to word features.
ing, provides mechanisms and techniques through                  In this paper, we examine whether term weight-
which this vast amount of information can be pro-             ing functions adopted from Information Retrieval
cessed and harnessed. Research in the field has               (IR) based on the standard tf.idf formula and
mainly, but not exclusively, focused in two sub-              adapted to the particular setting of sentiment anal-
problems: detecting whether a segment of text, ei-            ysis can help classification accuracy. We demon-
ther a whole document or a sentence, is subjective            strate that variants of the original tf.idf weighting
or objective, i.e. contains an expression of opin-            scheme provide significant increases in classifica-
ion, and detecting the overall polarity of the text,          tion performance. The advantages of the approach
i.e. positive or negative.                                    are that it is intuitive, computationally efficient


                                                        1386
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1386–1395,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


and doesn’t require additional human annotation        a Naive Bayes baseline using the whole text but
or external sources. Experiments conducted on a        only slight increase compared to using a SVM
number of publicly available data sets improve on      classifier on the entire document.
the previous state-of-the art.                            Mullen and Collier (2004) used SVMs and ex-
   The next section provides an overview of rel-       panded the feature set for representing documents
evant work in sentiment analysis. In section 3         with favorability measures from a variety of di-
we provide a brief overview of the original tf.idf     verse sources. They introduced features based on
weighting scheme along with a number of variants       Osgood’s Theory of Semantic Differentiation (Os-
and show how they can be applied to a classifica-      good, 1967) using WordNet to derive the values
tion scenario. Section 4 describes the corpora that    of potency, activity and evaluative of adjectives
were used to test the proposed weighting schemes       and Turney’s semantic orientation (Turney, 2002).
and section 5 discusses the results. Finally, we       Their results showed that using a hybrid SVM
conclude and propose future work in section 6.         classifier, that uses as features the distance of doc-
                                                       uments from the separating hyperplane, with all
2 Prior Work                                           the above features produces the best results.
Sentiment analysis has been a popular research            Whitelaw et al. (2005) added fine-grained se-
topic in recent years. Most of the work has fo-        mantic distinctions in the feature set. Their ap-
cused on analyzing the content of movie or gen-        proach was based on a lexicon created in a semi-
eral product reviews, but there are also applica-      supervised fashion and then manually refined It
tions to other domains such as debates (Thomas et      consists of 1329 adjectives and their modifiers cat-
al., 2006; Lin et al., 2006), news (Devitt and Ah-     egorized under several taxonomies of appraisal at-
mad, 2007) and blogs (Ounis et al., 2008; Mishne,      tributes based on Martin and White’s Appraisal
2005). The book of Pang and Lee (2008) presents        Theory (2005). They combined the produced ap-
a thorough overview of the research in the field.      praisal groups with unigram-based document rep-
This section presents the most relevant work.          resentations as features to a Support Vector Ma-
                                                       chine classifier (Witten and Frank, 1999), result-
    Pang et al. (2002) conducted early polarity
                                                       ing in significant increases in accuracy.
classification of reviews using supervised ap-
proaches. They employed Support Vector Ma-                Zaidan et al. (2007) introduced “annotator ra-
chines (SVMs), Naive Bayes and Maximum En-             tionales”, i.e. words or phrases that explain the
tropy classifiers using a diverse set of features,     polarity of the document according to human an-
such as unigrams, bigrams, binary and term fre-        notators. By deleting rationale text spans from the
quency feature weights and others. They con-           original documents they created several contrast
cluded that sentiment classification is more dif-      documents and constrained the SVM classifier to
ficult that standard topic-based classification and    classify them less confidently than the originals.
that using a SVM classifier with binary unigram-       Using the largest training set size, their approach
based features produces the best results.              significantly increased the accuracy on a standard
    A subsequent innovation was the detection and      data set (see section 4).
removal of the objective parts of documents and           Prabowo and Thelwall (2009) proposed a hy-
the application of a polarity classifier on the rest   brid classification process by combining in se-
(Pang and Lee, 2004). This exploited text coher-       quence several ruled-based classifiers with a SVM
ence with adjacent text spans which were assumed       classifier. The former were based on the Gen-
to belong to the same subjectivity or objectivity      eral Inquirer lexicon (Wilson et al., 2005), the
class. Documents were represented as graphs with       MontyLingua part-of-speech tagger (Liu, 2004)
sentences as nodes and association scores between      and co-occurrence statistics of words with a set
them as edges. Two additional nodes represented        of predefined reference words. Their experiments
the subjective and objective poles. The weights        showed that combining multiple classifiers can
between the nodes were calculated using three dif-     result in better effectiveness than any individual
ferent, heuristic decaying functions. Finding a par-   classifier, especially when sufficient training data
tition that minimized a cost function separated the    isn’t available.
objective from the subjective sentences. They re-         In contrast to machine learning approaches
ported a statistically significant improvement over    that require labeled corpora for training, Lin and


                                                   1387


He (2009) proposed an unsupervised probabilis-            3.1 The classic tf.idf weighting schemes
tic modeling framework, based on Latent Dirich-           The classic tf.idf formula assigns weight wi to
let Allocation (LDA). The approach assumes that           term i in document D as:
documents are a mixture of topics, i.e. proba-
bility distribution of words, according to which                                                   N
                                                                     wi = tfi · idfi = tfi · log            (1)
each document is generated through an hierarchi-                                                   dfi
cal process and adds an extra sentiment layer to
accommodate the opinionated nature (positive or           where tfi is the number of times term i occurs in
negative) of the document. Their best attained per-       D, idfi is the inverse document frequency of term
formance, using a filtered subjectivity lexicon and       i, N is the total number of documents and dfi is
removing objective sentences in a manner similar          the number of documents that contain term i.
to Pang and Lee (2004), is only slightly lower than          The utilization of tfi in classification is rather
that of a fully-supervised approach.                      straightforward and intuitive but, as previously
                                                          discussed, usually results in decreased accuracy
                                                          in sentiment analysis. On the other hand, using
                                                          idf to assign weights to features is less intuitive,
                                                          since it only provides information about the gen-
3 A study of non-binary weights
                                                          eral distribution of term i amongst documents of
                                                          all classes, without providing any additional evi-
                                                          dence of class preference. The utilization of idf
We use the terms “features”, “words” and “terms”          in information retrieval is based on its ability to
interchangeably in this paper, since we mainly fo-        distinguish between content-bearing words (words
cus on unigrams. The approach nonetheless can             with some semantical meaning) and simple func-
easily be extended to higher order n-grams. Each          tion words, but this behavior is at least ambiguous
document D therefore is represented
                                     as a bag-of-        in classification.
words feature vector: D = w1 , w2 , ..., w|V |
where |V | is the size of the vocabulary (i.e. the
                                                          Table 1: SMART notation for term frequency vari-
number of unique words) and wi , i = 1, . . . , |V |
                                                          ants. maxt (tf ) is the maximum frequency of any
is the weight of term i in document D.
                                                          term in the document and avg dl is the average
   Despite the significant attention that sentiment       number of terms in all the documents. For ease of
analysis has received in recent years, the best ac-       reference, we also include the BM25 tf scheme.
curacy without using complex features (Mullen             The k1 and b parameters of BM25 are set to their
and Collier, 2004; Whitelaw et al., 2005) or ad-          default values of 1.2 and 0.95 respectively (Jones
ditional human annotations (Zaidan et al., 2007) is       et al., 2000).
achieved by employing a binary weighting scheme                 Notation         Term frequency
(Pang et al., 2002), where wi = 1, if tfi > 0 and              n (natural)              tf
wi = 0, if tfi = 0, where tfi is the number of               l (logarithm)         1 + log(tf )
                                                                                          0.5·tf
times that term i appears in document D (hence-             a (augmented)        0.5 + max   t (tf )
                                                                               
forth raw term frequency) and utilizing a SVM                                     1, tf > 0
classifier. It is of particular interest that using tfi       b (boolean)
                                                                                  0, otherwise
in the document representation usually results in                                    1+log(tf )
                                                              L (log ave)          1+log(avg dl)
decreased accuracy, a result that appears to be in                                  (k1 +1)·tf 
contrast with topic classification (Mccallum and             o (BM25)           
                                                                                            dl
                                                                              k1 (1−b)+b· avg  dl
                                                                                                  +tf
Nigam, 1998; Pang et al., 2002).

    In this paper, we also utilize SVMs but our
study is centered on whether more sophisticated           3.2 Delta tf.idf
than binary or raw term frequency weighting func-         Martineau and Finin (2009) provide a solution to
tions can improve classification accuracy. We             the above issue of idf utilization in a classification
base our approach on the classic tf.idf weighting         scenario by localizing the estimation of idf to the
scheme from Information Retrieval (IR) and adapt          documents of one or the other class and subtract-
it to the domain of sentiment classification.             ing the two values. Therefore, the weight of term


                                                      1388


                                                              3.3 SMART and BM25 tf.idf variants
Table 2: SMART notation for inverse document
frequency variants. For ease of reference we also             The SMART retrieval system by Salton (1971) is
include the BM25 idf factor and also present the              a retrieval system based on the vector space model
extensions of the original formulations with their            (Salton and McGill, 1986). Salton and Buckley
∆ variants.                                                   (1987) provide a number of variants of the tf.idf
  Notation                  Inverse Document Fre-             weighting approach and present the SMART nota-
                            quency                            tion scheme, according to which each weighting
  n (no)                    1                                 function is defined by triples of letters; the first
                                N                             one denotes the term frequency factor, the sec-
  t (idf)                   log df
                                   −df                        ond one corresponds to the inverse document fre-
  p (prob idf)              log N df
                                     −df +0.5                 quency function and the last one declares the nor-
 k (BM25 idf)                  log N df +0.5                  malization that is being applied. The upper rows
                                     1 ·df2
 ∆(t) (Delta idf)              log N
                                   N2 ·df1                    of tables 1, 2 and 3 present the three most com-
                                     1 ·df2 +0.5
 ∆(t′ ) (Delta smoothed        log N
                                   N2 ·df1 +0.5
                                                              monly used weighting functions for each factor re-
 idf)                                                         spectively. For example, a binary document repre-
 ∆(p) (Delta prob idf)         log (N 1 −df1 )·df2            sentation would be equivalent to SM ART.bnn1
                                   df1 ·(N2 −df2 )
                                                              or more simply bnn, while a simple raw term fre-
 ∆(p′ ) (Delta smoothed        log (N 1 −df1 )·df2 +0.5
                                   (N2 −df2 )·df1 +0.5        quency based would be notated as nnn or nnc
 prob idf)                                                    with cosine normalization.
 ∆(k) (Delta BM25 idf)         log (N 1 −df1 +0.5)·df2 +0.5
                                   (N2 −df2 +0.5)·df1 +0.5
                                                                         Table 3: SMART normalization.
                                                                 Notation      Normalization
i in document D is estimated as:                                 n (none)           1
                                                                 c (cosine) √ 2 12          2
                                                                                   w1 +w2 +...+wn
                        N1                     N2
   wi   = tfi · log2 (       ) − tfi · log2 (       )
                       dfi,1                  dfi,2              Significant research has been done in IR on di-
                       N1 · dfi,2                             verse weighting functions and not all versions of
        = tfi · log2 (            )                   (2)
                       dfi,1 · N2                             SMART notations are consistent (Manning et al.,
                                                              2008). Zobel and Moffat (1998) provide an ex-
                                                              haustive study but in this paper, due to space con-
where Nj is the total number of training docu-                straints, we will follow the concise notation pre-
ments in class cj and dfi,j is the number of train-
                                                              sented by Singhal et al. (1995).
ing documents in class cj that contain term i. The
                                                                 The BM25 weighting scheme (Robertson et al.,
above weighting scheme was appropriately named
                                                              1994; Robertson et al., 1996) is a probabilistic
Delta tf.idf .
                                                              model for information retrieval and is one of the
   The produced results (Martineau and Finin,                 most popular and effective algorithms used in in-
2009) show that the approach produces better                  formation retrieval. For ease of reference, we in-
results than the simple tf or binary weighting                corporate the BM25 tf and idf factors into the
scheme. Nonetheless, the approach doesn’t take                SMART annotation scheme (last row of table 1
into consideration a number of tested notions from            and 4th row of table 2), therefore the weight wi
IR, such as the non-linearity of term frequency to            of term i in document D according to the BM25
document relevancy (e.g. Robertson et al. (2004))             scheme is notated as SM ART.okn or okn.
according to which, the probability of a document                Most of the tf weighting functions in SMART
being relevant to a query term is typically sub-              and the BM25 model take into consideration the
linear in relation to the number of times a query             non-linearity of document relevance to term fre-
term appears in the document. Additionally, their                 1
                                                                   Typically, a weighting function in the SMART system is
approach doesn’t provide any sort of smoothing                defined as a pair of triples, i.e. ddd.qqq where the first triple
for the dfi,j factor and is therefore susceptible to          corresponds to the document representation and the second
errors in corpora where a term occurs in docu-                to the query representation. In the context that the SMART
                                                              annotation is used here, we will use the prefix SM ART for
ments of only one or the other class and therefore            the first part and a triple for the document representation in
dfi,j = 0 .                                                   the second part, i.e. SM ART.ddd, or more simply ddd.


                                                          1389


quency and thus employ tf factors that scale sub-             above formulation would imply (see table 2). The
linearly in relation to term frequency. Addition-             above variation was made for two reasons: firstly,
ally, the BM25 tf variant also incorporates a scal-           when the dfi ’s are larger than 1 then the smooth-
ing for the length of the document, taking into con-          ing factor influences the final idf value only in a
sideration that longer documents will by definition           minor way in the revised formulation, since it is
have more term occurences2 . Effective weighting              added only after the multiplication of the dfi with
functions is a very active research area in infor-            Ni (or its variation). Secondly, when dfi = 0, then
mation retrieval and it is outside the scope of this          the smoothing factor correctly adds only a small
paper to provide an in-depth analysis but signifi-            mass, avoiding a potential division by zero, where
cant research can be found in Salton and McGill               otherwise it would add a much greater mass, be-
(1986), Robertson et al. (2004), Manning et al.               cause it would be multiplied by Ni .
(2008) or Armstrong et al. (2009) for a more re-                 According to this annotation scheme therefore,
cent study.                                                   the original approach by Martineau and Finin
                                                              (2009) can be represented as n∆(t)n.
3.4    Introducing SMART and BM25 Delta
                                                                 We hypothesize that the utilization of sophisti-
       tf.idf variants
                                                              cated term weighting functions that have proved
We apply the idea of localizing the estimation                effective in information retrieval, thus providing
of idf values to documents of one class but em-               an indication that they appropriately model the
ploy more sophisticated term weighting functions              distinctive power of terms to documents and the
adapted from the SMART retrieval system and                   smoothed, localized estimation of idf values will
the BM25 probabilistic model. The resulting idf               prove beneficial in sentiment classification.
weighting functions are presented in the lower part
of table 2. We extend the original SMART anno-
                                                              Table 4: Reported accuracies on the Movie Re-
tation scheme by adding Delta (∆) variants of the
                                                              view data set. Only the best reported accuracy for
original idf functions and additionally introduce
                                                              each approach is presented, measured by 10-fold
smoothed Delta variants of the idf and the prob
                                                              cross validation. The list is not exhaustive and be-
idf factors for completeness and comparative rea-
                                                              cause of differences in training/testing data splits
sons, noted by their accented counterparts. For
                                                              the results are not directly comparable. It is pro-
example, the weight of term i in document D ac-
                                                              duced here only for reference.
cording to the o∆(k)n weighting scheme where
                                                                Approach                                 Acc.
we employ the BM25 tf weighting function and
                                                                SVM with unigrams & binary 87.15%
utilize the difference of class-based BM25 idf val-
                                                                weights (Pang et al., 2002), reported
ues would be calculated as:
                                                                at (Pang and Lee, 2004)
          (k1 + 1) · tfi        N1 − dfi,1 + 0.5                Hybrid SVM with Turney/Osgood 86%
wi =                     · log(                  )
            K + tfi               dfi,1 + 0.5                   Lemmas (Mullen and Collier, 2004)
          (k1 + 1) · tfi        N2 − dfi,2 + 0.5                SVM with min-cuts (Pang and Lee, 87.2%
     −                   · log(                  )
            K + tfi               dfi,2 + 0.5                   2004)
          (k1 + 1) · tfi                                        SVM with appraisal groups                90.2%
     =                                                          (Whitelaw et al., 2005)
            K + tfi
             
               (N1 − dfi,1 + 0.5) · (dfi,2 + 0.5)
                                                               SVM with log likehood ratio feature 90.45%
      · log                                                     selection (Aue and Gamon, 2005)
               (N2 − dfi,2 + 0.5) · (dfi,1 + 0.5)
                                                              SVM with annotator rationales            92.2%
where K is defined as k1 (1 − b) + b · avgdl dl .               (Zaidan et al., 2007)
However, we used a minor variation of the above                 LDA with filtered lexicon, subjectiv- 84.6%
formulation for all the final accented weighting                ity detection (Lin and He, 2009)
functions in which the smoothing factor is added
to the product of dfi with Ni (or its variation for             The approach is straightforward, intuitive, com-
∆(p′ ) and ∆(k)), rather than to the dfi alone as the         putationally efficient, doesn’t require additional
   2
                                                              human effort and takes into consideration stan-
    We deliberately didn’t extract the normalization compo-
nent from the BM25 tf variant, as that would unnecessarily    dardized and tested notions from IR. The re-
complicate the notation.                                      sults presented in section 5 show that a number


                                                          1390


of weighting functions solidly outperform other                ties X, which may be people, companies, films
state-of-the-art approaches. In the next section, we           etc. The results are given to human assessors who
present the corpora that were used to study the ef-            then judge the content of the webpages (i.e. blog
fectiveness of different weighting schemes.                    post and comments) and assign each webpage a
                                                               score: “1” if the document contains relevant, fac-
4 Experimental setup                                           tual information about the entity but no expression
                                                               of opinion, “2” if the document contains an ex-
We have experimented with a number of publicly
                                                               plicit negative opinion towards the entity and “4”
available data sets.
                                                               is the document contains an explicit positive opin-
   The movie review dataset by Pang et al. (2002)
                                                               ion towards the entity. We used the produced as-
has been used extensively in the past by a number
                                                               sessments from all 3 years of the conference in our
of researchers (see Table 4), presenting the oppor-
                                                               data set, resulting in 150 different entity searches
tunity to compare the produced results with pre-
                                                               and, after duplicate removal, 7,930 negative docu-
vious approaches. The dataset comprises 2,000
                                                               ments (i.e. having an assessment of “2”) and 9,968
movie reviews, equally divided between positive
                                                               positive documents (i.e. having an assessment of
and negative, extracted from the Internet Movie
                                                               “4”), which were used as the “gold standard” 7 .
Database3 archive of the rec.arts.movies.reviews
                                                               Documents are annotated at the document-level,
newsgroup. In order to avoid reviewer bias, only
                                                               rather than at the post level, making this data set
20 reviews per author were kept, resulting in a to-
                                                               somewhat noisy. Additionally, the data set is par-
tal of 312 reviewers4 . The best attained accuracies
                                                               ticularly large compared to the other ones, making
by previous research on the specific data are pre-
                                                               classification especially challenging and interest-
sented in table 4. We do not claim that those re-
                                                               ing. More information about all data sets can be
sults are directly comparable to ours, because of
                                                               found at table 5.
potential subtle differences in tokenization, classi-
                                                                  We have kept the pre-processing of the docu-
fier implementations etc, but we present them here
                                                               ments to a minimum. Thus, we have lower-cased
for reference.
                                                               all words and removed all punctuation but we have
   The Multi-Domain Sentiment data set (MDSD)
                                                               not removed stop words or applied stemming. We
by Blitzer et al. (2007) contains Amazon reviews
                                                               have also refrained from removing words with
for four different product types: books, electron-
                                                               low or high occurrence. Additionally, for the
ics, DVDs and kitchen appliances. Reviews with
                                                               BLOGS06 data set, we have removed all html for-
ratings of 3 or higher, on a 5-scale system, were
                                                               matting.
labeled as positive and reviews with a rating less
                                                                  We utilize the implementation of a support vec-
than 3 as negative. The data set contains 1,000
                                                               tor classifier from the LIBLINEAR library (Fan et
positive and 1,000 negative reviews for each prod-
                                                               al., 2008). We use a linear kernel and default
uct category for a total of 8,000 reviews. Typically,
                                                               parameters. All results are based on leave-one
the data set is used for domain adaptation applica-
                                                               out cross validation accuracy. The reason for this
tions but in our setting we only split the reviews
                                                               choice of cross-validation setting, instead of the
between positive and negative5 .
                                                               most standard ten-fold, is that all of the proposed
   Lastly, we present results from the BLOGS06
                                                               approaches that use some form of idf utilize the
(Macdonald and Ounis, 2006) collection that is
                                                               training documents for extracting document fre-
comprised of an uncompressed 148GB crawl of
                                                               quency statistics, therefore more information is
approximately 100,000 blogs and their respective
                                                               available to them in this experimental setting.
RSS feeds. The collection has been used for 3 con-
                                                                  Because of the high number of possible combi-
secutive years by the Text REtrieval Conferences
                                                               nations between tf and idf variants (6·9·2 = 108)
(TREC)6 . Participants of the conference are pro-
                                                               and due to space constraints we only present re-
vided with the task of finding documents (i.e. web
                                                               sults from a subset of the most representative com-
pages) expressing an opinion about specific enti-
                                                               binations. Generally, we’ll use the cosine nor-
   3
     http://www.imdb.com                                       malized variants of unsmoothed delta weighting
   4
     The dataset can be found at: http://www.cs.cornell.edu/   schemes, since they perform better than their un-
People/pabo/movie-review-data/review polarity.tar.gz.
   5                                                              7
     The data set can be found at http://www.cs.jhu.edu/            More information about the data set, as well as in-
mdredze/datasets/sentiment/                                    formation on how it can be obtained can be found at:
   6
     http://www.trec.nist.gov                                  http://ir.dcs.gla.ac.uk/test collections/blogs06info.html


                                                           1391


                               Table 5: Statistics about the data sets used.
 Data set                      #Documents #Terms              #Unique Average #Terms
                                                                Terms        per Document
 Movie Reviews                 2,000            1,336,883     39,399         668
 Multi-Domain Sentiment        8,000            1,741,085     455,943        217
 Dataset (MDSD)
 BLOGS06                       17,898           51,252,850     367,899         2,832




                       Figure 1: Reported accuracy on the Movie Review data set.


normalized counterparts. We’ll avoid using nor-        tains an accuracy of 86.6% in comparison to at′ c’s
malization for the smoothed versions, in order to      88.25%, although the simpler at′ c is again as ef-
focus our attention on the results of smoothing,       fective than the BM25 tf (ot′ c), which performs at
rather than normalization.                             88%. The actual idf weighting function is of some
                                                       importance, e.g. ot′ c (88%) vs. okc (87.65%) and
5 Results                                              akc (88%) vs. at′ c (88.25%), with simpler idf fac-
                                                       tors performing similarly, although slightly better
Results for the Movie Reviews, Multi-Domain
                                                       than BM25.
Sentiment Dataset and BLOGS06 corpora are re-
ported in figures 1, 2 and 3 respectively.                Introducing smoothed, localized variants of idf
   On the Movie Review data set, the results re-       and scaled or binary tf weighting schemes pro-
confirm that using binary features (bnc) is bet-       duces significant advantages. In this setting,
ter than raw term frequency (nnc) (83.40%) fea-        smoothing plays a role, e.g. n∆(t)c8 (91.60%)
tures. For reference, in this setting the unnor-       vs. n∆(t′ )n (95.80%) and a∆(p)c (92.80%)
malized vector using the raw tf approach (nnn)         vs. a∆(p′ )n (96.55%), since we can expect zero
performs similar to the normalized (nnc) (83.40%       class-based estimations of idf values, supporting
vs. 83.60%), the former not present in the graph.      our initial hypothesis on its importance. Addition-
Nonetheless, using any scaled tf weighting func-       ally, using augmented, BM25 or binary tf weights
tion (anc or onc) performs as well as the binary       is always better than raw term frequency, pro-
approach (87.90% and 87.50% respectively). Of          viding further support on the advantages of us-
interest is the fact that although the BM25 tf algo-   ing sublinear tf weighting functions9 . In this set-
rithm has proved much more successful in IR, the       ting, the best accuracy of 96.90% is attained using
same doesn’t apply in this setting and its accuracy    BM25 tf weights with the BM25 delta idf variant,
is similar to the simpler augmented tf approach.       although binary or augmented tf weights using
   Incorporating un-localized variants of idf (mid-
                                                            8
dle graph section) produces only small increases              The original Delta tf.idf by Martineau and Finin (2009)
                                                       has a limitation of utilizing features with df > 2. In our
in accuracy. Smoothing also doesn’t provide any        experiments it performed similarly to n∆(t)n (90.60%) but
particular advantage, e.g. btc (88.20%) vs. bt′ c      still lower than the cosine normalized variant n∆(t)c in-
(88.45%), since no zero idf values are present.        cluded in the graph (91.60%).
                                                            9
                                                              Although not present in the graph, for completeness rea-
Again, using more sophisticated tf functions pro-      sons it should be noted that l∆(s)n and L∆(s)n also per-
vides an advantage over raw tf , e.g. nt′ c at-        form very well, both reaching accuracies of approx. 96%.


                                                   1392


                  Figure 2: Reported accuracy on the Multi-Domain Sentiment data set.


delta idf perform similarly (96.50% and 96.60%              Lastly, we present results on the BLOGS06
respectively). The results indicate that the tf and      dataset in figure 3. As previously noted, this data
the idf factor themselves aren’t of significant im-      set is particularly noisy, because it has been an-
portance, as long as the former are scaled and the       notated at the document-level rather than the post-
latter smoothed in some manner. For example,             level and as a result, the differences aren’t as pro-
a∆(p′ )n vs. a∆(t′ )n perform quite similarly.           found as in the previous corpora, although they
   The results from the Multi-Domain Sentiment           do follow the same patterns. Focusing on the
data set (figure 2) largely agree with the find-         delta idf variants, the importance of smoothing
ings on the Movie Review data set, providing a           becomes apparent, e.g. a∆(p)c vs. a∆(p′ )n and
strong indication that the approach isn’t limited        n∆(t)c vs. n∆(t′ )n. Additionally, because of the
to a specific domain. Binary weights outperform          fact that documents tend to be more verbose in
raw term frequency weights and perform similarly         this data set, the scaled tf variants also perform
with scaled tf ’s. Non-localized variants of idf         better than the simple raw tf ones, n∆(t′ )n vs.
weights do provide a small advantage in this data        a∆(t′ )n. Lastly, as previously, the smoothed lo-
set although the actual idf variant isn’t important,     calized idf variants perform better than their un-
e.g. btc, bt′ c, and okc all perform similarly. The      smoothed counterparts, e.g. n∆(t)n vs. n∆(t′ )n
utilized tf variant also isn’t important, e.g. at′ c     and a∆(p)c vs. a∆(p′ )n.
(88.39%) vs. bt′ c (88.25%).
   We focus our attention on the delta idf vari-         6 Conclusions
ants which provide the more interesting results.
The importance of smoothing becomes apparent
                                                         In this paper, we presented a study of document
when comparing the accuracy of a∆(p)c and its
                                                         representations for sentiment analysis using term
smoothed variant a∆(p′ )n (92.56% vs. 95.6%).
                                                         weighting functions adopted from information re-
Apart from that, all smoothed delta idf variants
                                                         trieval and adapted to classification. The pro-
perform very well in this data set, including some-
                                                         posed weighting schemes were tested on a num-
what surprisingly, n∆(t′ )n which uses raw tf
                                                         ber of publicly available datasets and a number
(94.54%). Considering that the average tf per
                                                         of them repeatedly demonstrated significant in-
document is approx. 1.9 in the Movie Review
                                                         creases in accuracy compared to other state-of-the-
data set and 1.1 in the MDSD, the results can be
                                                         art approaches. We demonstrated that for accurate
attributed to the fact that words tend to typically
                                                         classification it is important to use term weight-
appear only once per document in the latter, there-
                                                         ing functions that scale sublinearly in relation to
fore minimizing the difference of the weights at-
                                                         the number of times a term occurs in a document
tributed by different tf functions10 . The best at-
                                                         and that document frequency smoothing is a sig-
tained accuracy is 96.40% but as the MDSD has
                                                         nificant factor.
mainly been used for domain adaptation applica-
tions, there is no clear baseline to compare it with.       In the future we plan to test the proposed
                                                         weighting functions in other domains such as topic
  10
     For reference, the average tf per document in the   classification and additionally extend the approach
BLOGS06 data set is 2.4.                                 to accommodate multi-class classification.


                                                     1393


                           Figure 3: Reported accuracy on the BLOGS06 data set.


Acknowledgments                                            Stephan Greene and Philip Resnik. 2009. More than
                                                              words: Syntactic packaging and implicit sentiment.
This work was supported by a European Union                   In Proceedings of Human Language Technologies:
grant by the 7th Framework Programme, Theme                   The 2009 Annual Conference of the North American
3: Science of complex systems for socially intelli-           Chapter of the Association for Computational Lin-
                                                              guistics, pages 503–511, Boulder, Colorado, June.
gent ICT. It is part of the CyberEmotions Project             Association for Computational Linguistics.
(Contract 231323).
                                                           K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
                                                              A probabilistic model of information retrieval: de-
References                                                    velopment and comparative experiments. Inf. Pro-
                                                              cess. Manage., 36(6):779–808.
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
  2008. Sentiment analysis in multiple languages:          Chenghua Lin and Yulan He. 2009. Joint senti-
  Feature selection for opinion classification in web        ment/topic model for sentiment analysis. In CIKM
  forums. ACM Trans. Inf. Syst., 26(3):1–34.                 ’09: Proceeding of the 18th ACM conference on In-
                                                             formation and knowledge management, pages 375–
Timothy G. Armstrong, Alistair Moffat, William Web-          384, New York, NY, USA. ACM.
  ber, and Justin Zobel. 2009. Improvements that
  don’t add up: ad-hoc retrieval results since 1998.       Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
  In David Wai Lok Cheung, Il Y. Song, Wesley W.             Alexander Hauptmann. 2006. Which side are you
  Chu, Xiaohua Hu, Jimmy J. Lin, David Wai Lok               on? identifying perspectives at the document and
  Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu,             sentence levels. In Proceedings of the Conference
  and Jimmy J. Lin, editors, CIKM, pages 601–610,            on Natural Language Learning (CoNLL).
  New York, NY, USA. ACM.
                                                           Hugo Liu. 2004. MontyLingua: An end-to-end natural
Anthony Aue and Michael Gamon. 2005. Customiz-               language processor with common sense. Technical
  ing sentiment classifiers to new domains: A case           report, MIT.
  study. In Proceedings of Recent Advances in Nat-
  ural Language Processing (RANLP).                        C. Macdonald and I. Ounis. 2006. The trec blogs06
                                                              collection : Creating and analysing a blog test col-
John Blitzer, Mark Dredze, and Fernando Pereira.              lection. DCS Technical Report Series.
  2007. Biographies, bollywood, boom-boxes and
  blenders: Domain adaptation for sentiment classi-        Christopher D. Manning, Prabhakar Raghavan, and
  fication. In Proceedings of the 45th Annual Meet-          Hinrich Schütze. 2008. Introduction to Information
  ing of the Association of Computational Linguistics,       Retrieval. Cambridge University Press, 1 edition,
  pages 440–447, Prague, Czech Republic, June. As-           July.
  sociation for Computational Linguistics.
                                                           J. R. Martin and P. R. R. White. 2005. The language of
Ann Devitt and Khurshid Ahmad. 2007. Sentiment                evaluation : appraisal in English / J.R. Martin and
  polarity identification in financial news: A cohesion-      P.R.R. White. Palgrave Macmillan, Basingstoke :.
  based approach. In Proceedings of the 45th Annual
  Meeting of the Association of Computational Lin-         Justin Martineau and Tim Finin. 2009. Delta TFIDF:
  guistics, pages 984–991, Prague, Czech Republic,            An Improved Feature Space for Sentiment Analysis.
  June. Association for Computational Linguistics.            In Proceedings of the Third AAAI Internatonal Con-
                                                              ference on Weblogs and Social Media, San Jose, CA,
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-             May. AAAI Press. (poster paper).
  Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
  A library for large linear classification. Journal of    A. Mccallum and K. Nigam. 1998. A comparison of
  Machine Learning Research, 9:1871–1874.                    event models for naive bayes text classification.


                                                       1394


G. Mishne. 2005. Experiments with mood classifi-          Fabrizio Sebastiani. 2002. Machine learning in au-
  cation in blog posts. In 1st Workshop on Stylistic        tomated text categorization. ACM Computing Sur-
  Analysis Of Text For Information Access.                  veys, 34(1):1ñ47.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-      Amit Singhal, Gerard Salton, and Chris Buckley. 1995.
  ysis using support vector machines with diverse in-      Length normalization in degraded text collections.
  formation sources. In Dekang Lin and Dekai Wu,           Technical report, Ithaca, NY, USA.
  editors, Proceedings of EMNLP 2004, pages 412–
  418, Barcelona, Spain, July. Association for Com-       Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
  putational Linguistics.                                  out the vote: Determining support or opposition
                                                           from congressional floor-debate transcripts. CoRR,
Charles E. Osgood. 1967. The measurement of mean-          abs/cs/0607062.
  ing / [by] [Charles E. Osgood, George J. Suci [and]
  Percy H. Tannenbaum]. University of Illinois Press,     Peter D. Turney. 2002. Thumbs up or thumbs down?
  Urbana :, 2nd ed. edition.                                semantic orientation applied to unsupervised classi-
                                                            fication of reviews. In ACL, pages 417–424.
Iadh Ounis, Craig Macdonald, and Ian Soboroff. 2008.
   Overview of the trec-2008 blog trac. In The Seven-     Casey Whitelaw, Navendu Garg, and Shlomo Arga-
   teenth Text REtrieval Conference (TREC 2008) Pro-        mon. 2005. Using appraisal groups for sentiment
   ceedings. NIST.                                          analysis. In CIKM ’05: Proceedings of the 14th
                                                            ACM international conference on Information and
Bo Pang and Lillian Lee. 2004. A sentimental educa-         knowledge management, pages 625–631, New York,
  tion: Sentiment analysis using subjectivity summa-        NY, USA. ACM.
  rization based on minimum cuts. In In Proceedings
  of the ACL, pages 271–278.                              Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
                                                            2005. Recognizing contextual polarity in phrase-
B. Pang and L. Lee. 2008. Opinion Mining and Senti-         level sentiment analysis. In Proceedings of Human
   ment Analysis. Now Publishers Inc.                       Language Technologies Conference/Conference on
                                                            Empirical Methods in Natural Language Processing
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
                                                            (HLT/EMNLP 2005), Vancouver, CA.
  2002. Thumbs up? sentiment classification using
  machine learning techniques. In Proceedings of the      Ian H. Witten and Eibe Frank. 1999. Data Mining:
  2002 Conference on Empirical Methods in Natural            Practical Machine Learning Tools and Techniques
  Language Processing (EMNLP).                               with Java Implementations (The Morgan Kaufmann
Rudy Prabowo and Mike Thelwall. 2009. Sentiment              Series in Data Management Systems). Morgan
  analysis: A combined approach. Journal of Infor-           Kaufmann, 1st edition, October.
  metrics, 3(2):143–157, April.                           Alex Wright. 2009. Mining the web for feelings, not
Stephen E. Robertson, Steve Walker, Susan Jones,            facts. August 23, NY Times, last accessed October
   Micheline Hancock-Beaulieu, and Mike Gatford.            2, 2009, http://http://www.nytimes.com/2009/08/24/
   1994. Okapi at trec-3. In TREC, pages 0–.                technology/internet/ 24emotion.html? r=1.

S E Robertson, S Walker, S Jones, M M Hancock-            O.F. Zaidan, J. Eisner, and C.D. Piatko. 2007. Using
  Beaulieu, and M Gatford. 1996. Okapi at trec-2.           Annotator Rationales to Improve Machine Learn-
  In In The Second Text REtrieval Conference (TREC-         ing for Text Categorization. Proceedings of NAACL
  2), NIST Special Special Publication 500-215, pages       HLT, pages 260–267.
  21–34.
                                                          Justin Zobel and Alistair Moffat. 1998. Exploring the
Stephen Robertson, Hugo Zaragoza, and Michael Tay-           similarity space. SIGIR Forum, 32(1):18–34.
   lor. 2004. Simple bm25 extension to multiple
   weighted fields. In CIKM ’04: Proceedings of the
   thirteenth ACM international conference on Infor-
   mation and knowledge management, pages 42–49,
   New York, NY, USA. ACM.
Gerard Salton and Chris Buckley. 1987. Term weight-
  ing approaches in automatic text retrieval. Technical
  report, Ithaca, NY, USA.
Gerard Salton and Michael J. McGill. 1986. Intro-
  duction to Modern Information Retrieval. McGraw-
  Hill, Inc., New York, NY, USA.
G. Salton. 1971. The SMART Retrieval System—
  Experiments in Automatic Document Processing.
  Prentice-Hall, Inc., Upper Saddle River, NJ, USA.


                                                      1395

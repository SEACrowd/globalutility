 Supervised Noun Phrase Coreference Research: The First Fifteen Years

                                           Vincent Ng
                            Human Language Technology Research Institute
                                   University of Texas at Dallas
                                    Richardson, TX 75083-0688
                                  vince@hlt.utdallas.edu


                      Abstract                                published not only in general NLP conferences,
                                                              but also in specialized conferences (e.g., the bien-
    The research focus of computational                       nial Discourse Anaphora and Anaphor Resolution
    coreference resolution has exhibited a                    Colloquium (DAARC)) and workshops (e.g., the
    shift from heuristic approaches to machine                series of Bergen Workshop on Anaphora Resolu-
    learning approaches in the past decade.                   tion (WAR)). Being inherently a clustering task,
    This paper surveys the major milestones in                coreference has also received a lot of attention in
    supervised coreference research since its                 the machine learning community.
    inception fifteen years ago.                                 Fifteen years have passed since the first paper
                                                              on learning-based coreference resolution was pub-
1 Introduction                                                lished (Connolly et al., 1994). Our goal in this
Noun phrase (NP) coreference resolution, the task             paper is to provide NLP researchers with a sur-
of determining which NPs in a text or dialogue re-            vey of the major milestones in supervised coref-
fer to the same real-world entity, has been at the            erence research, focusing on the computational
core of natural language processing (NLP) since               models, the linguistic features, the annotated cor-
the 1960s. NP coreference is related to the task              pora, and the evaluation metrics that were devel-
of anaphora resolution, whose goal is to identify             oped in the past fifteen years. Note that several
an antecedent for an anaphoric NP (i.e., an NP                leading coreference researchers have published
that depends on another NP, specifically its an-              books (e.g., Mitkov (2002)), written survey arti-
tecedent, for its interpretation) [see van Deemter            cles (e.g., Mitkov (1999), Strube (2009)), and de-
and Kibble (2000) for a detailed discussion of the            livered tutorials (e.g., Strube (2002), Ponzetto and
difference between the two tasks]. Despite its sim-           Poesio (2009)) that provide a broad overview of
ple task definition, coreference is generally con-            coreference research. This survey paper aims to
sidered a difficult NLP task, typically involving             complement, rather than supersede, these previ-
the use of sophisticated knowledge sources and                ously published materials. In particular, while ex-
inference procedures (Charniak, 1972). Compu-                 isting survey papers discuss learning-based coref-
tational theories of discourse, in particular focus-          erence research primarily in the context of the in-
ing (see Grosz (1977) and Sidner (1979)) and cen-             fluential mention-pair model, we additionally sur-
tering (Grosz et al. (1983; 1995)), have heavily              vey recently proposed learning-based coreference
influenced coreference research in the 1970s and              models, which attempt to address the weaknesses
1980s, leading to the development of numerous                 of the mention-pair model. Due to space limita-
centering algorithms (see Walker et al. (1998)).              tions, however, we will restrict our discussion to
   The focus of coreference research underwent a              the most commonly investigated kind of corefer-
gradual shift from heuristic approaches to machine            ence relation: the identity relation for NPs, exclud-
learning approaches in the 1990s. This shift can              ing coreference among clauses and bridging refer-
be attributed in part to the advent of the statisti-          ences (e.g., part/whole and set/subset relations).
cal NLP era, and in part to the public availability
                                                              2 Annotated Corpora
of annotated coreference corpora produced as part
of the MUC-6 (1995) and MUC-7 (1998) confer-                  The widespread popularity of machine learning
ences. Learning-based coreference research has                approaches to coreference resolution can be at-
remained vibrant since then, with results regularly           tributed in part to the public availability of an-


                                                        1396
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1396–1411,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


notated coreference corpora. The MUC-6 and             3 Learning-Based Coreference Models
MUC-7 corpora, though relatively small (60 doc-
uments each) and homogeneous w.r.t. document           In this section, we examine three important classes
type (newswire articles only), have been exten-        of coreference models that were developed in the
sively used for training and evaluating coreference    past fifteen years, namely, the mention-pair model,
models. Equally popular are the corpora produced       the entity-mention model, and ranking models.
by the Automatic Content Extraction (ACE1 ) eval-
                                                       3.1 Mention-Pair Model
uations in the past decade: while the earlier ACE
corpora (e.g., ACE-2) consist of solely English        The mention-pair model is a classifier that deter-
newswire and broadcast news articles, the later        mines whether two NPs are coreferent. It was
ones (e.g., ACE 2005) have also included Chi-          first proposed by Aone and Bennett (1995) and
nese and Arabic documents taken from additional        McCarthy and Lehnert (1995), and is one of the
sources such as broadcast conversations, webblog,      most influential learning-based coreference mod-
usenet, and conversational telephone speech.           els. Despite its popularity, this binary classifica-
   Coreference annotations are also publicly avail-    tion approach to coreference is somewhat undesir-
able in treebanks. These include (1) the English       able: the transitivity property inherent in the coref-
Penn Treebank (Marcus et al., 1993), which is la-      erence relation cannot be enforced, as it is possible
beled with coreference links as part of the Onto-      for the model to determine that A and B are coref-
Notes project (Hovy et al., 2006); (2) the Tübingen   erent, B and C are coreferent, but A and C are not
Treebank (Telljohann et al., 2004), which is a         coreferent. Hence, a separate clustering mecha-
collection of German news articles consisting of       nism is needed to coordinate the pairwise classifi-
27,125 sentences; (3) the Prague Dependency            cation decisions made by the model and construct
Treebank (Hajic̆ et al., 2006), which consists of      a coreference partition.
3168 news articles taken from the Czech National          Another issue that surrounds the acquisition of
Corpus; (4) the NAIST Text Corpus (Iida et al.,        the mention-pair model concerns the way train-
2007b), which consists of 287 Japanese news arti-      ing instances are created. Specifically, to deter-
cles; (5) the AnCora Corpus (Recasens and Martı́,      mine whether a pair of NPs is coreferent or not,
2009), which consists of Spanish and Catalan jour-     the mention-pair model needs to be trained on a
nalist texts; and (6) the GENIA corpus (Ohta et al.,   data set where each instance represents two NPs
2002), which contains 2000 MEDLINE abstracts.          and possesses a class value that indicates whether
   Other publicly available coreference corpora of     the two NPs are coreferent. Hence, a natural way
interest include two annotated by Ruslan Mitkov’s      to assemble a training set is to create one instance
research group: (1) a 55,000-word corpus in            from each pair of NPs appearing in a training doc-
the domain of security/terrorism (Hasler et al.,       ument. However, this instance creation method is
2006); and (2) training data released as part of the   rarely employed: as most NP pairs in a text are not
2007 Anaphora Resolution Exercise (Orăsan et al.,     coreferent, this method yields a training set with a
2008), a coreference resolution shared task. There     skewed class distribution, where the negative in-
are also two that consist of spoken dialogues: the     stances significantly outnumber the positives.
TRAINS93 corpus (Heeman and Allen, 1995) and              As a result, in practical implementations of the
the Switchboard data set (Calhoun et al., in press).   mention-pair model, one needs to specify not only
   Additional coreference data will be available in    the learning algorithm for training the model and
the near future. For instance, the SemEval-2010        the linguistic features for representing an instance,
shared task on Coreference Resolution in Multiple      but also the training instance creation method for
Languages (Recasens et al., 2009) has promised to      reducing class skewness and the clustering algo-
release coreference data in six languages. In addi-    rithm for constructing a coreference partition.
tion, Massimo Poesio and his colleagues are lead-      3.1.1 Creating Training Instances
ing an annotation project that aims to collect large
                                                       As noted above, the primary purpose of train-
amounts of coreference data for English via a Web
                                                       ing instance creation is to reduce class skewness.
Collaboration game called Phrase Detectives2 .
                                                       Many heuristic instance creation methods have
   1
       http://www.itl.nist.gov/iad/mig/tests/ace/      been proposed, among which Soon et al.’s (1999;
   2
       http://www.phrasedetectives.org                 2001) is arguably the most popular choice. Given


                                                    1397


an anaphoric noun phrase3 , NPk , Soon et al.’s                  and support vector machines (Joachims, 1999)
method creates a positive instance between NPk                   have been increasingly used, in part due to their
and its closest preceding antecedent, NPj , and a                ability to provide a confidence value (e.g., in the
negative instance by pairing NPk with each of the                form of a probability) associated with a classifica-
intervening NPs, NPj+1 , . . ., NPk−1 .                          tion, and in part due to the fact that they can be
   With an eye towards improving the precision of                easily adapted to train recently proposed ranking-
a coreference resolver, Ng and Cardie (2002c) pro-               based coreference models (see Section 3.3).
pose an instance creation method that involves a
                                                                 3.1.3 Generating an NP Partition
single modification to Soon et al.’s method: if NPk
is non-pronominal, a positive instance should be                 After training, we can apply the resulting model
formed between NPk and its closest preceding non-                to a test text, using a clustering algorithm to co-
pronominal antecedent instead. This modification                 ordinate the pairwise classification decisions and
is motivated by the observation that it is not easy              impose an NP partition. Below we describe some
for a human, let alone a machine learner, to learn               commonly used coreference clustering algorithms.
from a positive instance where the antecedent of a                  Despite their simplicity, closest-first cluster-
non-pronominal NP is a pronoun.                                  ing (Soon et al., 2001) and best-first clustering
   To further reduce class skewness, some re-                    (Ng and Cardie, 2002c) are arguably the most
searchers employ a filtering mechanism on top of                 widely used coreference clustering algorithms.
an instance creation method, thereby disallowing                 The closest-first clustering algorithm selects as the
the creation of training instances from NP pairs                 antecedent for an NP, NPk , the closest preceding
that are unlikely to be coreferent, such as NP pairs             noun phrase that is classified as coreferent with it.4
that violate gender and number agreement (e.g.,                  However, if no such preceding noun phrase exists,
Strube et al. (2002), Yang et al. (2003)).                       no antecedent is selected for NPk . The best-first
   While many instance creation methods are                      clustering algorithm aims to improve the precision
heuristic in nature (see Uryupina (2004) and Hoste               of closest-first clustering, specifically by selecting
and Daelemans (2005)), some are learning-based.                  as the antecedent of NPk the most probable preced-
For example, motivated by the fact that some                     ing NP that is classified as coreferent with it.
coreference relations are harder to identify than                   One criticism of the closest-first and best-first
the others (see Harabagiu et al. (2001)), Ng and                 clustering algorithms is that they are too greedy.
Cardie (2002a) present a method for mining easy                  In particular, clusters are formed based on a small
positive instances, in an attempt to avoid the inclu-            subset of the pairwise decisions made by the
sion of hard training instances that may complicate              model. Moreover, positive pairwise decisions are
the acquisition of an accurate coreference model.                unjustifiably favored over their negative counter-
                                                                 parts. For example, three NPs are likely to end up
3.1.2    Training a Coreference Classifier                       in the same cluster in the resulting partition even if
Once a training set is created, we can train a coref-            there is strong evidence that A and C are not coref-
erence model using an off-the-shelf learning algo-               erent, as long as the other two pairs (i.e., (A,B) and
rithm. Decision tree induction systems (e.g., C5                 (B,C)) are classified as positive.
(Quinlan, 1993)) are the first and one of the most                  Several algorithms that address one or both of
widely used learning algorithms by coreference                   these problems have been used for coreference
researchers, although rule learners (e.g., RIPPER                clustering. Correlation clustering (Bansal et al.,
(Cohen, 1995)) and memory-based learners (e.g.,                  2002), which produces a partition that respects
TiMBL (Daelemans and Van den Bosch, 2005))                       as many pairwise decisions as possible, is used
are also popular choices, especially in early appli-             by McCallum and Wellner (2004), Zelenko et al.
cations of machine learning to coreference resolu-               (2004), and Finley and Joachims (2005). Graph
tion. In recent years, statistical learners such as              partitioning algorithms are applied on a weighted,
maximum entropy models (Berger et al., 1996),                    undirected graph where a vertex corresponds to
voted perceptrons (Freund and Schapire, 1999),                   an NP and an edge is weighted by the pairwise
                                                                 coreference scores between two NPs (e.g., Mc-
    3
      In this paper, we use the term anaphoric to describe any   Callum and Wellner (2004), Nicolae and Nico-
NP that is part of a coreference chain but is not the head of
                                                                    4
the chain. Hence, proper names can be anaphoric under this            If a probabilistic model is used, we can define a threshold
overloaded definition, but linguistically, they are not.         above which a pair of NPs is considered coreferent.


                                                             1398


lae (2006)). The Dempster-Shafer rule (Dempster,                performs as well as their proposed minimum-cut-
1968), which combines the positive and negative                 based graph partitioning algorithm.
pairwise decisions to score a partition, is used by
Kehler (1997) and Bean and Riloff (2004) to iden-               3.1.4 Determining NP Anaphoricity
tify the most probable NP partition.                            While coreference clustering algorithms attempt
    Some clustering algorithms bear a closer resem-             to resolve each NP encountered in a document,
blance to the way a human creates coreference                   only a subset of the NPs are anaphoric and there-
clusters. In these algorithms, not only are the NPs             fore need to be resolved. Hence, knowledge of the
in a text processed in a left-to-right manner, the              anaphoricity of an NP can potentially improve the
later coreference decisions are dependent on the                precision of a coreference resolver.
earlier ones (Cardie and Wagstaff, 1999; Klenner                   Traditionally, the task of anaphoricity determi-
and Ailloud, 2008).5 For example, to resolve an                 nation has been tackled independently of corefer-
NP, NPk , Cardie and Wagstaff’s algorithm consid-               ence resolution using a variety of techniques. For
ers each preceding NP, NPj , as a candidate an-                 example, pleonastic it has been identified using
tecedent in a right-to-left order. If NPk and NPj               heuristic approaches (e.g., Paice and Husk (1987),
are likely to be coreferent, the algorithm imposes              Lappin and Leass (1994), Kennedy and Bogu-
an additional check that NPk does not violate any               raev (1996)), supervised approaches (e.g., Evans
constraint on coreference (e.g., gender agreement)              (2001), Müller (2006), Versley et al. (2008a)),
with any NP in the cluster containing NPj before                and distributional methods (e.g., Bergsma et al.
positing that the two NPs are coreferent.                       (2008)); and non-anaphoric definite descriptions
    Luo et al.’s (2004) Bell-tree-based algorithm is            have been identified using rule-based techniques
another clustering algorithm where the later coref-             (e.g., Vieira and Poesio (2000)) and unsupervised
erence decisions are dependent on the earlier ones.             techniques (e.g., Bean and Riloff (1999)).
A Bell tree provides an elegant way of organizing                  Recently, anaphoricity determination has been
the space of NP partitions. Informally, a node in               evaluated in the context of coreference resolution,
the ith level of a Bell tree corresponds to an ith-             with results showing that training an anaphoric-
order partial partition (i.e., a partition of the first         ity classifier to identify and filter non-anaphoric
i NPs of the given document), and the ith level of              NPs prior to coreference resolution can improve
the tree contains all possible ith-order partial parti-         a learning-based resolver (e.g., Ng and Cardie
tions. Hence, a leaf node contains a complete par-              (2002b), Uryupina (2003), Poesio et al. (2004b)).
tition of the NPs, and the goal is to search for the            Compared to earlier work on anaphoricity deter-
leaf node that contains the most probable partition.            mination, recently proposed approaches are more
The search starts at the root, and a partitioning of            “global” in nature, taking into account the pair-
the NPs is incrementally constructed as we move                 wise decisions made by the mention-pair model
down the tree. Specifically, based on the corefer-              when making anaphoricity decisions. Examples
ence decisions it has made in the first i−1 levels of           of such approaches have exploited techniques in-
the tree, the algorithm determines at the ith level             cluding integer linear programming (ILP) (Denis
whether the ith NP should start a new cluster, or to            and Baldridge, 2007a), label propagation (Zhou
which preceding cluster it should be assigned.                  and Kong, 2009), and minimum cuts (Ng, 2009).
    While many coreference clustering algorithms                3.1.5 Combining Classification & Clustering
have been developed, there have only been a few
attempts to compare their effectiveness. For ex-                From a learning perspective, a two-step approach
ample, Ng and Cardie (2002c) report that best-                  to coreference — classification and clustering —
first clustering is better than closest-first cluster-          is undesirable. Since the classification model
ing. Nicolae and Nicolae (2006) show that best-                 is trained independently of the clustering algo-
first clustering performs similarly to Bell-tree-               rithm, improvements in classification accuracy
based clustering, but neither of these algorithms               do not guarantee corresponding improvements in
                                                                clustering-level accuracy. That is, overall perfor-
   5
     When applying closest-first and best-first clustering,     mance on the coreference task might not improve.
Soon et al. (2001) and Ng and Cardie (2002c) also process          To address this problem, McCallum and Well-
the NPs in a sequential manner, but since the later decisions
are not dependent on the earlier ones, the order in which the   ner (2004) and Finley and Joachims (2005) elimi-
NPs are processed does not affect their clustering results.     nate the classification step entirely, treating coref-


                                                            1399


erence as a supervised clustering task where a sim-           and “Clinton” were in the same cluster, it proba-
ilarity metric is learned to directly maximize clus-          bly would not have posited that “she” and “Clin-
tering accuracy. Klenner (2007) and Finkel and                ton” are coreferent. The aforementioned Cardie
Manning (2008) use ILP to ensure that the pair-               and Wagstaff algorithm attempts to address this
wise classification decisions satisfy transitivity.6          problem in a heuristic manner. It would be de-
                                                              sirable to learn a model that can classify whether
3.1.6    Weaknesses of the Mention-Pair Model
                                                              an NP to be resolved is coreferent with a preced-
While many of the aforementioned algorithms                   ing, possibly partially-formed, cluster. This model
for clustering and anaphoricity determination have            is commonly known as the entity-mention model.
been shown to improve coreference performance,
                                                                 Since the entity-mention model aims to classify
the underlying model with which they are used
                                                              whether an NP is coreferent with a preceding clus-
in combination — the mention-pair model — re-
                                                              ter, each of its training instances (1) corresponds
mains fundamentally weak. The model has two
                                                              to an NP, NPk , and a preceding cluster, Cj , and
commonly-cited weaknesses. First, since each
                                                              (2) is labeled with either POSITIVE or NEGATIVE,
candidate antecedent for an anaphoric NP to be
                                                              depending on whether NPk should be assigned to
resolved is considered independently of the oth-
                                                              Cj . Consequently, we can represent each instance
ers, the model only determines how good a candi-
                                                              by a set of cluster-level features (i.e., features that
date antecedent is relative to the anaphoric NP, but
                                                              are defined over an arbitrary subset of the NPs in
not how good a candidate antecedent is relative to
                                                              Cj ). A cluster-level feature can be computed from
other candidates. In other words, it fails to answer
                                                              a feature employed by the mention-pair model by
the question of which candidate antecedent is most
                                                              applying a logical predicate. For example, given
probable. Second, it has limitations in its expres-
                                                              the NUMBER AGREEMENT feature, which deter-
siveness: the information extracted from the two
                                                              mines whether two NPs agree in number, we can
NPs alone may not be sufficient for making an in-
                                                              apply the ALL predicate to create a cluster-level
formed coreference decision, especially if the can-
                                                              feature, which has the value YES if NPk agrees in
didate antecedent is a pronoun (which is semanti-
                                                              number with all of the NPs in Cj and NO other-
cally empty) or a mention that lacks descriptive in-
                                                              wise. Other commonly-used logical predicates for
formation such as gender (e.g., “Clinton”). Below
                                                              creating cluster-level features include relaxed ver-
we discuss how these weaknesses are addressed by
                                                              sions of the ALL predicate, such as MOST, which
the entity-mention model and ranking models.
                                                              is true if NPk agrees in number with more than half
3.2     Entity-Mention Model                                  of the NPs in Cj , and ANY, which is true as long as
                                                              NPk agrees in number with just one of the NPs in
The entity-mention model addresses the expres-
                                                              Cj . The ability of the entity-mention model to em-
siveness problem with the mention-pair model.
                                                              ploy cluster-level features makes it more expres-
To motivate the entity-mention model, consider
                                                              sive than its mention-pair counterpart.
an example taken from McCallum and Wellner
(2003), where a document consists of three NPs:                  Despite its improved expressiveness, the entity-
“Mr. Clinton,” “Clinton,” and “she.” The mention-             mention model has not yielded particularly en-
pair model may determine that “Mr. Clinton” and               couraging results. For example, Luo et al. (2004)
“Clinton” are coreferent using string-matching                apply the ANY predicate to generate cluster-level
features, and that “Clinton” and “she” are coref-             features for their entity-mention model, which
erent based on proximity and lack of evidence for             does not perform as well as the mention-pair
gender and number disagreement. However, these                model. Yang et al. (2004b; 2008a) also investi-
two pairwise decisions together with transitivity             gate the entity-mention model, which produces re-
imply that “Mr. Clinton” and “she” will end up in             sults that are only marginally better than those of
the same cluster, which is incorrect due to gen-              the mention-pair model. However, it appears that
der mismatch. This kind of error arises in part               they are not fully exploiting the expressiveness of
because the later coreference decisions are not de-           the entity-mention model, as cluster-level features
pendent on the earlier ones. In particular, had the           only comprise a small fraction of their features.
model taken into consideration that “Mr. Clinton”                Variants of the entity-mention model have been
   6
     Recently, however, Klenner and Ailloud (2009) have be-   investigated. For example, Culotta et al. (2007)
come less optimistic about ILP approaches to coreference.     present a first-order logic model that determines


                                                          1400


the probability that an arbitrary set of NPs are all     sible to train a mention ranker that ranks all of
co-referring. Their model resembles the entity-          the candidate antecedents simultaneously. While
mention model in that it enables the use of cluster-     mention rankers have consistently outperformed
level features. Daumé III and Marcu (2005) pro-         the mention-pair model (Versley, 2006; Denis and
pose an online learning model for constructing           Baldridge, 2007b), they are not more expressive
coreference chains in an incremental fashion, al-        than the mention-pair model, as they are unable
lowing later coreference decisions to be made by         to exploit cluster-level features, unlike the entity-
exploiting cluster-level features that are computed      mention model. To enable rankers to employ
over the coreference chains created thus far.            cluster-level features, Rahman and Ng (2009) pro-
                                                         pose the cluster-ranking model, which ranks pre-
3.3   Ranking Models                                     ceding clusters, rather than candidate antecedents,
                                                         for an NP to be resolved. Cluster rankers there-
While the entity-mention model addresses the             fore address both weaknesses of the mention-pair
expressiveness problem with the mention-pair             model, and have been shown to improve mention
model, it does not address the other problem: fail-      rankers. Cluster rankers are conceptually similar
ure to identify the most probable candidate an-          to Lappin and Leass’s (1994) heuristic pronoun re-
tecedent. Ranking models, on the other hand, al-         solver, which resolves an anaphoric pronoun to the
low us to determine which candidate antecedent           most salient preceding cluster.
is most probable given an NP to be resolved.
                                                            An important issue with ranking models that
Ranking is arguably a more natural reformula-
                                                         we have eluded so far concerns the identification
tion of coreference resolution than classification,
                                                         of non-anaphoric NPs. As a ranker simply im-
as a ranker allows all candidate antecedents to be
                                                         poses a ranking on candidate antecedents or pre-
considered simultaneously and therefore directly
                                                         ceding clusters, it cannot determine whether an NP
captures the competition among them. Another
                                                         is anaphoric (and hence should be resolved). To
desirable consequence is that there exists a nat-
                                                         address this problem, Denis and Baldridge (2008)
ural resolution strategy for a ranking approach:
                                                         apply an independently trained anaphoricity clas-
an anaphoric NP is resolved to the candidate an-
                                                         sifier to identify non-anaphoric NPs prior to rank-
tecedent that has the highest rank. This contrasts
                                                         ing, and Rahman and Ng (2009) propose a model
with classification-based approaches, where many
                                                         that jointly learns coreference and anaphoricity.
clustering algorithms have been employed to co-
ordinate the pairwise classification decisions, and
                                                         4 Knowledge Sources
it is still not clear which of them is the best.
    The notion of ranking candidate antecedents          Another thread of supervised coreference research
can be traced back to centering algorithms, many         concerns the development of linguistic features.
of which use grammatical roles to rank forward-          Below we give an overview of these features.
looking centers (see Walker et al. (1998)). Rank-           String-matching features can be computed ro-
ing is first applied to learning-based coreference       bustly and typically contribute a lot to the per-
resolution by Connolly et al. (1994; 1997), where        formance of a coreference system. Besides sim-
a model is trained to rank two candidate an-             ple string-matching operations such as exact string
tecedents. Each training instance corresponds to         match, substring match, and head noun match
the NP to be resolved, NPk , as well as two candi-       for different kinds of NPs (see Daumé III and
date antecedents, NPi and NPj , one of which is an       Marcu (2005)), slightly more sophisticated string-
antecedent of NPk and the other is not. Its class        matching facilities have been attempted, includ-
value indicates which of the two candidates is bet-      ing minimum edit distance (Strube et al., 2002)
ter. This model is referred to as the tournament         and longest common subsequence (Castaño et al.,
model by Iida et al. (2003) and the twin-candidate       2002). Yang et al. (2004a) treat the two NPs in-
model by Yang et al. (2003; 2008b). To resolve an        volved as two bags of words, and compute their
NP during testing, one way is to apply the model to      similarity using metrics commonly-used in infor-
each pair of its candidate antecedents, and the can-     mation retrieval, such as the dot product, with each
didate that is classified as better the largest number   word weighted by their TF-IDF value.
of times is selected as its antecedent.                     Syntactic features are computed based on a
    Advances in machine learning have made it pos-       syntactic parse tree. Ge et al. (1998) implement


                                                     1401


a Hobbs distance feature, which encodes the rank        2007) as well as the semantic class of a noun (Ng,
assigned to a candidate antecedent for a pronoun        2007a; Huang et al., 2009). One difficulty with
by Hobbs’s (1978) seminal syntax-based pronoun          deriving knowledge from WordNet is that one has
resolution algorithm. Luo and Zitouni (2005) ex-        to determine which sense of a given word to use.
tract features from a parse tree for implement-         Some researchers simply use the first sense (Soon
ing Binding Constraints (Chomsky, 1988). Given          et al., 2001) or all possible senses (Ponzetto and
an automatically parsed corpus, Bergsma and Lin         Strube, 2006a), while others overcome this prob-
(2006) extract from each parse tree a dependency        lem with word sense disambiguation (Nicolae and
path, which is represented as a sequence of nodes       Nicolae, 2006). Knowledge has also been mined
and dependency labels connecting a pronoun and          from Wikipedia for measuring the semantic relat-
a candidate antecedent, and collect statistical in-     edness of two NPs, NPj and NPk (Ponzetto and
formation from these paths to determine the like-       Strube (2006a; 2007)), such as: whether NPj/k ap-
lihood that a pronoun and a candidate antecedent        pears in the first paragraph of the Wiki page that
connected by a given path are coreferent. Rather        has NPk/j as the title or in the list of categories to
than deriving features from parse trees, Iida et al.    which this page belongs, and the degree of overlap
(2006) and Yang et al. (2006) employ these trees        between the two pages that have the two NPs as
directly as structured features for pronoun resolu-     their titles (see Poesio et al. (2007) for other uses
tion. Specifically, Yang et al. define tree kernels     of encyclopedic knowledge for coreference reso-
for efficiently computing the similarity between        lution). Contextual roles (Bean and Riloff, 2004),
two parse trees, and Iida et al. use a boosting-based   semantic relations (Ji et al., 2005), semantic roles
algorithm to compute the usefulness of a subtree.       (Ponzetto and Strube, 2006b; Kong et al., 2009),
                                                        and animacy (Orăsan and Evans, 2007) have also
   Grammatical features encode the grammati-
                                                        been exploited to improve coreference resolution.
cal properties of one or both NPs involved in an
instance. For example, Ng and Cardie’s (2002c)             Lexico-syntactic patterns have been used to
resolver employs 34 grammatical features. Some          capture the semantic relatedness between two NPs
features determine NP type (e.g., are both NPs def-     and hence the likelihood that they are coreferent.
inite or pronouns?). Some determine the grammat-        For instance, given the pattern X is a Y (which is
ical role of one or both of the NPs. Some encode        highly indicative that X and Y are coreferent), we
traditional linguistic (hard) constraints on corefer-   can instantiate it with a pair of NPs and search
ence. For example, coreferent NPs have to agree         for the instantiated pattern in a large corpus or
in number and gender and cannot span one an-            the Web (Daumé III and Marcu, 2005; Haghighi
other (e.g., “Google” and “Google employees”).          and Klein, 2009). The more frequently the pat-
There are also features that encode general linguis-    tern occurs, the more likely they are coreferent.
tic preferences either for or against coreference.      This technique has been applied to resolve dif-
For example, an indefinite NP (that is not in ap-       ferent kinds of anaphoric references, including
position to an anaphoric NP) is not likely to be        other-anaphora (Modjeska et al., 2003; Markert
coreferent with any NP that precedes it.                and Nissim, 2005) and bridging references (Poesio
                                                        et al., 2004a). While these patterns are typically
   There has been an increasing amount of work on
                                                        hand-crafted (e.g., Garera and Yarowsky (2006)),
investigating semantic features for coreference
                                                        they can also be learned from an annotated cor-
resolution. One of the earliest kinds of seman-
                                                        pus (Yang and Su, 2007) or bootstrapped from an
tic knowledge employed for coreference resolu-
                                                        unannotated corpus (Bean and Riloff, 2004).
tion is perhaps selectional preference (Dagan and
Itai, 1990; Kehler et al., 2004b; Yang et al., 2005;       Despite the large amount of work on discourse-
Haghighi and Klein, 2009): given a pronoun to be        based anaphora resolution in the 1970s and
resolved, its governing verb, and its grammatical       1980s (see Hirst (1981)), learning-based resolvers
role, we prefer a candidate antecedent that can be      have only exploited shallow discourse-based fea-
governed by the same verb and be in the same role.      tures, which primarily involve characterizing the
Semantic knowledge has also been extracted from         salience of a candidate antecedent by measuring
WordNet and unannotated corpora for computing           its distance from the anaphoric NP to be resolved
the semantic compatibility/similarity between two       or determining whether it is in a prominent gram-
common nouns (Harabagiu et al., 2001; Versley,          matical role (e.g., subject). A notable exception


                                                    1402


is Iida et al. (2009), who train a ranker to rank              these three extraction methods typically produce
the candidate antecedents for an anaphoric pro-                different numbers of NPs: the NPs extracted from
noun by their salience. It is worth noting that                a parser tend to significantly outnumber the system
Tetreault (2005) has employed Grosz and Sid-                   mentions, which in turn outnumber the gold NPs.
ner’s (1986) discourse theory and Veins Theory                 The reasons are two-fold. First, in some corefer-
(Ide and Cristea, 2000) to identify and remove                 ence corpora (e.g., MUC-6 and MUC-7), the NPs
candidate antecedents that are not referentially ac-           that are not part of any coreference chain are not
cessible to an anaphoric pronoun in his heuristic              annotated. Second, in corpora such as those pro-
pronoun resolvers. It would be interesting to in-              duced by the ACE evaluations, only the NPs that
corporate this idea into a learning-based resolver.            belong to one of the ACE entity types (e.g., PER -
   There are also features that do not fall into any           SON , ORGANIZATION , LOCATION ) are annotated.
of the preceding categories. For example, a mem-                   Owing in large part to the difference in the num-
orization feature is a word pair composed of the               ber of NPs extracted by these three methods, a
head nouns of the two NPs involved in an in-                   coreference resolver can produce substantially dif-
stance (Bengtson and Roth, 2008). Memoriza-                    ferent results when applied to the resulting three
tion features have been used as binary-valued fea-             sets of NPs, with gold NPs yielding the best results
tures indicating the presence or absence of their              and NPs extracted from a parser yielding the worst
words (Luo et al., 2004) or as probabilistic fea-              (Nicolae and Nicolae, 2006). While researchers
tures indicating the probability that the two heads            who evaluate their resolvers on gold NPs point out
are coreferent according to the training data (Ng,             that the results can more accurately reflect the per-
2007b). An anaphoricity feature indicates whether              formance of their coreference algorithm, Stoyanov
an NP to be resolved is anaphoric, and is typ-                 et al. (2009) argue that such evaluations are unre-
ically computed using an anaphoricity classifier               alistic, as NP extraction is an integral part of an
(Ng, 2004), hand-crafted patterns (Daumé III and              end-to-end fully-automatic resolver.
Marcu, 2005), and automatically acquired pat-                      Whichever NP extraction method is employed,
terns (Bean and Riloff, 1999). Finally, the outputs            it is clear that the use of gold NPs can considerably
of rule-based pronoun and coreference resolvers                simplify the coreference task, and hence resolvers
have also been used as features for learning-based             employing different extraction methods should not
coreference resolution (Ng and Cardie, 2002c).                 be compared against each other.
   For an empirical evaluation of the contribution
of a subset of these features to the mention-pair              5.2 Scoring a Coreference Partition
model, see Bengtson and Roth (2008).
                                                               The MUC scorer (Vilain et al., 1995) is the first
5 Evaluation Issues                                            program developed for scoring coreference parti-
                                                               tions. It has two often-cited weaknesses. As a link-
Two important issues surround the evaluation of a              based measure, it does not reward correctly iden-
coreference resolver. First, how do we obtain the              tified singleton clusters since there is no corefer-
set of NPs that a resolver will partition? Second,             ence link in these clusters. Also, it tends to under-
how do we score the partition it produces?                     penalize partitions with overly large clusters.
                                                                   To address these problems, two coreference
5.1   Extracting Candidate Noun Phrases
                                                               scoring programs have been developed: B3
To obtain the set of NPs to be partitioned by a re-            (Bagga and Baldwin, 1998) and CEAF (Luo,
solver, three methods are typically used. In the               2005). Note that both scorers have only been de-
first method, the NPs are extracted automatically              fined for the case where the key partition has the
from a syntactic parser. The second method in-                 same set of NPs as the response partition. To apply
volves extracting the NPs directly from the gold               these scorers to automatically extracted NPs, dif-
standard. In the third method, a mention detec-                ferent methods have been proposed (see Rahman
tor is first trained on the gold-standard NPs in the           and Ng (2009) and Stoyanov et al. (2009)).
training texts, and is then applied to automatically               Since coreference is a clustering task, any
extract system mentions in a test text.7 Note that             general-purpose method for evaluating a response
  7
    An exception is Daumé III and Marcu (2005), whose         partition against a key partition (e.g., Kappa (Car-
model jointly learns to extract NPs and perform coreference.   letta, 1996)) can be used for coreference scor-


                                                           1403


ing (see Popescu-Belis et al. (2004)). In practice,      cent systems are becoming more sophisticated, we
these general-purpose methods are typically used         suggest that researchers make their systems pub-
to provide scores that complement those obtained         licly available in order to facilitate performance
via the three coreference scorers discussed above.       comparisons. Publicly available coreference sys-
It is worth mentioning that there is a trend to-         tems currently include JavaRAP (Qiu et al., 2004),
wards evaluating a resolver against multiple scor-       GuiTaR (Poesio and Kabadjov, 2004), BART (Ver-
ers, which can indirectly help to counteract the         sley et al., 2008b), CoRTex (Denis and Baldridge,
bias inherent in a particular scorer. For further dis-   2008), the Illinois Coreference Package (Bengt-
cussion on evaluation issues, see Byron (2001).          son and Roth, 2008), CherryPicker (Rahman and
                                                         Ng, 2009), Reconcile (Stoyanov et al., 2010), and
6 Concluding Remarks                                     Charniak and Elsner’s (2009) pronoun resolver.
                                                            We conclude with a discussion of two ques-
While we have focused our discussion on super-           tions regarding supervised coreference research.
vised approaches, coreference researchers have           First, what is the state of the art? This is not an
also attempted to reduce a resolver’s reliance on        easy question, as researchers have been evaluat-
annotated data by combining a small amount of            ing their resolvers on different corpora using dif-
labeled data and a large amount of unlabeled             ferent evaluation metrics and preprocessing tools.
data using general-purpose semi-supervised learn-        In particular, preprocessing tools can have a large
ing algorithms such as co-training (Müller et al.,      impact on the performance of a resolver (Barbu
2002), self-training (Kehler et al., 2004a), and EM      and Mitkov, 2001). Worse still, assumptions about
(Cherry and Bergsma, 2005; Ng, 2008). Interest-          whether gold or automatically extracted NPs are
ingly, recent results indicate that unsupervised ap-     used are sometimes not explicitly stated, poten-
proaches to coreference resolution (e.g., Haghighi       tially causing results to be interpreted incorrectly.
and Klein (2007; 2010), Poon and Domingos                To our knowledge, however, the best results on the
(2008)) rival their supervised counterparts, casting     MUC-6 and MUC-7 data sets using automatically
doubts on whether supervised resolvers are mak-          extracted NPs are reported by Yang et al. (2003)
ing effective use of the available labeled data.         (71.3 MUC F-score) and Ng and Cardie (2002c)
    Another issue that we have not focused on but        (63.4 MUC F-score), respectively;8 and the best
which is becoming increasingly important is mul-         results on the ACE data sets using gold NPs can
tilinguality. While many of the techniques dis-          be found in Luo (2007) (88.4 ACE-value).
cussed in this paper were originally developed for          Second, what lessons can we learn from fifteen
English, they have been applied to learn coref-          years of learning-based coreference research?
erence models for other languages, such as Chi-          The mention-pair model is weak because it makes
nese (e.g., Converse (2006)), Japanese (e.g., Iida       coreference decisions based on local informa-
(2007)), Arabic (e.g., Luo and Zitouni (2005)),          tion (i.e., information extracted from two NPs).
Dutch (e.g., Hoste (2005)), German (e.g., Wun-           Expressive models (e.g., those that can exploit
sch (2010)), Swedish (e.g., Nilsson (2010)), and         cluster-level features) generally offer better perfor-
Czech (e.g., Ngu.y et al. (2009)). In addition, re-      mance, and so are models that are “global” in na-
searchers have developed approaches that are tar-        ture. Global coreference models may refer to any
geted at handling certain kinds of anaphora present      kind of models that can exploit non-local infor-
in non-English languages, such as zero anaphora          mation, including models that can consider mul-
(e.g., Iida et al. (2007a), Zhao and Ng (2007)).         tiple candidate antecedents simultaneously (e.g.,
    As Mitkov (2001) puts it, coreference resolution     ranking models), models that allow joint learning
is a “difficult, but not intractable problem,” and       for coreference resolution and related tasks (e.g.,
we have been making “slow, but steady progress”          anaphoricity determination), models that can di-
on improving machine learning approaches to the          rectly optimize clustering-level (rather than classi-
problem in the past fifteen years. To ensure fur-        fication) accuracy, and models that can coordinate
ther progress, researchers should compare their re-      with other components of a resolver, such as train-
sults against a baseline that is stronger than the       ing instance creation and clustering.
commonly-used Soon et al. (2001) system, which              8
                                                             These results by no means suggest that no progress has
relies on a weak model (i.e., the mention-pair           been made since 2003: most of the recently proposed coref-
model) and a small set of linguistic features. As re-    erence models were evaluated on the ACE data sets.


                                                     1404


Acknowledgments                                             Shane Bergsma, Dekang Lin, and Randy Goebel.
                                                              2008. Distributional identification of non-referential
We thank the three anonymous reviewers for their              pronouns. In Proceedings of ACL-08: HLT, pages
invaluable comments on an earlier draft of the pa-            10–18.
per. This work was supported in part by NSF                 Donna Byron. 2001. The uncommon denominator: A
Grant IIS-0812261. Any opinions, findings, and                proposal for consistent reporting of pronoun resolu-
conclusions or recommendations expressed are                  tion results. Computational Linguistics, 27(4):569–
those of the author and do not necessarily reflect            578.
the views or official policies, either expressed or         Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
implied, of the NSF.                                          Mayo, Dan Jurafsky, Mark Steedman, and David
                                                              Beaver. (in press). The NXT-format Switchboard
                                                              corpus: A rich resource for investigating the syn-
References                                                    tax, semantics, pragmatics and prosody of dialogue.
                                                              Language Resources and Evaluation.
Chinatsu Aone and Scott William Bennett. 1995.
  Evaluating automated and manual acquisition of            Claire Cardie and Kiri Wagstaff. 1999. Noun phrase
  anaphora resolution strategies. In Proceedings of the       coreference as clustering. In Proceedings of the
  33rd Annual Meeting of the Association for Compu-           1999 Joint SIGDAT Conference on Empirical Meth-
  tational Linguistics, pages 122–129.                        ods in Natural Language Processing and Very Large
                                                              Corpora, pages 82–89.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
 scoring coreference chains. In Proceedings of the          Jean Carletta. 1996. Assessing agreement on classi-
 LREC Workshop on Linguistic Coreference, pages                fication tasks: the kappa statistic. Computational
 563–566.                                                      Linguistics, 22(2):249–254.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2002.         José Castaño, Jason Zhang, and James Pustejovsky.
  Correlation clustering. In Proceedings of the 43rd           2002. Anaphora resolution in biomedical literature.
  Annual IEEE Symposium on Foundations of Com-                 In Proceedings of the 2002 International Symposium
  puter Science, pages 238–247.                                on Reference Resolution.
Catalina Barbu and Ruslan Mitkov. 2001. Evaluation          Eugene Charniak and Micha Elsner. 2009. EM works
  tool for rule-based anaphora resolution methods. In         for pronoun anaphora resolution. In Proceedings of
  Proceedings of the 39th Annual Meeting of the Asso-         the 12th Conference of the European Chapter of the
  ciation for Computational Linguistics, pages 34–41.         Association for Computational Linguistics, pages
                                                              148–156.
David Bean and Ellen Riloff. 1999. Corpus-based
  identification of non-anaphoric noun phrases. In          Eugene Charniak. 1972. Towards a Model of Chil-
  Proceedings of the 37th Annual Meeting of the As-           dren’s Story Comphrension. AI-TR 266, Artificial
  sociation for Computational Linguistics, pages 373–         Intelligence Laboratory, Massachusetts Institute of
  380.                                                        Technology, USA.
David Bean and Ellen Riloff. 2004. Unsupervised             Colin Cherry and Shane Bergsma. 2005. An expecta-
  learning of contextual role knowledge for corefer-          tion maximization approach to pronoun resolution.
  ence resolution. In Human Language Technologies             In Proceedings of the Ninth Conference on Compu-
  2004: The Conference of the North American Chap-            tational Natural Language Learning, pages 88–95.
  ter of the Association for Computational Linguistics;
  Proceedings of the Main Conference, pages 297–            Noam Chomsky. 1988. Language and Problems of
  304.                                                        Knowledge. The Managua Lectures. MIT Press,
                                                              Cambridge, Massachusetts.
Eric Bengtson and Dan Roth. 2008. Understanding the
   values of features for coreference resolution. In Pro-   William Cohen. 1995. Fast effective rule induction. In
   ceedings of the 2008 Conference on Empirical Meth-         Proceedings of the 12th International Conference on
   ods in Natural Language Processing, pages 294–             Machine Learning, pages 115–123.
   303.
                                                            Dennis Connolly, John D. Burger, and David S. Day.
Adam L. Berger, Stephen A. Della Pietra, and Vin-             1994. A machine learning approach to anaphoric
  cent J. Della Pietra. 1996. A maximum entropy               reference. In Proceedings of International Con-
  approach to natural language processing. Compu-             ference on New Methods in Language Processing,
  tational Linguistics, 22(1):39–71.                          pages 255–261.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping           Dennis Connolly, John D. Burger, and David S. Day.
  path-based pronoun resolution. In Proceedings of            1997. A machine learning approach to anaphoric
  the 21st International Conference on Computational          reference. In D. Jones and H. Somers, editors, New
  Linguistics and the 44th Annual Meeting of the Asso-        Methods in Language Processing, pages 133–144.
  ciation for Computational Linguistics, pages 33–40.         UCL Press.


                                                        1405


Susan Converse. 2006. Pronominal Anaphora Resolu-         Yoav Freund and Robert E. Schapire. 1999. Large
  tion in Chinese. Ph.D. thesis, University of Pennsyl-     margin classification using the perceptron algorithm.
  vania, USA.                                               Machine Learning, 37(3):277–296.

Aron Culotta, Michael Wick, and Andrew McCallum.          Nikesh Garera and David Yarowsky. 2006. Resolving
  2007. First-order probabilistic models for corefer-       and generating definite anaphora by modeling hy-
  ence resolution. In Human Language Technologies           pernymy using unlabeled corpora. In Proceedings
  2007: The Conference of the North American Chap-          of the Tenth Conference on Computational Natural
  ter of the Association for Computational Linguistics;     Language Learning, pages 37–44.
  Proceedings of the Main Conference, pages 81–88.
                                                          Niyu Ge, John Hale, and Eugene Charniak. 1998. A
Walter Daelemans and Antal Van den Bosch. 2005.             statistical approach to anaphora resolution. In Pro-
  Memory-Based Language Processing. Cambridge               ceedings of the Sixth Workshop on Very Large Cor-
  University Press, Cambridge, UK.                          pora, pages 161–170.

Ido Dagan and Alon Itai. 1990. Automatic processing       Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
   of large corpora for the resolution of anaphora ref-     tion, intentions, and the structure of discourse. Com-
   erences. In Proceedings of the 13th International        putational Linguistics, 12(3):175–204.
   Conference on Computational Linguistics, pages
   330–332.                                               Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
                                                            stein. 1983. Providing a unified account of definite
Hal Daumé III and Daniel Marcu. 2005. A large-             noun phrases in discourse. In Proceedings of the
  scale exploration of effective global features for a      21st Annual Meeting of the Association for Compu-
  joint entity detection and tracking model. In Pro-        tational Linguistics, pages 44–50.
  ceedings of the Human Language Technology Con-
                                                          Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
  ference and the Conference on Empirical Methods
                                                            stein. 1995. Centering: A framework for model-
  in Natural Language Processing, pages 97–104.
                                                            ing the local coherence of discourse. Computational
Arthur Dempster. 1968. A generalization of Bayesian         Linguistics, 21(2):203–226.
  inference. Journal of the Royal Statistical Society,    Barbara J. Grosz. 1977. The representation and use of
  30:205–247.                                               focus in a system for understanding dialogs. In Pro-
                                                            ceedings of the Fifth International Joint Conference
Pascal Denis and Jason Baldridge. 2007a. Global,            on Artificial Intelligence, pages 67–76.
  joint determination of anaphoricity and coreference
  resolution using integer programming. In Human          Aria Haghighi and Dan Klein. 2007. Unsupervised
  Language Technologies 2007: The Conference of             coreference resolution in a nonparametric bayesian
  the North American Chapter of the Association for         model. In Proceedings of the 45th Annual Meet-
  Computational Linguistics; Proceedings of the Main        ing of the Association of Computational Linguistics,
  Conference, pages 236–243.                                pages 848–855.
Pascal Denis and Jason Baldridge. 2007b. A ranking        Aria Haghighi and Dan Klein. 2009. Simple coref-
  approach to pronoun resolution. In Proceedings of         erence resolution with rich syntactic and semantic
  the Twentieth International Conference on Artificial      features. In Proceedings of the 2009 Conference on
  Intelligence, pages 1588–1593.                            Empirical Methods in Natural Language Process-
                                                            ing, pages 1152–1161.
Pascal Denis and Jason Baldridge. 2008. Special-
  ized models and ranking for coreference resolution.     Aria Haghighi and Dan Klein. 2010. Coreference
  In Proceedings of the 2008 Conference on Empiri-          resolution in a modular, entity-centered model. In
  cal Methods in Natural Language Processing, pages         Proceedings of Human Language Technologies: The
  660–669.                                                  2010 Annual Conference of the North American
                                                            Chapter of the Association for Computational Lin-
Richard Evans. 2001. Applying machine learning to-          guistics.
  ward an automatic classification of it. Literary and
  Linguistic Computing, 16(1):45–57.                      Jan Hajic̆, Jarmila Panevová, Eva Hajic̆ová, Jarmila
                                                             Panevová, Petr Sgall, Petr Pajas, Jan Stĕpánek, Jir̆ı́
Jenny Rose Finkel and Christopher Manning. 2008.             Havelka, and Marie Mikulová. 2006. The Prague
   Enforcing transitivity in coreference resolution. In      Dependency Treebank 2.0. In Linguistic Data Con-
   Proceedings of ACL-08: HLT, Short Papers, pages           sortium.
   45–48.
                                                          Sanda Harabagiu, Răzvan Bunescu, and Steven Maio-
Thomas Finley and Thorsten Joachims. 2005. Super-           rano. 2001. Text and knowledge mining for corefer-
  vised clustering with support vector machines. In         ence resolution. In Proceedings of the 2nd Meeting
  Proceedings of the 22nd International Conference          of the North American Chapter of the Association
  on Machine Learning, pages 217–224.                       for Computational Linguistics, pages 55–62.


                                                      1406


Laura Hasler, Constantin Orasan, and Karin Naumann.       Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2009.
  2006. NPs for events: Experiments in coreference          Capturing salience with a trainable cache model for
  annotation. In Proceedings of the 5th International       zero-anaphora resolution. In Proceedings of the
  Conference on Language Resources and Evaluation,          Joint Conference of the 47th Annual Meeting of the
  pages 1167–1172.                                          ACL and the 4th International Joint Conference on
                                                            Natural Language Processing of the AFNLP, pages
Peter Heeman and James Allen. 1995. The TRAINS              647–655.
  spoken dialog corpus. CD-ROM, Linguistic Data
  Consortium.                                             Ryu Iida. 2007. Combining Linguistic Knowledge and
                                                            Machine Learning for Anaphora Resolution. Ph.D.
Graeme Hirst. 1981. Discourse-oriented anaphora             thesis, Nara Institute of Science and Technology,
  resolution in natural language understanding: A re-       Japan.
  view. American Journal of Computational Linguis-
  tics, 7(2):85–98.                                       Heng Ji, David Westbrook, and Ralph Grishman. 2005.
                                                            Using semantic relations to refine coreference deci-
Jerry Hobbs. 1978. Resolving pronoun references.
                                                            sions. In Proceedings of the Human Language Tech-
   Lingua, 44:311–338.
                                                            nology Conference and the Conference on Empiri-
Véronique Hoste and Walter Daelemans. 2005. Com-           cal Methods in Natural Language Processing, pages
   paring learning approaches to coreference resolu-        17–24.
   tion. There is more to it than bias. In Proceedings
   of the ICML Workshop on Meta-Learning.                 Thorsten Joachims. 1999. Making large-scale SVM
                                                            learning practical. In Bernhard Scholkopf and
Véronique Hoste. 2005. Optimization Issues in Ma-          Alexander Smola, editors, Advances in Kernel Meth-
   chine Learning of Coreference Resolution. Ph.D.          ods - Support Vector Learning, pages 44–56. MIT
   thesis, University of Antewerp, Belgium.                 Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance        Andrew Kehler, Douglas Appelt, Lara Taylor, and
  Ramshaw, and Ralph Weischedel. 2006. Ontonotes:           Aleksandr Simma. 2004a. Competitive self-trained
  The 90% solution. In Proceedings of the Human             pronoun interpretation. In Proceedings of HLT-
  Language Technology Conference of the NAACL,              NAACL 2004: Short Papers, pages 33–36.
  Companion Volume: Short Papers, pages 57–60.
                                                          Andrew Kehler, Douglas Appelt, Lara Taylor, and
Zhiheng Huang, Guangping Zeng, Weiqun Xu, and               Aleksandr Simma. 2004b. The (non)utility of
  Asli Celikyilmaz. 2009. Accurate semantic class           predicate-argument frequencies for pronoun inter-
  classifier for coreference resolution. In Proceedings     pretation. In Human Language Technologies 2004:
  of the 2009 Conference on Empirical Methods in            The Conference of the North American Chapter of
  Natural Language Processing, pages 1232–1240.             the Association for Computational Linguistics; Pro-
                                                            ceedings of the Main Conference, pages 289–296.
Nancy Ide and Dan Cristea. 2000. A hierarchical ac-
  count of referential accessibility. In Proceedings of   Andrew Kehler. 1997. Probabilistic coreference in in-
  the 38th Annual Meeting of the Association for Com-       formation extraction. In Proceedings of the Second
  putational Linguistics, pages 416–424.                    Conference on Empirical Methods in Natural Lan-
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji           guage Processing, pages 163–173.
  Matsumoto. 2003. Incorporating contextual cues
                                                          Christopher Kennedy and Branimir Boguraev. 1996.
  in trainable models for coreference resolution. In
                                                            Anaphor for everyone: Pronominal anaphora resolu-
  Proceedings of the EACL Workshop on The Compu-
                                                            tion without a parser. In Proceedings of the 16th In-
  tational Treatment of Anaphora.
                                                            ternational Conference on Computational Linguis-
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006.           tics, pages 113–118.
  Exploting syntactic patterns as clues in zero-
  anaphora resolution. In Proceedings of the 21st In-     Manfred Klenner and Étienne Ailloud. 2008. Enhanc-
  ternational Conference on Computational Linguis-         ing coreference clustering. In Proceedings of the
  tics and the 44th Annual Meeting of the Association      Second Workshop on Anaphora Resolution, pages
  for Computational Linguistics, pages 625–632.            31–40.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007a.        Manfred Klenner and Étienne Ailloud. 2009. Op-
  Zero-anaphora resolution by learning rich syntactic      timization in coreference resolution is not needed:
  pattern features. ACM Transactions on Asian Lan-         A nearly-optimal algorithm with intensional con-
  guage Information Processing, 6(4).                      straints. In Proceedings of the 12th Conference of
                                                           the European Chapter of the Association for Com-
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji           putational Linguistics, pages 442–450.
  Matsumoto. 2007b. Annotating a Japanese text cor-
  pus with predicate-argument and coreference rela-       Manfred Klenner. 2007. Enforcing consistency on
  tions. In Proceedings of the ACL Workshop ’Lin-          coreference sets. In Proceedings of Recent Ad-
  guistic Annotation Workshop’, pages 132–139.             vances in Natural Language Processing.


                                                      1407


Fang Kong, GuoDong Zhou, and Qiaoming Zhu. 2009.         Ruslan Mitkov. 2001. Outstanding issues in anaphora
  Employing the centering theory in pronoun resolu-        resolution. In Al. Gelbukh, editor, Computational
  tion from the semantic perspective. In Proceedings       Linguistics and Intelligent Text Processing, pages
  of the 2009 Conference on Empirical Methods in           110–125. Springer.
  Natural Language Processing, pages 987–996.
                                                         Ruslan Mitkov. 2002. Anaphora Resolution. Long-
Shalom Lappin and Herbert Leass. 1994. An algo-            man.
  rithm for pronominal anaphora resolution. Compu-
  tational Linguistics, 20(4):535–562.                   Natalia N. Modjeska, Katja Markert, and Malvina Nis-
                                                           sim. 2003. Using the web in machine learning
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual        for other-anaphora resolution. In Proceedings of the
  coreference resolution with syntactic features. In       2003 Conference on Empirical Methods in Natural
  Proceedings of the Human Language Technology             Language Processing, pages 176–183.
  Conference and the Conference on Empirical Meth-
  ods in Natural Language Processing, pages 660–         MUC-6. 1995. Proceedings of the Sixth Message Un-
  667.                                                    derstanding Conference.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda      MUC-7. 1998. Proceedings of the Seventh Message
  Kambhatla, and Salim Roukos. 2004. A mention-           Understanding Conference.
  synchronous coreference resolution algorithm based
  on the Bell tree. In Proceedings of the 42nd Annual    Christoph Müller, Stefan Rapp, and Michael Strube.
  Meeting of the Association for Computational Lin-        2002. Applying co-training to reference resolution.
  guistics, pages 135–142.                                 In Proceedings of the 40th Annual Meeting of the As-
                                                           sociation for Computational Linguistics, pages 352–
Xiaoqiang Luo. 2005. On coreference resolution per-        359.
  formance metrics. In Proceedings of the Human
  Language Technology Conference and the Confer-         Christoph Müller. 2006. Automatic detection of non-
  ence on Empirical Methods in Natural Language            referential it in spoken multi-party dialog. In Pro-
  Processing, pages 25–32.                                 ceedings of the 11th Conference of the European
                                                           Chapter of the Association for Computational Lin-
Xiaoqiang Luo. 2007. Coreference or not: A twin            guistics, pages 49–56.
  model for coreference resolution. In Human Lan-
  guage Technologies 2007: The Conference of the         Vincent Ng and Claire Cardie. 2002a. Combining
  North American Chapter of the Association for            sample selection and error-driven pruning for ma-
  Computational Linguistics; Proceedings of the Main       chine learning of coreference rules. In Proceedings
  Conference, pages 73–80.                                 of the 2002 Conference on Empirical Methods in
                                                           Natural Language Processing, pages 55–62.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
  Marcinkiewicz. 1993. Building a large annotated        Vincent Ng and Claire Cardie. 2002b. Identifying
  corpus of English: The Penn Treebank. Computa-           anaphoric and non-anaphoric noun phrases to im-
  tional Linguistics, 19(2):313–330.                       prove coreference resolution. In Proceedings of
Katja Markert and Malvina Nissim. 2005. Comparing          the 19th International Conference on Computational
  knowledge sources for nominal anaphora resolution.       Linguistics, pages 730–736.
  Computational Linguistics, 31(3):367–402.
                                                         Vincent Ng and Claire Cardie. 2002c. Improving ma-
Andrew McCallum and Ben Wellner. 2003. Toward              chine learning approaches to coreference resolution.
  conditional models of identity uncertainty with ap-      In Proceedings of the 40th Annual Meeting of the As-
  plication to proper noun coreference. In Proceed-        sociation for Computational Linguistics, pages 104–
  ings of the IJCAI Workshop on Information Integra-       111.
  tion on the Web.
                                                         Vincent Ng. 2004. Learning noun phrase anaphoricity
Andrew McCallum and Ben Wellner. 2004. Condi-              to improve conference resolution: Issues in repre-
  tional models of identity uncertainty with applica-      sentation and optimization. In Proceedings of the
  tion to noun coreference. In Advances in Neural In-      42nd Annual Meeting of the Association for Compu-
  formation Proceesing Systems.                            tational Linguistics, pages 151–158.

Joseph McCarthy and Wendy Lehnert. 1995. Using           Vincent Ng. 2007a. Semantic class induction and
   decision trees for coreference resolution. In Pro-      coreference resolution. In Proceedings of the 45th
   ceedings of the Fourteenth International Conference     Annual Meeting of the Association of Computational
   on Artificial Intelligence, pages 1050–1055.            Linguistics, pages 536–543.

Ruslan Mitkov. 1999. Anaphora resolution: The            Vincent Ng. 2007b. Shallow semantics for coreference
  state of the art. Technical Report (Based on the         resolution. In Proceedings of the Twentieth Inter-
  COLING/ACL-98 tutorial on anaphora resolution),          national Joint Conference on Artificial Intelligence,
  University of Wolverhampton, Wolverhampton.              pages 1689–1694.


                                                     1408


Vincent Ng. 2008. Unsupervised models for corefer-           Proeedings of the ACL Workshop on Reference Res-
  ence resolution. In Proceedings of the 2008 Con-           olution.
  ference on Empirical Methods in Natural Language
  Processing, pages 640–649.                              Massimo Poesio, David Day, Ron Artstein, Jason Dun-
                                                           can, Vladimir Eidelman, Claudio Giuliano, Rob
Vincent Ng. 2009. Graph-cut-based anaphoricity de-         Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,
  termination for coreference resolution. In Proceed-      Stanley Yong Wai Keong, Gideon Mann, Alessan-
  ings of Human Language Technologies: The 2009            dro Moschitti, Simone Ponzetto, Jason Smith, Josef
  Annual Conference of the North American Chap-            Steinberger, Michael Strube, Jian Su, Yannick Vers-
  ter of the Association for Computational Linguistics,    ley, Xiaofeng Yang, and Michael Wick. 2007. EL-
  pages 575–583.                                           ERFED: Final report of the research group on Ex-
                                                           ploiting Lexical and Encyclopedic Resources For
Giang Linh Ngu.y, Václav Novák, and Zdeněk              Entity Disambiguation. Technical report, Summer
  Žabokrtský. 2009. Comparison of classification and     Workshop on Language Engineering, Center for
  ranking approaches to pronominal anaphora resolu-        Language and Speech Processing, Johns Hopkins
  tion in Czech. In Proceedings of the SIGDIAL 2009        University, Baltimore, MD.
  Conference, pages 276–285.
                                                          Simone Paolo Ponzetto and Massimo Poesio. 2009.
Cristina Nicolae and Gabriel Nicolae. 2006. Best-           State-of-the-art NLP approaches to coreference res-
  Cut: A graph algorithm for coreference resolution.        olution: Theory and practical recipes. In Tutorial
  In Proceedings of the 2006 Conference on Empiri-          Abstracts of ACL-IJCNLP 2009, page 6.
  cal Methods in Natural Language Processing, pages
  275–283.                                                Simone Paolo Ponzetto and Michael Strube. 2006a.
                                                            Exploiting semantic role labeling, WordNet and
Kristina Nilsson. 2010. Hybrid Methods for Coref-           Wikipedia for coreference resolution. In Human
  erence Resolution in Swedish. Ph.D. thesis, Stock-        Language Technologies 2006: The Conference of
  holm University, Sweden.                                  the North American Chapter of the Association for
                                                            Computational Linguistics; Proceedings of the Main
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.          Conference, pages 192–199.
  The GENIA corpus: An annotated research abstract
  corpus in molecular biology domain. In Proceed-         Simone Paolo Ponzetto and Michael Strube. 2006b.
  ings of the Second International Conference on Hu-        Semantic role labeling for coreference resolution. In
  man Language Technology Research, pages 82–86.            Proceedings of the 11th Conference of the European
                                                            Chapter of the Association for Computational Lin-
Constantin Orăsan and Richard Evans. 2007. NP ani-         guistics, pages 143–146.
  macy identification for anaphora resolution. Journal
                                                          Simone Paolo Ponzetto and Michael Strube. 2007.
  of Artificial Intelligence Research, 29:79 – 103.
                                                            Knowledge derived from Wikipedia for computing
Constantin Orăsan, Dan Cristea, Ruslan Mitkov, and         semantic relatedness. Journal of Artificial Intelli-
  António H. Branco. 2008. Anaphora Resolution             gence Research, 30:181–212.
  Exercise: An overview. In Proceedings of the 6th        Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
  Language Resources and Evaluation Conference,             pervised coreference resolution with Markov Logic.
  pages 2801–2805.                                          In Proceedings of the 2008 Conference on Empiri-
Chris Paice and Gareth Husk. 1987. Towards the au-          cal Methods in Natural Language Processing, pages
  tomatic recognition of anaphoric features in English      650–659.
  text: the impersonal pronoun ’it’. Computer Speech      Andrei Popescu-Belis, Loı̈s Rigouste, Susanne
  and Language, 2:109–132.                                  Salmon-Alt, and Laurent Romary. 2004. Online
                                                            evaluation of coreference resolution. In Proceedings
Massimo Poesio and Mijail A. Kabadjov. 2004. A
                                                            of the 4th International Conference on Language
 general-purpose, off-the-shelf anaphora resolution
                                                            Resources and Evaluation, pages 1507–1510.
 module: Implementation and preliminary evalua-
 tion. In Proceedings of the 4th International Confer-    Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004.
 ence on Language Resources and Evaluation, pages           A public reference implementation of the RAP
 663–668.                                                   anaphora resolution algorithm. In Proceedings of
                                                            the 4th International Conference on Language Re-
Massimo Poesio, Rahul Mehta, Axel Maroudas, and             sources and Evaluation, pages 291–294.
 Janet Hitzeman. 2004a. Learning to resolve bridg-
 ing references. In Proceedings of the 42nd Annual        John Ross Quinlan. 1993. C4.5: Programs for Ma-
 Meeting of the Association for Computational Lin-          chine Learning. Morgan Kaufmann, San Mateo,
 guistics, pages 143–150.                                   CA.
Massimo Poesio, Olga Uryupina, Renata Vieira, Mijail      Altaf Rahman and Vincent Ng. 2009. Supervised mod-
 Alexandrov-Kabadjov, and Rodrigo Goulart. 2004b.           els for coreference resolution. In Proceedings of the
 Discourse-new detectors for definite description res-      2009 Conference on Empirical Methods in Natural
 olution: A survey and a preliminary proposal. In           Language Processing, pages 968–977.


                                                      1409


Marta Recasens and M. Antónia Martı́. 2009. AnCora-      Olga Uryupina. 2003. High-precision identification of
 CO: Coreferentially annotated corpora for Spanish          discourse new and unique noun phrases. In Proceed-
 and Catalan. Language Resources and Evaluation,            ings of the ACL Student Research Workshop, pages
 43(4).                                                     80–86.
Marta Recasens, Toni Martı́, Mariona Taulé, Lluı́s       Olga Uryupina. 2004. Linguistically motivated sample
 Màrquez, and Emili Sapena. 2009. SemEval-                 selection for coreference resolution. In Proceedings
 2010 Task 1: Coreference resolution in multiple lan-       of the 5th Discourse Anaphora and Anaphor Reso-
 guages. In Proceedings of the Workshop on Seman-           lution Colloquium.
 tic Evaluations: Recent Achievements and Future
 Directions (SEW-2009), pages 70–75.                      Kees van Deemter and Rodger Kibble. 2000. On core-
                                                            ferring: Coreference in MUC and related annotation
Candace Sidner. 1979. Towards a Computational The-          schemes. Computational Linguistics, 26(4):629–
  ory of Definite Anaphora Comprehension in English         637.
  Discourse. Ph.D. thesis, Massachusetts Institute of
  Technology, USA.                                        Yannick Versley, Alessandro Moschitti, Massimo Poe-
                                                            sio, and Xiaofeng Yang. 2008a. Coreference sys-
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.             tems based on kernels methods. In Proceedings
  1999. Corpus-based learning for noun phrase coref-        of the 22nd International Conference on Computa-
  erence resolution. In Proceedings of the 1999 Joint       tional Linguistics, pages 961–968.
  SIGDAT Conference on Empirical Methods in Nat-
  ural Language Processing and Very Large Corpora,        Yannick Versley, Simone Paolo Ponzetto, Massimo
  pages 285–291.                                            Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
                                                            Xiaofeng Yang, and Alessandro Moschitti. 2008b.
Wee Meng Soon, Hwee Tou Ng, and Daniel                      BART: A modular toolkit for coreference resolution.
  Chung Yong Lim. 2001. A machine learning ap-              In Proceedings of the ACL-08: HLT Demo Session,
  proach to coreference resolution of noun phrases.         pages 9–12.
  Computational Linguistics, 27(4):521–544.
                                                          Yannick Versley. 2006. A constraint-based approach
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and        to noun phrase coreference resolution in German
  Ellen Riloff. 2009. Conundrums in noun phrase             newspaper text. In Konferenz zur Verarbeitung
  coreference resolution: Making sense of the state-        Natürlicher Sprache.
  of-the-art. In Proceedings of the Joint Conference of
  the 47th Annual Meeting of the ACL and the 4th In-      Yannick Versley. 2007. Antecedent selection tech-
  ternational Joint Conference on Natural Language          niques for high-recall coreference resolution. In
  Processing of the AFNLP, pages 656–664.                   Proceedings of the 2007 Joint Conference on Empir-
                                                            ical Methods in Natural Language Processing and
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen      Computational Natural Language Learning, pages
  Riloff, David Buttler, and David Hysom. 2010.             496–505.
  Coreference resolution with Reconcile. In Proceed-
  ings of the ACL 2010 Conference Short Papers.           Renata Vieira and Massimo Poesio. 2000. Process-
                                                            ing definite descriptions in corpora. In S. Botley
Michael Strube, Stefan Rapp, and Christoph Müller.         and A. McEnery, editors, Corpus-based and Compu-
  2002. The influence of minimum edit distance on           tational Approaches to Discourse Anaphora, pages
  reference resolution. In Proceedings of the 2002          189–212. UCL Press.
  Conference on Empirical Methods in Natural Lan-
  guage Processing, pages 312–319.                        Marc Vilain, John Burger, John Aberdeen, Dennis Con-
                                                           nolly, and Lynette Hirschman. 1995. A model-
Michael Strube. 2002. NLP approaches to reference          theoretic coreference scoring scheme. In Proceed-
  resolution. In Tutorial Abstracts of ACL 2002, page      ings of the Sixth Message Understanding Confer-
  124.                                                     ence, pages 45–52.
Michael Strube. 2009. Anaphernresolution. In Com-         Marilyn Walker, Aravind Joshi, and Ellen Prince, edi-
  puterlinguistik und Sprachtechnologie. Eine Ein-         tors. 1998. Centering Theory in Discourse. Oxford
  fuhrung. Springer, Heidelberg, Germany, 3rd edi-         University Press.
  tion.
                                                          Holger Wunsch. 2010. Rule-based and Memory-based
Heike Telljohann, Erhard Hinrichs, and Sandra Kübler.      Pronoun Resolution for German: A Comparison and
  2004. The tüba-d/z treebank: Annotating German           Assessment of Data Sources. Ph.D. thesis, Univer-
  with a context-free backbone. In Proceedings of           sity of Tübingen, Germany.
  the 4th International Conference on Language Re-
  sources and Evaluation, pages 2229–2235.                Xiaofeng Yang and Jian Su. 2007. Coreference reso-
                                                            lution using semantic relatedness information from
Joel Tetreault. 2005. Empirical Evaluations of              automatically discovered patterns. In Proceedings
  Pronoun Resolution. Ph.D. thesis, University of           of the 45th Annual Meeting of the Association for
  Rochester, USA.                                           Computational Linguistics, pages 528–535.


                                                      1410


Xiaofeng Yang, Guodong Zhou, Jian Su, and
  Chew Lim Tan. 2003. Coreference resolution us-
  ing competitive learning approach. In Proceedings
  of the 41st Annual Meeting of the Association for
  Computational Linguistics, pages 176–183.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2004a.
  Improving noun phrase coreference resolution by
  matching strings. In Proceedings of the First In-
  ternational Joint Conference on Natural Language
  Processing, pages 22–31.
Xiaofeng Yang, Jian Su, GuoDong Zhou, and
  Chew Lim Tan. 2004b. An NP-cluster based ap-
  proach to coreference resolution. In Proceedings of
  the 20th International Conference on Computational
  Linguistics, pages 226–232.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
  Improving pronoun resolution using statistics-based
  semantic compatibility information. In Proceedings
  of the 43rd Annual Meeting of the Association for
  Computational Linguistics, pages 165–172.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
  Kernel based pronoun resolution with structured
  syntactic knowledge. In Proceedings of the 21st In-
  ternational Conference on Computational Linguis-
  tics and the 44th Annual Meeting of the Association
  for Computational Linguistics, pages 41–48.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
  and Sheng Li. 2008a. An entity-mention model
  for coreference resolution with inductive logic pro-
  gramming. In Proceedings of ACL-08: HLT, pages
  843–851.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2008b. A
  twin-candidate model for learning-based anaphora
  resolution. Computational Linguistics, 34(3):327–
  356.
Dmitry Zelenko, Chinatsu Aone, and Jason Tibbetts.
 2004. Coreference resolution for information ex-
 traction. In Proceedings of the ACL Workshop on
 Reference Resolution and its Applications, pages 9–
 16.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
  tion and resolution of Chinese zero pronouns: A ma-
  chine learning approach. In Proceedings of the 2007
  Joint Conference on Empirical Methods on Natu-
  ral Language Processing and Computational Natu-
  ral Language Learning, pages 541–550.
GuoDong Zhou and Fang Kong. 2009. Global learn-
  ing of noun phrase anaphoricity in coreference res-
  olution via label propagation. In Proceedings of the
  2009 Conference on Empirical Methods in Natural
  Language Processing, pages 978–986.




                                                     1411

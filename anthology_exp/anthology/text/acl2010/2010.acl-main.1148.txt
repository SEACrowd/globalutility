                           Detecting Experiences from Weblogs


                   Keun Chan Park, Yoonjae Jeong and Sung Hyon Myaeng
                              Department of Computer Science
                      Korea Advanced Institute of Science and Technology
                    {keunchan, hybris, myaeng}@kaist.ac.kr




                                                             personal experiences tend to be found in weblogs
                     Abstract                                more often than other web documents like news
                                                             articles, home pages, and scientific papers. As
    Weblogs are a source of human activity know-             such, we have begun to see some research efforts
    ledge comprising valuable information such as            in mining experience-related attributes such as
    facts, opinions and personal experiences. In             time, location, topic, and experiencer, and their
    this paper, we propose a method for mining               relations from weblogs (Inui et al., 2008; Kura-
    personal experiences from a large set of web-
                                                             shima et al., 2009).
    logs. We define experience as knowledge em-
    bedded in a collection of activities or events              Mined experiences can be of practical use in
    which an individual or group has actually un-            wide application areas. For example, a collection
    dergone. Based on an observation that expe-              of experiences from the people who visited a
    rience-revealing sentences have a certain lin-           resort area would help planning what to do and
    guistic style, we formulate the problem of de-           how to do things correctly without having to
    tecting experience as a classification task us-          spend time sifting through a variety of resources
    ing various features including tense, mood, as-          or rely on commercially-oriented sources.
    pect, modality, experiencer, and verb classes.           Another example would be a public service de-
    We also present an activity verb lexicon con-            partment gleaning information about how a park
    struction method based on theories of lexical
                                                             is being used at a specific location and time.
    semantics. Our results demonstrate that the ac-
    tivity verb lexicon plays a pivotal role among              Experiences can be recorded around a frame
    selected features in the classification perfor-          like “who did what, when, where, and why” al-
    mance and shows that our proposed method                 though opinions and emotions can be also linked.
    outperforms the baseline significantly.                  Therefore attributes such as location, time, and
                                                             activity and their relations must be extracted by
1    Introduction                                            devising a method for selecting experience-
                                                             containing sentences based on verbs that have a
In traditional philosophy, human beings are                  particular linguistics case frame or belong to a
known to acquire knowledge mainly by reason-                 “do” class (Kurashima et al., 2009). However,
ing and experience. Reasoning allows us to draw              this kind of method may extract the following
a conclusion based on evidence, but people tend              sentences as containing an experience:
to believe it firmly when they experience or ob-
serve it in the physical world. Despite the fact             [1]   If Jason arrives on time, I’ll buy him a drink.
                                                             [2]   Probably, she will laugh and dance in his funeral.
that direct experiences play a crucial role in mak-
                                                             [3]   Can anyone explain what is going on here?
ing a firm decision and solving a problem,                   [4]   Don’t play soccer on the roads!
people often resort to indirect experiences by
reading written materials or asking around.                  None of the sentences contain actual experiences
Among many sources people resort to, the Web                 because hypotheses, questions, and orders have
has become the largest one for human expe-                   not actually happened in the real world. For ex-
riences, especially with the proliferation of web-           perience mining, it is important to ensure a sen-
logs.                                                        tence mentions an event or passes a factuality
   While Web documents contain various types                 test to contain experience (Inui et al., 2008).
of information including facts, encyclopedic                     In this paper, we focus on the problem of de-
knowledge, opinions, and experiences in general,             tecting experiences from weblogs. We formulate


                                                        1464
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1464–1472,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


               Class                 Examples                     larity detection, and holder extraction under the
         State                 like, know, believe                names of opinion mining and sentiment analysis
         Activity              run, swim, walk                    (Pang and Lee, 2008).
         Achievement           recognize, realize                    In summary, our contribution lies in three as-
                               paint (a picture),                 pects: 1) conception of experience detection,
         Accomplishment
                               build (a house)                    which is a precursor for experience mining, and
            Table 1. Vendler class examples                       specific related tasks that can be tackled with a
                                                                  high performance machine learning based solu-
the problem as a classification task using various                tion; 2) examination and identification of salient
linguistic features including tense, mood, aspect,                linguistic features for experience detection; 3) a
modality, experiencer, and verb classes.                          novel lexicon construction method with identifi-
   Based on our observation that experience-                      cation of key features to be used for verb classi-
revealing sentences tend to have a certain lin-                   fication.
guistic style (Jijkoun et al., 2010), we investigate                 The remainder of the paper is organized as fol-
on the roles of various features. The ability to                  lows. Section 2 presents our lexicon construction
detect experience-revealing sentences should be                   method with experiments. Section 3 describes
a precursor for ensuring the quality of extracting                the experience detection method, including expe-
various elements of actual experiences.                           rimental setup, evaluation, and results. In Section
    Another issue addressed in this paper is au-                  4, we discuss related work, before we close with
tomatic construction of a lexicon for verbs re-                   conclusion and future work in Section 5.
lated to activities and events. While there have
been well-known studies about classifying verbs                   2     Lexicon Construction
based on aspectual features (Vendler, 1967),
thematic roles and selectional restrictions (Fill-                Since our definition of experience is based on
more, 1968; Somers, 1987; Kipper et al., 2008),                   activities and events, it is critical to determine
valence alternations and intuitions (Levin, 1993)                 whether a sentence contains a predicate describ-
and conceptual structures (Fillmore and Baker,                    ing an activity or an event. To this end, it is quite
2001), we found that none of the existing lexical                 conceivable that a lexicon containing activity /
resources such as Framenet (Baker et al., 2003)                   event verbs would play a key role. Given that
and Verbnet (Kipper et al., 2008) are sufficient                  our ultimate goal is to extract experiences from a
for identifying experience-revealing verbs. We                    large amount of weblogs, we opt for increased
introduce a method for constructing an activi-                    coverage by automatically constructing a lexicon
ty/event verb lexicon based on Vendler’s theory                   rather than high precision obtainable by manual-
and statistics obtained by utilizing a web search                 ly crafted lexicon.
engine.                                                              Based on the theory of Vendler (1967), we
    We define experience as knowledge embed-                      classify a given verb or a verb phrase into one of
ded in a collection of activities or events which                 the two categories: activity and state. We consid-
an individual or group has actually undergone1. It                er all the verbs and verb phrases in WordNet
can be subjective as in opinions as well as objec-                (Fellbaum, 1998) which is the largest electronic
tive, but our focus in this article lies in objective             lexical database. In addition to the linguistic
knowledge. The following sentences contain ob-                    schemata features based on Vendler’s theory, we
jective experiences:                                              used thematic role features and an external
                                                                  knowledge feature.
[5] I ran with my wife 3 times a week until we
    moved to Washington, D.C.                                     2.1    Background
[6] Jane and I hopped on a bus into the city centre.
[7] We went to a restaurant near the central park.
                                                                  Vendler (1967) proposes that verb meanings can
                                                                  be categorized into four basic classes, states, ac-
Whereas sentences like the following contain                      tivities, achievements, and accomplishments, de-
subjective knowledge:                                             pending on interactions between the verbs and
[8] I like your new style. You’re beautiful!                      their aspectual and temporal modifiers. Table 1
[9] The food was great, the interior too.                         shows some examples for the classes.
                                                                     Vendler (1967) and Dowty (1979) introduce
Subject knowledge has been studied extensively                    linguistic schemata that serve as evidence for the
for various functions such as identification, po-                 classes.
1
    http://en.wikipedia.org/wiki/Experience_(disambiguation)


                                                               1465


       Linguistic
                     bs     prs   prp   pts   ptp
                                                       perception level. On the other hand, activity and
       Schemata                                        accomplishment are processes (transeunt opera-
      No schema      ■      ■       ■   ■     ■        tions) in traditional philosophy. We henceforth
      Progressive                   ■                  call the first genus activity and the latter state.
      Force          ■                                 Our aim is to classify verbs into the two genuses.
      Persuade       ■
                                                       2.2   Features based on Linguistic Schemata
      Stop                          ■
      For            ■      ■       ■   ■     ■        We developed a relatively simple computational
      Carefully      ■      ■       ■   ■     ■        testing method for the schemata. Assuming that
                                                       an awkward expression like, “John is liking
Table 2. Query matrix. The “■” indicates that the      something” won’t occur frequently, for example,
query is applied. No Schema indicates that no          we generated a co-occurrence based test for the
schema is applied when the word itself is a query.     first linguistic schema using the Web as a corpus.
bs, prs, prp, pts, ptp correspond to base form,        By issuing a search query, ((be OR am OR is OR
present simple (3rd person singular), present par-     was OR were OR been) and ? ing) where ‘?’
ticiple, past simple and past participle, respect-     represents the verb at hand, to a search engine,
fully.                                                 we can get an estimate about how the verb is
                                                       likely to belong to state. A test can be generated
Below are the six schemata we chose because            for each of the schemata in a similar way.
they can be tested automatically: progressive,             For completeness, we considered all the verb
force, persuade, stop, for, and carefully (An aste-    forms (i.e., 3rd person singular present, present
risk denotes that the statement is awkward).
                                                       participle, simple past, past participle) available.
   • States cannot occur in progressive tense:         However, some of the patterns cannot be applied
         John is running.
                                                       to some forms. For example, other forms except
         John is liking.*
                                                       the base form cannot come as a complement of
  •    States cannot occur as complements of
                                                       force (e.g., force to runs.*). Therefore, we
       force and persuade:
                                                       created a query matrix which represents all query
         John forced harry to run.
         John forced harry to know.*                   patterns we have applied, in table 2.
         John persuaded harry to know.*                   Based on the query matrix in table 2, we is-
  •    Achievements cannot occur as comple-            sued queries for all the verbs and verb phrases
       ments of stop:                                  from WordNet to a search engine. We used the
         John stopped running.                         Google news archive search for two reasons.
         John stopped realizing.*                      First, since news articles are written rather for-
  •    Achievements cannot occur with time ad-         mally compared to weblogs and other web pages,
       verbial for:                                    the statistics obtained for a test would be more
         John ran for an hour.                         reliable. Second, Google provides an advanced
         John realized for an hour.*                   option to retrieve snippets containing the query
  •    State and achievement cannot occur with         word. Normally, a snippet is composed of 3~5
       adverb carefully:                               sentences.
         John runs carefully.                             The basic statistics we consider are hit count,
         John knows carefully.*                        candidate sentence count and correct sentence
The schemata are not perfect because verbs can         count which we use the notations Hij(w), Sij(w),
shift classes due to various contextual factors        and Cij(w), respectfully, where w is a word, i the
such as arguments and senses. However, a verb          linguistic schema and j the verb form from the
certainly has its fundamental class that is its most   query matrix in table 2. Hij(w) was directly ga-
natural category at least in its dominant use.         thered from the Google search engine. Sij(w) is
   The four classes can further be grouped into        the number of sentences containing the word w
two genuses: a genus of processes going on in          in the search result snippets. Cij(w) is the number
time and the other that refers to non-processes.       of correct sentences matching the query pattern
Activity and accomplishment belong to the for-         among the candidate sentences. For example, the
mer whereas state and achievement belong to the        progressive schema for a verb “build” can re-
latter. As can be seen in table 1, states are rather   trieve the following sentences.
immanent operations and achievements are those         [10] …, New-York, is building one of the largest …
occur in a single moment or operations related to      [11] Is building an artifact?



                                                    1466


“Building” in the first example is a progressive           The subject of a state verb is dative (D) as in [12]
verb, but the one in second is a noun, which does          whereas the subject for an action verb takes the
not satisfy the linguistic schema. For a POS and           agent (A) role. In addition, a verb with the in-
grammatical check of a candidate sentence, we              strument (I) role tends to be an action verb. From
used the Stanford POS tagger (Toutanova et al.,            these observations, we can use the distribution of
2003) and Stanford dependency parser (Klein                cases (thematic roles) for a verb in a corpus. Ac-
and Manning, 2003).                                        tivity verbs are expected to have high frequency
   For each linguistic schema, we derived three            of agent and instrument roles than state verbs.
features: Absolute hit ratio, Relative hit ratio and       Although a verb may have more than one case
Valid ratio for which we use the notations Ai(w),          frame, it is possible to determine which thematic
Ri(w) and Vi(w), respectfully, where w is a word           roles used more dominantly.
and i a linguistic schema. The index j for summa-             We utilize two major resources of lexical se-
tions represents the j-th verb form. They are              mantics, Verbnet (Kipper et al., 2008) based on
computed as follows.                                       the theory of Levin (1993), and Framenet (Baker
                                                           et al., 2003), which is based on Fillmore (1968).
            Ai ( w ) =
                         ∑   j
                                 H ij ( w )
                                                           Levin (1993) demonstrated that syntactic alterna-
                         H i ( *)                          tions can be the basis for groupings of verbs se-
                                                           mantically and accord reasonably well with lin-
            Ri ( w ) =
                             ∑      j
                                        H ij ( w )
                                                     (1)   guistic intuitions. Verbnet provides 274 verb
                     ∑H      j
                              ( w)  No Scheme              classes with 23 thematic roles covering 3,769
                                                           verbs based on their alternation behaviors with
            V ( w) =
                     ∑ C ( w)j     ij                      thematic roles annotated. Framenet defines 978
                     ∑ S ( w)                              semantic frames with 7,124 unique semantic
              i
                             j    ij
                                                           roles, covering 11,583 words including verbs,
Absolute hit ratio is computes the extent to               nouns, adverbs, etc.
which the target word w occurs with the i-th                  Using Verbnet alone does not suit our needs
schema over all occurrences of the schema. The             because it has a relatively small number of ex-
denominator is the hit count of wild card “*”              ample sentences. Framenet contains a much larg-
matching any single word with the schema pat-              er number of examples but the vast number of
tern from Google (e.g., H1(*), the progressive             semantic roles presents a problem. In order to get
test hit count is 3.82 × 108). Relative hit ratio          meaningful distributions for a manageable num-
computes the extent to which the target word w             ber of thematic roles, we used Semlink (Loper et
occurs with the i-th schema over all occurrences           al., 2007) that provides a mapping between Fra-
of the word. The denominator is the sum of all             menet and Verbnet and uses a total of 23 themat-
verb forms. Valid ratio means the fraction of cor-         ic roles of Verbnet for the annotated corpora of
rect sentences among candidate sentences. The              the two resources. By the mapping, we obtained
weight of a linguistic schema increases as the             distributions of the thematic roles for 2,868
valid ratio gets high. With the three different            unique verbs that exist in both of the resources.
ratios, Ai(w), Ri(w) and Vi(w), for each test, we          For example, the verb “construct” has high fre-
can generate a total of 18 features.                       quencies with agent, material and product roles.
2.3    Features based on case frames                       2.4     Features based on how-to instructions
Since the hit count via Google API sometimes               Ryu et al. (2010) presented a method for extract-
returns unreliable results (e.g., when the query           ing action steps for how-to goals from eHow2 a
becomes too long in case of long verb phrases),            website containing a large number of how-to in-
we also consider additional features. While our            structions. The authors attempted to extract ac-
initial observation indicated that the existing lex-       tions comprising a verb and some ingredients
ical resources would not be sufficient for our             like an object entity from the documents based
goal, it occurred to us that the linguistic theory         on syntactic patterns and a CRF based model.
behind them would be worth exploring as gene-                 Since each extracted action has its probability,
rating additional features for categorizing verbs          we can use the value as a feature for state / activ-
for the two classes. Consider the following ex-            ity verb classification. However, a verb may ap-
amples:                                                    pear in different contexts and can have multiple
[12] John(D) believed(V) the story(O).
                                                           2
[13] John(A) hit(V) him(O) with a bat(I).                      http://www.ehow.com

                                                       1467


                          ME               SVM             matic role distributions, and one from eHow. In
      Feature
                  Prec.     Recall     Prec. Recall        order to examine which features are discrimina-
      All 43       68%       50%       83%       75%       tive for the classification, we used two well
      Top 30       72%       52%       83%       75%       known feature selection methods, Chi-square and
      Top 20       83%       76%       85%       77%
                                                           information gain.
      Top 10       89%       88%       91%       78%

         Table 3. Classification Performance               2.6    Results

         Class                  Examples
                                                           Table 3 shows the classification performance
                                                           values for different feature selection methods.
                    act, battle, build, carry, chase,
       Activity     drive, hike, jump, kick, sky           The evaluation was done on the training data
                    dive, tap dance, walk, …               with 10-fold cross validation.
                    admire, believe, know, like,              Note that the precision and recall are macro-
       State                                               averaged values across the two classes, activity
                    love, …
                                                           and state. The most discriminative features were
               Table 4. Classified Examples                absolute ratio and relative ratio in conjunction
probability values. To generate a single value for         with the force, stop, progressive, and persuade
a verb, we combine multiple probability values             schemata, the role distribution of experiencer,
using the following sigmoid function:                      and the eHow evidence.
                           1                                  It is noteworthy that eHow evidence and the
               E ( w) =                                    distribution of experiencer got into the top 10.
                        1 + e−t                (2)         Other thematic roles did not perform well be-
                    t = ∑ d ∈D Pd ( w)                     cause of the data sparseness. Only a few roles
                                   w

                                                           (e.g., experience, agent, topic, location) among
Evidence of a word w being an action in eHow is            the 23 had frequency values other than 0 for
denoted as E(w) where variable t is the sum of             many verbs. Data sparseness affected the linguis-
individual action probability values in Dw the set         tic schemata as well. Many of the verbs had zero
of documents from which the word w has been                hit counts for the for and carefully schemata. It is
extracted as an action. The higher probability a           also interesting that the validity ratio Vi(w) was
word gets and the more frequent the word has
                                                           not shown to be a good feature-generating statis-
been extracted as an action, the more evidence
                                                           tic.
we get.
                                                              We finally trained our model with the top 10
2.5     Classification                                     features and classified all WordNet verbs and
                                                           verb phrases. For actual construction of the lex-
For training, we selected 80 seed verbs from               icon, 11,416 verbs and verb phrases were classi-
Dowty’s list (1979) which are representative               fied into the two classes roughly equally. We
verbs for each Vendler (1967) class. The selec-            randomly sampled 200 items and examined how
tion was based on the lack of word sense ambi-             accurately the classification was done. A total of
guity.                                                     164 items were correctly classified, resulting in
   One of our classifiers is based on Maximum              82% accuracy. Some examples from the classifi-
Entropy (ME) models that implement the intui-              cation are shown in table 4.
tion that the best model will be the one that is              A further analysis of the results show that
consistent with the set of constraints imposed by          most of the errors occurred with domain-specific
the evidence, but otherwise is as uniform as               verbs (e.g., ablactate, alkalify, and transaminate
possible (Berger et al., 1996). ME models are              in chemistry) and multi-word verb phrases (e.g.,
widely used in natural language processing tasks           turn a nice dime; keep one’s shoulder to the
for its flexibility to incorporate a diverse range of      wheel). Since many features are computed based
features. The other one is based on Support Vec-           on Web resources, rare verbs cannot be classified
tor Machine (Chang and Lin, 2001) which is the             correctly when their hit rations are very low. The
state-of-the-art algorithm for many classification         domain-specific words rarely appear in Framenet
tasks. We used RBF kernel with the default set-            or e-how, either.
tings (Hsu et al., 2009) because it is been known
to show moderate performance using multiple                3     Experience Detection
feature compositions.
   The features we considered are a total of 42            As mentioned earlier, experience-revealing sen-
real values: 18 from linguistic schemata, 23 the-          tences tend to have a certain linguistic style.


                                                        1468


Having converted the problem of experience de-        [14] The stranger messed up the entire garden.
tection for sentences to a classification task, we    [15] His presence messed up the whole situation.
focus on the extent to which various linguistic       The first sentence is considered an experience
features contribute to the performance of the bi-     since the subject is a person. However, the
nary classifier for sentences. We also explain the    second sentence with the same verb is not, be-
experimental setting for evaluation, including the    cause the subject is a non-animate abstract con-
classifier and the test corpus.                       cept. That is, a non-animate noun can hardly
3.1   Linguistic features                             constitute an experience. In order to make a dis-
                                                      tinction, we use the dependency parser and a
In addition to the verb class feature available in    named-entity recognizer (Finkel et al., 2005) that
the verb lexicon constructed automatically, we        can recognize person pronouns and person names.
used tense, mood, aspect, modality, and expe-
riencer features.                                     3.2     Classification
   Verb class: The feature comes directly from        To train our classifier, we first crawled weblogs
the lexicon since a verb has been classified into a   from Wordpress3, one of the most popular blog
state or activity verb. The predicate part of the     sites in use today. Worpress provides an interface
sentence to be classified for experience is looked    to search blog posts with queries. In selecting
up in the lexicon without sense disambiguation.       experience-containing blog pots, we used loca-
   Tense: The tense of a sentence is important        tion names such as Central Park, SOHO, Seoul
since an experience-revealing sentence tends to       and general place names such as airport, subway
use past and present tense. Future tenses are not     station, and restaurant because blog posts with
experiences in most cases. We use POS tagging         some places are expected to describe experiences
(Toutanova et al., 2003) for tense determination,     rather than facts or thoughts.
but since the Penn tagset provides no future             We crawled 6,000 blog posts. After deleting
tenses, they are determined by exploiting modal       non-English and multi-media blog posts for
verbs such as “will” and future expressions such      which we could not obtain any meaningful text
“going to”.                                           data, the number became 5,326. We randomly
   Mood: It is one of distinctive forms that are      sampled 1,000 sentences4 and asked three anno-
used to signal the modal status of a sentence. We     tators to judge whether or not individual sen-
consider three mood categories: indicative, im-       tences are considered containing an experience
perative and subjunctive. We determine the            based on our definition. For maximum accuracy,
mood of a sentence by a small set of heuristic        we decided to use only those sentences all the
rules using the order of POS occurrences and          three annotators agreed, resulting in a total of
punctuation marks.                                    568 sentences.
   Aspect: It defines the temporal flow of a verb        While we tested several classifiers, we chose
in the activity or state. Two categories are used:    to use two different classifiers based on SVM
progressive and perfective. This feature is deter-    and Logistic Regression for the final experimen-
mined by the POS of the predicate in a sentence.      tal results because they showed the best perfor-
   Modality: In linguistics, modals are expres-       mance.
sions broadly associated with notions of possibil-
ity. While modality can be classified at a fine       3.3     Results
level (e.g., epistemic and deontic), we simply        For comparison purposes, we take the method of
determine whether or not a sentence includes a        Kurashima et al. (2005) as our baseline because
modal marker that is involved in the main predi-      the method was used in subsequent studies (Ku-
cate of the sentence. In other words, this binary     rashima et al., 2006; Kurashima et al., 2009)
feature is determined based on the existence of a     where experience attributes are extracted. We
model verb like “can”, “shall”, “must”, and “may”     briefly describe the method and present how we
or a phrase like “have to” or “need to”. The de-      implemented it.
pendency parser is used to ensure a modal mark-          The method first extracts all verbs and their
er is indeed associated with the main predicate.      dependent phrasal unit from candidate sentences.
   Experiencer: A sentence can or cannot be
treated as containing an experience depending on
                                                      3
the subject or experiencer of the verb (note that     4
                                                        http://wordpress.com
this is different from the experiencer role in a        It was due to the limited human resources, but when we
                                                      increased the number at a later stage, the performance in-
case frame). Consider the following sentences:        crease was almost negligible.


                                                 1469


                       Logistic                                                         Logistic
                                               SVM                                                       SVM
    Feature          Regression                                         Feature       Regression
                    Prec. Recall         Prec.      Recall                           Prec. Recall    Prec.   Recall
  Baseline         32.0%      55.1%      25.3%      44.4%             Baseline       32.0%   55.1%   25.3%   44.4%
  Lexicon          77.5%      76.0%      77.5%      76.0%             -Lexicon       84.6%   84.6%   83.1%   81.2%
  Tense            75.1%      75.1%      75.1%      75.1%             -Tense         87.3%   87.1%   86.8%   86.5%
  Mood             75.8%      60.3%      75.8%      60.3%             -Mood          89.5%   89.5%   89.3%   89.2%
  Aspect           26.7%      51.7%      26.7%      51.7%             -Aspect        90.8%   90.5%   89.0%   88.6%
  Modality         79.8%      70.5%      79.8%      70.5%             -Modality      89.5%   89.5%   82.8%   82.8%
  Experiencer      54.3%      53.5%      54.3%      53.5%             -Experiencer   91.5%   91.4%   91.1%   90.8%
  All included     91.9%      91.7%      91.7%      91.4%             All included   91.9%   91.7%   91.7%   91.4%

   Table 5. Experience Detection Performance                      Table 6. Experience Detection Performance
                                                                  without Individual Features
The candidate goes through three filters before it
is treated as experience-containing sentence.                     evaluated individual features for their importance
First, the candidates that do not have an objective               in experience detection (classification). The
case (Fillmore, 1968) are eliminated because                      evaluation was conducted with 10-fold cross va-
their definition of experience as “action + object”.              lidation. The results are shown in table 5.
This was done by identifying the object-                             The performance, especially precision, of the
indicating particle (case marker) in Japanese.                    baseline is much lower than those of the others.
Next, the candidates belonging to “become” and                    The method devised for Japanese doesn’t seem
“be” statements based on Japanese verb types are                  suitable for English. It seems that the linguistic
filtered out. Finally, the candidate sentences in-                styles shown in experience expressions are dif-
cluding a verb that indicates a movement are                      ferent from each other. In addition, the lexicon
eliminated because the main interest was to iden-                 we constructed for the baseline (i.e., using the
tify an activity in a place.                                      WordNet) contains more errors than our activity
   Although their definition of experience is                     lexicon for activity verbs. Some hyponyms of an
somewhat different from ours (i.e., “action + ob-                 activity verb may not be activity verbs. (e.g.,
ject”), they used the method to generate candi-                   “appear” is a hyponym of “do”).
date sentences from which various experience                         There is almost no difference between the Lo-
attributes are extracted. From this perspective,                  gistic Regression and SVM classifiers for our
the method functioned like our experience detec-                  methods although SVM was inferior for the
tion. Put differently, the definition and the me-                 baseline. The performance for the best case with
thod by which it is determined were much cruder                   all the features included is very promising,
than the one we are using, which seems close to                   closed to 92% precision and recall. Among the
our general understanding.5                                       features, the lexicon, i.e., verb classes, gave the
   The three filtering steps were implemented as                  best result when each is used alone, followed by
follows. We used the dependency parser for ex-                    modality, tense, and mood. Aspect was the worst
tracting objective cases using the direct object                  but close to the baseline. This result is very en-
relation. The second step, however, could not be                  couraging for the automatic lexicon construction
applied because there is no grammatical distinc-                  work because the lexicon plays a pivotal role in
tion among “do, be, become” statements in Eng-                    the overall performance.
lish. We had to alter this step by adopting the                      In order to see the effect of including individ-
approach of Inui et al. (2008). The authors pro-                  ual features in the feature set, precision and re-
pose a lexicon of experience expression by col-                   call were measured after eliminating a particular
lecting hyponyms from a hierarchically struc-                     feature from the full set. The results are shown in
tured dictionary. We collected all hyponyms of                    table 6. Although the absence of the lexicon fea-
words “do” and “act”, from WordNet (Fellbaum,                     ture hurt the performance most badly, still the
1998). Lastly, we removed all the verbs that are                  performance was reasonably high (roughly 84 %
under the hierarchy of “move” from WordNet.                       in precision and recall for the Logistic Regres-
   We not only compared our results with the                      sion case). Similar to table 5, the aspect and ex-
baseline in terms of precision and recall but also                perience features were the least contributors as
                                                                  the performance drops are almost negligible.
5
  This is based on our observation that the three annotators
found their task of identifying experience sentences not
difficulty, resulting in a high degree of agreements.


                                                               1470


4    Related Work                                     determining whether individual sentences con-
                                                      tain experience or not. Viewing the task as a
Experience mining in its entirety is a relatively     classification problem, we focused on identifica-
new area where various natural language               tion and examination of various linguistic fea-
processing and text mining techniques can play a      tures such as verb class, tense, aspect, mood,
significant role. While opinion mining or senti-      modality, and experience, all of which were
ment analysis, which can be considered an im-         computed automatically. For verb classes, in par-
portant part of experience mining, has been stu-      ticular, we devised a method for classifying all
died quite extensively (see Pang and Lee’s excel-     the verbs and verb phrases in WordNet into the
lent survey (2008)), another sub-area, factuality     activity and state classes. The experimental re-
analysis, begins to gain some popularity (Inui et     sults show that verb and verb phrase classifica-
al., 2008; Saurí, 2008). Very few studies have        tion method is reasonably accurate with 91%
focused explicitly on extracting various entities     precision and 78% recall with manually con-
that constitute experiences (Kurashima et al.,        structed gold standard consisting of 80 verbs and
2009) or detecting experience-containing parts of     82% accuracy for a random sample of all the
text although many NLP research areas such as         WordNet entries. For experience detection, the
named entity recognition and verb classification      performance was very promising, closed to 92%
are strongly related. The previous work on expe-      in precision and recall when all the features were
rience detection relies on a handcrafted lexicon.     used. Among the features, the verb classes, or the
   There have been a number of studies for verb       lexicon we constructed, contributed the most.
classification (Fillmore, 1968; Vendler, 1967;            In order to increase the coverage even further
Somers, 1982; Levin, 1993; Fillmore and Baker,        and reduce the errors in lexicon construction, i.e.,
2001; Kipper et al., 2008) that are essential for     verb classification, caused by data sparseness, we
construction of an activity verb lexicon, which in    need to devise a different method, perhaps using
turn is important for experience detection. Most      domain specific resources.
similar to our work was done by Siegel and                Given that experience mining is a relatively
McKeown (2000), who attempted to categorize           new research area, there are many areas to ex-
verbs into state or event classes based on 14 tests   plore. In addition to refinements of our work, our
similar to those of Vendler’s. They attempted to      next step is to develop a method for representing
compute co-occurrence statistics from a corpus.       and extracting actual experiences from expe-
The event class, however, includes activity, ac-      rience-revealing sentences. Furthermore, consi-
complishment, and achievement. Similarly, Za-         dering that only 13% of the blog data we
crone and Lenci (2008) attempted to categorize        processed contain experiences, an interesting
verbs in Italian into the four Vendler classes us-    extension is to apply the methodology to extract
ing the Vendler tests by using a tagged corpus.       other types of knowledge such as facts, which
They focused on existence of arguments such as        are not necessarily experiences.
subject and object that should co-occur with the
linguistic features in the tests.                     Acknowledgments
   The main difference between the previous
work and ours lies in the goal and scope of the       This research was supported by the IT R&D pro-
work. Since our work is specifically geared to-       gram of MKE/KEIT under grant KI001877 [Lo-
ward domain-independent experience detection,         cational/Societal Relation-Aware Social Media
we attempted to maximize the coverage by using        Service Technology], and by the MKE (The
all the verbs in WordNet, as opposed to the verbs     Ministry of Knowledge Economy), Korea, under
appearing in a particular domain-specific corpus      the ITRC (Information Technology Research
(e.g., medicine domain) as done in the previous       Center) support program supervised by the NIPA
work. Another difference is that while we are not     (National IT Industry Promotion Agency) [NI-
limited to a particular domain, we did not use        PA-2010-C1090-1011-0008].
extensive human-annotated corpus other than
using the 80 seed verbs and existing lexical re-      Reference
sources.                                              Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
                                                         Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhi-
5    Conclusion and Future Work                          ko Ohe. 2009. TEXT2TABLE: Medical Text
                                                         Summarization System based on Named Entity
We defined experience detection as an essential
task for experience mining, which is restated as

                                                  1471


  Recognition and Modality Identification. In Pro-       Takeshi Kurashima, Ko Fujimura, and Hidenori Oku-
  ceedings of the Workshop on BioNLP.                      da. 2009. Discovering Association Rules on Expe-
                                                           riences from Large-Scale Blog Entries. In Proceed-
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
                                                           ings of ECIR.
  2003. The Structure of the Framenet Database. In-
  ternational Journal of Lexicography.                   Takeshi Kurashima, Taro Tezuka, and Katsumi Tana-
                                                           ka. 2005. Blog Map of Experiences: Extracting and
Adam L. Berger, Stephen A. Della Pietra, and Vin-
                                                           Geographically Mapping Visitor Experiences from
  cent J. Della Pietra. 1996. A Mximum Entropy
                                                           Urban Blogs. In Proceedings of WISE.
  Approach to Natural Language Processing. Com-
  putational Linguistics.                                Takeshi Kurashima, Taro Tezuka, and Katsumi Tana-
                                                           ka. 2006. Mining and Visualizing Local Expe-
Chih-Chung Chang and Chih-Jen Lin. 2001.                   riences from Blog Entries. In Proceedings of
  LIBSVM : a Library for Support Vector Machines.          DEXA.
  http://www.csie.ntu.edu.tw/~cjlin/libsvm.
                                                         John Lafferty, Andew McCallum, and Fernando Pe-
David R. Dowty. 1979. Word meaning and Montague            reira. 2001. Conditional Random Fields: Probabil-
  Grammar. Reidel, Dordrecht.                              istic Models for Segmenting and Labeling Se-
Christiane Fellbaum. 1998. WordNet: An Electronic          quence Data. In Proceedings of ICML.
  Lexical Database. MIT Press.                           Beth Levin. 1993. English verb classes and alterna-
Charles J. Fillmore. 1968. The Case for Case. In Bach      tions: A Preliminary investigation. University of
  and Harms (Ed.): Universals in Linguistic Theory.        Chicago press.

Charles J. Fillmore and Collin F. Baker. 2001. Frame     Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
  Semantics for Text Understanding. In Proceedings         Combining Lexical Resources: Mapping Between
  of WordNet and Other Lexical Resources Work-             PropBank and Verbnet. In Proceedings of the In-
  shop, NAACL.                                             ternational Workshop on Computational Linguis-
                                                           tics.
Jenny R. Finkel, Trond Grenager, and Christopher D.
   Manning. 2005. Incorporating Non-local Informa-       Bo Pang and Lillian Lee. 2008. Opinion Mining and
   tion into Information Extraction Systems by Gibbs       Sentiment Analysis, Foundations and Trends in In-
   Sampling. In Proceedings of ACL.                        formation Retrieval.

Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.        Jihee Ryu, Yuchul Jung, Kyung-min Kim and Sung H.
  2009. A Practical Guide to Support Vector Classi-         Myaeng. 2010. Automatic Extraction of Human
  fication. http://www.csie.ntu.edu.tw/~cjlin/libsvm.       Activity Knowledge from Method-Describing Web
                                                            Articles. In Proceedings of the 1st Workshop on Au-
Kentaro Inui, Shuya Abe, Kazuo Hara, Hiraku Morita,         tomated Knowledge Base Construction.
  Chitose Sao, Megumi Eguchi, Asuka Sumida, Koji
  Murakami, and Suguru Matsuyoshi. 2008. Expe-           Roser Saurí. 2008. A Factuality Profiler for Eventuali-
  rience Mining: Building a Large-Scale Database of        ties in Text. PhD thesis, Brandeis University.
  Personal Experiences and Opinions from Web             Eric V. Siegel and Kathleen R. McKeown. 2000.
  Documents. In Proceedings of the International            Learing Methods to Combine Linguistic Indicators:
  Conference on Web Intelligence.                           Improving Aspectual Classification and Revealing
Valentin Jijkoun, Maarten de Rijke, Wouter Weer-            Linguistic Insights. In Computational Linguistics.
  kamp, Paul Ackermans and Gijs Geleijnse. 2010.         Harold L. Somers. 1987. Valency and Case in Com-
  Mining User Experiences from Online Forums: An           putational Linguistics. Edinburgh University Press.
  Exploration. In Proceedings of NAACL HLT Work-
  shop on Computational Linguistics in a World of        Kristina Toutanova, Dan Klein, Christopher D. Man-
  Social Media.                                            ning, and Yoram Singer. 2003. Feature-Rich Part-
                                                           of-Speech Tagging with a Cyclic Dependency
Karin Kipper, Anna Korhonen, Neville Ryant, and            Network. In Proceedings of HLT-NAACL.
  Martha Palmer. 2008. A Large-scale Classification
  of English Verbs. Language Resources and Evalu-        Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
  ation Journal.                                           nell University Press.
Dan Klein and Christopher D. Manning. 2003. Accu-        Alessandra Zarcone and Alessandro Lenci. 2008.
  rate Unlexicalized Parsing. In Proceedings of ACL.       Computational Models of Event Type Classifica-
                                                           tion in Context. In Proceedings of LREC.




                                                     1472

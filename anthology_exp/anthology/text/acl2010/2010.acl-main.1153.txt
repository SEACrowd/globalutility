       A Generalized-Zero-Preserving Method for Compact Encoding of
                             Concept Lattices
          Matthew Skala                       Victoria Krakovna                             Gerald Penn
                                                János Kramár
    School of Computer Science                Dept. of Mathematics                 Dept. of Computer Science
      University of Waterloo                  University of Toronto                  University of Toronto
      mskala@cs.toronto.edu             {vkrakovna,jkramar}@gmail.com                 gpenn@cs.toronto.edu



                       Abstract                                   • With large type signatures, the table that in-
                                                                    dexes unifiable pairs of types may be so large
     Constructing an encoding of a concept lat-                     that it pushes working parsing memory into
     tice using short bit vectors allows for ef-                    swap. This loss of locality of reference costs
     ficient computation of join operations on                      time.
     the lattice. Join is the central operation
     any unification-based parser must support.                   Why isn’t everyone using bit vectors? For the
     We extend the traditional bit vector encod-               most part, the reason is their size. The classical
     ing, which represents join failure using the              encoding given by Aı̈t-Kaci et al. (1989) is at least
     zero vector, to count any vector with less                as large as the number of meet-irreducible types,
     than a fixed number of one bits as failure.               which in the parlance of HPSG type signatures
     This allows non-joinable elements to share                is the number of unary-branching types plus the
     bits, resulting in a smaller vector size. A               number of maximally specific types. For the En-
     constraint solver is used to construct the                glish Resource Grammar (ERG) (Copestake and
     encoding, and a variety of techniques are                 Flickinger, 2000), these are 314 and 2474 respec-
     employed to find near-optimal solutions                   tively. While some systems use them nonetheless
     and handle timeouts. An evaluation is pro-                (PET (Callmeier, 2000) does, as a very notable ex-
     vided comparing the extended representa-                  ception), it is clear that the size of these codes is a
     tion of failure with traditional bit vector               source of concern.
     techniques.                                                  Again, it has been so since the very beginning:
                                                               Aı̈t-Kaci et al. (1989) devoted several pages to
1    Introduction                                              a discussion of how to “modularize” type codes,
The use of bit vectors is almost as old as HPSG                which typically achieves a smaller code in ex-
parsing itself. Since they were first suggested in             change for a larger-time operation than bitwise
the programming languages literature (Aı̈t-Kaci et             AND as the implementation of type unification.
al., 1989) as a method for computing the unifica-              However, in this and later work on the subject
tion of two types without table lookup, bit vectors            (e.g. (Fall, 1996)), one constant has been that we
have been attractive because of three speed advan-             know our unification has failed when the imple-
tages:                                                         mentation returns the zero vector. Zero preserva-
                                                               tion (Mellish, 1991; Mellish, 1992), i.e., detect-
    • The classical bit vector encoding uses bitwise           ing a type unification failure, is just as important
      AND to calculate type unification. This is               as obtaining the right answer quickly when it suc-
      hard to beat.                                            ceeds.
                                                                  The approach of the present paper borrows
    • Hash tables, the most common alternative,                from recent statistical machine translation re-
      involve computing the Dedekind-MacNeille                 search, which addresses the problem of efficiently
      completion (DMC) at compile time if the in-              representing large-scale language models using a
      put type hierarchy is not a bounded-complete             mathematical construction called a Bloom filter
      partial order. That is exponential time in the           (Talbot and Osborne, 2007). The approach is best
      worst case; most bit vector methods avoid ex-            combined with modularization in order to further
      plicitly computing it.                                   reduce the size of the codes, but its novelty lies in


                                                         1512
        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1512–1521,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


the observation that counting the number of one         u, v ∈ X:
bits in an integer is implemented in the basic in-
                                                                      u v v ⇒ f (u)  f (v)                (1)
struction sets of many CPUs. The question then
arises whether smaller codes would be obtained               defined (u t v) ⇒ g(f (u), f (v)) = 1         (2)
by relaxing zero preservation so that any resulting         ¬defined (u t v) ⇒ g(f (u), f (v)) = 0         (3)
vector with at most λ bits is interpreted as failure,             u t v = w ⇔ f (u) g f (v) = f (w)        (4)
with λ ≥ 1.
                                                           With property (1), the embedding is said to pre-
   Penn (2002) generalized join-preserving encod-
                                                        serve order; with property (2), it preserves suc-
ings of partial orders to the case where more than
                                                        cess; with property (3), it preserves failure; and
one code can be used to represent the same ob-
                                                        with property (4), it preserves joins.
ject, but the focus there was on codes arising from
successful unifications; there was still only one       2    Bit-vector encoding
representative for failure. To our knowledge, the
present paper is the first generalization of zero       Intuitively, taking the join of two types in a type hi-
preservation in CL or any other application do-         erarchy is like taking the intersection of two sets.
main of partial order encodings.                        Types often represent sets of possible values, and
   We note at the outset that we are not using          the type represented by the join really does repre-
Bloom filters as such, but rather a derandomized        sent the intersection of the sets that formed the in-
encoding scheme that shares with Bloom filters          put. So it seems natural to embed a partial order of
the essential insight that λ can be greater than zero   types hX, vi into a partial order (in fact, a lattice)
without adverse consequences for the required al-       of sets hY, i, where Y is the power set of some
gebraic properties of the encoding. Deterministic       set Z, and  is the superset relation ⊇. Then join
variants of Bloom filters may in turn prove to be       g is simply set intersection ∩. The embedding
of some value in language modelling.                    function g, which indicates whether a join exists,
                                                        can be naturally defined by g(f (u), f (v)) = 0 if
1.1   Notation and definitions                          and only if f (u) ∩ f (v) = ∅. It remains to choose
                                                        the underlying set Z and embedding function f .
A partial order hX, vi consists of a set X and a           Aı̈t-Kaci et al. (1989) developed what has be-
reflexive, antisymmetric, and transitive binary re-     come the standard technique of this type. They
lation v. We use u t v to denote the unique least       set Z to be the set of all meet irreducible elements
upper bound or join of u, v ∈ X, if one exists, and     in X; and f (u) = {v ∈ Z|v w u}, that is, the
u u v for the greatest lower bound or meet. If we       meet irreducible elements greater than or equal to
need a second partial order, we use  for its order     u. The resulting embedding preserves order, suc-
relation and g for its join operation. We are espe-     cess, failure, and joins. If Z is chosen to be the
cially interested in a class of partial orders called   maximal elements of X instead, then join preser-
meet semilattices, in which every pair of elements      vation is lost but the embedding still preserves or-
has a unique meet. In a meet semilattice, the join      der, success, and failure. The sets can be repre-
of two elements is unique when it exists at all, and    sented efficiently by vectors of bits. We hope to
there is a unique globally least element ⊥ (“bot-       minimize the size of the largest set f (⊥), which
tom”).                                                  determines the vector length.
   A successor of an element u ∈ X is an element           It follows from the work of Markowsky (1980)
v 6= u ∈ X such that u v v and there is no w ∈ X        that the construction of Aı̈t-Kaci et al. is optimal
with w 6= u, w 6= v, and u v w v v, i.e., v fol-        among encodings that use sets with intersection
lows u in X with no other elements in between. A        for meet and empty set for failure: with Y defined
maximal element has no successor. A meet irre-          as the power set of some set Z, v as ⊇, t as ∩, and
ducible element is an element u ∈ X such that for       g(f (u), f (v)) = 0 if and only if f (u) ∩ f (v) = ∅,
any v, w ∈ X, if u = v u w then u = v or u = w.         then the smallest Z that will preserve order, suc-
A meet irreducible has at most one successor.           cess, failure, and joins is the set of all meet irre-
   Given two partial orders hX, vi and hY, i, an       ducible elements of X. No shorter bit vectors are
embedding of X into Y is a pair of functions            possible.
f : X → Y and g : (Y × Y ) → {0, 1}, which                 We construct shorter bit vectors by modifying
may have some of the following properties for all       the definition of g, so that the minimality results


                                                    1513


no longer apply. In the following discussion we                     1
                                                                        1             ?               1                     1

present first an intuitive and then a technical de-
scription of our approach.

2.1   Intuition from Bloom filters                              1            1   1        1   1   1



Vectors generated by the above construction tend
to be quite sparse, or if not sparse, at least bor-                              Figure 1: A Bloom filter
ing. Consider a meet semilattice containing only
the bottom element ⊥ and n maximal elements all           return zero. Bloom describes how to calculate the
incomparable to each other. Then each bit vector          optimal value of k, and the necessary length of
would consist of either all ones, or all zeroes ex-       the hashed array, to achieve any desired bound on
cept for a single one. We would thus be spending          the error rate. In general, the hashed array can
n bits to represent a choice among n + 1 alterna-         be much smaller than the original unhashed ar-
tives, which should fit into a logarithmic number         ray (Bloom, 1970).
of bits. The meet semilattices that occur in prac-           Classical Bloom filtering applied to the sparse
tice are more complicated than this example, but          vectors of the embedding would create some per-
they tend to contain things like it as a substruc-        centage of incorrect join results, which would then
ture. With the traditional bit vector construction,       have to be handled by other techniques. Our work
each of the maximal elements consumes its own             described here combines the idea of using k hash
bit, even though those bits are highly correlated.        functions to reduce the error rate, with perfect
   The well-known technique called Bloom fil-             hashes designed in a precomputation step to bring
tering (Bloom, 1970) addresses a similar issue.           the error rate to zero.
There, it is desired to store a large array of bits
subject to two considerations. First, most of the         2.2       Modified failure detection
bits are zeroes. Second, we are willing to accept         In the traditional bit vector construction, types
a small proportion of one-sided errors, where ev-         map to sets, join is computed by intersection of
ery query that should correctly return one does so,       sets, and the empty set corresponds to failure
but some queries that should correctly return zero        (where no join exists). Following the lead of
might actually return one instead.                        Bloom filters, we change the embedding function
   The solution proposed by Bloom and widely              g(f (u), f (v)) to be 0 if and only if |f (u)∩f (v)| ≤
used in the decades since is to map the entries in        λ for some constant λ. With λ = 0 this is the same
the large bit array pseudorandomly (by means of           as before. Choosing greater values of λ allows us
a hash function) into the entries of a small bit ar-      to re-use set elements in different parts of the type
ray. To store a one bit we find its hashed location       hierarchy while still avoiding collisions.
and store it there. If we query a bit for which the          Figure 2 shows an example meet semilattice. In
answer should be zero but it happens to have the          the traditional construction, to preserve joins we
same hashed location as another query with the an-        must assign one bit to each of the meet-irreducible
swer one, then we return a one and that is one of         elements {d, e, f, g, h, i, j, k, l, m}, for a total of
our tolerated errors.                                     ten bits. But we can use eight bits and still pre-
   To reduce the error rate we can elaborate the          serve joins by setting g(f (u), f (v)) = 0 if and
construction further: with some fixed k, we use           only if |f (u) ∩ f (v)| ≤ λ = 1, and f as follows.
k hash functions to map each bit in the large array
                                                                            f (⊥) = {1, 2, 3, 4, 5, 6, 7, 8}
to several locations in the small one. Figure 1 il-
lustrates the technique with k = 3. Each bit has                                     f (a) = {1, 2, 3, 4, 5}
three hashed locations. On a query, we check all                    f (b) = {1, 6, 7, 8}                   f (c) = {1, 2, 3}
three; they must all contain ones for the query to              f (d) = {2, 3, 4, 5}                       f (e) = {1, 6}
return a one. There will be many collisions of indi-                                                                            (5)
                                                                f (f ) = {1, 7}                           f (g) = {1, 8}
vidual hashed locations, as shown; but the chances
are good that when we query a bit we did not in-                f (h) = {6, 7}                             f (i) = {6, 8}
tend to store in the filter, at least one of its hashed             f (j) = {1, 2}                        f (k) = {1, 3}
locations will still be empty, and so the query will                f (l) = {2, 3}                        f (m) = {2, 3, 4}

                                                      1514


      j     k       l                                  the types lacks a module, we observe that the mod-
                                                       ule bottoms and non-module types form a tree, and
                        m             g                the join can be computed in that tree. If x is a type
                                 f         h
            c               e                   i
                                                       in the module whose bottom is y, and z has no
                        d
                                                       module, then x t z = y t z unless y t z = y
                                                       in which case x t z = x; so it only remains to
                    a                 b
                                                       compute joins within the tree. Our implementa-
                                                       tion does that by table lookup. More sophisticated
                                                       approaches could be appropriate on larger trees.

                                                       3   Set programming
Figure 2: An example meet semilattice; ⊥ is the
most general type.                                     Ideally, we would like to have an efficient algo-
                                                       rithm for finding the best possible encoding of any
                                                       given meet semilattice. The encoding can be rep-
   As a more general example, consider the very        resented as a collection of sets of integers (repre-
simple meet semilattice consisting of just a least     senting bit indices that contain ones), and an opti-
element ⊥ with n maximal elements incompara-           mal encoding is the collection of sets whose over-
ble to each other. For a given λ we can represent      all union is smallest subject to the constraint that
       b bits by choosing the smallest b such that
this in                                                the collection forms an encoding at all. This com-
   b
 λ+1 ≥ n and assigning each maximal element a
                                                       binatorial optimization problem is a form of set
distinct choice of the bits. With optimal choice of    programming; and set programming problems are
λ, b is logarithmic in n.                              widely studied. We begin by defining the form of
                                                       set programming we will use.
2.3       Modules                                      Definition 1 Choose    S set variables S1 , S2 , . . . , Sn
As Aı̈t-Kaci et al. (1989) described, partial or-      to minimize b = | ni=1 Si | subject to some con-
ders encountered in practice often resemble trees.     straints of the forms |Si | ≥ ri , Si ⊆ Sj , Si + Sj ,
Both their technique and ours are at a disadvantage    |Si ∩ Sj | ≤ λ, and Si ∩ Sj = Sk . The constant
when applied to large trees; in particular, if the     λ is the same for all constraints. Set elements may
bottom of the partial order has successors which       be arbitrary, but we generally assume they are the
are not joinable with each other, then those will be   integers {1 . . . b} for convenience.
assigned large sets with little overlap, and bits in      The reduction of partial order representation to
the vectors will tend to be wasted.                    set programming is clear: we create a set variable
   To avoid wasting bits, we examine the partial       for every type, force the maximal types’ sets to
order X in a precomputation step to find the mod-      contain at least λ + 1 elements, and then use sub-
ules, which are the smallest upward-closed sub-        set to enforce that every type is a superset of all
sets of X such that for any x ∈ X, if x has at         its successors (preserving order and success). We
least two joinable successors, then x is in a mod-     limit the maximum intersection of incomparable
ule. This is similar to ALE’s definition of mod-       types to preserve failure. To preserve joins, if that
ule (Penn, 1999), but not the same. The definition     property is desired, we add a constraint Si + Sj
of Aı̈t-Kaci et al. (1989) also differs from ours.     for every pair of types xi 6v xj and one of the
Under our definition, every module has a unique        form Si ∩ Sj = Sk for every xi , xj , xk such that
least element, and not every type is in a module.      xi t xj = xk ..
For instance, in Figure 2, the only module has a          Given a constraint satisfaction problem like this
as its least element. In the ERG’s type hierarchy,     one, we can ask two questions: is there a feasi-
there are 11 modules, with sizes ranging from 10       ble solution, assigning values to the variables so
to 1998 types.                                         all constraints are satisfied; and if so what is the
   To find the join of two types in the same mod-      optimal solution, producing the best value of the
ule, we find the intersection of their encodings and   objective while remaining feasible? In our prob-
check whether it is of size greater than λ. If the     lem, there is always a feasible solution we can
types belong to two distinct modules, there is no      find by the generalized Aı̈t-Kaci et al. construc-
join. For the remaining cases, where at least one of   tion (GAK), which consists of assigning λ bits


                                                    1515


shared among all types; adding enough unshared          being choke-vertices. Maximal and minimal ele-
new bits to maximal elements to satisfy cardinal-       ments are always choke-vertices.
ity constraints; adding one new bit to each non-           Choke-vertices are important because the op-
maximal meet irreducible type; and propagating          timal bit assignment for elements after a choke-
all the bits down the hierarchy to satisfy the subset   vertex u is almost independent of the bit assign-
constraints. Since the GAK solution is feasible, it     ment elsewhere in the partial order. Removing
provides a useful upper bound on the result of the      the redundant constraints means there are no con-
set programming.                                        straints between elements after u and elements
   Ongoing research on set programming has pro-         before, or incomparable with, u. All constraints
duced a variety of software tools for solving these     across u must involve u directly. As a result, we
problems. However, at first blush our instances are     can solve a smaller instance consisting of u and
much too large for readily-available set program-       everything after it, to find the minimal number of
ming tools. Grammars like ERG contain thou-             bits ru for representing u. Then we solve the rest
sands of types. We use binary constraints be-           of the problem with a constraint |Su | = ru , ex-
tween every pair of types, for a total of millions      cluding all partial order elements after u, and then
of constraints—and these are variables and con-         combine the two solutions with any arbitrary bi-
straints over a domain of sets, not integers or re-     jection between the set elements assigned to u in
als. General-purpose set programming software           each solution. Assuming optimal solutions to both
cannot handle such instances.                           sub-problems, the result is an optimal solution to
                                                        the original problem.
3.1   Simplifying the instances
                                                        3.2   Splitting into components
First of all, we only use minimum cardinality con-
straints |Si | ≥ ri for maximal types; and every        If we cut the partial order at every choke-vertex,
ri ≥ λ + 1. Given a feasible bit assignment for a       we reduce the huge and impractical encoding
maximal type with more than ri elements in its set      problem to a collection of smaller ones. The cut-
Si , we can always remove elements until it has ex-     ting expresses the original partial order as a tree
actly ri elements, without violating the other con-     of components, each of which corresponds to a set
straints. As a result, instead of using constraints     programming instance. Components are shown by
|Si | ≥ ri we can use constraints |Si | = ri . Doing    the dashed lines in Figure 2. We can find an op-
so reduces the search space.                            timal encoding for the entire partial order by opti-
   Subset is transitive; so if we have constraints      mally encoding the components, starting with the
Si ⊆ Sj and Sj ⊆ Sk , then Si ⊆ Sk is implied           leaves of that tree and working our way back to the
and we need not specify it as a constraint. Simi-       root.
larly, if we have Si ⊆ Sj and Si * Sk , then we            The division into components creates a collec-
have Sj * Sk . Furthermore, if Si and Sj have           tion of set programming instances with a wide
maximum intersection λ, then any subset of Si           range of sizes and difficulty; we examine each in-
also has maximum intersection λ with any subset         stance and choose appropriate techniques for each
of Sk , and we need not specify those constraints       one. Table 1 summarizes the rules used to solve an
either.                                                 instance, and shows the number of times each rule
   Now, let a choke-vertex in the partial order         was applied in a typical run with the modules ex-
hX, vi be an element u ∈ X such that for ev-            tracted from ERG, a ten-minute timeout, and each
ery v, w ∈ X where v is a successor of w and            λ from 0 to 10.
u v v, we have u v w. That is, any chain of suc-           In many simple cases, GAK is provably opti-
cessors from elements not after u to elements after     mal. These include when λ = 0 regardless of the
u, must pass through u. Figure 2 shows choke-           structure of the component; when the component
vertices as squares. We call these choke-vertices       consists of a bottom and zero, one, or two non-
by analogy with the graph theoretic concept of          joinable successors; and when there is one element
cut-vertices in the Hasse diagram of the partial or-    (a top) greater than all other elements in the com-
der; but note that some vertices (like j and k) can     ponent. We can easily recognize these cases and
be choke-vertices without being cut-vertices, and       apply GAK to them.
some vertices (like c) can be cut-vertices without         Another important special case is when the


                                                    1516


 Condition       Succ.    Fail.   Method                  This process of removing ULs, solving, and
 λ=0              216             GAK (optimal)        adding them back in, may in general produce sub-
 ∃ top            510             GAK (optimal)        optimal solutions, so we use it only when the
 2 successors     850             GAK (optimal)        solver cannot find a solution on the full-sized prob-
 3 or 4            70             exponential          lem. In practical experiments, the solver gener-
 successors                       variable             ally either produces an optimal or very nearly op-
 only ULs          420            b-choose-(λ+1)       timal solution within a time limit on the order of
                                  special case         ten minutes; or fails to produce a feasible solu-
 before UL         251      59    ic_sets              tion at all, even with a much longer limit. Testing
 removal                                               whether it finds a solution is then a useful way to
 after UL             9     50    ic_sets              determine whether UL removal is worthwhile.
 removal                                                  Recall that in an instance consisting of k ULs
 remaining           50           GAK                  and a bottom, an optimal solution
                                                                                       b
                                                                                          consists of find-
                                                       ing the smallest b such that λ+1    is at least k; that
Table 1: Rules for solving an instance in the ERG
                                                       is the number of bits for the bottom, and we can
                                                       choose any k distinct subsets of size λ + 1 for the
component consists of a bottom and some num-           ULs. Augmenting an existing solution to include
ber k of pairwise non-joinable successors, and the     additional ULs involves a similar calculation.
successors all have required cardinality λ + 1.           To add a UL x as the successor of an element
Then the optimal encoding  comes from finding the     y without increasing the total number of bits, we
                       b
smallest b such that λ+1 is at least k, and giving     must find a choice of λ + 1 of the bits already as-
each successor a distinct combination of the b bits.   signed to y, sharing at most λ bits with any of y’s
                                                       other successors. Those successors are in general
3.3   Removing unary leaves
                                                       sets of arbitrary size, but all that matters for as-
For components that do not have one of the spe-        signing x is how many subsets of size λ + 1 they
cial forms described above, it becomes necessary       already cover. The UL can use any such subset
to solve the set programming problem. Some of          not covered by an existing successor of y. Our al-
our instances are small enough to apply constraint     gorithm counts the subsets already covered, and
solving software directly; but for larger instances,   compares that with the number of choices of λ + 1
we have one more technique to bring them into the      bits from the bits assigned to y. If enough choices
tractable range.                                       remain, we use them; otherwise, we add bits until
Definition 2 A unary leaf (UL) is an element x in      there are enough choices.
a partial order hX, vi such that x is maximal and
x is the successor of exactly one other element.       3.4   Solving
   ULs are special because their set programming       For instances with a small number of sets and rela-
constraints always take a particular form: if x is a   tively large number of elements in the sets, we use
UL and a successor of y, then the constraints on       an exponential variable solver. This encodes the
its set Sx are exactly that |Sx | = λ + 1, Sx ⊆ Sy ,   set programming problem into integer program-
and Sx has intersection of size at most λ with the     ming. For each element x ∈ {1, 2, . . . , b}, let
set for any other successor of y. Other constraints    c(x) = {i|x ∈ Si }; that is, c(x) represents the
disappear by the simplifications described earlier.    indices of all the sets in the problem that contain
   Furthermore, ULs occur frequently in the par-       the element x. There are 2n − 1 possible values
tial orders we consider in practice; and by increas-   of c(x), because each element must be in at least
ing the number of sets in an instance, they have       one set. We create an integer variable for each of
a disproportionate effect on the difficulty of solv-   those values. Each element is counted once, so the
ing the set programming problem. We therefore          sum of the integer variables is b. The constraints
implement a special solution process for instances     translate into simple inequalities on sums of the
containing ULs: we remove them all, solve the re-      variables; and the system of constraints can be
sulting instance, and then add them back one at a      solved with standard integer programming tech-
time while attempting to increase the overall num-     niques. After solving the integer programming
ber of elements as little as possible.                 problem we can then assign elements arbitrarily


                                                   1517


to the appropriate combinations of sets.                the entire process to more than compensate for
   Where applicable, the exponential variable ap-       the improved propagation. We also evaluated the
proach works well, because it breaks all the sym-       Cardinal solver included in ECLi PSe , which of-
metries between set elements. It also continues to      fers stronger propagation of cardinality informa-
function well even when the sets are large, since       tion; it lacked other needed features and seemed
nothing in the problem directly grows when we           no more efficient than ic sets. Among these
increase b. The wide domains of the variables           three solvers, the improvements associated with
may be advantageous for some integer program-           our custom variable and value heuristics greatly
ming solvers as well. However, it creates an in-        outweighed the baseline differences between the
teger programming problem of size exponential in        solvers; and the differences were in optimization
the number of sets. As a result, it is only applica-    time rather than quality of the returned solutions.
ble to instances with a very few set variables.            Solvers with available source code were pre-
   For more general set programming instances,          ferred for ease of customization, and free solvers
we feed the instance directly into a solver de-         were preferred for economy, but a license for
signed for such problems. We used the ECLi PSe          ILOG CPLEX (IBM, 2008) was available and we
logic programming system (Cisco Systems, 2008),         tried using it with the natural encoding of sets as
which offers several set programming solvers as         vectors of binary variables. It solved small in-
libraries, and settled on the ic sets library. This     stances to optimality in time comparable to that
is a straightforward set programming solver based       of ECLi PSe . However, for medium to large in-
on containment bounds. We extended the solver           stances, CPLEX proved impractical. An instance
by adding a lightweight not-subset constraint, and      with n sets of up to b bits, dense with pairwise
customized heuristics for variable and value selec-     constraints like subset and maximum intersection,
tion designed to guide the solver to a feasible so-     requires Θ(n2 b) variables when encoded into in-
lution as soon as possible. We choose variables         teger programming in the natural way. CPLEX
near the top of the instance first, and prefer to as-   stores a copy of the relaxed problem, with signifi-
sign values that share exactly λ bits with exist-       cant bookkeeping information per variable, for ev-
ing assigned values. We also do limited symme-          ery node in the search tree. It is capable of storing
try breaking, in that whenever we assign a bit not      most of the tree in compressed form on disk, but in
shared with any current assignment, the choice of       our larger instances even a single node is too large;
bit is arbitrary so we assume it must be the lowest-    CPLEX exhausts memory while loading its input.
index bit. That symmetry breaking speeds up the         The ECLi PSe solver also stores each set variable
search significantly.                                   in a data structure that increases linearly with the
                                                        number of elements, so that the size of the prob-
    The present work is primarily on the benefits
                                                        lem as stored by ECLi PSe is also Θ(n2 b); but the
of nonzero λ, and so a detailed study of gen-
                                                        constant for ECLi PSe appears to be much smaller,
eral set programming techniques would be inap-
                                                        and its search algorithm stores only incremental
propriate; but we made informal tests of several
                                                        updates (with nodes per set instead of per element)
other set-programming solvers. We had hoped that
                                                        on a stack as it explores the tree. As a result, the
a solver using containment-lexicographic hybrid
                                                        ECLi PSe solver can process much larger instances
bounds as described by Sadler and Gervet (Sadler
                                                        than CPLEX without exhausting memory.
and Gervet, 2008) would offer good performance,
and chose the ECLi PSe framework partly to gain            Encoding into SAT would allow use of the so-
access to its ic hybrid sets implementation of such     phisticated solvers available for that problem. Un-
bounds. In practice, however, ic hybrid sets gave       fortunately, cardinality constraints are notoriously
consistently worse performance than ic sets (typi-      difficult to encode in Boolean logic. The obvi-
cally by an approximate factor of two). It appears      ous encoding of our problem into CNFSAT would
that in intuitive terms, the lexicographic bounds       require O(n2 bλ) clauses and variables. Encod-
rarely narrowed the domains of variables much un-       ings into Boolean variables with richer constraints
til the variables were almost entirely labelled any-    than CNFSAT (we tried, for instance, the SICS-
way, at which point containment bounds were al-         tus Prolog clp(FD) implementation (Carlsson et
most as good; and meanwhile the increased over-         al., 1997)) generally exhausted memory on much
head of maintaining the extra bounds slowed down        smaller instances than those handled by the set-


                                                    1518


    Module               n       b0      λ    bλ          Encoding               length    time     space
    mrs_min             10        7      0     7          Lookup table               n/a    140    72496
    conj                13        8      1     7          Modular, best λ        0–357      321       203
    list                27       15      1    11          Modular, λ = 0        0–1749      747       579
    local_min           27       21      1    10          Non-mod, λ = 0          2788     4651      1530
    cat_min             30       17      1    14          Non-mod, λ = 1          1243     2224       706
    individual          33       15      0    15          Non-mod, λ = 2          1140     2008       656
    head_min           247       55      0    55          Non-mod, λ = 9          1069     1981       622
    *sort*             247      129      3   107          Non-mod, λ = 140          985    3018       572
    synsem_min         612      255      0   255
    sign_min          1025      489      3   357       Table 3: Query performance. Vector length in bits,
    mod_relation      1998     1749      6   284       time in milliseconds, space in Kbytes.
    entire ERG        4305     2788    140   985

Table 2: Best encodings of the ERG and its mod-        for the modules where the solver is failing any-
ules: n is number of types, b0 is vector length with   way. One important lesson seems to be that further
λ = 0, and λ is parameter that gives the shortest      work on set programming solvers would be bene-
vector length bλ .                                     ficial: any future more capable set programming
                                                       solver could be applied to the unsolved instances
                                                       and would be expected to save more bits.
variable solvers, while offering no improvement           Table 3 and Figure 3 show the performance of
in speed.                                              the join query with various encodings. These re-
                                                       sults are from a simple implementation in C that
4    Evaluation                                        tests all ordered pairs of types for joinability. As
Table 2 shows the size of our smallest encodings       well as testing the non-modular ERG encoding for
to date for the entire ERG without modularization,     different values of λ, we tested the modularized
and for each of its modules. These were found          encoding with λ = 0 for all modules (to show the
by running the optimization process of the previ-      effect of modularization alone) and with λ cho-
ous section on Intel Xeon servers with a timeout       sen per-module to give the shortest vectors. For
of 30 minutes for each invocation of the solver        comparison, we also tested a simple lookup table.
(which may occur several times per module). Un-        The same implementation sufficed for all these
der those conditions, some modules take a long         tests, by means of putting all types in one mod-
time to optimize—as much as two hours per tested       ule for the non-modular bit vectors or no types
value of λ for sign_min. The Xeon’s hyper-             in any module for the pure lookup table. The
threading feature makes reproducibility of timing      times shown are milliseconds of user CPU time
results difficult, but we found that results almost    to test all join tests (roughly 18.5 million of them),
never improved with additional time allowance be-      on a non-hyperthreading Intel Pentium 4 with a
yond the first few seconds in any case, so the prac-   clock speed of 2.66GHz and 1G of RAM, run-
tical effect of the timing variations should be min-   ning Linux. Space consumption shown is the total
imal.                                                  amount of dynamically-allocated memory used to
   These results show some significant improve-        store the vectors and lookup table.
ments in vector length for the larger modules.            The non-modular encoding with λ = 0 is the
However, they do not reveal the entire story. In       basic encoding of Aı̈t-Kaci et al. (1989). As Ta-
particular, the apparent superiority of λ = 0 for      ble 3 shows, we achieved more than a factor of
the synsem_min module should not be taken              two improvement from that, in both time and vec-
as indicating that no higher λ could be better:        tor length, just by setting λ = 1. Larger values
rather, that module includes a very difficult set      offered further small improvements in length up to
programming instance on which the solver failed        λ = 140, which gave the minimum vector length
and fell back to GAK. For the even larger modules,     of 985. That is a shallow minimum; both λ = 120
nonzero λ proved helpful despite solver failures,      and λ = 160 gave vector lengths of 986, and the
because of the bits saved by UL removal. UL re-        length slowly increased with greater λ.
moval is clearly a significant advantage, but only        However, the fastest bit-count on this architec-


                                                   1519


                           5000


                           4500


                           4000
      user CPU time (ms)




                           3500


                           3000


                           2500


                           2000


                           1500
                                   0               50               100                 150                 200
                                                                lambda (bits)


                                  Figure 3: Query performance for the ERG without modularization.


ture, using a technique first published by Weg-                    all storage space and improve speed.
ner (1960), requires time increasing with the num-                    A good encoding requires a kind of perfect
ber of nonzero bits it counts; and a similar effect                hash, the design of which maps naturally to con-
would appear on a word-by-word basis even if we                    straint programming over sets of integers. We
used a constant-time per-word count. As a result,                  have described a practical framework for solving
there is a time cost associated with using larger λ,               the instances of constraint programming thus cre-
so that the fastest value is not necessarily the one               ated, in which we can apply existing or future con-
that gives the shortest vectors. In our experiments,               straint solvers to the subproblems for which they
λ = 9 gave the fastest joins for the non-modular                   are best suited; and a technique for modularizing
encoding of the ERG. As shown in Figure 3, all                     practical type hierarchies to get better value from
small nonzero λ gave very similar times.                           the bit vector encodings. We have evaluated the re-
   Modularization helps a lot, both with λ = 0,                    sulting encodings on the ERG’s type system, and
and when we choose the optimal λ per module.                       examined the performance of the associated unifi-
Here, too, the use of optimal λ improves both time                 cation test. Modularization, and the use of nonzero
and space by more than a factor of two. Our best                   λ, each independently provide significant savings
bit-vector encoding, the modularized one with per-                 in both time and vector length.
module optimal λ, is only a little less than half                     The modified failure detection concept suggests
the speed of the lookup table; and this test favours               several directions for future work, including eval-
the lookup table by giving it a full word for every                uation of the new encodings in the context of a
entry (no time spent shifting and masking bits) and                large-scale HPSG parser; incorporation of further
testing the pairs in a simple two-level loop (almost               developments in constraint solvers; and the possi-
purely sequential access).                                         bility of approximate encodings that would permit
                                                                   one-sided errors as in traditional Bloom filtering.
5   Conclusion
We have described a generalization of conven-                      References
tional bit vector concept lattice encoding tech-
                                                                   Hassan Aı̈t-Kaci, Robert S. Boyer, Patrick Lincoln, and
niques to the case where all vectors with λ or fewer                 Roger Nasr. 1989. Efficient implementation of lat-
one bits represent failure; traditional encodings are                tice operations. ACM Transactions on Programming
the case λ = 0. Increasing λ can reduce the over-                    Languages and Systems, 11(1):115–146, January.


                                                               1520


Burton H. Bloom. 1970. Space/time trade-offs in hash        Peter Wegner. 1960. A technique for counting ones
  coding with allowable errors. Communications of             in a binary computer. Communications of the ACM,
  the ACM, 13(7):422–426, July.                               3(5):322.

Ulrich Callmeier. 2000. PET – a platform for ex-
  perimentation with efficient HPSG processing tech-
  niques. Natural Language Engineering, 6(1):99–
  107.

Mats Carlsson, Greger Ottosson, and Björn Carlson.
 1997. An open-ended finite domain constraint
 solver. In H. Glaser, P. Hartel, and H. Kucken, ed-
 itors, Programming Languages: Implementations,
 Logics, and Programming, volume 1292 of Lec-
 ture Notes in Computer Science, pages 191–206.
 Springer-Verlag, September.

Cisco Systems. 2008. ECLi PSe 6.0. Computer soft-
  ware. Online http://eclipse-clp.org/.

Ann Copestake and Dan Flickinger. 2000. An
  open-source grammar development environment
  and broad-coverage English grammar using HPSG.
  In Proceedings of the Second Conference on Lan-
  guage Resources and Evaluation (LREC 2000).

Andrew Fall. 1996. Reasoning with Taxonomies.
  Ph.D. thesis, Simon Fraser University.

IBM. 2008. ILOG CPLEX 11. Computer software.

George Markowsky. 1980. The representation of
  posets and lattices by sets. Algebra Universalis,
  11(1):173–192.

Chris Mellish. 1991. Graph-encodable description
  spaces. Technical report, University of Edinburgh
  Department of Artificial Intelligence. DYANA De-
  liverable R3.2B.

Chris Mellish. 1992. Term-encodable description
  spaces. In D.R. Brough, editor, Logic Program-
  ming: New Frontiers, pages 189–207. Kluwer.

Gerald Penn. 1999. An optimized prolog encoding of
  typed feature structures. In D. De Schreye, editor,
  Logic programming: proceedings of the 1999 Inter-
  national Conference on Logic Programming (ICLP),
  pages 124–138.

Gerald Penn. 2002. Generalized encoding of descrip-
  tion spaces and its application to typed feature struc-
  tures. In Proceedings of the 40th Annual Meeting of
  the Association for Computational Linguistics (ACL
  2002), pages 64–71.

Andrew Sadler and Carmen Gervet. 2008. Enhanc-
  ing set constraint solvers with lexicographic bounds.
  Journal of Heuristics, 14(1).

David Talbot and Miles Osborne. 2007. Smoothed
  Bloom filter language models: Tera-scale LMs on
  the cheap. In Proceedings of the 2007 Joint Con-
  ference on Empirical Methods in Natural Language
  Processing and Computational Natural Language
  Learning (EMNLP-CoNLL), pages 468–476.


                                                        1521

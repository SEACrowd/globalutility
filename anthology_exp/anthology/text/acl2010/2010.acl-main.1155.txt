   All Words Domain Adapted WSD: Finding a Middle Ground between
                    Supervision and Unsupervision

  Mitesh M. Khapra    Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya
                       Indian Institute of Technology Bombay,
                              Mumbai - 400076, India.
             {miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in


                     Abstract                                of language competence, topic comprehension and
                                                             domain sensitivity. This makes supervised ap-
   In spite of decades of research on word                   proaches to WSD a difficult proposition (Agirre
   sense disambiguation (WSD), all-words                     et al., 2009b; Agirre et al., 2009a; McCarthy et
   general purpose WSD has remained a dis-                   al., 2007). Unsupervised and knowledge based ap-
   tant goal. Many supervised WSD systems                    proaches have been tried with the hope of creating
   have been built, but the effort of creat-                 WSD systems with no need for sense marked cor-
   ing the training corpus - annotated sense                 pora (Koeling et al., 2005; McCarthy et al., 2007;
   marked corpora - has always been a matter                 Agirre et al., 2009b). However, the accuracy fig-
   of concern. Therefore, attempts have been                 ures of such systems are low.
   made to develop unsupervised and knowl-                      Our work here is motivated by the desire to de-
   edge based techniques for WSD which do                    velop annotation-lean all-words domain adapted
   not need sense marked corpora. However                    techniques for supervised WSD. It is a common
   such approaches have not proved effective,                observation that domain specific WSD exhibits
   since they typically do not better Word-                  high level of accuracy even for the all-words sce-
   net first sense baseline accuracy. Our re-                nario (Khapra et al., 2010) - provided training and
   search reported here proposes to stick to                 testing are on the same domain. Also domain
   the supervised approach, but with far less                adaptation - in which training happens in one do-
   demand on annotation. We show that if                     main and testing in another - often is able to attain
   we have ANY sense marked corpora, be it                   good levels of performance, albeit on a specific set
   from mixed domain or a specific domain, a                 of target words (Chan and Ng, 2007; Agirre and
   small amount of annotation in ANY other                   de Lacalle, 2009). To the best of our knowledge
   domain can deliver the goods almost as                    there does not exist a system that solves the com-
   if exhaustive sense marking were avail-                   bined problem of all words domain adapted WSD.
   able in that domain. We have tested our                   We thus propose the following:
   approach across Tourism and Health do-
   main corpora, using also the well known                     a. For any target domain, create a small amount
   mixed domain SemCor corpus. Accuracy                           of sense annotated corpus.
   figures close to self domain training lend
   credence to the viability of our approach.                  b. Mix it with an existing sense annotated cor-
   Our contribution thus lies in finding a con-                   pus – from a mixed domain or specific do-
   venient middle ground between pure su-                         main – to train the WSD engine.
   pervised and pure unsupervised WSD. Fi-
                                                             This procedure tested on four adaptation scenar-
   nally, our approach is not restricted to any
                                                             ios, viz., (i) SemCor (Miller et al., 1993) to
   specific set of target words, a departure
                                                             Tourism, (ii) SemCor to Health, (iii) Tourism to
   from a commonly observed practice in do-
                                                             Health and (iv) Health to Tourism has consistently
   main specific WSD.
                                                             yielded good performance (to be explained in sec-
                                                             tions 6 and 7).
1 Introduction
                                                                The remainder of this paper is organized as fol-
Amongst annotation tasks, sense marking surely               lows. In section 2 we discuss previous work in the
takes the cake, demanding as it does high level              area of domain adaptation for WSD. In section 3


                                                       1532
      Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


we discuss three state of art supervised, unsuper-    al. (2009b) for unsupervised and knowledge based
vised and knowledge based algorithms for WSD.         approaches respectively have cast a doubt on the
Section 4 discusses the injection strategy for do-    viability of supervised approaches which rely on
main adaptation. In section 5 we describe the         sense tagged corpora. However, these conclusions
dataset used for our experiments. We then present     were drawn only from the performance on certain
the results in section 6 followed by discussions in   target words, leaving open the question of their
section 7. Section 8 examines whether there is any    utility in all words WSD.
need for intelligent choice of injections. Section       We believe our work contributes to the WSD
9 concludes the paper highlighting possible future    research in the following way: (i) it shows that
directions.                                           there is promise in supervised approach to all-
                                                      word WSD, through the instrument of domain
2 Related Work                                        adaptation; (ii) it places in perspective some very
                                                      recently reported unsupervised and knowledge
Domain specific WSD for selected target words         based techniques of WSD; (ii) it answers some
has been attempted by Ng and Lee (1996), Agirre       questions arising out of the debate between super-
and de Lacalle (2009), Chan and Ng (2007), Koel-      vision and unsupervision in WSD; and finally (iv)
ing et al. (2005) and Agirre et al. (2009b). They     it explores a convenient middle ground between
report results on three publicly available lexical    unsupervised and supervised WSD – the territory
sample datasets, viz., DSO corpus (Ng and Lee,        of “annotate-little and inject” paradigm.
1996), MEDLINE corpus (Weeber et al., 2001)
and the corpus made available by Koeling et al.       3 WSD algorithms employed by us
(2005). Each of these datasets contains a handful
of target words (41-191 words) which are sense        In this section we describe the knowledge based,
marked in the corpus.                                 unsupervised and supervised approaches used for
   Our main inspiration comes from the target-        our experiments.
word specific results reported by Chan and Ng
                                                      3.1 Knowledge Based Approach
(2007) and Agirre and de Lacalle (2009). The
former showed that adding just 30% of the target      Agirre et al. (2009b) showed that a graph based
data to the source data achieved the same perfor-     algorithm which uses only the relations between
mance as that obtained by taking the entire source    concepts in a Lexical Knowledge Base (LKB) can
and target data. Agirre and de Lacalle (2009) re-     outperform supervised approaches when tested on
ported a 22% error reduction when source and          specific domains (for a set of chosen target words).
target data were combined for training a classi-      We employ their method which involves the fol-
fier, as compared to the case when only the target    lowing steps:
data was used for training the classifier. However,      1. Represent Wordnet as a graph where the con-
both these works focused on target word specific            cepts (i.e., synsets) act as nodes and the re-
WSD and do not address all-words domain spe-                lations between concepts define edges in the
cific WSD.                                                  graph.
   In the unsupervised setting, McCarthy et al.          2. Apply a context-dependent Personalized
(2007) showed that their predominant sense acqui-           PageRank algorithm on this graph by intro-
sition method gives good results on the corpus of           ducing the context words as nodes into the
Koeling et al. (2005). In particular, they showed           graph and linking them with their respective
that the performance of their method is compa-              synsets.
rable to the most frequent sense obtained from a
tagged corpus, thereby making a strong case for          3. These nodes corresponding to the context
unsupervised methods for domain-specific WSD.               words then inject probability mass into the
More recently, Agirre et al. (2009b) showed that            synsets they are linked to, thereby influencing
knowledge based approaches which rely only on               the final relevance of all nodes in the graph.
the semantic relations captured by the Wordnet
                                                         We used the publicly available implementation
graph outperform supervised approaches when ap-
                                                      of this algorithm1 for our experiments.
plied to specific domains. The good results ob-
                                                         1
tained by McCarthy et al. (2007) and Agirre et               http://ixa2.si.ehu.es/ukb/


                                                  1533


3.2   Unsupervised Approach                                   (such as, semantic similarity, conceptual dis-
McCarthy et al. (2007) used an untagged corpus to             tance, etc.)
construct a thesaurus of related words. They then                                     X
                                                                S ∗ = arg max(θi Vi +    Wij ∗ Vi ∗ Vj )
found the predominant sense (i.e., the most fre-                           i
                                                                                      j∈J
quent sense) of each target word using pair-wise
                                                                                                            (1)
Wordnet based similarity measures by pairing the
target word with its top-k neighbors in the the-
                                                              where,
saurus. Each target word is then disambiguated
by assigning it its predominant sense – the moti-
                                                                 i ∈ Candidate Synsets
vation being that the predominant sense is a pow-
erful, hard-to-beat baseline. We implemented their              J = Set of disambiguated words
method using the following steps:                               θi = BelongingnessT oDominantConcept(Si)
1. Obtain a domain-specific untagged corpus (we                Vi = P (Si |word)
   crawled a corpus of approximately 9M words                 Wij = CorpusCooccurrence(Si , Sj )
   from the web).                                                 ∗ 1/W N ConceptualDistance(Si , Sj )
2. Extract grammatical relations from this text us-               ∗ 1/W N SemanticGraphDistance(Si , Sj )
   ing a dependency parser2 (Klein and Manning,
   2003).                                                  4. Select the candidate synset with maximizes the
3. Use the grammatical relations thus extracted to            above score as the winner sense.
   construct features for identifying the k nearest
   neighbors for each word using the distributional        4 Injections for Supervised Adaptation
   similarity score described in (Lin, 1998).              This section describes the main interest of our
4. Rank the senses of each target word in the test         work i.e. adaptation using injections. For su-
   set using a weighted sum of the distributional          pervised adaptation, we use the supervised algo-
   similarity scores of the neighbors. The weights         rithm described above (Khapra et al., 2010) in the
   in the sum are based on Wordnet Similarity              following 3 settings as proposed by Agirre et al.
   scores (Patwardhan and Pedersen, 2003).                 (2009a):
5. Each target word in the test set is then disam-         a. Source setting: We train the algorithm on a
   biguated by simply assigning it its predominant            mixed-domain corpus (SemCor) or a domain-
   sense obtained using the above method.                     specific corpus (say, Tourism) and test it on a
3.3   Supervised approach                                     different domain (say, Health). A good perfor-
                                                              mance in this setting would indicate robustness
Khapra et al. (2010) proposed a supervised algo-              to domain-shifts.
rithm for domain-specific WSD and showed that it
beats the most frequent corpus sense and performs          b. Target setting: We train and test the algorithm
on par with other state of the art algorithms like            using data from the same domain. This gives the
PageRank. We implemented their iterative algo-                skyline performance, i.e., the best performance
rithm which involves the following steps:                     that can be achieved if sense marked data from
1. Tag all monosemous words in the sentence.                  the target domain were available.
2. Iteratively disambiguate the remaining words in         c. Adaptation setting: This setting is the main fo-
   the sentence in increasing order of their degree           cus of interest in the paper. We augment the
   of polysemy.                                               training data which could be from one domain
3. At each stage rank the candidate senses of                 or mixed domain with a small amount of data
   a word using the scoring function of Equa-                 from the target domain. This combined data is
   tion (1) which combines corpus based param-                then used for training. The aim here is to reach
   eters (such as, sense distributions and corpus             as close to the skyline performance using as lit-
   co-occurrence) and Wordnet based parameters                tle data as possible. For injecting data from the
    2
      We used the Stanford parser        -   http://nlp.      target domain we randomly select some sense
stanford.edu/software/lex-parser.shtml                        marked words from the target domain and add


                                                       1534


                 Polysemous words     Monosemous words                      Avg. no. of instances per polysemous word
  Category       Tourism Health       Tourism   Health          Category    Health           Tourism          SemCor
  Noun             53133    15437       23665     6979          Noun          7.06              12.56            10.98
  Verb             15528     7348        1027      356          Verb          7.47               9.76            11.95
  Adjective        19732     5877       10569     2378          Adjective     5.74              12.07             8.67
  Adverb            6091     1977        4323     1694          Adverb        9.11              19.78            25.44
  All              94484    30639       39611    11407          All           6.94              12.17            11.25

        Table 1: Polysemous and Monosemous words per               Table 2: Average number of instances per polyse-
        category in each domain                                    mous word per category in the 3 domains

                     Avg. degree of Wordnet polysemy                              Avg. degree of Corpus polysemy
                           for polysemous words                                        for polysemous words
        Category     Health Tourism          SemCor                  Category     Health Tourism         SemCor
        Noun           5.24         4.95         5.60                Noun           1.92        2.60         3.41
        Verb          10.60       10.10          9.89                Verb           3.41        4.55         4.73
        Adjective      5.52         5.08         5.40                Adjective      2.04        2.57         2.65
        Adverb         3.64         4.16         3.90                Adverb         2.16        2.82         3.09
        All            6.49         5.77         6.43                All            2.31        2.93         3.56

        Table 3: Average degree of Wordnet polysemy of             Table 4: Average degree of Corpus polysemy of
        polysemous words per category in the 3 domains             polysemous words per category in the 3 domains


  them to the training data. An obvious ques-               Note that we do not use the monosemous words
  tion which arises at this point is “Why were the          while calculating precision and recall of our algo-
  words selected at random?” or “Can selection              rithms.
  of words using some active learning strategy                 Table 2 shows the average number of instances
  yield better results than a random selection?”            per polysemous word in the 3 corpora. We note
  We discuss this question in detail in Section 7           that the number of instances per word in the
  and show that a random set of injections per-             Tourism domain is comparable to that in the Sem-
  forms no worse than a craftily selected set of            Cor corpus whereas the number of instances per
  injections.                                               word in the Health corpus is smaller due to the
                                                            overall smaller size of the Health corpus.
5 DataSet Preparation
                                                               Tables 3 and 4 summarize the average degree
Due to the lack of any publicly available all-words         of Wordnet polysemy and corpus polysemy of the
domain specific sense marked corpora we set upon            polysemous words in the corpus. Wordnet poly-
the task of collecting data from two domains, viz.,         semy is the number of senses of a word as listed
Tourism and Health. The data for Tourism do-                in the Wordnet, whereas corpus polysemy is the
main was downloaded from Indian Tourism web-                number of senses of a word actually appearing in
sites whereas the data for Health domain was ob-            the corpus. As expected, the average degree of
tained from two doctors. This data was manu-                corpus polysemy (Table 4) is much less than the
ally sense annotated by two lexicographers adept            average degree of Wordnet polysemy (Table 3).
in English. Princeton Wordnet 2.13 (Fellbaum,               Further, the average degree of corpus polysemy
1998) was used as the sense inventory. A total              (Table 4) in the two domains is less than that in the
of 1,34,095 words from the Tourism domain and               mixed-domain SemCor corpus, which is expected
42,046 words from the Health domain were man-               due to the domain specific nature of the corpora.
ually sense marked. Some files were sense marked            Finally, Table 5 summarizes the number of unique
by both the lexicographers and the Inter Tagger             polysemous words per category in each domain.
Agreement (ITA) calculated from these files was
83% which is comparable to the 78% ITA reported                                  No. of unique polysemous words
on the SemCor corpus considering the domain-                       Category      Health Tourism         SemCor
                                                                   Noun           2188        4229         5871
specific nature of the corpus.                                     Verb             984       1591         2565
   We now present different statistics about the                   Adjective      1024        1635         2640
                                                                   Adverb           217        308          463
corpora. Table 1 summarizes the number of poly-                    All            4413        7763        11539
semous and monosemous words in each category.
                                                                Table 5: Number of unique polysemous words per category
  3                                                             in each domain.
      http://wordnetweb.princeton.edu/perl/webwn


                                                         1535


The data is currently being enhanced by manu-                       untagged Health corpus which is needed for con-
ally sense marking more words from each domain                      structing the thesaurus. The results are summa-
and will be soon freely available4 for research pur-                rized in Table 7.
poses.
                                                                        Domain    Algorithm    P(%)     R(%)     F(%)
6 Results                                                               Tourism   McCarthy     51.85    49.32    50.55
                                                                                    WFS        62.50    62.50    62.50
We tested the 3 algorithms described in section 4
using SemCor, Tourism and Health domain cor-                            Table 7: Comparing the performance of unsuper-
pora. We did a 2-fold cross validation for su-                          vised approach with Wordnet First Sense Baseline
pervised adaptation and report the average perfor-                      (WFS)
mance over the two folds. Since the knowledge
based and unsupervised methods do not need any
                                                                    6.3 Supervised adaptation
training data we simply test it on the entire corpus
from the two domains.                                               We report results in the source setting, target set-
                                                                    ting and adaptation setting as described earlier
6.1      Knowledge Based approach                                   using the following four combinations for source
The results obtained by applying the Personalized                   and target data:
PageRank (PPR) method to Tourism and Health                         1. SemCor to Tourism (SC→T) where SemCor is
data are summarized in Table 6. We also report                         used as the source domain and Tourism as the
the Wordnet first sense baseline (WFS).                                target (test) domain.

Domain           Algorithm        P(%)      R(%)          F(%)      2. SemCor to Health (SC→H) where SemCor is
Tourism             PPR           53.1      53.1          53.1         used as the source domain and Health as the tar-
                   WFS            62.5      62.5          62.5         get (test) domain.
 Health             PPR           51.1      51.1          51.1      3. Tourism to Health (T→H) where Tourism is
                   WFS            65.5      65.5          65.5         used as the source domain and Health as the tar-
                                                                       get (test) domain.
 Table 6: Comparing the performance of Person-
 alized PageRank (PPR) with Wordnet First Sense                     4. Health to Tourism (H→T) where Health is
 Baseline (WFS)                                                        used as the source domain and Tourism as the
                                                                       target (test) domain.
                                                                       In each case, the target domain data was divided
6.2      Unsupervised approach
                                                                    into two folds. One fold was set aside for testing
The predominant sense for each word in the two                      and the other for injecting data in the adaptation
domains was calculated using the method de-                         setting. We increased the size of the injected target
scribed in section 4.2. McCarthy et al. (2004)                      examples from 1000 to 14000 words in increments
reported that the best results were obtained us-                    of 1000. We then repeated the same experiment by
ing k = 50 neighbors and the Wordnet Similar-                       reversing the role of the two folds.
ity jcn measure (Jiang and Conrath, 1997). Fol-                        Figures 1, 2, 3 and 4 show the graphs of the av-
lowing them, we used k = 50 and observed that                       erage F-score over the 2-folds for SC→T, SC→H,
the best results for nouns and verbs were obtained                  T→H and H→T respectively. The x-axis repre-
using the jcn measure and the best results for ad-                  sents the amount of training data (in words) in-
jectives and adverbs were obtained using the lesk                   jected from the target domain and the y-axis rep-
measure (Banerjee and Pedersen, 2002). Accord-                      resents the F-score. The different curves in each
ingly, we used jcn for nouns and verbs and lesk                     graph are as follows:
for adjectives and adverbs. Each target word in
                                                                    a. only random : This curve plots the perfor-
the test set is then disambiguated by simply as-
                                                                       mance obtained using x randomly selected
signing it its predominant sense obtained using
                                                                       sense tagged words from the target domain and
the above method. We tested this approach only
                                                                       zero sense tagged words from the source do-
on Tourism domain due to unavailability of large
                                                                       main (x was varied from 1000 to 14000 words
   4
       http://www.cfilt.iitb.ac.in/wsd/annotated corpus                in increments of 1000).


                                                                 1536


                                 Injection Size v/s F-score                                              Injection Size v/s F-score
              80                                                                      80
              75        tsky                                                          75        tsky
              70                                                                      70
                                                                                                srcb
              65                                                                      65        wfs
F-score (%)




                                                                        F-score (%)
                        wfs
              60        srcb                                                          60
              55                                                                      55
              50                                                                      50
              45                                                                      45
              40                                   only_random                        40                                   only_random
                                                random+semcor                                                           random+semcor
              35                                                                      35
                   0   2000    4000   6000   8000 10000 12000 14000                        0   2000    4000   6000   8000 10000 12000 14000
                                  Injection Size (words)                                                  Injection Size (words)
                   Figure 1: Supervised adaptation from                                    Figure 2: Supervised adaptation from
                   SemCor to Tourism using injections                                      SemCor to Health using injections
                                 Injection Size v/s F-score                                              Injection Size v/s F-score
              80                                                                      80
              75        tsky                                                          75        tsky
              70                                                                      70
              65        wfs                                                           65
F-score (%)




                                                                        F-score (%)

                        srcb                                                                    wfs
              60                                                                      60
                                                                                                srcb
              55                                                                      55
              50                                                                      50
              45                                                                      45
              40                                   only_random                        40                                   only_random
                                                random+tourism                                                           random+health
              35                                                                      35
                   0   2000    4000 6000 8000 10000 12000 14000                            0   2000    4000 6000 8000 10000 12000 14000
                                   Injection Size (words)                                                  Injection Size (words)
                   Figure 3: Supervised adaptation from                                    Figure 4: Supervised adaptation from
                   Tourism to Health using injections                                      Health to Tourism using injections


b. random+source : This curve plots the perfor-                                   7 Discussions
   mance obtained by mixing x randomly selected
   sense tagged words from the target domain with                                 We discuss the performance of the three ap-
   the entire training data from the source domain                                proaches.
   (again x was varied from 1000 to 14000 words
   in increments of 1000).
c. source baseline (srcb) : This represents the F-                                7.1 Knowledge Based and Unsupervised
   score obtained by training on the source data                                      approaches
   alone without mixing any examples from the
   target domain.                                                                 It is apparent from Tables 6 and 7 that knowl-
                                                                                  edge based and unsupervised approaches do not
d. wordnet first sense (wfs) : This represents the                                perform well when compared to the Wordnet first
   F-score obtained by selecting the first sense                                  sense (which is freely available and hence can be
   from Wordnet, a typically reported baseline.                                   used for disambiguation). Further, we observe that
                                                                                  the performance of these approaches is even less
e. target skyline (tsky) : This represents the av-
                                                                                  than the source baseline (i.e., the case when train-
   erage 2-fold F-score obtained by training on
                                                                                  ing data from a source domain is applied as it is
   one entire fold of the target data itself (Health:
                                                                                  to a target domain - without using any injections).
   15320 polysemous words; Tourism: 47242 pol-
                                                                                  These observations bring out the weaknesses of
   ysemous words) and testing on the other fold.
                                                                                  these approaches when used in an all-words set-
  These graphs along with other results are dis-                                  ting and clearly indicate that they come nowhere
cussed in the next section.                                                       close to replacing a supervised system.


                                                                      1537


7.2   Supervised adaptation                                        Health) to another specific domain (Health or
1. The F-score obtained by training on SemCor                      Tourism) gives the same performance as that ob-
   (mixed-domain corpus) and testing on the two                    tained by adapting from a mixed-domain (Sem-
   target domains without using any injections                     Cor) to a specific domain (Tourism, Health).
   (srcb) – F-score of 61.7% on Tourism and F-                     This is an interesting observation as it suggests
   score of 65.5% on Health – is comparable to the                 that as long as data from one domain is avail-
   best result reported on the SEMEVAL datasets                    able it is easy to build a WSD engine that works
   (65.02%, where both training and testing hap-                   for other domains by injecting a small amount
   pens on a mixed-domain corpus (Snyder and                       of data from these domains.
   Palmer, 2004)). This is in contrast to previ-
                                                         To verify that the results are consistent, we ran-
   ous studies (Escudero et al., 2000; Agirre and
                                                         domly selected 5 different sets of injections from
   Martinez, 2004) which suggest that instead of
                                                         fold-1 and tested the performance on fold-2. We
   adapting from a generic/mixed domain to a spe-
                                                         then repeated the same experiment by reversing
   cific domain, it is better to completely ignore
                                                         the roles of the two folds. The results were in-
   the generic examples and use hand-tagged data
                                                         deed consistent irrespective of the set of injections
   from the target domain itself. The main rea-
                                                         used. Due to lack of space we have not included
   son for the contrasting results is that the ear-
                                                         the results for these 5 different sets of injections.
   lier work focused only on a handful of target
   words whereas we focus on all words appearing         7.3 Quantifying the trade-off between
   in the corpus. So, while the behavior of a few            performance and corpus size
   target words would change drastically when the
                                                         To correctly quantify the benefit of adding injec-
   domain changes, a majority of the words will
                                                         tions from the target domain, we calculated the
   exhibit the same behavior (i.e., same predomi-
                                                         amount of target data (peak size) that is needed
   nant sense) even when the domain changes. We
                                                         to reach the skyline F-score (peak F) in the ab-
   agree that the overall performance is still lower
                                                         sence of any data from the source domain. The
   than that obtained by training on the domain-
                                                         peak size was found to be 35000 (Tourism) and
   specific corpora. However, it is still better than
                                                         14000 (Health) corresponding to peak F values of
   the performance of unsupervised and knowl-
                                                         74.2% (Tourism) and 73.4% (Health). We then
   edge based approaches which tilts the scale in
                                                         plotted a graph (Figure 5) to capture the rela-
   favor of supervised approaches even when only
                                                         tion between the size of injections (expressed as
   mixed domain sense marked corpora is avail-
                                                         a percentage of the peak size) and the F-score (ex-
   able.
                                                         pressed as a percentage of the peak F).
2. Adding injections from the target domain im-
   proves the performance. As the amount of in-                                      Size v/s Performance
                                                                   105
   jection increases the performance approaches
   the skyline, and in the case of SC→H and T→H
                                                                   100
   it even crosses the skyline performance showing
   that combining the source and target data can                    95
                                                        % peak_F




   give better performance than using the target
   data alone. This is consistent with the domain                   90
   adaptation results reported by Agirre and de La-
   calle (2009) on a specific set of target words.                  85                                      SC --> H
                                                                                                             T --> H
                                                                                                            SC --> T
3. The performance of random+source is always                       80
                                                                                                             H --> T

   better than only random indicating that the data                      0     20       40         60           80     100
                                                                                         % peak_size
   from the source domain does help to improve
                                                                      Figure 5: Trade-off between performance
   performance. A detailed analysis showed that
                                                                      and corpus size
   the gain obtained by using the source data is at-
   tributable to reducing recall errors by increasing
                                                            We observe that by mixing only 20-40% of the
   the coverage of seen words.
                                                         peak size with the source domain we can obtain up
4. Adapting from one specific domain (Tourism or         to 95% of the performance obtained by using the


                                                    1538


entire target data (peak size). In absolute terms,     Table 8 summarizes the percentage of words that
the size of the injections is only 7000-9000 poly-     fall in each category in each of the three adapta-
semous words which is a very small price to pay        tion scenarios. The fact that nearly 50-60% of the
considering the performance benefits.                  words fall in the “conformist” category once again
                                                       makes a strong case for reusing sense tagged data
8 Does the choice of injections matter?                from one domain to another domain.

An obvious question which arises at this point is      Category                SC→T         SC→H         T→H
“Why were the words selected at random?” or            WD2                      7.14%        5.45%      13.61%
“Can selection of words using some active learn-       Conformists             49.54%       60.43%      54.31%
ing strategy yield better results than a random        Non-Conformists         43.30%       34.11%      32.06%
selection?” An answer to this question requires
a more thorough understanding of the sense-               Table 8: Percentage of Words belonging to each
behavior exhibited by words across domains. In            category in the three settings.
any scenario involving a shift from domain D1 to
domain D2 , we will always encounter words be-         The above characterization suggests that an ideal
longing to the following 4 categories:                 domain adaptation strategy should focus on in-
                                                       jecting WD2 and WD1D2non−conf ormists as these
a. WD1 : This class includes words which are en-       would yield maximum benefits if injected into the
   countered only in the source domain D1 and do       training data. While it is easy to identify the
   not appear in the target domain D2 . Since we       WD2 words, “identifying non-conformists” is a
   are interested in adapting to the target domain     hard problem which itself requires some type of
   and since these words do not appear in the tar-     WSD5 . However, just to prove that a random in-
   get domain, it is quite obvious that they are not   jection strategy does as good as an ideal strategy
   important for the problem of domain adapta-         we assume the presence of an oracle which iden-
   tion.                                               tifies the WD1D2non−conf ormists . We then augment
                                                       the training data with 5-8 instances for WD2 and
b. WD2 : This class includes words which are en-       WD1D2non−conf ormists words thus identified. We
   countered only in the target domain D2 and do       observed that adding more than 5-8 instances per
   not appear in the source domain D1 . Again, it      word does not improve the performance. This is
   is quite obvious that these words are important     due to the “one sense per domain” phenomenon –
   for the problem of domain adaptation. They fall     seeing only a few instances of a word is sufficient
   in the category of unseen words and need han-       to identify the predominant sense of the word. Fur-
   dling from that point of view.                      ther, to ensure a better overall performance, the
                                                       instances of the most frequent words are injected
c. WD1D2conf ormists : This class includes words       first followed by less frequent words till we ex-
   which are encountered in both the domains and       haust the total size of the injections (1000, 2000
   exhibit the same predominant sense in both the      and so on). We observed that there was a 75-
   domains. Correct identification of these words      80% overlap between the words selected by ran-
   is important so that we can use the predomi-        dom strategy and oracle strategy. This is because
   nant sense learned from D1 for disambiguating       oracle selects the most frequent words which also
   instances of these words appearing in D2 .          have a high chance of getting selected when a ran-
                                                       dom sampling is done.
d. WD1D2non−conf ormists : This class includes             Figures 6, 7, 8 and 9 compare the performance
   words which are encountered in both the do-         of the two strategies. We see that the random strat-
   mains but their predominant sense in the tar-       egy does as well as the oracle strategy thereby sup-
   get domain D2 does not conform to the pre-          porting our claim that if we have sense marked
   dominant sense learned from the source domain       corpus from one domain then simply injecting ANY
   D1 . Correct identification of these words is im-   small amount of data from the target domain will
   portant so that we can ignore the predominant           5
                                                            Note that the unsupervised predominant sense acquisi-
   senses learned from D1 while disambiguating         tion method of McCarthy et al. (2007) implicitly identifies
   instances of these words appearing in D2 .          conformists and non-conformists


                                                   1539


                                  Injection Size v/s F-score                                               Injection Size v/s F-score
              80                                                                       80
              75         tsky                                                          75         tsky
              70                                                                       70
                                                                                                  srcb
              65                                                                       65         wfs
                         wfs
F-score (%)




                                                                         F-score (%)
              60         srcb                                                          60
              55                                                                       55
              50                                                                       50
              45                                                                       45
              40                                 random+semcor                         40                                 random+semcor
                                                   oracle+semcor                                                            oracle+semcor
              35                                                                       35
                   0   2000     4000   6000   8000 10000 12000 14000                        0   2000     4000   6000   8000 10000 12000 14000
                                   Injection Size (words)                                                   Injection Size (words)
                   Figure 6: Comparing random strategy                                      Figure 7: Comparing random strategy
                   with oracle based ideal strategy for Sem-                                with oracle based ideal strategy for Sem-
                   Cor to Tourism adaptation                                                Cor to Health adaptation
                                  Injection Size v/s F-score                                               Injection Size v/s F-score
              80                                                                       80
              75         tsky                                                          75         tsky
              70                                                                       70
              65         wfs                                                           65
F-score (%)




                                                                         F-score (%)



                         srcb                                                                     wfs
              60                                                                       60
                                                                                                  srcb
              55                                                                       55
              50                                                                       50
              45                                                                       45
              40                                 random+tourism                        40                                  random+health
                                                   oracle+tourism                                                            oracle+health
              35                                                                       35
                   0   2000     4000 6000 8000 10000 12000 14000                            0   2000     4000 6000 8000 10000 12000 14000
                                    Injection Size (words)                                                   Injection Size (words)
                   Figure 8: Comparing random strat-                                        Figure 9: Comparing random strat-
                   egy with oracle based ideal strategy for                                 egy with oracle based ideal strategy for
                   Tourism to Health adaptation                                             Health to Tourism adaptation


do the job.                                                                    3. Supervised adaptation from a mixed domain to
                                                                                  a specific domain gives the same performance
9 Conclusion and Future Work                                                      as that from one specific domain (Tourism) to
                                                                                  another specific domain (Health).
Based on our study of WSD in 4 domain adap-
tation scenarios, we make the following conclu-
sions:                                                                         4. Supervised adaptation is not sensitive to the
                                                                                  type of data being injected. This is an interest-
1. Supervised adaptation by mixing small amount                                   ing finding with the following implication: as
   of data (7000-9000 words) from the target do-                                  long as one has sense marked corpus - be it from
   main with the source domain gives nearly the                                   a mixed or specific domain - simply injecting
   same performance (F-score of around 70% in                                     ANY small amount of data from the target do-
   all the 4 adaptation scenarios) as that obtained                               main suffices to beget good accuracy.
   by training on the entire target domain data.

2. Unsupervised and knowledge based approaches                                     As future work, we would like to test our work on
   which use distributional similarity and Word-                                   the Environment domain data which was released
   net based similarity measures do not compare                                    as part of the SEMEVAL 2010 shared task on “All-
   well with the Wordnet first sense baseline per-                                 words Word Sense Disambiguation on a Specific
   formance and do not come anywhere close to                                      Domain”.
   the performance of supervised adaptation.


                                                                       1540


References                                                Dan Klein and Christopher D. Manning. 2003. Ac-
                                                            curate unlexicalized parsing. In IN PROCEEDINGS
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-           OF THE 41ST ANNUAL MEETING OF THE ASSO-
  pervised domain adaption for wsd. In EACL ’09:            CIATION FOR COMPUTATIONAL LINGUISTICS,
  Proceedings of the 12th Conference of the European        pages 423–430.
  Chapter of the Association for Computational Lin-
  guistics, pages 42–50, Morristown, NJ, USA. Asso-       Rob Koeling, Diana McCarthy, and John Carroll.
  ciation for Computational Linguistics.                    2005. Domain-specific sense distributions and pre-
                                                            dominant sense acquisition. In HLT ’05: Proceed-
Eneko Agirre and David Martinez. 2004. The effect of        ings of the conference on Human Language Tech-
  bias on an automatically-built word sense corpus. In      nology and Empirical Methods in Natural Language
  Proceedings of the 4rd International Conference on        Processing, pages 419–426, Morristown, NJ, USA.
  Languages Resources and Evaluations (LREC).               Association for Computational Linguistics.

Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-     Dekang Lin. 1998. Automatic retrieval and cluster-
  baum, Andrea Marchetti, Antonio Toral, and Piek           ing of similar words. In Proceedings of the 17th
  Vossen. 2009a. Semeval-2010 task 17: all-words            international conference on Computational linguis-
  word sense disambiguation on a specific domain. In        tics, pages 768–774, Morristown, NJ, USA. Associ-
  DEW ’09: Proceedings of the Workshop on Seman-            ation for Computational Linguistics.
  tic Evaluations: Recent Achievements and Future
  Directions, pages 123–128, Morristown, NJ, USA.         Diana McCarthy, Rob Koeling, Julie Weeds, and John
  Association for Computational Linguistics.                Carroll. 2004. Finding predominant word senses in
                                                            untagged text. In ACL ’04: Proceedings of the 42nd
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.       Annual Meeting on Association for Computational
  2009b. Knowledge-based wsd on specific domains:           Linguistics, page 279, Morristown, NJ, USA. Asso-
  Performing better than generic supervised wsd. In         ciation for Computational Linguistics.
  In Proceedings of IJCAI.
                                                          Diana McCarthy, Rob Koeling, Julie Weeds, and John
                                                            Carroll. 2007. Unsupervised acquisition of predom-
Satanjeev Banerjee and Ted Pedersen. 2002. An
                                                            inant word senses. Comput. Linguist., 33(4):553–
  adapted lesk algorithm for word sense disambigua-
                                                            590.
  tion using wordnet. In CICLing ’02: Proceedings
  of the Third International Conference on Compu-         George A. Miller, Claudia Leacock, Randee Tengi, and
  tational Linguistics and Intelligent Text Processing,     Ross T. Bunker. 1993. A semantic concordance. In
  pages 136–145, London, UK. Springer-Verlag.               HLT ’93: Proceedings of the workshop on Human
                                                            Language Technology, pages 303–308, Morristown,
Yee Seng Chan and Hwee Tou Ng. 2007. Do-                    NJ, USA. Association for Computational Linguis-
  main adaptation with active learning for word sense       tics.
  disambiguation. In Proceedings of the 45th An-
  nual Meeting of the Association of Computational        Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
  Linguistics, pages 49–56, Prague, Czech Republic,         multiple knowledge sources to disambiguate word
  June. Association for Computational Linguistics.          sense: an exemplar-based approach. In Proceedings
                                                            of the 34th annual meeting on Association for Com-
Gerard Escudero, Lluı́s Màrquez, and German Rigau.         putational Linguistics, pages 40–47, Morristown,
  2000. An empirical study of the domain depen-             NJ, USA. Association for Computational Linguis-
  dence of supervised word sense disambiguation sys-        tics.
  tems. In Proceedings of the 2000 Joint SIGDAT con-
  ference on Empirical methods in natural language        Siddharth Patwardhan and Ted Pedersen.           2003.
  processing and very large corpora, pages 172–180,          The cpan wordnet::similarity package. http://search
  Morristown, NJ, USA. Association for Computa-              .cpan.org/ sid/wordnet-similarity/.
  tional Linguistics.
                                                          Benjamin Snyder and Martha Palmer. 2004. The en-
C. Fellbaum. 1998. WordNet: An Electronic Lexical           glish all-words task. In Rada Mihalcea and Phil
   Database.                                                Edmonds, editors, Senseval-3: Third International
                                                            Workshop on the Evaluation of Systems for the Se-
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-        mantic Analysis of Text, pages 41–43, Barcelona,
   ity based on corpus statistics and lexical taxonomy.     Spain, July. Association for Computational Linguis-
   In Proc. of the Int’l. Conf. on Research in Computa-     tics.
   tional Linguistics, pages 19–33.
                                                          Marc Weeber, James G. Mork, and Alan R. Aronson.
                                                           2001. Developing a test collection for biomedical
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
                                                           word sense disambiguation. In In Proceedings of
  pak Bhattacharyya. 2010. Domain-specific word
                                                           the AMAI Symposium, pages 746–750.
  sense disambiguation combining corpus based and
  wordnet based parameters. In 5th International
  Conference on Global Wordnet (GWC2010).


                                                      1541

                Phrase-based Statistical Language Generation using
                      Graphical Models and Active Learning

                     François Mairesse, Milica Gašić, Filip Jurčı́ček,
               Simon Keizer, Blaise Thomson, Kai Yu and Steve Young∗
Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK
    {f.mairesse, mg436, fj228, sk561, brmt2, ky219, sjy}@eng.cam.ac.uk



                      Abstract                                utterance quality (Walker et al., 2002), or align-
                                                              ment models trained on speaker-specific corpora
    Most previous work on trainable language                  (Isard et al., 2006).
    generation has focused on two paradigms:
                                                                 A second line of research has focused on intro-
    (a) using a statistical model to rank a
                                                              ducing statistics at the generation decision level,
    set of generated utterances, or (b) using
                                                              by training models that find the set of genera-
    statistics to inform the generation deci-
                                                              tion parameters maximising an objective function,
    sion process. Both approaches rely on
                                                              e.g. producing a target linguistic style (Paiva and
    the existence of a handcrafted generator,
                                                              Evans, 2005; Mairesse and Walker, 2008), gener-
    which limits their scalability to new do-
                                                              ating the most likely context-free derivations given
    mains. This paper presents BAGEL, a sta-
                                                              a corpus (Belz, 2008), or maximising the expected
    tistical language generator which uses dy-
                                                              reward using reinforcement learning (Rieser and
    namic Bayesian networks to learn from
                                                              Lemon, 2009). While such methods do not suffer
    semantically-aligned data produced by 42
                                                              from the computational cost of an overgeneration
    untrained annotators. A human evalua-
                                                              phase, they still require a handcrafted generator to
    tion shows that BAGEL can generate nat-
                                                              define the generation decision space within which
    ural and informative utterances from un-
                                                              statistics can be used to find an optimal solution.
    seen inputs in the information presentation
                                                                 This paper presents BAGEL (Bayesian networks
    domain. Additionally, generation perfor-
                                                              for generation using active learning), an NLG sys-
    mance on sparse datasets is improved sig-
                                                              tem that can be fully trained from aligned data.
    nificantly by using certainty-based active
                                                              While the main requirement of the generator is to
    learning, yielding ratings close to the hu-
                                                              produce natural utterances within a dialogue sys-
    man gold standard with a fraction of the
                                                              tem domain, a second objective is to minimise the
    data.
                                                              overall development effort. In this regard, a major
1   Introduction                                              advantage of data-driven methods is the shift of
                                                              the effort from model design and implementation
The field of natural language generation (NLG) is
                                                              to data annotation. In the case of NLG systems,
one of the last areas of computational linguistics to
                                                              learning to produce paraphrases can be facilitated
embrace statistical methods. Over the past decade,
                                                              by collecting data from a large sample of annota-
statistical NLG has followed two lines of research.
                                                              tors. Our meaning representation should therefore
The first one, pioneered by Langkilde and Knight
                                                              (a) be intuitive enough to be understood by un-
(1998), introduces statistics in the generation pro-
                                                              trained annotators, and (b) provide useful gener-
cess by training a model which reranks candi-
                                                              alisation properties for generating unseen inputs.
date outputs of a handcrafted generator. While
                                                              Section 2 describes BAGEL’s meaning represen-
their HAL OGEN system uses an n-gram language
                                                              tation, which satisfies both requirements. Sec-
model trained on news articles, other systems have
                                                              tion 3 then details how our meaning representation
used hierarchical syntactic models (Bangalore and
                                                              is mapped to a phrase sequence, using a dynamic
Rambow, 2000), models trained on user ratings of
                                                              Bayesian network with backoff smoothing.
    ∗
    This research was partly funded by the UK EPSRC un-          Within a given domain, the same semantic
der grant agreement EP/F013930/1 and funded by the EU
FP7 Programme under grant agreement 216594 (CLASSiC           concept can occur in different utterances. Sec-
project: www.classic-project.org).                            tion 4 details how BAGEL exploits this redundancy


                                                        1552
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552–1561,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


to improve generation performance on sparse             (a) general attributes of the object under discus-
datasets, by guiding the data collection process        sion (e.g., inform(area) in Table 1), or (b) to
using certainty-based active learning (Lewis and        concepts that are not in the input at all, which are
Catlett, 1994). We train BAGEL in the informa-          associated with the singleton stack inform (e.g.,
tion presentation domain, from a corpus of utter-       phrases expressing the dialogue act type, or clause
ances produced by 42 untrained annotators (see          aggregation operations). For example, the stack
Section 5.1). An automated evaluation metric is         sequence in Table 1 contains 3 intermediary stacks
used to compare preliminary model and training          for t = 2, 5 and 7.
configurations in Section 5.2, while Section 5.3           BAGEL’s granularity is defined by the semantic
shows that the resulting system produces natural        annotation in the training data, rather than external
and informative utterances, according to 18 hu-         linguistic knowledge about what constitutes a unit
man judges. Finally, our human evaluation shows         of meaning, i.e. contiguous words belonging to
that training using active learning significantly im-   the same semantic stack are modelled as an atomic
proves generation performance on sparse datasets,       observation unit or phrase.1 In contrast with word-
yielding results close to the human gold standard       level models, a major advantage of phrase-based
using a fraction of the data.                           generation models is that they can model long-
                                                        range dependencies and domain-specific idiomatic
2   Phrase-based generation from                        phrases with fewer parameters.
    semantic stacks
                                                        3   Dynamic Bayesian networks for NLG
BAGEL uses a stack-based semantic representa-
tion to constrain the sequence of semantic con-         Dynamic Bayesian networks have been used suc-
cepts to be searched. This representation can be        cessfully for speech recognition, natural language
seen as a linearised semantic tree similar to the       understanding, dialogue management and text-to-
one previously used for natural language under-         speech synthesis (Rabiner, 1989; He and Young,
standing in the Hidden Vector State model (He           2005; Lefèvre, 2006; Thomson and Young, 2010;
and Young, 2005). A stack representation provides       Tokuda et al., 2000). Such models provide a
useful generalisation properties (see Section 3.1),     principled framework for predicting elements in a
while the resulting stack sequences are relatively      large structured space, such as required for non-
easy to align (see Section 5.1). In the context of      trivial NLG tasks. Additionally, their probabilistic
dialogue systems, Table 1 illustrates how the input     nature makes them suitable for modelling linguis-
dialogue act is first mapped to a set of stacks of      tic variation, i.e. there can be multiple valid para-
semantic concepts, and then aligned with a word         phrases for a given input.
sequence. The bottom concept in the stack will             BAGEL models the generation task as finding
typically be a dialogue act type, e.g. an utterance     the most likely sequence of realisation phrases
providing information about the object under dis-       R∗ = (r1 ...rL ) given an unordered set of manda-
cussion (inform) or specifying that the request         tory semantic stacks Sm , with |Sm | ≤ L. BAGEL
of the user cannot be met (reject). Other con-          must thus derive the optimal sequence of semantic
cepts include attributes of that object (e.g., food,    stacks S∗ that will appear in the utterance given
area), values for those attributes (e.g., Chinese,      Sm , i.e. by inserting intermediary stacks if needed
riverside), as well as special symbols for negat-       and by performing content ordering. Any num-
ing underlying concepts (e.g., not) or specifying       ber of intermediary stacks can be inserted between
that they are irrelevant (e.g., dontcare).              two consecutive mandatory stacks, as long as all
   The generator’s goal is thus finding the             their concepts are included in either the previous
most likely realisation given an unordered              or following mandatory stack, and as long as each
set of mandatory semantic stacks Sm derived             stack transition leads to a different stack (see ex-
from the input dialogue act.         For example,       ample in Table 1). Let us define the set of possi-
s =inform(area(centre)) is a mandatory stack            ble stack sequences matching these constraints as
associated with the dialogue act in Table 1 (frame      Seq(Sm ) ⊆ {S = (s1 ...sL ) s.t. st ∈ Sm ∪ Si }.
8). While mandatory stacks must all be conveyed            We propose a model which estimates the dis-
in the output realisation, Sm does not contain the         1
                                                             The term phrase is thus defined here as any sequence of
optional intermediary stacks Si that can refer to       one or more words.


                                                    1553


     Charlie Chan           is a       Chinese         restaurant      near           Cineworld          in the       centre of town
    Charlie Chan                       Chinese        restaurant                     Cineworld                           centre
        name                            food             type         near              near            area              area
       inform           inform         inform           inform       inform            inform          inform           inform
        t=1              t=2            t=3              t=4          t=5               t=6             t=7               t=8

Table 1:       Example semantic stacks aligned with an utterance for the dialogue act
inform(name(Charlie Chan) type(restaurant) area(centre) food(Chinese) near(Cineworld)).
                                                                            Mandatory
stacks are in bold.

tribution P (R|Sm ) from a training set of real-                     last mandatory
                                                                          stack
isation phrases aligned with semantic stack se-
                                                                        stack set
quences, by marginalising over all stack sequences                      validator
in Seq(Sm ):                                                            semantic
                                                                         stack s

                        X                                            stack set tracker
     P (R|Sm ) =                   P (R, S|Sm )                                          first frame      repeated frame        final frame
                   S∈Seq(Sm )
                        X
               =                   P (R|S, Sm )P (S|Sm )            Figure 1: Graphical model for the semantic decod-
                   S∈Seq(Sm )
                        X                                           ing phase. Plain arrows indicate smoothed proba-
               =                   P (R|S)P (S|Sm )         (1)     bility distributions, dashed arrows indicate deter-
                   S∈Seq(Sm )
                                                                    ministic relations, and shaded nodes are observed.
                                                                    The generation of the end semantic stack symbol
   Inference over the model defined in (1) requires                 deterministically triggers the final frame.
the decoding algorithm to consider all possible or-
derings over Seq(Sm ) together with all possible
realisations, which is intractable for non-trivial do-              to depend only on the previous two stacks and the
mains. We thus make the additional assumption                       last mandatory stack su ∈ Sm with 1 ≤ u < t:
that the most likely sequence of semantic stacks
S∗ given Sm is the one yielding the optimal reali-                                        Q
                                                                                             T
                                                                                             t=1 P (st |st−1 , st−2 , su )
                                                                                         
sation phrase sequence:                                                      P (S|Sm ) =         if S ∈ Seq(Sm )                              (6)
                                                                                           0     otherwise
                                                                                         
                                                                                           T
           P (R|Sm ) ≈ P (R|S∗ )P (S∗ |Sm )                 (2)              P (R|S∗ ) =
                                                                                           Y
                                                                                                 P (rt |rt−1 , s∗t−1 , s∗t , s∗t+1 )          (7)
                    ∗
             with S = argmax P (S|Sm )                      (3)                            t=1
                         S∈Seq(Sm )


                                                                       While dynamic Bayesian networks typically
   The semantic stacks are therefore decoded first
                                                                    take sequential inputs, mapping a set of seman-
using the model in Fig. 1 to solve the argmax
                                                                    tic stacks to a sequence of phrases is achieved
in (3). The decoded stack sequence S∗ is then
                                                                    by keeping track of the mandatory stacks that
treated as observed in the realisation phase, in
                                                                    were visited in the current sequence (see stack set
which the model in Fig. 2 is used to find the real-
                                                                    tracker variable in Fig. 1), and pruning any se-
isation phrase sequence R∗ maximising P (R|S∗ )
                                                                    quence that has not included all mandatory input
over all phrase sequences of length L = |S∗ | in
                                                                    stacks on reaching the final frame (see observed
our vocabulary:
                                                                    stack set validator variable in Fig. 1). Since the
                                                                    number of intermediary stacks is not known at de-
         R∗ = argmax P (R|S∗ )P (S∗ |Sm )                   (4)     coding time, the network is unrolled for a fixed
               R=(r1 ...rL )

             = argmax P (R|S∗ )                             (5)     number of frames T defining the maximum num-
               R=(r1 ...rL )                                        ber of phrases that can be generated (e.g., T =
                                                                    50). The end of the stack sequence is then deter-
   In order to reduce model complexity, we fac-                     mined by a special end symbol, which can only
torise our model by conditioning the realisation                    be emitted within the T frames once all mandatory
phrase at time t on the previous phrase rt−1 ,                      stacks have been visited. The probability of the re-
and the previous, current, and following semantic                   sulting utterance is thus computed over all frames
stacks. The semantic stack st at time t is assumed                  up to the end symbol, which determines the length


                                                              1554


L of S∗ and R∗ . While the decoding constraints           realisation
enforce that L > |Sm |, the search for S∗ requires         phrase r

comparing sequences of different lengths. A con-
sequence is that shorter sequences containing only        stack tail l

mandatory stacks are likely to be favoured. While
future work should investigate length normalisa-
tion strategies, we find that the learned transition
                                                          !"#$%&& '(")*+
                                                          stack head h

                                                           semantic
                                                            stack s
                                                                                    first frame                   repeated frame                       final frame
probabilities are skewed enough to favour stack
sequences including intermediary stacks.               Figure 2: Graphical model for the realisation
                                                       phase. Dashed arrows indicate deterministic re-
   Once the topology and the decoding constraints
                                                       lations, and shaded node are observed.
of the network have been defined, any inference al-
gorithm can be used to search for S∗ and R∗ . We                                                   rt | ht , lt , rt −1 , lt −1 , lt +1 , st , st −1 , st +1
use the junction tree algorithm implemented in the                                                         rt | ht , lt , rt −1 , lt −1 , lt +1 , st
Graphical Model ToolKit (GMTK) for our exper-                            st | st −1 , st −2 , su             rt | ht , lt , rt −1 , lt −1 , lt +1
iments (Bilmes and Zweig, 2002), however both                              st | st −1 , st − 2                          rt | ht , lt
problems can be solved using a standard Viterbi                                st | st −1                                 rt | ht
search given the appropriate state representation.                                 st                                         rt
In terms of computational complexity, it is impor-
tant to note that the number of stack sequences        Figure 3: Backoff graphs for the semantic decod-
Seq(Sm ) to search over increases exponentially        ing and realisation models.
with the number of input mandatory stacks. Nev-
ertheless, we find that real-time performance can
be achieved by pruning low probability sequences,                                            L
without affecting the quality of the solution.
                                                                                             Y
                                                                   P (R|S∗ ) =                        P (rt |rt−1 , ht , lt−1 , lt , lt+1 ,
                                                                                             t=1

                                                                                                                  s∗t−1 , s∗t , s∗t+1 )                              (8)

3.1   Generalisation to unseen semantic stacks
                                                          Conditional probability distributions are repre-
                                                       sented as factored language models smoothed us-
In order to generalise to semantic stacks which        ing Witten-Bell interpolated backoff smoothing
have not been observed during training, the re-        (Bilmes and Kirchhoff, 2003), according to the
alisation phrase r is made dependent on under-         backoff graphs in Fig. 3. Variables which are the
specified stack configurations, i.e. the tail l        furthest away in time are dropped first, and par-
and the head h of the stack. For example, the          tial stack variables are dropped last as they are ob-
last stack in Table 1 is associated with the head      served the most.
centre and the tail inform(area). As a re-                It is important to note that generating unseen se-
sult, BAGEL assigns non-zero probabilities to re-      mantic stacks requires all possible mandatory se-
alisation phrases in unseen semantic contexts, by      mantic stacks in the target domain to be prede-
backing off to the head and the tail of the stack.     fined, in order for all stack unigrams to be assigned
A consequence is that BAGEL’s lexical realisa-         a smoothed non-zero probability.
tion can generalise across contexts. For exam-
ple, if reject(area(centre)) was never ob-             3.2       High cardinality concept abstraction
served at training time, P (r = centre of town|s =     While one should expect a trainable generator
reject(area(centre))) will be estimated by             to learn multiple lexical realisations for low-
backing off to P (r = centre of town|h =               cardinality semantic concepts, learning lexical
centre). BAGEL can thus generate ‘there are            realisations for high-cardinality database entries
no venues in the centre of town’ if the phrase         (e.g., proper names) would increase the number of
‘centre of town’ was associated with the con-          model parameters prohibitively. We thus divide
cept centre in a different context, such as            pre-terminal concepts in the semantic stacks into
inform(area(centre)). The final realisation            two types: (a) enumerable attributes whose val-
model is illustrated in Fig. 2:                        ues are associated with distinct semantic stacks in


                                                   1555


our model (e.g., inform(pricerange(cheap))),                        Since each active learning iteration requires gen-
and (b) non-enumerable attributes whose values                      erating all training utterances in our domain, they
are replaced by a generic symbol before train-                      are generated using a larger clique pruning thresh-
ing in both the utterance and the semantic stack                    old than the test utterances used for evaluation.
(e.g., inform(name(X)). These symbolic values
are then replaced in the surface realisation by the                 5.1      Corpus collection
corresponding value in the input specification. A                   We train BAGEL in the context of a dialogue
consequence is that our model can only learn syn-                   system providing information about restaurants
onymous lexical realisations for enumerable at-                     in Cambridge. The domain contains two dia-
tributes.                                                           logue act types: (a) inform: presenting infor-
                                                                    mation about a restaurant (see Table 1), and (b)
4     Certainty-based active learning
                                                                    reject: informing that the user’s constraints can-
A major issue with trainable NLG systems is the                     not be met (e.g., ‘There is no cheap restaurant
lack of availability of domain-specific data. It is                 in the centre’). Our domain contains 8 restau-
therefore essential to produce NLG models that                      rant attributes: name, food, near, pricerange,
minimise the data annotation cost.                                  postcode, phone, address, and area, out of
   BAGEL supports the optimisation of the data                      which food, pricerange, and area are treated
collection process through active learning, in                      as enumerable.3 Our input semantic space is ap-
which the next semantic input to annotate is de-                    proximated by the set of information presentation
termined by the current model. The probabilis-                      dialogue acts produced over 20,000 simulated di-
tic nature of BAGEL allows the use of certainty-                    alogues between our statistical dialogue manager
based active learning (Lewis and Catlett, 1994),                    (Young et al., 2010) and an agenda-based user
by querying the k semantic inputs for which the                     simulator (Schatzmann et al., 2007), which results
model is the least certain about its output real-                   in 202 unique dialogue acts after replacing non-
isation. Given a finite semantic input space I                      enumerable values by a generic symbol. Each di-
representing all possible dialogue acts in our do-                  alogue act contains an average of 4.48 mandatory
main (i.e., the set of all sets of mandatory seman-                 semantic stacks.
tic stacks Sm ), BAGEL’s active learning training                      As one of our objectives is to test whether
process iterates over the following steps:                          BAGEL can learn from data provided by a large
                                                                    sample of untrained annotators, we collected a
    1. Generate an utterance for each semantic input Sm ∈ I
       using the current model.2                                    corpus of semantically-aligned utterances using
                                               1     k
                                                                    Amazon’s Mechanical Turk data collection ser-
    2. Annotate the k semantic inputs {Sm        ...Sm } yielding
       the lowest realisation probability, i.e. for q ∈ (1..k)      vice. A crucial aspect of data collection for
                                                                    NLG is to ensure that the annotators under-
            q
           Sm =         argmin          (max P (R|Sm ))       (9)   stand the meaning of the semantics to be con-
                           1 ...S q−1       R
                   Sm ∈I\{Sm      m     }
                                                                    veyed. Annotators were first asked to provide
       with P (R|Sm ) defined in (2).                               an utterance matching an abstract description
                                                                    of the dialogue act, regardless of the order in
    3. Retrain the model with the additional k data points.
                                                                    which the constraints are presented (e.g., Offer
   The number of utterances to be queried k should                  the venue Taj Mahal and provide the information
depend on the flexibility of the annotators and the                 type(restaurant), area(riverside), food(Indian),
time required for generating all possible utterances                near(The Red Lion)). The order of the constraints
in the domain.                                                      in the description was randomised to reduce the
                                                                    effect of priming. The annotators were then asked
5     Experimental method                                           to align the attributes (e.g., Indicate the region of
                                                                    the utterance related to the concept ‘area’), and
BAGEL’s factored language models are trained us-
                                                                    the attribute values (e.g., Indicate only the words
ing the SRILM toolkit (Stolcke, 2002), and de-
                                                                    related to the concept ‘riverside’). Two para-
coding is performed using GMTK’s junction tree
                                                                    phrases were collected for each dialogue act in
inference algorithm (Bilmes and Zweig, 2002).
                                                                    our domain, resulting in a total of 404 aligned ut-
    2
      Sampling methods can be used if I is infinite or too
                                                                       3
large.                                                                     With the exception of areas defined as proper nouns.


                                                                1556


                      rt                st                                                 ht                  lt
                      <s>               START                                              START               START
                      The Rice Boat     inform(name(X))                                    X                   inform(name)
                      is a              inform                                             inform              EMPTY
                      restaurant        inform(type(restaurant))                           restaurant          inform(type)
                      in the            inform(area)                                       area                inform
                      riverside         inform(area(riverside))                            riverside           inform(area)
                      area              inform(area)                                       area                inform
                      that              inform                                             inform              EMPTY
                      serves            inform(food)                                       food                inform
                      French            inform(food(French))                               French              inform(food)
                      food              inform(food)                                       food                inform
                      </s>              END                                                END                 END

Table 2: Example utterance annotation used to estimate the conditional probability distributions of the
models in Figs. 1 and 2 ( rt =realisation phrase, st =semantic stack, ht =stack head, lt =stack tail).

                                                                                    !"%$
terances produced by 42 native speakers of En-
                                                                                     !"%
glish. After manually checking and normalising
the dataset,4 the layered annotations were auto-                                    !"#$



                                                                  !"#$%&'()%*+,-"
matically mapped to phrase-level semantic stacks                                     !"#

by splitting the utterance into phrases at annotation                               !"$$

boundaries. Each annotated utterance is then con-                                    !"$

verted into a sequence of symbols such as in Ta-                                    !".$                             &'(()*+,-(
                                                                                                                     /+)01234)5234+66
ble 2, which are used to estimate the conditional                                    !".                             /+)01234)5234+667)8+)6'1'9-)0-*281:30
probability distributions defined in (6) and (8).                                   !";$
The resulting vocabulary consists of 52 distinct se-                                       <!   =!   .!   #!    >!     <!! <=! <$! =!! =$! ;!! ;#=

mantic stacks and 109 distinct realisation phrases,                                                              .-#/$/$0%*"1%*/2"

with an average of 8.35 phrases per utterance.                    Figure 4: BLEU score averaged over a 10-fold
                                                                  cross-validation for different training set sizes and
                                                                  network topologies, using random sampling.
5.2    BLEU score evaluation

We first evaluate BAGEL using the BLEU auto-
mated metric (Papineni et al., 2002), which mea-                  Results: Fig. 4 shows that adding a dependency
sures the word n-gram overlap between the gen-                    on the future semantic stack improves perfor-
erated utterances and the 2 reference paraphrases                 mances for all training set sizes, despite the added
over a test corpus (with n up to 4). While BLEU                   model complexity. Backing off to partial stacks
suffers from known issues such as a bias towards                  also improves performance, but only for sparse
statistical NLG systems (Reiter and Belz, 2009), it               training sets.
provides useful information when comparing sim-                      Fig. 5 compares the full model trained using
ilar systems. We evaluate BAGEL for different                     random sampling in Fig. 4 with the same model
training set sizes, model dependencies, and active                trained using certainty-based active learning, for
learning parameters. Our results are averaged over                different values of k. As our dataset only con-
a 10-fold cross-validation over distinct dialogue                 tains two paraphrases per dialogue act, the same
acts, i.e. dialogue acts used for testing are not seen            dialogue act can only be queried twice during the
at training time,5 and all systems are tested on the              active learning procedure. A consequence is that
same folds. The training and test sets respectively               the training set used for active learning converges
contain an average of 181 and 21 distinct dialogue                towards the randomly sampled set as its size in-
acts, and each dialogue act is associated with two                creases. Results show that increasing the train-
paraphrases, resulting in 362 training utterances.                ing set one utterance at a time using active learn-
                                                                  ing (k = 1) significantly outperforms random
   4
      The normalisation process took around 4 person-hour for     sampling when using 40, 80, and 100 utterances
404 utterances.                                                   (p < .05, two-tailed). Increasing the number of
    5
      We do not evaluate performance on dialogue acts used
for training, as the training examples can trivially be used as   utterances to be queried at each iteration to k = 10
generation templates.                                             results in a smaller performance increase. A possi-


                                                              1557


                   !"%#
                                                                                             5.3   Human evaluation
                    !"%
                                                                                             While automated metrics provide useful informa-
                   !"$#
                                                                                             tion for comparing different systems, human feed-
!"#$%&'()%*+,-"



                    !"$
                                                                                             back is needed to assess (a) the quality of BAGEL’s
                   !"##
                                                                                             outputs, and (b) whether training models using ac-
                    !"#
                                                                   &'()*+,-'+./0(1
                                                                                             tive learning has a significant impact on user per-
                   !"3#
                                                                   7890:;,/;'<(0(1,=>4       ceptions. We evaluate BAGEL through a large-
                    !"3                                            7890:;,/;'<(0(1,=>4!      scale subjective rating experiment using Amazon’s
                   !"2#
                                                                                             Mechanical Turk service.
                          4!   5!   3!   $!   6!   4!! 45! 4#! 5!! 5#! 2!! 2$5
                                                                                                For each dialogue act in our domain, partici-
                                               .-#/$/$0%*"1%*/2"
                                                                                             pants are presented with a ‘gold standard’ human
Figure 5: BLEU score averaged over a 10-fold                                                 utterance from our dataset, which they must com-
cross-validation for different numbers of queries                                            pare with utterances generated by models trained
per iteration, using the full model with the query                                           with and without active learning on a set of 20, 40,
selection criterion (9).                                                                     100, and 362 utterances (full training set), as well
                                                                                             as with the second human utterance in our dataset.
                   !"%#
                                                                                             See example utterances in Table 3. The judges are
                    !"%
                                                                                             then asked to evaluate the informativeness and nat-
                   !"$#
                                                                                             uralness of each of the 8 utterances on a 5 point
 !"#$%&'()%*+,-"




                    !"$
                                                                                             likert-scale. Naturalness is defined as whether the
                   !"##
                                                                                             utterance could have been produced by a human,
                    !"#
                                                                                             and informativeness is defined as whether it con-
                   !"0#                                            &'(()(*+,-*.
                                                                   4*+,-*.),5-)6-785         tains all the information in the gold standard utter-
                    !"0                                            4*9+5:;)<9,';)6<-:;       ance. Each utterance is taken from the test folds of
                   !"/#
                                                                                             the cross-validation experiment presented in Sec-
                          1!   2!   0!   $!   3!   1!! 12! 1#! 2!! 2#! /!! /$2
                                                                                             tion 5.2, i.e. the models are trained on up to 90%
                                               .-#/$/$0%*"1%*/2"
                                                                                             of the data and the training set does not contain the
Figure 6: BLEU score averaged over a 10-fold                                                 dialogue act being tested.
cross-validation for different query selection cri-
teria, using the full model with k = 1.                                                      Results: Figs. 7 and 8 compare the naturalness
                                                                                             and informativeness scores of each system aver-
                                                                                             aged over all 202 dialogue acts. A paired t-test
ble explanation is that the model is likely to assign                                        shows that models trained on 40 utterances or
low probabilities to similar inputs, thus any value                                          less produce utterances that are rated significantly
above k = 1 might result in redundant queries                                                lower than human utterances for both naturalness
within an iteration.                                                                         and informativeness (p < .05, two-tailed). How-
   As the length of the semantic stack sequence                                              ever, models trained on 100 utterances or more do
is not known before decoding, the active learn-                                              not perform significantly worse than human utter-
ing selection criterion presented in (9) is biased                                           ances for both dimensions, with a mean difference
towards longer utterances, which tend to have a                                              below .10 over 202 comparisons. Given the large
lower probability. However, Fig. 6 shows that                                                sample size, this result suggests that BAGEL can
normalising the log probability by the number of                                             successfully learn our domain using a fraction of
semantic stacks does not improve overall learn-                                              our initial dataset.
ing performance. Although a possible explanation                                                As far as the learning method is concerned, a
is that longer inputs tend to contain more infor-                                            paired t-test shows that models trained on 20 and
mation to learn from, Fig. 6 shows that a base-                                              40 utterances using active learning significantly
line selecting the largest remaining semantic input                                          outperform models trained using random sam-
at each iteration performs worse than the active                                             pling, for both dimensions (p < .05). The largest
learning scheme for training sets above 20 utter-                                            increase is observed using 20 utterances, i.e. the
ances. The full log probability selection criterion                                          naturalness increases by .49 and the informative-
defined in (9) is therefore used throughout the rest                                         ness by .37. When training on 100 utterances, the
of the paper (with k = 1).                                                                   effect of active learning becomes insignificant. In-


                                                                                          1558


 Input                           inform(name(the Fountain) near(the Arts Picture House) area(centre) pricerange(cheap))
 Human                           There is an inexpensive restaurant called the Fountain in the centre of town near the Arts Picture House
 Rand-20                         The Fountain is a restaurant near the Arts Picture House located in the city centre cheap price range
 Rand-40                         The Fountain is a restaurant in the cheap city centre area near the Arts Picture House
 AL-20                           The Fountain is a restaurant near the Arts Picture House in the city centre cheap
 AL-40                           The Fountain is an affordable restaurant near the Arts Picture House in the city centre
 Full set                        The Fountain is a cheap restaurant in the city centre near the Arts Picture House
 Input                           reject(area(Barnwell) near(Saint Mary0 s Church))
 Human                           I am sorry but I know of no venues near Saint Mary’s Church in the Barnwell area
 Full set                        I am sorry but there are no venues near Saint Mary’s Church in the Barnwell area
 Input                           inform(name(the Swan)area(Castle Hill) pricerange(expensive))
 Human                           The Swan is a restaurant in Castle Hill if you are seeking something expensive
 Full set                        The Swan is an expensive restaurant in the Castle Hill area
 Input                           inform(name(Browns) area(centre) near(the Crowne Plaza) near(El Shaddai) pricerange(cheap))
 Human                           Browns is an affordable restaurant located near the Crowne Plaza and El Shaddai in the centre of the city
 Full set                        Browns is a cheap restaurant in the city centre near the Crowne Plaza and El Shaddai

Table 3: Example utterances for different input dialogue acts and system configurations. AL-20 = active
learning with 20 utterances, Rand = random sampling.

                            $                                                                                               +




                                                                                              !"#$%&$'()*#+&,"$"--%-.()"
                           *"$                                                                                             #"+      #"&!
                                        *"%'                                                                                                            #"%$
  !"#$%$#&'(#)$"**%*+,("




                                                                   *"%%                                                                    !"()
                            *                  !")*                                                                         #    !"'&
                                   !"(%                             !"&'       *"%#                                                                       #"%&       #"%#
                           !"$                                                                                             !"+                !"$$
                                                       !"$%                                                                        !"##
                            !                                                                                               !
                                    !"##
                           +"$                                                                                             *"+
                                                              ,-./01                                                        *                        ,-./01
                            +
                                                              234567897-:.5.;                                                                        234567897-:.5.;
                           #"$                                                                                             &"+
                                                              <=1-.8=447:-.378>8*"%'                                                                 <=1-.8=447:-.378>8#"&!
                            #                                                                                               &
                                   +%                 *%          #%%         !(+                                                 *%         #%          &%%         !)*
                                                 -(#.$.$/%*"&%*.0"                                                                         /)#&$&$0%-"+%-&1"


Figure 7: Naturalness mean opinion scores for dif-                                        Figure 8: Informativeness mean opinion scores for
ferent training set sizes, using random sampling                                          different training set sizes, using random sampling
and active learning. Differences for training set                                         and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).                                         sizes of 20 and 40 are all significant (p < .05).


terestingly, while models trained on 100 utterances                                       6                     Related work
outperform models trained on 40 utterances using
random sampling (p < .05), they do not signifi-                                           While most previous work on trainable NLG re-
cantly outperform models trained on 40 utterances                                         lies on a handcrafted component (see Section 1),
using active learning (p = .15 for naturalness and                                        recent research has started exploring fully data-
p = .41 for informativeness). These results sug-                                          driven NLG models.
gest that certainty-based active learning is benefi-                                         Factored language models have recently been
cial for training a generator from a limited amount                                       used for surface realisation within the OpenCCG
of data given the domain size.                                                            framework (White et al., 2007; Espinosa et al.,
   Looking back at the results presented in Sec-                                          2008). More generally, chart generators for
tion 5.2, we find that the BLEU score correlates                                          different grammatical formalisms have been
with a Pearson correlation coefficient of .42 with                                        trained from syntactic treebanks (White et al.,
the mean naturalness score and .35 with the mean                                          2007; Nakanishi et al., 2005), as well as from
informativeness score, over all folds of all systems                                      semantically-annotated treebanks (Varges and
tested (n = 70, p < .01). This is lower than                                              Mellish, 2001). However, a major difference with
previous correlations reported by Reiter and Belz                                         our approach is that BAGEL uses domain-specific
(2009) in the shipping forecast domain with non-                                          data to generate a surface form directly from se-
expert judges (r = .80), possibly because our do-                                         mantic concepts, without any syntactic annotation
main is larger and more open to subjectivity.                                             (see Section 7 for further discussion).


                                                                                       1559


   This work is strongly related to Wong and           data is limited, according to ratings from 18 hu-
Mooney’s WASP−1 generation system (2007),              man judges.6 These results suggest that the pro-
which combines a language model with an in-            posed framework can largely reduce the develop-
verted synchronous CFG parsing model, effec-           ment time of NLG systems.
tively casting the generation task as a translation       While this paper only evaluates the most likely
problem from a meaning representation to natu-         realisation given a dialogue act, we believe that
ral language. WASP−1 relies on G IZA ++ to align       BAGEL’s probabilistic nature and generalisation
utterances with derivations of the meaning repre-      capabilities are well suited to model the linguis-
sentation (Och and Ney, 2003). Although early          tic variation resulting from the diversity of annota-
experiments showed that G IZA ++ did not perform       tors. Our first objective is thus to evaluate the qual-
well on our data—possibly because of the coarse        ity of BAGEL’s n-best outputs, and test whether
granularity of our semantic representation—future      sampling from the output distribution can improve
work should evaluate the generalisation perfor-        naturalness and user satisfaction within a dialogue.
mance of synchronous CFGs in a dialogue system            Our results suggest that explicitly modelling
domain.                                                syntax is not necessary for our domain, possi-
   Although we do not know of any work on ac-          bly because of the lack of syntactic complexity
tive learning for NLG, previous work has used          compared with formal written language. Never-
active learning for semantic parsing and informa-      theless, future work should investigate whether
tion extraction (Thompson et al., 1999; Tang et al.,   syntactic information can improve performance in
2002), spoken language understanding (Tur et al.,      more complex domains. For example, the reali-
2003), speech recognition (Hakkani-Tür et al.,        sation phrase can easily be conditioned on syntac-
2002), word alignment (Sassano, 2002), and more        tic constructs governing that phrase, and the recur-
recently for statistical machine translation (Blood-   sive nature of syntax can be modelled by keeping
good and Callison-Burch, 2010). While certainty-       track of the depth of the current embedded clause.
based methods have been widely used, future work       While syntactic information can be included with
should investigate the performance of committee-       no human effort by using syntactic parsers, their
based active learning for NLG, in which examples       robustness to dialogue system utterances must first
are selected based on the level of disagreement be-    be evaluated.
tween models trained on subsets of the data (Fre-         Finally, recent years have seen HMM-based
und et al., 1997).                                     synthesis models become competitive with unit se-
                                                       lection methods (Tokuda et al., 2000). Our long
7   Discussion and conclusion                          term objective is to take advantage of those ad-
                                                       vances to jointly optimise the language genera-
This paper presents and evaluates BAGEL, a sta-        tion and the speech synthesis process, by combin-
tistical language generator that can be trained en-    ing both components into a unified probabilistic
tirely from data, with no handcrafting required be-    concept-to-speech generation model.
yond the semantic annotation. All the required
subtasks—i.e. content ordering, aggregation, lex-      References
ical selection and realisation—are learned from        S. Bangalore and O. Rambow. Exploiting a probabilistic hi-
data using a unified model. To train BAGEL in a di-       erarchical model for generation. In Proceedings of the
                                                          18th International Conference on Computational Linguis-
alogue system domain, we propose a stack-based            tics (COLING), pages 42–48, 2000.
semantic representation at the phrase level, which
                                                       A. Belz. Automatic generation of weather forecast texts us-
is expressive enough to generate natural utterances       ing comprehensive probabilistic generation-space models.
from unseen inputs, yet simple enough for data to         Natural Language Engineering, 14(4):431–455, 2008.
be collected from 42 untrained annotators with a       J. Bilmes and K. Kirchhoff. Factored language models and
minimal normalisation step. A human evaluation            generalized parallel backoff. In Proceedings of HLT-
                                                          NAACL, short papers, 2003.
over 202 dialogue acts does not show any differ-
                                                       J. Bilmes and G. Zweig. The Graphical Models ToolKit: An
ence in naturalness and informativeness between           open source software system for speech and time-series
BAGEL’s outputs and human utterances. Addition-           processing. In Proceedings of ICASSP, 2002.
ally, we find that the data collection process can        6
                                                            The full training corpus and the generated
be optimised using active learning, resulting in a     utterances used for evaluation are available at
significant increase in performance when training      http://mi.eng.cam.ac.uk/∼farm2/bagel.


                                                   1560


M. Bloodgood and C. Callison-Burch. Bucking the trend:              Proceedings of the 40th Annual Meeting of the Association
  Large-scale cost-focused active learning for statistical ma-      for Computational Linguistics (ACL), 2002.
  chine translation. In Proceedings of the 48th Annual           J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
  Meeting of the Association for Computational Linguistics          S. Young. Agenda-based user simulation for bootstrap-
  (ACL), 2010.                                                      ping a POMDP dialogue system. In Proceedings of HLT-
D. Espinosa, M. White, and D. Mehay. Hypertagging: Su-              NAACL, short papers, pages 149–152, 2007.
   pertagging for surface realization with CCG. In Proceed-      A. Stolcke. SRILM – an extensible language modeling
   ings of the 46th Annual Meeting of the Association for          toolkit. In Proceedings of the International Conference
   Computational Linguistics (ACL), 2008.                          on Spoken Language Processing, 2002.
Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective       M. Tang, X. Luo, and S. Roukos. Active learning for statis-
   sampling using the query by committee algorithm. Ma-            tical natural language parsing. In Proceedings of the 40th
   chine Learning, 28:133–168, 1997.                               Annual Meeting of the Association for Computational Lin-
D. Hakkani-Tür, G. Riccardi, and A. Gorin. Active learn-          guistics (ACL), 2002.
   ing for automatic speech recognition. In Proceedings of       C. Thompson, M. E. Califf, and R. J. Mooney. Active learn-
   ICASSP, 2002.                                                    ing for natural language parsing and information extrac-
Y. He and S. Young. Semantic processing using the Hidden            tion. In Proceedings of ICML, 1999.
   Vector State model. Computer Speech & Language, 19            B. Thomson and S. Young. Bayesian update of dialogue state:
   (1):85–106, 2005.                                                A POMDP framework for spoken dialogue systems. Com-
A. Isard, C. Brockmann, and J. Oberlander. Individuality and        puter Speech & Language, 24(4):562–588, 2010.
   alignment in generated dialogues. In Proceedings of the       Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and
   4th International Natural Language Generation Confer-            T. Kitamura. Speech parameter generation algorithms for
   ence (INLG), pages 22–29, 2006.                                  HMM-based speech synthesis. In Proceedings of ICASSP,
I. Langkilde and K. Knight. Generation that exploits corpus-        2000.
    based statistical knowledge. In Proceedings of the 36th      G. Tur, R. E. Schapire, and D. Hakkani-Tür. Active learn-
    Annual Meeting of the Association for Computational Lin-        ing for spoken language understanding. In Proceedings of
    guistics (ACL), pages 704–710, 1998.                            ICASSP, 2003.
F. Lefèvre. A DBN-based multi-level stochastic spoken lan-      S. Varges and C. Mellish. Instance-based natural language
   guage understanding system. In Proceedings of the IEEE           generation. In Proceedings of the Annual Meeting of the
   Workshop on Spoken Language Technology (SLT), 2006.              North American Chapter of the ACL (NAACL), 2001.
D. D. Lewis and J. Catlett. Heterogeneous uncertainty am-        M. A. Walker, O. Rambow, and M. Rogati. Training a sen-
   pling for supervised learning. In Proceedings of ICML,          tence planner for spoken dialogue using boosting. Com-
   1994.                                                           puter Speech and Language, 16(3-4), 2002.
F. Mairesse and M. A. Walker. Trainable generation of Big-       M. White, R. Rajkumar, and S. Martin. Towards broad cov-
   Five personality styles through data-driven parameter esti-     erage surface realization with CCG. In Proceedings of the
   mation. In Proceedings of the 46th Annual Meeting of the        Workshop on Using Corpora for NLG: Language Genera-
   Association for Computational Linguistics (ACL), 2008.          tion and Machine Translation, 2007.
H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods   Y. W. Wong and R. Mooney. Generation by inverting a se-
   for disambiguation of an HPSG-based chart generator. In          mantic parser that uses statistical machine translation. In
   Proceedings of the IWPT, 2005.                                   Proceedings of HLT-NAACL, 2007.
F. J. Och and H. Ney. A systematic comparison of various         S. Young, M. Gašić, S. Keizer, F. Mairesse, J. Schatzmann,
   statistical alignment models. Computational Linguistics,         B. Thomson, and K. Yu. The Hidden Information State
   29(1):19–51, 2003.                                               model: a practical framework for POMDP-based spoken
D. S. Paiva and R. Evans. Empirically-based control of nat-         dialogue management. Computer Speech and Language,
   ural language generation. In Proceedings of the 43rd An-         24(2):150–174, 2010.
   nual Meeting of the Association for Computational Lin-
   guistics (ACL), pages 58–65, 2005.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a
   method for automatic evaluation of machine translation. In
   Proceedings of the 40th Annual Meeting of the Association
   for Computational Linguistics (ACL), 2002.
L. R. Rabiner. Tutorial on Hidden Markov Models and se-
   lected applications in speech recognition. Proceedings of
   the IEEE, 77(2):257–285, 1989.
E. Reiter and A. Belz. An investigation into the validity
   of some metrics for automatically evaluating natural lan-
   guage generation systems. Computational Linguistics, 25:
   529–558, 2009.
V. Rieser and O. Lemon. Natural language generation as
   planning under uncertainty for spoken dialogue systems.
   In Proceedings of the Annual Meeting of the European
   Chapter of the ACL (EACL), 2009.
M. Sassano. An empirical study of active learning with sup-
  port vector machines for japanese word segmentation. In


                                                             1561

         Automated planning for situated natural language generation

                           Konstantina Garoufi and Alexander Koller
                  Cluster of Excellence “Multimodal Computing and Interaction”
                            Saarland University, Saarbrücken, Germany
                      {garoufi,koller}@mmci.uni-saarland.de



                      Abstract                                instruction follower (IF), which is adapted from
    We present a natural language genera-                     the SCARE corpus (Stoia et al., 2008):
    tion approach which models, exploits, and                  (1) IG: Walk forward and then turn right.
    manipulates the non-linguistic context in                      IF: (walks and turns)
    situated communication, using techniques                       IG: OK. Now hit the button in the middle.
    from AI planning. We show how to gen-
    erate instructions which deliberately guide                  In this example, the IG plans to refer to an ob-
    the hearer to a location that is convenient               ject (here, a button); and in order to do so, gives a
    for the generation of simple referring ex-                navigation instruction to guide the IF to a conve-
    pressions, and how to generate referring                  nient location at which she can then use a simple
    expressions with context-dependent adjec-                 referring expression (RE). That is, there is an inter-
    tives. We implement and evaluate our                      action between navigation instructions (intended
    approach in the framework of the Chal-                    to manipulate the non-linguistic context in a cer-
    lenge on Generating Instructions in Vir-                  tain way) and referring expressions (which exploit
    tual Environments, finding that it performs               the non-linguistic context). Although such subdi-
    well even under the constraints of real-                  alogues are common in SCARE, we are not aware
    time generation.                                          of any previous research that can generate them in
                                                              a computationally feasible manner.
1   Introduction
                                                                 This paper presents an approach to generation
The problem of situated natural language gen-                 which is able to model the effect of an utter-
eration (NLG)—i.e., of generating natural lan-                ance on the non-linguistic context, and to inten-
guage in the context of a physical (or virtual)               tionally generate utterances such as the above as
environment—has received increasing attention in              part of a process of referring to objects. Our ap-
the past few years. On the one hand, this is be-              proach builds upon the CRISP generation system
cause it is the foundation of various emerging ap-            (Koller and Stone, 2007), which translates gener-
plications, including human-robot interaction and             ation problems into planning problems and solves
mobile navigation systems, and is the focus of a              these with an AI planner. We extend the CRISP
current evaluation effort, the Challenges on Gener-           planning operators with the perlocutionary effects
ating Instructions in Virtual Environments (GIVE;             that uttering a particular word has on the physi-
(Koller et al., 2010b)). On the other hand, situated          cal environment if it is understood correctly; more
generation comes with interesting theoretical chal-           specifically, on the position and orientation of the
lenges: Compared to the generation of pure text,              hearer. This allows the planner to predict the non-
the interpretation of expressions in situated com-            linguistic context in which a later part of the ut-
munication is sensitive to the non-linguistic con-            terance will be interpreted, and therefore to search
text, and this context can change as easily as the            for contexts that allow the use of simple REs. As a
user can move around in the environment.                      result, the work of referring to an object gets dis-
   One interesting aspect of situated communica-              tributed over multiple utterances of low cognitive
tion from an NLG perspective is that this non-                load rather than a single complex noun phrase.
linguistic context can be manipulated by the                     A second contribution of our paper is the gen-
speaker. Consider the following segment of dis-               eration of REs involving context-dependent adjec-
course between an instruction giver (IG) and an               tives: A button can be described as “the left blue


                                                        1573
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1573–1582,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


button” even if there is a red button to its left. We   tion and referring (Stoia et al., 2006). In this ma-
model adjectives whose interpretation depends on        chine learning approach, Stoia et al. train classi-
the nominal phrases they modify, as well as on the      fiers that signal when the context conditions (e.g.
non-linguistic context, by keeping track of the dis-    visibility of target and distractors) are appropriate
tractors that remain after uttering a series of mod-    for the generation of an RE. This method can be
ifiers. Thus, unlike most other RE generation ap-       then used as part of a content selection component
proaches, we are not restricted to building an RE       of an NLG system. Such a component, however,
by simply intersecting lexically specified sets rep-    can only inform a system on whether to choose
resenting the extensions of different attributes, but   navigation over RE generation at a given point of
can correctly generate expressions whose mean-          the discourse, and is not able to help it decide
ing depends on the context in a number of ways.         what kind of navigational instructions to generate
In this way we are able to refer to objects earlier     so that subsequent REs become simple.
and more flexibly.
                                                           To our knowledge, the only previous research
   We implement and evaluate our approach in
                                                        on generating REs with context-dependent modi-
the context of a GIVE NLG system, by using
                                                        fiers is van Deemter’s (2006) algorithm for gener-
the GIVE-1 software infrastructure and a GIVE-1
                                                        ating vague adjectives. Unlike van Deemter, we
evaluation world. This shows that our system gen-
                                                        integrate the RE generation process tightly with
erates an instruction-giving discourse as in (1) in
                                                        the syntactic realization, which allows us to gen-
about a second. It outperforms a mostly non-
                                                        erate REs with more than one context-dependent
situated baseline significantly, and compares well
                                                        modifier and model the effect of their linear or-
against a second baseline based on one of the
                                                        der on the meaning of the phrase. In modeling
top-performing systems of the GIVE-1 Challenge.
                                                        the context, we focus on the non-linguistic con-
Next to the practical usefulness this evaluation es-
                                                        text and the influence of each of the RE’s words;
tablishes, we argue that our approach to jointly
                                                        this is in contrast to previous research on context-
modeling the grammatical and physical effects of
                                                        sensitive generation of REs, which mainly focused
a communicative action can also inform new mod-
                                                        on the discourse context (Krahmer and Theune,
els of the pragmatics of speech acts.
                                                        2002). Our interpretation of context-dependent
Plan of the paper. We discuss related work in           modifiers picks up ideas by Kamp and Partee
Section 2, and review the CRISP system, on which        (1995) and implements them in a practical system,
our work is based, in Section 3. We then show           while our method of ordering modifiers is linguis-
in Section 4 how we extend CRISP to generate            tically informed by the class-based paradigm (e.g.,
navigation-and-reference discourses as in (1), and      Mitchell (2009)).
add context-dependent adjectives in Section 5. We          On the other hand, our work also stands in a tra-
evaluate our system in Section 6; Section 7 con-        dition of NLG research that is based on AI plan-
cludes and points to future work.                       ning. Early approaches (Perrault and Allen, 1980;
                                                        Appelt, 1985) provided compelling intuitions for
2   Related work
                                                        this connection, but were not computationally vi-
The research reported here can be seen in the           able. The research we report here can be seen
wider context of approaches to generating refer-        as combining Appelt’s idea of using planning for
ring expressions. Since the foundational work of        sentence-level NLG with a computationally be-
Dale and Reiter (1995), there has been a consider-      nign variant of Perrault et al.’s approach of model-
able amount of literature on this topic. Our work       ing the intended perlocutionary effects of a speech
departs from the mainstream in two ways. First, it      act as the effects of a planning operator. Our work
exploits the situated communicative setting to de-      is linked to a growing body of very recent work
liberately modify the context in which an RE is         that applies modern planning research to various
generated. Second, unlike most other RE genera-         problems in NLG (Steedman and Petrick, 2007;
tion systems, we allow the contribution of a modi-      Brenner and Kruijff-Korbayová, 2008; Benotti,
fier to an RE to depend both on the context and on      2009). It is directly based on Koller and Stone’s
the rest of the RE.                                     (2007) reimplementation of the SPUD generator
   We are aware of only one earlier study on gen-       (Stone et al., 2003) with planning. As far as we
eration of REs with focus on interleaving naviga-       know, ours is the first system in the SPUD tradi-


                                                    1574


       (a)         S:self                                                                       (b)       S:e
                                                               NP:self
       NP:subj ↓                     VP:self                                                     NP:j ↓
                                                                      N:self                                          VP:e
                                                       the
                            V:self        NP:obj ↓                                                                           NP:b1 ↓
                                                                      button                                    V:e
                            pushes                                                               NP:j
                                                                                                                pushes
                                                                                                                              NP:b1
        semcontent: {push(self,subj,obj)}            semcontent: {button(self)}                  John
                                                                                                                                   N:b1
                                                               NP:self                                                the
                            N:self
                                                                                                                  N:b1
              red                    N*                        John                                                                button
             semcontent: {red(self)}                 semcontent: {John(self)}                               red               N*


      Figure 1: (a) An example grammar; (b) a derivation of “John pushes the red button” using (a).


tion that explicitly models the context change ef-                             combine here for simplicity the entries for “the”
fects of an utterance.                                                         and “button” into “the button”.
   While nothing in our work directly hinges on                                    For generation, we assume as input a knowl-
this, we implemented our approach in the context                               edge base and a communicative goal in addition to
of an NLG system for the GIVE Challenge (Koller                                the grammar. The goal is to compute a derivation
et al., 2010b), that is, as an instruction giving sys-                         that expresses the communicative goal in a sen-
tem for virtual worlds. This makes our system                                  tence that is grammatically correct and complete;
comparable with other approaches to instruction                                whose meaning is justified by the knowledge base;
giving implemented in the GIVE framework.                                      and in which all REs can be resolved to unique
                                                                               individuals in the world by the hearer. Let’s say,
3     Sentence generation as planning                                          for example, that we have a knowledge base
                                                                               {push(e, j, b1 ), John(j), button(b1 ), button(b2 ),
Our work is based on the CRISP system (Koller
                                                                               red(b1 )}. Then we can combine instances of the
and Stone, 2007), which encodes sentence gener-
                                                                               trees for “John”, “pushes”, and “the button” into
ation with tree-adjoining grammars (TAG; (Joshi
                                                                               a grammatically complete derivation. However,
and Schabes, 1997)) as an AI planning problem
                                                                               because both b1 and b2 satisfy the semantic
and solves that using efficient planners. It then
                                                                               content of “the button”, we must adjoin “red” into
decodes the resulting plan into a TAG derivation,
                                                                               the derivation to make the RE refer uniquely to
from which it can read off a sentence. In this sec-
                                                                               b1 . The complete derivation is shown in Fig. 1b;
tion, we briefly recall how this works. For space
                                                                               we can read off the output sentence “John pushes
reasons, we will present primarily examples in-
                                                                               the red button” from the leaves of the derived tree
stead of definitions.
                                                                               we build in this way.
3.1    TAG sentence generation
                                                                               3.2   TAG generation as planning
The CRISP generation problem (like that of SPUD
(Stone et al., 2003)) assumes a lexicon of entries                             In the CRISP system, Koller and Stone (2007)
consisting of a TAG elementary tree annotated                                  show how this generation problem can be solved
with semantic and pragmatic information. An ex-                                by converting it into a planning problem (Nau et
ample is shown in Fig. 1a. In addition to the el-                              al., 2004). The basic idea is to encode the partial
ementary tree, each lexicon entry specifies its se-                            derivation in the planning state, and to encode the
mantic content and possibly a semantic require-                                action of adding each elementary tree in the plan-
ment, which can express certain presuppositions                                ning operators. The encoding of our example as a
triggered by this entry. The nodes in the tree may                             planning problem is shown in Fig. 2.
be labeled with argument names such as semantic                                   In the example, we start with an initial state
roles, which specify the participants in the rela-                             which contains the entire knowledge base, plus
tion expressed by the lexicon entry; in the exam-                              atoms subst(S, root) and ref(root, e) expressing
ple, every entry uses the semantic role self repre-                            that we want to generate a sentence about the event
senting the event or individual itself, and the en-                            e. We can then apply the (instantiated) action
try for “pushes” furthermore uses subj and obj for                             pushes(root, n1 , n2 , n3 , e, j, b1 ), which models the
the subject and object argument, respectively. We                              act of substituting the elementary tree for “pushes”


                                                                       1575


pushes(u, u1 , u2 , un , x, x1 , x2 ):
  Precond: subst(S, u), ref(u, x), push(x, x1 , x2 ),
            current(u1 ), next(u1 , u2 ), next(u2 , un )                              f1       b2       b3
  Effect: ¬subst(S, u), subst(NP, u1 ), subst(NP, u2 ),
                                                                          4
          ref(u1 , x1 ), ref(u2 , x2 ), ∀y.distractor(u1 , y),
          ∀y.distractor(u2 , y)
                                                                          3
John(u, x):
  Precond: subst(NP, u), ref(u, x), John(x)                                      b1                              north
  Effect: ¬subst(NP, u), ∀y.¬John(y) → ¬distractor(u, y)                  2
the-button(u, x):
  Precond: subst(NP, u), ref(u, x), button(x)
  Effect: ¬subst(NP, u), canadjoin(N, u),                                 1      1         2        3        4
          ∀y.¬button(y) → ¬distractor(u, y)

red(u, x):                                                          Figure 3: An example map for instruction giving.
  Precond: canadjoin(N, u), ref(u, x), red(x)
  Effect: ∀y.¬red(y) → ¬distractor(u, y)
                                                                    3.3       Decoding the plan
Figure 2: CRISP planning operators for the ele-                     An AI planner such as FF (Hoffmann and Nebel,
mentary trees in Fig. 1.                                            2001) can compute a plan for a planning problem
                                                                    that consists of the planning operators in Fig. 2
into the substitution node root: It can only be                     and a specification of the initial state and the goal.
applied because root is an unfilled substitution                    We can then decode this plan into the TAG deriva-
node (precondition subst(S, root)), and its effect                  tion shown in Fig. 1b. The basic idea of this
is to remove subst(S, root) from the planning state                 decoding step is that an action with a precondi-
while adding two new atoms subst(NP, n1 ) and                       tion subst(A, u) fills the substitution node u, while
subst(NP, n2 ) for the substitution nodes of the                    an action with a precondition canadjoin(A, u) ad-
“pushes” tree. The planning state maintains in-                     joins into a node of category A in the elementary
formation about which individual each node refers                   tree that was substituted into u. CRISP allows
to in the ref atoms. The current and next atoms                     multiple trees to adjoin into the same node. In this
are needed to select unused names for newly in-                     case, the decoder executes the adjunctions in the
troduced syntax nodes.1 Finally, the action in-                     order in which they occur in the plan.
troduces a number of distractor atoms including
                                                                    4     Context manipulation
distractor(n2 , e) and distractor(n2 , b2 ), express-
ing that the RE at n2 can still be misunderstood                    We are now ready to describe our NLG ap-
by the hearer as e or b2 .                                          proach, SCRISP (“Situated CRISP”), which ex-
   In this new state, all subst and distractor                      tends CRISP to take the non-linguistic context of
atoms for n1 can be eliminated with the ac-                         the generated utterance into account, and deliber-
tion John(n1 , j). We can also apply the action                     ately manipulate it to simplify RE generation.
the-button(n2 , b1 ) to eliminate subst(NP, n2 )                       As a simplified version of our introductory in-
and distractor(n2 , e), since e is not a button.                    struction giving example (1), consider the map in
However distractor(n2 , b2 ) remains. Now be-                       Fig. 3. The instruction follower (IF), who is lo-
cause the action the-button also introduced the                     cated on the map at position pos3,2 facing north,
atom canadjoin(N, n2 ), we can remove the fi-                       sees the scene from the first-person perspective as
nal distractor atom by applying red(n2 , b1 ).                      in Fig. 7. Now an instruction giver (IG) could in-
This brings us into a goal state, and we                            struct the IF to press the button b1 in this scene by
are done.       Goal states in CRISP planning                       saying “push the button on the wall to your left”.
problems are characterized by axioms such as                        Interpreting this instruction is difficult for the IF
∀A∀u.¬subst(A, u) (encoding grammatical com-                        because it requires her to either memorize the RE
pleteness) and ∀u∀x.¬distractor(u, x) (requiring                    until she has turned to see the button, or to per-
unique reference).                                                  form a mental rotation task to visualize b1 inter-
   1
                                                                    nally. Alternatively, the IG can first instruct the
    This is a different solution to the name-selection problem
than in Koller and Stone (2007). It is simpler and improves         IF to “turn left”; once the IF has done this, the IG
computational efficiency.                                           can then simply say “now push the button in front


                                                                 1576


             S:self             semreq: visible(p, o, obj)               turnleft(u, x, o1 , o2 ):
                                nonlingcon: player–pos(p),                 Precond: subst(S, u), ref(u, x), player–ori(o1 ),
      V:self     NP:obj ↓                    player–ori(o)                            next–ori–left(o1 , o2 ), . . .
                                impeff: push(obj)                          Effect: ¬subst(S, u), ¬player–ori(o1 ), player–ori(o2 ),
      push                                                                         to–do(turnleft), . . .

             S:self             nonlingcon: player–ori(o1 ),             push(u, u1 , un , x, x1 , p, o):
                                            next–ori–left(o1 , o2 )        Precond: subst(S, u), ref(u, x), player–pos(p),
      V:self      Adv           nonlingeff: ¬player–ori(o1 ),                         player–ori(o), visible(p, o, x1 ), . . .
                                            player–ori(o2 )                Effect: ¬subst(S, u), subst(NP, u1 ), ref(u1 , x1 ),
      turn        left          impeff: turnleft                                   ∀y.(y 6= x1 ∧ visible(p, o, y) → distractor(u1 , y)),
                                                                                   to–do(push(x1 )), canadjoin(S, u), . . .
                      S:self
                                                                         and(u, u1 , un , e1 , e2 ):
                                                                           Precond: canadjoin(S, u), ref(u, e1 ), . . .
  S:self *      and      S:other ↓
                                                                           Effect: subst(S, u1 ), ref(u1 , e2 ), . . .
        Figure 4: An example SCRISP lexicon.
                                                                         Figure 5: SCRISP planning operators for the lexi-
of you”. This lowers the cognitive load on the IF,                       con in Fig. 4.
and presumably improves the rate of correctly in-
terpreted REs.                                                           only refer to objects that are currently visible.
   SCRISP is capable of deliberately generat-                            Similarly, “turn left” puts turning left on the IF’s
ing such context-changing navigation instructions.                       agenda. In addition, the lexicon entry for “turn
The key idea of our approach is to extend the                            left” specifies that, under the assumption that the
CRISP planning operators with preconditions and                          IF understands and follows the instruction, they
effects that describe the (simulated) physical envi-                     will turn 90 degrees to the left after hearing it. The
ronment: A “turn left” action, for example, mod-                         planning operators are written in a way that as-
ifies the IF’s orientation in space and changes the                      sumes that the intended (perlocutionary) effects of
set of visible objects; a “push” operator can then                       an utterance actually come true. This assumption
pick up this changed set and restrict the distractors                    is crucial in connecting the non-linguistic effects
of the forthcoming RE it introduces (i.e. “the but-                      of one SCRISP action to the non-linguistic pre-
ton”) to only objects that are visible in the changed                    conditions of another, and generalizes to a scalable
context. We also extend CRISP to generate imper-                         model of planning perlocutionary acts. We discuss
ative rather than declarative sentences.                                 this in more detail in Koller et al. (2010a).
                                                                            We then translate a SCRISP generation prob-
4.1      Situated CRISP                                                  lem into a planning problem. In addition to what
We define a lexicon for SCRISP to be a CRISP                             CRISP does, we translate all non-linguistic condi-
lexicon in which every lexicon entry may also de-                        tions into preconditions and all non-linguistic ef-
scribe non-linguistic conditions, non-linguistic ef-                     fects into effects of the planning operator, adding
fects and imperative effects. Each of these is a                         any free variables to the operator’s parameters.
set of atoms over constants, semantic roles, and                         An imperative effect P is translated into an ef-
possibly some free variables. Non-linguistic con-                        fect to–do(P ). The operators for the example lex-
ditions specify what must be true in the world                           icon of Fig. 4 are shown in Fig. 5. Finally, we
so a particular instance of a lexicon entry can be                       add information about the situated environment to
uttered felicitously; non-linguistic effects specify                     the initial state, and specify the planning goal by
what changes uttering the word brings about in the                       adding to–do(P ) atoms for each atom P that is to
world; and imperative effects contribute to the IF’s                     be placed on the IF’s agenda.
“to-do list” (Portner, 2007) by adding the proper-
ties they denote.                                                        4.2    An example
   A small lexicon for our example is shown in                           Now let’s look at how this generates the appropri-
Fig. 4. This lexicon specifies that saying “push                         ate instructions for our example scene of Fig. 3.
X” puts pushing X on the IF’s to-do list, and car-                       We encode the state of the world as depicted
ries the presupposition that X must be visible from                      in the map in an initial state which contains,
the location where “push X” is uttered; this re-                         among others, the atoms player–pos(pos3,2 ),
flects our simplifying assumption that the IG can                        player–ori(north),     next–ori–left(north, west),


                                                                      1577


visible(pos3,2 , west, b1 ), etc.2 We want the IF to              left(u, x):
                                                                     Precond: ∀y.¬(distractor(u, y) ∧ left–of(y, x)),
press b1 , so we add to–do(push(b1 )) to the goal.                              canadjoin(N, u), ref(u, x)
   We can start by applying the action                               Effect: ∀y.(left–of(x, y) → ¬distractor(u, y)),
                                                                              premod–index(u, 2), . . .
turnleft(root, e, north, west) to the initial
state. Next to the ordinary grammatical effects                   red(u, x):
from CRISP, this action makes player–ori(west)                      Precond: red(x), canadjoin(N, u), ref(u, x),
                                                                               ¬premod–index(u, 2)
true. The new state does not contain any subst                      Effect: ∀y.(¬red(y) → ¬distractor(u, y)),
atoms, but we can continue the sentence by                                   premod–index(u, 1), . . .
adjoining “and”, i.e. by applying the action
and(root, n1 , n2 , e, e1 ). This produces a new                  Figure 6:    SCRISP operators for context-
atom subst(S, e1 ), which satisfies one precon-                   dependent and context-independent adjectives.
dition of push(n1 , n2 , n3 , e1 , b1 , pos3,2 , west).
Because turnleft changed the player orientation,
the visible precondition of push is now satisfied                 depend on the meaning of the phrase they modify:
too (unlike in the initial state, in which b1 was not             “the left button” is not necessarily both a button
visible). Applying the action push now introduces                 and further to the left than all other objects, it is
the need to substitute a noun phrase for the object,              only the leftmost object among the buttons.
which we can eliminate with an application of                        We will now show how to extend SCRISP so it
the-button(n2 , b1 ) as in Subsection 3.2.                        can generate REs that use such context-dependent
   Since there are no other visible buttons from                  adjectives.
pos3,2 facing west, there are no remaining
                                                                  5.1   Context-dependence of adjectives in
distractor atoms at this point, and a goal state
                                                                        SCRISP
has been reached. Together, this four-step plan
decodes into the sentence “turn left and push                     As a planning-based approach to NLG, SCRISP
the button”. The final state contains the atoms                   is not limited to simply intersecting sets of po-
to–do(push(b1 )) and to–do(turnleft), indicating                  tential referents that only depend on the attributes
that an IF that understands and accepts this in-                  that contribute to an RE: Distractors are removed
struction also accepts these two commitments into                 by applying operators which may have context-
their to-do list.                                                 sensitive conditions depending on the referent and
                                                                  the distractors that are still left.
5    Generating context-dependent                                    Our encoding of context-dependent adjectives
     adjectives                                                   as planning operators is shown in Fig. 6. We only
                                                                  show the operators here for lack of space; they can
Now consider if we wanted to instruct the IF to
                                                                  of course be computed automatically from lexicon
press b2 in Fig. 3 instead of b1 , say with the
                                                                  entries. In addition to the ordinary CRISP precon-
instruction “push the left button”. This is still
                                                                  ditions, the left operator has a precondition requir-
challenging, because (like most other approaches
                                                                  ing that no current distractor for the RE u is to the
to RE generation) CRISP interprets adjectives by
                                                                  left of x, capturing a presupposition of the adjec-
simply intersecting all their extensions. In the case
                                                                  tive. Its effect is that everything that is to the right
of “left”, the most reasonable way to do this would
                                                                  of x is no longer a distractor for u. Notice that we
be to interpret it as “leftmost among all visible ob-
                                                                  allow that there may still be distractors after left
jects”; but this is f1 in the example, and so there is
                                                                  has been applied (above or below x); we only re-
no distinguishing RE for b2 .
                                                                  quire unique reference in the goal state. (Ignore
   In truth, spatial adjectives like “left” and “up-
                                                                  the premod–index part of the effect for now; we
per” depend on the context in two different ways.
                                                                  will get to that in a moment.)
On the one hand, they are interpreted with respect
                                                                     Let’s say that we are computing a plan for re-
to the current spatio-visual context, in that what is
                                                                  ferring to b2 in the example map of Fig. 3, starting
on the left depends on the current position and ori-
                                                                  with push(root, n1 , n2 , e, b2 , pos3,1 , north) and
entation of the hearer. On the other hand, they also
                                                                  the-button(n1 , b2 ). The state after these two ac-
   2
     In a more complex situation, it may be infeasible to ex-     tions is not a goal state, because it still contains
haustively model visibility in this way. This could be fixed by
connecting the planner to an external spatial reasoner (Dorn-     the atom distractor(n1 , b3 ) (the plant f1 was re-
hege et al., 2009).                                               moved as a distractor by the action the-button).


                                                              1578


Now assume that we have modeled the spatial
relations between all objects in the initial state
in left–of and above atoms; in particular, we
have left–of(b2 , b3 ). Then the action instance
left(n1 , b2 ) is applicable in this state, as there is
no other object that is still a distractor in this state
and that is to the left of b2 . Applying left removes
distractor(n1 , b3 ) from the state. Thus we have
reached a goal state; the complete plan decodes to
the sentence “push the left button”.
   This system is sensitive to the order in which
operators for context-dependent adjectives are ap-
plied. To generate the RE “the upper left but-             Figure 7: The IF’s view of the scene in Fig. 3, as
ton”, for instance, we first apply the left action and     rendered by the GIVE client.
then the upper action, and therefore upper only
needs to remove distractors in the leftmost posi-
tion. On the other hand, the RE “the left upper            tion of the target referent (cf. “upper left” vs. “left
button” corresponds to first applying upper and            upper” example above). However, there are also
then left. These action sequences succeed in re-           other constraints at work: “? the red left button” is
moving all distractors for different context states,       rather odd even when it is a semantically correct
which is consistent with the difference in meaning         description, whereas “the left red button” is fine.
between the two REs.                                          To ensure that SCRISP chooses to generate
   Furthermore, notice that the adjective operators        these adjectives correctly, we follow a class-based
themselves do not interact directly with the en-           approach to the premodifier ordering problem
coding of the context in atoms like visible and            (Mitchell, 2009). In our lexicon we assign adjec-
player–pos, just like the noun operators in Sec-           tives denoting spatial relations (“left”) to one class
tion 4 didn’t. The REs to which the adjectives and         and adjectives denoting color (“red”) to another;
nouns contribute are introduced by verb operators;         then we require that spatial adjectives must always
it is these verb operators that inspect the current        precede color adjectives. We enforce this by keep-
context and initialize the distractor set for the new      ing track of the current premodifier index of the RE
RE appropriately. This makes the correctness of            in atoms of the form premod–index. Any newly
the generated sentence independent of the order in         generated RE node starts off with a premodifier
which noun and adjective operators occur in the            index of zero; adjoining an adjective of a certain
plan. We only need to ensure that the verbs are            class then raises this number to the index for that
ordered correctly, and the workload of modeling            class. As the operators in Fig. 6 illustrate, color
interactions with the non-linguistic context is lim-       adjectives such as “red” have index one and can
ited to a single place in the encoding.                    only be used while the index is not higher; once
                                                           an adjective from a higher class (such as “left”, of
5.2   Adjective word order                                 a class with index two) is used, the premod–index
                                                           precondition of the “red” operator will fail. For
One final challenge that arises in our system is to        this reason, we can generate a plan for “the left
generate the adjectives in the correct order, which        red button”, but not for “? the red left button”, as
on top of semantically valid must be linguisti-            desired.
cally acceptable. In particular, it is known that
some types of adjectives are limited with respect          6   Evaluation
to the word order in which they can occur in a
noun phrase. For instance, “large foreign finan-           To establish the quality of the generated instruc-
cial firms” sounds perfectly acceptable, but “? for-       tions, we implemented SCRISP as part of a gener-
eign large financial firms” sounds odd (Shaw and           ation system in the GIVE-1 framework, and eval-
Hatzivassiloglou, 1999). In our setting, some ad-          uated it against two baselines. GIVE-1 was the
jective orders are forbidden because only one or-          First Challenge on Generating Instructions in Vir-
der produces a correct and distinguishing descrip-         tual Environments, which was completed in 2009


                                                       1579


                   1. Turn right and move one step.                               success                RE
      SCRISP
                   2. Push the right red button.                              rate      time   success     distance
                   1. Press the right red button on the          SCRISP       69%       306     71%          2.49
      Baseline A
                   wall to your right.                           Baseline A   16%** 230         49%**        1.97*
                   1. Turn right.                                Baseline B   84%       288     81%*         2.00*
                   2. Walk forward 3 steps.
                   3. Turn right.
      Baseline B                                             Table 2: Evaluation results. Differences to
                   4. Walk forward 1 step.
                   5. Turn left.                             SCRISP are significant at *p < .05, **p < .005
                   6. Good! Now press the left button.       (Pearson’s chi-square test for system success rates;
                                                             unpaired two-sample t-test for the rest).
Table 1: Example system instructions generated in
the same scene. REs for the target are typeset in
boldface.                                                    nicative goals for SCRISP. Then, for each of the
                                                             communicative goals, it generates instructions us-
                                                             ing SCRISP, segments them into navigation and
(Koller et al., 2010b). In this challenge, sys-
                                                             action parts, and presents these to the user as sep-
tems must generate real-time instructions that help
                                                             arate instructions sequentially (see Table 1).
users perform a task in a treasure-hunt virtual en-
                                                                For each instruction, SCRISP thus draws from
vironment such as the one shown in Fig. 7.
                                                             a knowledge base of about 1500 facts and a gram-
   We conducted our evaluation in World 2 from
                                                             mar of about 30 lexicon entries. We use the
GIVE-1, which was deliberately designed to be
                                                             FF planner (Hoffmann and Nebel, 2001; Koller
challenging for RE generation. The world con-
                                                             and Hoffmann, 2010) to solve the planning prob-
sists of one room filled with several objects and
                                                             lems. The maximum planning time for any in-
buttons, most of which cannot be distinguished by
                                                             struction is 1.03 seconds on a 3.06 GHz Intel Core
simple descriptions. Moreover, some of those may
                                                             2 Duo CPU. So although our planning-based sys-
activate an alarm and cause the player to lose the
                                                             tem tackles a very difficult search problem, FF is
game. The player’s moves and turns are discrete
                                                             very good at solving it—fast enough to generate
and the NLG system has complete and accurate
                                                             instructions in real time.
real-time information about the state of the world.
Instructions that each of the three systems under            6.2   Comparison with Baseline A
comparison generated in an example scene of the
evaluation world are presented in Table 1.                   Baseline A is a very basic system designed to sim-
   The evaluation took place online via the Ama-             ulate the performance of a classical RE genera-
zon Mechanical Turk, where we collected 25                   tion module which does not attempt to manipu-
games for each system. We focus on four mea-                 late the visual context. We hand-coded a correct
sures of evaluation: success rates for solving the           distinguishing RE for each target button in the
task and resolving the generated REs, average                world; the only way in which Baseline A reacts
task completion time (in seconds) for successful             to changes of the context is to describe on which
games, and average distance (in steps) between the           wall the button is with respect to the user’s current
IF and the referent at the time when the RE was              orientation (e.g. “Press the right red button on the
generated. As in the challenge, the task is consid-          wall to your right”).
ered as solved if the player has correctly been led             As Table 2 shows, our system guided 69% of
through manipulating all target objects required to          users to complete the task successfully, compared
discover and collect the treasure; in World 2, the           to only 16% for Baseline A (difference is statis-
minimum number of such targets is eight. An RE               tically significant at p < .005; Pearson’s chi-
is successfully resolved if it results in the manipu-        square test). This is primarily because only 49%
lation of the referent, whereas manipulation of an           of the REs generated by Baseline A were success-
alarm-triggering distractor ends the game unsuc-             ful. This comparison illustrates the importance of
cessfully.                                                   REs that minimize the cognitive load on the IF to
                                                             avoid misunderstandings.
6.1     The SCRISP system
                                                             6.3   Comparison with Baseline B
Our system receives as input a plan for what the
IF should do to solve the task, and successively             Baseline B is a corrected and improved version
takes object-manipulating actions as the commu-              of the “Austin” system (Chen and Karpov, 2009),


                                                          1580


one of the best-performing systems of the GIVE-1         topic for future work, for instance, is to expand our
Challenge. Baseline B, like the original “Austin”        notion of context by taking visual and discourse
system, issues navigation instructions by precom-        salience into account when generating REs. In ad-
puting the shortest path from the IF’s current lo-       dition, we plan to experiment with assigning costs
cation to the target, and generates REs using the        to planning operators in a metric planning problem
description logic based algorithm of Areces et al.       (Hoffmann, 2002) in order to model the cognitive
(2008). Unlike the original system, which inflex-        cost of an RE (Krahmer et al., 2003) and compute
ibly navigates the user all the way to the target,       minimal-cost instruction sequences.
Baseline B starts off with navigation, and oppor-           On a more theoretical level, the SCRISP actions
tunistically instructs the IF to push a button once it   model the physical effects of a correctly under-
has become visible and can be described by a dis-        stood and grounded instruction directly as effects
tinguishing RE. We fixed bugs in the original im-        of the planning operator. This is computationally
plementation of the RE generation module, so that        much less complex than classical speech act plan-
Baseline B generates only unambiguous REs. The           ning (Perrault and Allen, 1980), in which the in-
module nonetheless naively treats all adjectives as      tended physical effect comes at the end of a long
intersective and is not sensitive to the context of      chain of inferences. But our approach is also very
their comparison set. Specifically, a button can-        optimistic in estimating the perlocutionary effects
not be referred to as “the right red button” if it is    of an instruction, and must be complemented by an
not the rightmost of all visible objects—which ex-       appropriate model of execution monitoring. What
plains the long chain of navigational instructions       this means for a novel scalable approach to the
the system produced in Table 1.                          pragmatics of speech acts (Koller et al., 2010a)
   We did not find any significant differences in        is, we believe, an interesting avenue for future re-
the success rates or task completion times between       search.
this system and SCRISP, but the former achieved
a higher RE success rate (see Table 2). However,         Acknowledgments. We are grateful to Jörg
a closer analysis shows that SCRISP was able to          Hoffmann for improving the efficiency of FF in the
generate REs from significantly further away. This       SCRISP domain at a crucial time, and to Margaret
means that SCRISP’s RE generator solves a harder         Mitchell, Matthew Stone and Kees van Deemter
problem, as it typically has to deal with more vis-      for helping us expand our view of the context-
ible distractors. Furthermore, because of the in-        dependent adjective generation problem. We also
creased distance, the system’s execution monitor-        thank Ines Rehbein and Josef Ruppenhofer for
ing strategies (e.g. for detection and repair of mis-    testing early implementations of our system, and
understandings) become increasingly important,           Andrew Gargett as well as the reviewers for their
and this was not a focus of this work. In summary,       helpful comments.
then, we take the results to mean that SCRISP per-
forms quite capably in comparison to a top-ranked
                                                         References
GIVE-1 system.
                                                         Douglas E. Appelt. 1985. Planning English sentences.
7   Conclusion                                             Cambridge University Press, Cambridge, England.

In this paper, we have shown how situated instruc-       Carlos Areces, Alexander Koller, and Kristina Strieg-
                                                           nitz. 2008. Referring expressions as formulas of
tions can be generated using AI planning. We ex-           description logic. In Proceedings of the 5th Inter-
ploited the planner’s ability to model the perlocu-        national Natural Language Generation Conference,
tionary effects of communicative actions for effi-         pages 42–49, Salt Fork, Ohio, USA.
cient generation. We showed how this made it pos-
                                                         Luciana Benotti. 2009. Clarification potential of in-
sible to generate instructions that manipulate the         structions. In Proceedings of the SIGDIAL 2009
non-linguistic context in convenient ways, and to          Conference, pages 196–205, London, UK.
generate correct REs with context-dependent ad-
jectives.                                                Michael Brenner and Ivana Kruijff-Korbayová. 2008.
                                                           A continual multiagent planning approach to situ-
   We believe that this illustrates the power of           ated dialogue. In Proceedings of the 12th Workshop
a planning-based approach to NLG to flexibly               on the Semantics and Pragmatics of Dialogue, Lon-
model very different phenomena. An interesting             don, UK.


                                                     1581


David Chen and Igor Karpov.      2009.     The                editors, Information Sharing: Reference and Pre-
  GIVE-1 Austin system.        In The First                   supposition in Language Generation and Interpre-
  GIVE   Challenge:     System    descriptions.               tation, pages 223–264. CSLI Publications.
  http://www.give-challenge.org/
  research/files/GIVE-09-Austin.pdf.                       Emiel Krahmer, Sebastiaan van Erk, and André Verleg.
                                                             2003. Graph-based generation of referring expres-
Robert Dale and Ehud Reiter. 1995. Computational             sions. Computational Linguistics, 29(1):53–72.
  interpretations of the Gricean maxims in the genera-
  tion of referring expressions. Cognitive Science, 19.    Margaret Mitchell. 2009. Class-based ordering of
                                                            prenominal modifiers. In Proceedings of the 12th
Christian Dornhege, Patrick Eyerich, Thomas Keller,         European Workshop on Natural Language Genera-
  Sebastian Trüg, Michael Brenner, and Bernhard            tion, pages 50–57, Athens, Greece.
  Nebel. 2009. Semantic attachments for domain-
  independent planning systems. In Proceedings of          Dana Nau, Malik Ghallab, and Paolo Traverso. 2004.
  the 19th International Conference on Automated             Automated Planning: Theory and Practice. Morgan
  Planning and Scheduling, pages 114–121.                    Kaufmann.

Jörg Hoffmann and Bernhard Nebel. 2001. The               C. Raymond Perrault and James F. Allen. 1980. A
    FF planning system: Fast plan generation through         plan-based analysis of indirect speech acts. Amer-
    heuristic search. Journal of Artificial Intelligence     ican Journal of Computational Linguistics, 6(3–
    Research, 14:253–302.                                    4):167–182.
                                                           Paul Portner. 2007. Imperatives and modals. Natural
Jörg Hoffmann. 2002. Extending FF to numerical state
                                                             Language Semantics, 15(4):351–383.
    variables. In Proceedings of the 15th European Con-
    ference on Artificial Intelligence, Lyon, France.      James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
                                                             dering among premodifiers. In Proceedings of the
Aravind K. Joshi and Yves Schabes. 1997. Tree-               37th Annual Meeting of the Association for Compu-
  Adjoining Grammars. In G. Rozenberg and A. Salo-           tational Linguistics, pages 135–143, College Park,
  maa, editors, Handbook of Formal Languages, vol-           Maryland, USA.
  ume 3, pages 69–123. Springer-Verlag, Berlin, Ger-
  many.                                                    Mark Steedman and Ronald P. A. Petrick. 2007. Plan-
                                                            ning dialog actions. In Proceedings of the 8th SIG-
Hans Kamp and Barbara Partee. 1995. Prototype the-          dial Workshop on Discourse and Dialogue, pages
  ory and compositionality. Cognition, 57(2):129 –          265–272, Antwerp, Belgium.
  191.
                                                           Laura Stoia, Donna K. Byron, Darla Magdalene Shock-
Alexander Koller and Jörg Hoffmann. 2010. Waking            ley, and Eric Fosler-Lussier. 2006. Sentence
  up a sleeping rabbit: On natural-language sentence         planning for realtime navigational instructions. In
  generation with FF. In Proceedings of the 20th In-         NAACL ’06: Proceedings of the Human Language
  ternational Conference on Automated Planning and           Technology Conference of the NAACL, pages 157–
  Scheduling, Toronto, Canada.                               160, Morristown, NJ, USA.
Alexander Koller and Matthew Stone. 2007. Sentence         Laura Stoia, Darla M. Shockley, Donna K. Byron,
  generation as planning. In Proceedings of the 45th         and Eric Fosler-Lussier. 2008. SCARE: A sit-
  Annual Meeting of the Association of Computational         uated corpus with annotated referring expressions.
  Linguistics, Prague, Czech Republic.                       In Proceedings of the 6th International Conference
                                                             on Language Resources and Evaluation, Marrakech,
Alexander Koller, Andrew Gargett, and Konstantina            Morocco.
  Garoufi. 2010a. A scalable model of planning per-
  locutionary acts. In Proceedings of the 14th Work-       Matthew Stone, Christine Doran, Bonnie Webber, To-
  shop on the Semantics and Pragmatics of Dialogue,         nia Bleam, and Martha Palmer. 2003. Microplan-
  Poznan, Poland.                                           ning with communicative intentions: The SPUD
                                                            system. Computational Intelligence, 19(4):311–
Alexander Koller, Kristina Striegnitz, Donna Byron,         381.
  Justine Cassell, Robert Dale, Johanna Moore, and
  Jon Oberlander. 2010b. The First Challenge on            Kees van Deemter. 2006. Generating referring ex-
  Generating Instructions in Virtual Environments.           pressions that involve gradable properties. Compu-
  In M. Theune and E. Krahmer, editors, Empir-               tational Linguistics, 32(2).
  ical Methods in Natural Language Generation,
  volume 5790 of LNCS, pages 337–361. Springer,
  Berlin/Heidelberg. To appear.

Emiel Krahmer and Mariet Theune. 2002. Effi-
  cient context-sensitive generation of referring ex-
  pressions. In Kees van Deemter and Rodger Kibble,


                                                       1582

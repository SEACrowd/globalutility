                               Beyond NomBank:
              A Study of Implicit Arguments for Nominal Predicates
                                  Matthew Gerber and Joyce Y. Chai
                                    Department of Computer Science
                                       Michigan State University
                                     East Lansing, Michigan, USA
                                 {gerberm2,jchai}@cse.msu.edu


                      Abstract                                producer and arg1 is the produced entity. The sec-
                                                              ond sentence contains an instance of the nominal
    Despite its substantial coverage, Nom-                    predicate shipping that is not associated with argu-
    Bank does not account for all within-                     ments in NomBank (Meyers, 2007).
    sentence arguments and ignores extra-                        From the sentences in Example 1, the reader can
    sentential arguments altogether. These ar-                infer that The two companies refers to the agents
    guments, which we call implicit, are im-                  (arg0 ) of the shipping predicate. The reader can
    portant to semantic processing, and their                 also infer that market pulp, containerboard and
    recovery could potentially benefit many                   white paper refers to the shipped entities (arg1
    NLP applications. We present a study of                   of shipping).1 These extra-sentential arguments
    implicit arguments for a select group of                  have not been annotated for the shipping predi-
    frequent nominal predicates. We show that                 cate and cannot be identified by a system that re-
    implicit arguments are pervasive for these                stricts the argument search space to the sentence
    predicates, adding 65% to the coverage of                 containing the predicate. NomBank also ignores
    NomBank. We demonstrate the feasibil-                     many within-sentence arguments. This is shown
    ity of recovering implicit arguments with                 in the second sentence of Example 1, where The
    a supervised classification model. Our re-                goods can be interpreted as the arg1 of shipping.
    sults and analyses provide a baseline for                 These examples demonstrate the presence of argu-
    future work on this emerging task.                        ments that are not included in NomBank and can-
                                                              not easily be identified by systems trained on the
1   Introduction
                                                              resource. We refer to these arguments as implicit.
Verbal and nominal semantic role labeling (SRL)                  This paper presents our study of implicit ar-
have been studied independently of each other                 guments for nominal predicates. We began our
(Carreras and Màrquez, 2005; Gerber et al., 2009)            study by annotating implicit arguments for a se-
as well as jointly (Surdeanu et al., 2008; Hajič et          lect group of predicates. For these predicates, we
al., 2009). These studies have demonstrated the               found that implicit arguments add 65% to the ex-
maturity of SRL within an evaluation setting that             isting role coverage of NomBank.2 This increase
restricts the argument search space to the sentence           has implications for tasks (e.g., question answer-
containing the predicate of interest. However, as             ing, information extraction, and summarization)
shown by the following example from the Penn                  that benefit from semantic analysis. Using our an-
TreeBank (Marcus et al., 1993), this restriction ex-          notations, we constructed a feature-based model
cludes extra-sentential arguments:                            for automatic implicit argument identification that
                                                              unifies standard verbal and nominal SRL. Our re-
(1) [arg0 The two companies] [pred produce]                   sults indicate a 59% relative (15-point absolute)
    [arg1 market pulp, containerboard and white               gain in F1 over an informed baseline. Our analy-
    paper]. The goods could be manufactured                   ses highlight strengths and weaknesses of the ap-
    closer to customers, saving [pred shipping]               proach, providing insights for future work on this
    costs.                                                    emerging task.
The first sentence in Example 1 includes the Prop-               1
                                                                    In PropBank and NomBank, the interpretation of each
Bank (Kingsbury et al., 2002) analysis of the ver-            role (e.g., arg0 ) is specific to a predicate sense.
                                                                  2
bal predicate produce, where arg0 is the agentive                   Role coverage indicates the percentage of roles filled.



                                                        1583
       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


   In the following section, we review related re-     analysis of naturally occurring coreference pat-
search, which is historically sparse but recently      terns to aid implicit argument identification.
gaining traction. We present our annotation effort        Most recently, Ruppenhofer et al. (2009) con-
in Section 3, and follow with our implicit argu-       ducted SemEval Task 10, “Linking Events and
ment identification model in Section 4. In Section     Their Participants in Discourse”, which evaluated
5, we describe the evaluation setting and present      implicit argument identification systems over a
our experimental results. We analyze these results     common test set. The task organizers annotated
in Section 6 and conclude in Section 7.                implicit arguments across entire passages, result-
                                                       ing in data that cover many distinct predicates,
2   Related work                                       each associated with a small number of annotated
Palmer et al. (1986) made one of the earliest at-      instances. In contrast, our study focused on a se-
tempts to automatically recover extra-sentential       lect group of nominal predicates, each associated
arguments. Their approach used a fine-grained do-      with a large number of annotated instances.
main model to assess the compatibility of candi-
                                                       3     Data annotation and analysis
date arguments and the slots needing to be filled.
   A phenomenon similar to the implicit argu-          3.1    Data annotation
ment has been studied in the context of Japanese       Implicit arguments have not been annotated within
anaphora resolution, where a missing case-marked       the Penn TreeBank, which is the textual and syn-
constituent is viewed as a zero-anaphoric expres-      tactic basis for NomBank. Thus, to facilitate
sion whose antecedent is treated as the implicit ar-   our study, we annotated implicit arguments for
gument of the predicate of interest. This behavior     instances of nominal predicates within the stan-
has been annotated manually by Iida et al. (2007),     dard training, development, and testing sections of
and researchers have applied standard SRL tech-        the TreeBank. We limited our attention to nom-
niques to this corpus, resulting in systems that       inal predicates with unambiguous role sets (i.e.,
are able to identify missing case-marked expres-       senses) that are derived from verbal role sets. We
sions in the surrounding discourse (Imamura et         then ranked this set of predicates using two pieces
al., 2009). Sasano et al. (2004) conducted sim-        of information: (1) the average difference between
ilar work with Japanese indirect anaphora. The         the number of roles expressed in nominal form (in
authors used automatically derived nominal case        NomBank) versus verbal form (in PropBank) and
frames to identify antecedents. However, as noted      (2) the frequency of the nominal form in the cor-
by Iida et al., grammatical cases do not stand in      pus. We assumed that the former gives an indica-
a one-to-one relationship with semantic roles in       tion as to how many implicit roles an instance of
Japanese (the same is true for English).               the nominal predicate might have. The product of
   Fillmore and Baker (2001) provided a detailed       (1) and (2) thus indicates the potential prevalence
case study of implicit arguments (termed null in-      of implicit arguments for a predicate. To focus our
stantiations in that work), but did not provide con-   study, we ranked the predicates in NomBank ac-
crete methods to account for them automatically.       cording to this product and selected the top ten,
Previously, we demonstrated the importance of fil-     shown in Table 1.
tering out nominal predicates that take no local ar-      We annotated implicit arguments document-by-
guments (Gerber et al., 2009); however, this work      document, selecting all singular and plural nouns
did not address the identification of implicit ar-     derived from the predicates in Table 1. For each
guments. Burchardt et al. (2005) suggested ap-         missing argument position of each predicate in-
proaches to implicit argument identification based     stance, we inspected the local discourse for a suit-
on observed coreference patterns; however, the au-     able implicit argument. We limited our attention to
thors did not implement and evaluate such meth-        the current sentence as well as all preceding sen-
ods. We draw insights from all three of these          tences in the document, annotating all mentions of
studies. We show that the identification of im-        an implicit argument within this window.
plicit arguments for nominal predicates leads to          In the remainder of this paper, we will use iargn
fuller semantic interpretations when compared to       to refer to an implicit argument position n. We
traditional SRL methods. Furthermore, motivated        will use argn to refer to an argument provided by
by Burchardt et al., our model uses a quantitative     PropBank or NomBank. We will use p to mark


                                                   1584


                                   Pre-annotation                           Post-annotation
                                              Role average
  Predicate        #      Role coverage (%) Noun Verb           Role coverage (%)     Noun role average
  price           217            42.4          1.7    1.7              55.3                 2.2
  sale            185            24.3          1.2    2.0              42.0                 2.1
  investor        160            35.0          1.1    2.0              54.6                 1.6
  fund            109            8.7           0.4    2.0              21.6                 0.9
  loss            104            33.2          1.3    2.0              46.9                 1.9
  plan            102            30.9          1.2    1.8              49.3                 2.0
  investment      102            15.7          0.5    2.0              33.3                 1.0
  cost            101            26.2          1.1    2.3              47.5                 1.9
  bid             88             26.9          0.8    2.2              72.0                 2.2
  loan            85             22.4          1.1    2.5              41.2                 2.1
  Overall        1,253           28.0          1.1    2.0              46.2                 1.8

Table 1: Predicates targeted for annotation. The second column gives the number of predicate instances
annotated. Pre-annotation numbers only include NomBank annotations, whereas Post-annotation num-
bers include NomBank and implicit argument annotations. Role coverage indicates the percentage of
roles filled. Role average indicates how many roles, on average, are filled for an instance of a predicate’s
noun form or verb form within the TreeBank. Verbal role averages were computed using PropBank.


predicate instances. Below, we give an example          of roles in the predicate’s lexicon entry. Role cov-
annotation for an instance of the investment predi-     erage for the marked predicate in Example 2 is
cate:                                                   0/3 for NomBank-only arguments and 3/3 when
                                                        the annotated implicit arguments are also consid-
(2) [iarg0 Participants] will be able to transfer       ered. Returning to Table 1, the third column gives
    [iarg1 money] to [iarg2 other investment            role coverage percentages for NomBank-only ar-
    funds]. The [p investment] choices are              guments. The sixth column gives role coverage
    limited to [iarg2 a stock fund and a                percentages when both NomBank arguments and
    money-market fund].                                 the annotated implicit arguments are considered.
NomBank does not associate this instance of in-         Overall, the addition of implicit arguments created
vestment with any arguments; however, we were           a 65% relative (18-point absolute) gain in role cov-
able to identify the investor (iarg0 ), the thing in-   erage across the 1,253 predicate instances that we
vested (iarg1 ), and two mentions of the thing in-      annotated.
vested in (iarg2 ).                                        The predicates in Table 1 are typically associ-
  Our data set was also independently annotated         ated with fewer arguments on average than their
by an undergraduate linguistics student. For each       corresponding verbal predicates. When consid-
missing argument position, the student was asked        ering NomBank-only arguments, this difference
to identify the closest acceptable implicit argu-       (compare columns four and five) varies from zero
ment within the current and preceding sentences.        (for price) to a factor of five (for fund). When im-
The argument position was left unfilled if no ac-       plicit arguments are included in the comparison,
ceptable constituent could be found. For a miss-        these differences are reduced and many nominal
ing argument position, the student’s annotation         predicates express approximately the same num-
agreed with our own if both identified the same         ber of arguments on average as their verbal coun-
constituent or both left the position unfilled. Anal-   terparts (compare the fifth and seventh columns).
ysis indicated an agreement of 67% using Cohen’s           In addition to role coverage and average count,
kappa coefficient (Cohen, 1960).                        we examined the location of implicit arguments.
                                                        Figure 1 shows that approximately 56% of the im-
3.2   Annotation analysis                               plicit arguments in our data can be resolved within
Role coverage for a predicate instance is equal to      the sentence containing the predicate. The remain-
the number of filled roles divided by the number        ing implicit arguments require up to forty-six sen-


                                                    1585


                        1                                                   from the surrounding discourse that constituent c
 Implicit arguments
                      0.9                                                   (referring to Mexico) is the thing being invested in
                                                                            (the iarg2 ). When determining whether c is the
                      0.8
                                                                            iarg2 of investment, one can draw evidence from
      resolved


                      0.7                                                   other mentions in c’s coreference chain. Example
                      0.6                                                   3 states that Mexico needs investment. Example
                                                                            4 states that Mexico regulates investment. These
                      0.5
                                                                            propositions, which can be derived via traditional
                      0.4                                                   SRL analyses, should increase our confidence that
                            0   2   4   6   8   10   12   18   28   46
                                                                            c is the iarg2 of investment in Example 5.
                                        Sentences prior
                                                                               Thus, the unit of classification for a candi-
Figure 1: Location of implicit arguments. For                               date constituent c is the three-tuple hp, iargn , c0 i,
missing argument positions with an implicit filler,                         where c0 is a coreference chain comprising c and
the y-axis indicates the likelihood of the filler be-                       its coreferent constituents.3 We defined a binary
ing found at least once in the previous x sentences.                        classification function P r(+| hp, iargn , c0 i) that
                                                                            predicts the probability that the entity referred to
                                                                            by c fills the missing argument position iargn of
tences for resolution; however, a vast majority of                          predicate instance p. In the remainder of this pa-
these can be resolved within the previous few sen-                          per, we will refer to c as the primary filler, dif-
tences. Section 6 discusses implications of this                            ferentiating it from other mentions in the corefer-
skewed distribution.                                                        ence chain c0 . In the following section, we present
                                                                            the feature set used to represent each three-tuple
4            Implicit argument identification                               within the classification function.
4.1                   Model formulation                                     4.2   Model features
In our study, we assumed that each sentence in a                            Starting with a wide range of features, we per-
document had been analyzed for PropBank and                                 formed floating forward feature selection (Pudil
NomBank predicate-argument structure. Nom-                                  et al., 1994) over held-out development data com-
Bank includes a lexicon listing the possible ar-                            prising implicit argument annotations from section
gument positions for a predicate, allowing us to                            24 of the Penn TreeBank. As part of the feature
identify missing argument positions with a simple                           selection process, we conducted a grid search for
lookup. Given a nominal predicate instance p with                           the best per-class cost within LibLinear’s logistic
a missing argument position iargn , the task is to                          regression solver (Fan et al., 2008). This was done
search the surrounding discourse for a constituent                          to reduce the negative effects of data imbalance,
c that fills iargn . Our model conducts this search                         which is severe even when selecting candidates
over all constituents annotated by either PropBank                          from the current and previous few sentences. Ta-
or NomBank with non-adjunct labels.                                         ble 2 shows the selected features, which are quite
   A candidate constituent c will often form a                              different from those used in our previous work to
coreference chain with other constituents in the                            identify traditional semantic arguments (Gerber et
discourse. Consider the following abridged sen-                             al., 2009).4 Below, we give further explanations
tences, which are adjacent in their Penn TreeBank                           for some of the features.
document:                                                                      Feature 1 models the semantic role relationship
(3) [Mexico] desperately needs investment.                                  between each mention in c0 and the missing argu-
                                                                            ment position iargn . To reduce data sparsity, this
(4) Conservative Japanese investors are put off                             feature generalizes predicates and argument posi-
    by [Mexico’s] investment regulations.                                   tions to their VerbNet (Kipper, 2005) classes and
(5) Japan is the fourth largest investor in                                     3
                                                                                  We used OpenNLP for coreference identification:
    [c Mexico], with 5% of the total                                        http://opennlp.sourceforge.net
                                                                                4
    [p investments].                                                              We have omitted many of the lowest-ranked features.
                                                                            Descriptions of these features can be obtained by contacting
NomBank does not associate the labeled instance                             the authors.
of investment with any arguments, but it is clear


                                                                         1586


 #         Feature value description
 1*        For every f , the VerbNet class/role of pf /argf concatenated with the class/role of p/iargn .
 2*        Average pointwise mutual information between hp, iargn i and any hpf , argf i.
 3         Percentage of all f that are definite noun phrases.
 4         Minimum absolute sentence distance from any f to p.
 5*        Minimum pointwise mutual information between hp, iargn i and any hpf , argf i.
 6         Frequency of the nominal form of p within the document that contains it.
 7         Nominal form of p concatenated with iargn .
 8         Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.
 9         Number of mentions in c0 .
 10*       Head word of p’s right sibling node.
 11        For every f , the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn .
 12        Part of speech of the head of p’s parent node.
 13        Average absolute sentence distance from any f to p.
 14*       Discourse relation whose two discourse units cover c (the primary filler) and p.
 15        Number of left siblings of p.
 16        Whether p is the head of its parent node.
 17        Number of right siblings of p.

Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting a
f iller) in the coreference chain c0 , we define pf and argf to be the predicate and argument position of f .
Features are sorted in descending order of feature selection gain. Unless otherwise noted, all predicates
were normalized to their verbal form and all argument positions (e.g., argn and iargn ) were interpreted
as labels instead of word content. Features marked with an asterisk are explained in Section 4.2.


semantic roles using SemLink.5 For explanation           the caption for Table 2):
purposes, consider again Example 1, where we are
trying to fill the iarg0 of shipping. Let c0 contain        pmi(hp, iargn i , hpf , argf i) =
a single mention, The two companies, which is the                     Pcoref (hp, iargn i , hpf , argf i)
                                                            log
arg0 of produce. As described in Table 2, fea-                  Pcoref (hp, iargn i , ∗)Pcoref (hpf , argf i , ∗)
ture 1 is instantiated with a value of create.agent-                                                            (6)
send.agent, where create and send are the VerbNet
classes that contain produce and ship, respectively.     To compute Equation 6, we first labeled a subset of
In the conversion to LibLinear’s instance repre-         the Gigaword corpus (Graff, 2003) using the ver-
sentation, this instantiation is converted into a sin-   bal SRL system of Punyakanok et al. (2008) and
gle binary feature create.agent-send.agent whose         the nominal SRL system of Gerber et al. (2009).
value is one. Features 1 and 11 are instantiated         We then identified coreferent pairs of arguments
once for each mention in c0 , allowing the model         using OpenNLP. Suppose the resulting data has
to consider information from multiple mentions of        N coreferential pairs of argument positions. Also
the same entity.                                         suppose that M of these pairs comprise hp, argn i
   Features 2 and 5 are inspired by the work             and hpf , argf i. The numerator in Equation 6 is
of Chambers and Jurafsky (2008), who inves-              defined as MN . Each term in the denominator is
tigated unsupervised learning of narrative event         obtained similarly, except that M is computed as
sequences using pointwise mutual information             the total number of coreference pairs compris-
(PMI) between syntactic positions. We used a sim-        ing an argument position (e.g., hp, argn i) and any
ilar PMI score, but defined it with respect to se-       other argument position. Like Chambers and Ju-
mantic arguments instead of syntactic dependen-          rafsky, we also used the discounting method sug-
cies. Thus, the values for features 2 and 5 are          gested by Pantel and Ravichandran (2004) for low-
computed as follows (the notation is explained in        frequency observations. The PMI score is some-
   5
                                                         what noisy due to imperfect output, but it provides
       http://verbs.colorado.edu/semlink
                                                         information that is useful for classification.


                                                     1587


   Feature 10 does not depend on c0 and is specific       annotated as filling the missing argument position.
to each predicate. Consider the following exam-           To factor out errors from standard SRL analyses,
ple:                                                      the model used gold-standard argument labels pro-
                                                          vided by PropBank and NomBank. As shown in
(7) Statistics Canada reported that its [arg1             Figure 1 (Section 3.2), implicit arguments tend to
    industrial-product] [p price] index dropped           be located in close proximity to the predicate. We
    2% in September.                                      found that using all candidate constituents c within
The “[p price] index” collocation is rarely associ-       the current and previous two sentences worked
ated with an arg0 in NomBank or with an iarg0 in          best on our development data.
our annotations (both argument positions denote             We compared our supervised model with the
the seller). Feature 10 accounts for this type of be-     simple baseline heuristic defined below:6
havior by encoding the syntactic head of p’s right               Fill iargn for predicate instance p
sibling. The value of feature 10 for Example 7 is                with the nearest constituent in the two-
price:index. Contrast this with the following:                   sentence candidate window that fills
(8) [iarg0 The company] is trying to prevent                     argn for a different instance of p, where
    further [p price] drops.                                     all nominal predicates are normalized to
                                                                 their verbal forms.
The value of feature 10 for Example 8 is
price:drop. This feature captures an important dis-       The normalization allows an existing arg0 for the
tinction between the two uses of price: the for-          verb invested to fill an iarg0 for the noun in-
mer rarely takes an iarg0 , whereas the latter often      vestment. We also evaluated an oracle model
does. Features 12 and 15-17 account for predicate-        that made gold-standard predictions for candidates
specific behaviors in a similar manner.                   within the two-sentence prediction window.
   Feature 14 identifies the discourse relation (if          We evaluated these models using the methodol-
any) that holds between the candidate constituent         ogy proposed by Ruppenhofer et al. (2009). For
c and the filled predicate p. Consider the following      each missing argument position of a predicate in-
example:                                                  stance, the models were required to either (1) iden-
                                                          tify a single constituent that fills the missing argu-
(9) [iarg0 SFE Technologies] reported a net loss          ment position or (2) make no prediction and leave
    of $889,000 on sales of $23.4 million.                the missing argument position unfilled. We scored
                                                          predictions using the Dice coefficient, which is de-
(10) That compared with an operating [p loss] of
                                                          fined as follows:
     [arg1 $1.9 million] on sales of $27.4 million                                       T
     in the year-earlier period.                                        2 ∗ |P redicted T rue|
                                                                                                            (11)
                                                                         |P redicted| + |T rue|
In this case, a comparison discourse relation (sig-
naled by the underlined text) holds between the           P redicted is the set of tokens subsumed by the
first and sentence sentence. The coherence pro-           constituent predicted by the model as filling a
vided by this relation encourages an inference that       missing argument position. T rue is the set of
identifies the marked iarg0 (the loser). Through-         tokens from a single annotated constituent that
out our study, we used gold-standard discourse re-        fills the missing argument position. The model’s
lations provided by the Penn Discourse TreeBank           prediction receives a score equal to the maxi-
(Prasad et al., 2008).                                    mum Dice overlap across any one of the annotated
                                                          fillers. Precision is equal to the summed predic-
5   Evaluation                                            tion scores divided by the number of argument po-
We trained the feature-based logistic regression          sitions filled by the model. Recall is equal to the
model over 816 annotated predicate instances as-          summed prediction scores divided by the number
sociated with 650 implicitly filled argument posi-        of argument positions filled in our annotated data.
tions (not all predicate instances had implicit ar-       Predictions not covering the head of a true filler
guments). During training, a candidate three-tuple        were assigned a score of zero.
hp, iargn , c0 i was given a positive label if the can-      6
                                                                This heuristic outperformed a more complicated heuris-
didate implicit argument c (the primary filler) was       tic that relied on the PMI score described in section 4.2.



                                                      1588


                                        Baseline             Discriminative                      Oracle
                    #    Imp. #     P      R        F1      P      R      F1         p         R       F1
    sale           64      60      50.0 28.3       36.2    47.2 41.7 44.2          0.118      80.0    88.9
    price          121     53      24.0 11.3       15.4    36.0 32.6 34.2          0.008      88.7    94.0
    investor       78      35      33.3    5.7     9.8     36.8 40.0 38.4         < 0.001     91.4    95.5
    bid            19      26     100.0 19.2       32.3    23.8 19.2 21.3          0.280      57.7    73.2
    plan           25      20      83.3 25.0       38.5    78.6 55.0 64.7          0.060      82.7    89.4
    cost           25      17      66.7 23.5       34.8    61.1 64.7 62.9          0.024      94.1    97.0
    loss           30      12      71.4 41.7       52.6    83.3 83.3 83.3          0.020      100.0 100.0
    loan           11      9       50.0 11.1       18.2    42.9 33.3 37.5          0.277      88.9    94.1
    investment     21      8       0.0     0.0     0.0     40.0 25.0 30.8          0.182      87.5    93.3
    fund           43      6       0.0     0.0     0.0     14.3 16.7 15.4          0.576      50.0    66.7
    Overall        437    246      48.4 18.3       26.5    44.5 40.4 42.3         < 0.001     83.1    90.7

Table 3: Evaluation results. The second column gives the number of predicate instances evaluated.
The third column gives the number of ground-truth implicitly filled argument positions for the predicate
instances (not all instances had implicit arguments). P , R, and F1 indicate precision, recall, and F-
measure (β = 1), respectively. p-values denote the bootstrapped significance of the difference in F1
between the baseline and discriminative models. Oracle precision (not shown) is 100% for all predicates.


   Our evaluation data comprised 437 predicate in-                               Percent change (p-value)
stances associated with 246 implicitly filled ar-          Configuration          P          R        F1
gument positions. Table 3 presents the results.            Remove 1,2,5         -35.3      -36.1     -35.7
Predicates with the highest number of implicit ar-                             (< 0.01) (< 0.01) (< 0.01)
guments - sale and price - showed F1 increases             Use 1,2,5 only       -26.3      -11.9     -19.2
of 8 points and 18.8 points, respectively. Over-                               (< 0.01) (0.05) (< 0.01)
all, the discriminative model increased F1 perfor-         Remove 14             0.2        1.0       0.7
mance 15.8 points (59.6%) over the baseline.                                    (0.95)    (0.66)    (0.73)
   We measured human performance on this task
by running our undergraduate assistant’s annota-          Table 4: Feature ablation results. The first column
tions against the evaluation data. Our assistant          lists the feature configurations. All changes are
achieved an overall F1 score of 58.4% using the           percentages relative to the full-featured discrimi-
same candidate window as the baseline and dis-            native model. p-values for the changes are indi-
criminative models. The difference in F1 between          cated in parentheses.
the discriminative and human results had an ex-
act p-value of less than 0.001. All significance
                                                          of the missing argument position (first configura-
testing was performed using a two-tailed bootstrap
                                                          tion). The second configuration tested the effect of
method similar to the one described by Efron and
                                                          using only the SRL-based features. This also re-
Tibshirani (1993).
                                                          sulted in significant performance losses, suggest-
6     Discussion                                          ing that the other features contribute useful infor-
                                                          mation. Lastly, we tested the effect of removing
6.1    Feature ablation                                   discourse relations (feature 14), which are likely
We conducted an ablation study to measure the             to be difficult to extract reliably in a practical set-
contribution of specific feature sets. Table 4            ting. As shown, this feature did not have a statis-
presents the ablation configurations and results.         tically significant effect on performance and could
For each configuration, we retrained and retested         be excluded in future applications of the model.
the discriminative model using the features de-
                                                          6.2   Unclassified true implicit arguments
scribed. As shown, we observed significant losses
when excluding features that relate the seman-            Of all the errors made by the system, approxi-
tic roles of mentions in c0 to the semantic role          mately 19% were caused by the system’s failure to



                                                    1589


generate a candidate constituent c that was a cor-       This is precisely what happened for the fund pred-
rect implicit argument. Without such a candidate,        icate, where the model incorrectly identified many
the system stood no chance of identifying a cor-         implicit arguments for “stock [p fund]” and “mu-
rect implicit argument. Two factors contributed to       tual [p fund]”. The left context of fund should help
this type of error, the first being our assumption       the model avoid this type of error; however, our
that implicit arguments are also core (i.e., argn )      feature selection process did not identify any over-
arguments to traditional SRL structures. Approxi-        all gains from including this information.
mately 8% of the overall error was due to a failure
of this assumption. In many cases, the true im-          6.4    Improvements versus the baseline
plicit argument filled a non-core (i.e., adjunct) role   The baseline heuristic covers the simple case
within PropBank or NomBank.                              where identical predicates share arguments in the
   More frequently, however, true implicit argu-         same position. Thus, it is interesting to examine
ments were missed because the candidate window           cases where the baseline heuristic failed but the
was too narrow. This accounts for 12% of the             discriminative model succeeded. Consider the fol-
overall error. Oracle recall (second-to-last col-        lowing sentence:
umn in Table 3) indicates the nominals that suf-
fered most from windowing errors. For exam-              (12) Mr. Rogers recommends that [p investors]
ple, the sale predicate was associated with the               sell [iarg2 takeover-related stock].
highest number of true implicit arguments, but           Neither NomBank nor the baseline heuristic asso-
only 80% of those could be resolved within the           ciate the marked predicate in Example 12 with any
two-sentence candidate window. Empirically, we           arguments; however, the feature-based model was
found that extending the candidate window uni-           able to correctly identify the marked iarg2 as the
formly for all predicates did not increase perfor-       entity being invested in. This inference captured a
mance on the development data. The oracle re-            tendency of investors to sell the things they have
sults suggest that predicate-specific window set-        invested in.
tings might offer some advantage.                           We conclude our discussion with an example of
6.3   The investment and fund predicates                 an extra-sentential implicit argument:

In Section 4.2, we discussed the price predicate,        (13) [iarg0 Olivetti] has denied that it violated
which frequently occurs in the “[p price] index”              the rules, asserting that the shipments were
collocation. We observed that this collocation                properly licensed. However, the legality of
is rarely associated with either an overt arg0 or             these [p sales] is still an open question.
an implicit iarg0 . Similar observations can be
made for the investment and fund predicates. Al-         As shown in Example 13, the system was able to
though these two predicates are frequent, they are       correctly identify Olivetti as the agent in the sell-
rarely associated with implicit arguments: invest-       ing event of the second sentence. This inference
ment takes only eight implicit arguments across its      involved two key steps. First, the system identified
21 instances, and fund takes only six implicit ar-       coreferent mentions of Olivetti that participated in
guments across its 43 instances. This behavior is        exporting and supplying events (not shown). Sec-
due in large part to collocations such as “[p in-        ond, the system identified a tendency for exporters
vestment] banker”, “stock [p fund]”, and “mutual         and suppliers to also be sellers. Using this knowl-
[p fund]”, which use predicate senses that are not       edge, the system extracted information that could
eventive. Such collocations also violate our as-         not be extracted by the baseline heuristic or a tra-
sumption that differences between the PropBank           ditional SRL system.
and NomBank argument structure for a predicate           7     Conclusions and future work
are indicative of implicit arguments (see Section
3.1 for this assumption).                                Current SRL approaches limit the search for ar-
   Despite their lack of implicit arguments, it is       guments to the sentence containing the predicate
important to account for predicates such as in-          of interest. Many systems take this assumption
vestment and fund because incorrect prediction of        a step further and restrict the search to the predi-
implicit arguments for them can lower precision.         cate’s local syntactic environment; however, pred-
                                                         icates and the sentences that contain them rarely


                                                     1590


exist in isolation. As shown throughout this paper,             References
they are usually embedded in a coherent and se-                 Aljoscha Burchardt, Anette Frank, and Manfred
mantically rich discourse that must be taken into                 Pinkal. 2005. Building text meaning representa-
account. We have presented a preliminary study                    tions from contextually related frames - a case study.
of implicit arguments for nominal predicates that                 In Proceedings of the Sixth International Workshop
                                                                  on Computational Semantics.
focused specifically on this problem.
   Our contribution is three-fold. First, we have               Xavier Carreras and Lluı́s Màrquez. 2005. Introduc-
created gold-standard implicit argument annota-                   tion to the CoNLL-2005 shared task: Semantic role
                                                                  labeling.
tions for a small set of pervasive nominal predi-
cates.7 Our analysis shows that these annotations               Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
add 65% to the role coverage of NomBank. Sec-                     pervised learning of narrative event chains. In Pro-
ond, we have demonstrated the feasibility of re-                  ceedings of the Association for Computational Lin-
                                                                  guistics, pages 789–797, Columbus, Ohio, June. As-
covering implicit arguments for many of the pred-                 sociation for Computational Linguistics.
icates, thus establishing a baseline for future work
on this emerging task. Third, our study suggests                Jacob Cohen. 1960. A coefficient of agreement
                                                                   for nominal scales. Educational and Psychological
a few ways in which this research can be moved
                                                                   Measurement, 20(1):3746.
forward. As shown in Section 6, many errors were
caused by the absence of true implicit arguments                Bradley Efron and Robert J. Tibshirani. 1993. An In-
within the set of candidate constituents. More in-                troduction to the Bootstrap. Chapman & Hall, New
                                                                  York.
telligent windowing strategies in addition to al-
ternate candidate sources might offer some im-                  Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
provement. Although we consistently observed                      Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
                                                                  A Library for Large Linear Classification. Journal
development gains from using automatic coref-
                                                                  of Machine Learning Research, 9:1871–1874.
erence resolution, this process creates errors that
need to be studied more closely. It will also be                Christiane Fellbaum. 1998. WordNet: An Electronic
important to study implicit argument patterns of                  Lexical Database (Language, Speech, and Commu-
                                                                  nication). The MIT Press, May.
non-verbal predicates such as the partitive percent.
These predicates are among the most frequent in                 C.J. Fillmore and C.F. Baker. 2001. Frame semantics
the TreeBank and are likely to require approaches                 for text understanding. In Proceedings of WordNet
                                                                  and Other Lexical Resources Workshop, NAACL.
that differ from the ones we pursued.
   Finally, any extension of this work is likely to             Matthew Gerber, Joyce Y. Chai, and Adam Meyers.
encounter a significant knowledge acquisition bot-               2009. The role of implicit argumentation in nominal
tleneck. Implicit argument annotation is difficult               SRL. In Proceedings of the North American Chap-
                                                                 ter of the Association for Computational Linguistics,
because it requires both argument and coreference                pages 146–154, Boulder, Colorado, USA, June.
identification (the data produced by Ruppenhofer
et al. (2009) is similar). Thus, it might be produc-            David Graff. 2003. English Gigaword. Linguistic
                                                                  Data Consortium, Philadelphia.
tive to focus future work on (1) the extraction of
relevant knowledge from existing resources (e.g.,               Jan Hajič, Massimiliano Ciaramita, Richard Johans-
our use of coreference patterns from Gigaword) or                  son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
(2) semi-supervised learning of implicit argument                  Màrquez, Adam Meyers, Joakim Nivre, Sebastian
                                                                   Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
models from a combination of labeled and unla-                     Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
beled data.                                                        2009 shared task: Syntactic and semantic dependen-
                                                                   cies in multiple languages. In Proceedings of the
Acknowledgments                                                    Thirteenth Conference on Computational Natural
                                                                   Language Learning (CoNLL 2009): Shared Task,
We would like to thank the anonymous review-                       pages 1–18, Boulder, Colorado, June. Association
ers for their helpful questions and comments. We                   for Computational Linguistics.
would also like to thank Malcolm Doering for his                Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
annotation effort. This work was supported in part                Matsumoto. 2007. Annotating a Japanese text cor-
by NSF grants IIS-0347548 and IIS-0840538.                        pus with predicate-argument and coreference rela-
   7
                                                                  tions. In Proceedings of the Linguistic Annotation
     Our annotation data can be freely downloaded at              Workshop in ACL-2007, page 132139.
http://links.cse.msu.edu:8000/lair/projects/semanticrole.html



                                                            1591


Kenji Imamura, Kuniko Saito, and Tomoko Izumi.            Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
  2009.     Discriminative approach to predicate-           hashi. 2004. Automatic construction of nominal
  argument structure analysis with zero-anaphora res-       case frames and its application to indirect anaphora
  olution. In Proceedings of the ACL-IJCNLP 2009            resolution. In Proceedings of Coling 2004, pages
  Conference Short Papers, pages 85–88, Suntec, Sin-        1201–1207, Geneva, Switzerland, Aug 23–Aug 27.
  gapore, August. Association for Computational Lin-        COLING.
  guistics.
                                                          Mihai Surdeanu, Richard Johansson, Adam Meyers,
P. Kingsbury, M. Palmer, and M. Marcus. 2002.               Lluı́s Màrquez, and Joakim Nivre. 2008. The
   Adding semantic annotation to the Penn TreeBank.         CoNLL 2008 shared task on joint parsing of syn-
   In Proceedings of the Human Language Technology          tactic and semantic dependencies. In CoNLL 2008:
   Conference (HLT’02).                                     Proceedings of the Twelfth Conference on Computa-
                                                            tional Natural Language Learning, pages 159–177,
Karin Kipper. 2005. VerbNet: A broad-coverage, com-         Manchester, England, August. Coling 2008 Orga-
  prehensive verb lexicon. Ph.D. thesis, Department         nizing Committee.
  of Computer and Information Science University of
  Pennsylvania.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
  Marcinkiewicz. 1993. Building a large annotated
  corpus of English: the Penn TreeBank. Computa-
  tional Linguistics, 19:313–330.
Adam Meyers. 2007. Annotation guidelines for
  NomBank - noun argument structure for PropBank.
  Technical report, New York University.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
 man, Lynette Hirschman, Marcia Linebarger, and
 John Dowding. 1986. Recovering implicit infor-
 mation. In Proceedings of the 24th annual meeting
 on Association for Computational Linguistics, pages
 10–19, Morristown, NJ, USA. Association for Com-
 putational Linguistics.
Patrick Pantel and Deepak Ravichandran.       2004.
  Automatically labeling semantic classes.        In
  Daniel Marcu Susan Dumais and Salim Roukos, ed-
  itors, HLT-NAACL 2004: Main Proceedings, pages
  321–328, Boston, Massachusetts, USA, May 2 -
  May 7. Association for Computational Linguistics.

Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
  sakaki, Geraud Campion, Aravind Joshi, and Bonnie
  Webber. 2008. Penn discourse treebank version 2.0.
  Linguistic Data Consortium, February.

P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating
   search methods in feature selection. Pattern Recog-
   nition Letters, 15:1119–1125.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
  The importance of syntactic parsing and infer-
  ence in semantic role labeling. Comput. Linguist.,
  34(2):257–287.

Josef Ruppenhofer, Caroline Sporleder, Roser
   Morante, Collin Baker, and Martha Palmer. 2009.
   Semeval-2010 task 10: Linking events and their
   participants in discourse.     In Proceedings of
   the Workshop on Semantic Evaluations: Recent
   Achievements and Future Directions (SEW-2009),
   pages 106–111, Boulder, Colorado, June. Associa-
   tion for Computational Linguistics.




                                                      1592

       Learning Lexicalized Reordering Models from Reordering Graphs

                   Jinsong Su, Yang Liu, Yajuan Lü, Haitao Mi, Qun Liu
                      Key Laboratory of Intelligent Information Processing
                              Institute of Computing Technology
                                Chinese Academy of Sciences
                            P.O. Box 2704, Beijing 100190, China
                {sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cn



                       Abstract

     Lexicalized reordering models play a crucial
     role in phrase-based translation systems. They
     are usually learned from the word-aligned
     bilingual corpus by examining the reordering
     relations of adjacent phrases. Instead of just
     checking whether there is one phrase adjacent            Figure 1: Occurrence of a swap with different numbers
     to a given phrase, we argue that it is important         of adjacent bilingual phrases: only one phrase in (a) and
     to take the number of adjacent phrases into              three phrases in (b). Black squares denote word align-
     account for better estimations of reordering             ments and gray rectangles denote bilingual phrases. [s,t]
     models. We propose to use a structure named              indicates the target-side span of bilingual phrase bp and
     reordering graph, which represents all phrase            [u,v] represents the source-side span of bilingual phrase
     segmentations of a sentence pair, to learn lex-          bp.
     icalized reordering models efficiently. Exper-
     imental results on the NIST Chinese-English
                                                              al., 2007; Galley and Manning, 2008). These mod-
     test sets show that our approach significantly
     outperforms the baseline method.                         els are learned from a word-aligned corpus to pre-
                                                              dict three orientations of a phrase pair with respect
                                                              to the previous bilingual phrase: monotone (M ),
1   Introduction                                              swap (S), and discontinuous (D). Take the bilingual
                                                              phrase bp in Figure 1(a) for example. The word-
Phrase-based translation systems (Koehn et al.,               based reordering model (Koehn et al., 2007) ana-
2003; Och and Ney, 2004) prove to be the state-               lyzes the word alignments at positions (s − 1, u − 1)
of-the-art as they have delivered translation perfor-         and (s − 1, v + 1). The orientation of bp is set
mance in recent machine translation evaluations.              to D because the position (s − 1, v + 1) contains
While excelling at memorizing local translation and           no word alignment. The phrase-based reordering
reordering, phrase-based systems have difficulties in         model (Tillmann, 2004) determines the presence
modeling permutations among phrases. As a result,             of the adjacent bilingual phrase located in position
it is important to develop effective reordering mod-          (s − 1, v + 1) and then treats the orientation of bp as
els to capture such non-local reordering.                     S. Given no constraint on maximum phrase length,
    The early phrase-based paradigm (Koehn et al.,            the hierarchical phrase reordering model (Galley and
2003) applies a simple distance-based distortion              Manning, 2008) also analyzes the adjacent bilingual
penalty to model the phrase movements. More re-               phrases for bp and identifies its orientation as S.
cently, many researchers have presented lexicalized              However, given a bilingual phrase, the above-
reordering models that take advantage of lexical              mentioned models just consider the presence of an
information to predict reordering (Tillmann, 2004;            adjacent bilingual phrase rather than the number of
Xiong et al., 2006; Zens and Ney, 2006; Koehn et              adjacent bilingual phrases. See the examples in Fig-




                                                        12
                         Proceedings of the ACL 2010 Conference Short Papers, pages 12–16,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph. In (b), we denote
each bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the source
and target spans of this bilingual phrase respectively. M = monotone (solid lines), S = swap (dotted line), and D =
discontinuous (segmented lines). The bilingual phrases marked in the gray constitute a reordering example.


ure 1 for illustration. In Figure 1(a), bp is in a swap    ing probabilities.
order with only one bilingual phrase. In Figure 1(b),
bp swaps with three bilingual phrases. Lexicalized         2.1 Lexicalized Reordering Model
reordering models do not distinguish different num-        Given a phrase pair bp = (ei , f ai ), where ai de-
bers of adjacent phrase pairs, and just give bp the
                                                           fines that the source phrase f ai is aligned to the
same count in the swap orientation.
                                                           target phrase ei , the traditional lexicalized reorder-
   In this paper, we propose a novel method to better
                                                           ing model computes the reordering count of bp in
estimate the reordering probabilities with the con-
                                                           the orientation o based on the word alignments of
sideration of varying numbers of adjacent bilingual
                                                           boundary words. Specifically, the model collects
phrases. Our method uses reordering graphs to rep-
                                                           bilingual phrases and distinguishes their orientations
resent all phrase segmentations of parallel sentence
                                                           with respect to the previous bilingual phrase into
pairs, and then gets the fractional counts of bilin-
                                                           three categories:
gual phrases for orientations from reordering graphs
in an inside-outside fashion. Experimental results                        
indicate that our method achieves significant im-                         
                                                                          M         ai − ai−1 = 1
provements over the traditional lexicalized reorder-                    o= S         ai − ai−1 = −1             (1)
                                                                          
                                                                          
ing model (Koehn et al., 2007).                                            D         |ai − ai−1 | 6= 1
   This paper is organized as follows: in Section 2,
we first give a brief introduction to the traditional        Using the relative-frequency approach, the re-
lexicalized reordering model. Then we introduce            ordering probability regarding bp is
our method to estimate the reordering probabilities
from reordering graphs. The experimental results                                      Count(o, bp)
                                                                       p(o|bp) = P              0
                                                                                                                (2)
are reported in Section 3. Finally, we end with a                                     o0 Count(o , bp)
conclusion and future work in Section 4.
                                                           2.2 Reordering Graph
2   Estimation of Reordering Probabilities
                                                           For a parallel sentence pair, its reordering graph in-
    Based on Reordering Graph
                                                           dicates all possible translation derivations consisting
In this section, we first describe the traditional lexi-   of the extracted bilingual phrases. To construct a
calized reordering model, and then illustrate how to       reordering graph, we first extract bilingual phrases
construct reordering graphs to estimate the reorder-       using the way of (Och, 2003). Then, the adjacent




                                                      13


bilingual phrases are linked according to the target-               src span     trg span    α      β
side order. Some bilingual phrases, which have                        [0, 0]       [0, 0]    1      9
                                                                      [1, 1]       [1, 1]    1      8
no adjacent bilingual phrases because of maximum
                                                                      [1, 7]       [1, 7]    1      1
length limitation, are linked to the nearest bilingual                [4, 4]       [2, 2]    1      1
phrases in the target-side order.                                     [4, 5]       [2, 3]    1      3
   Shown in Figure 2(b), the reordering graph for                     [4, 6]       [2, 4]    1      1
the parallel sentence pair (Figure 2(a)) can be rep-                  [4, 7]       [2, 5]    1      2
                                                                      [2, 7]       [2, 7]    1      1
resented as an undirected graph, where each rect-
                                                                      [5, 5]       [3, 3]    1      1
angle corresponds to a phrase pair, each link is the                  [6, 6]       [4, 4]    2      1
orientation relationship between adjacent bilingual                   [6, 7]       [4, 5]    1      2
phrases, and two distinguished rectangles bs and be                   [7, 7]       [5, 5]    3      1
indicate the beginning and ending of the parallel sen-                [2, 2]       [6, 6]    5      1
                                                                      [2, 3]       [6, 7]    2      1
tence pair, respectively. With the reordering graph,
                                                                      [3, 3]       [7, 7]    5      1
we can obtain all reordering examples containing                      [8, 8]       [8, 8]    9      1
the given bilingual phrase. For example, the bilin-
gual phrase hzhengshi huitan, formal meetingsi (see       Table 1: The α and β values of the bilingual phrases
Figure 2(a)), corresponding to the rectangle labeled      shown in Figure 2.
with the source span [6,7] and the target span [4,5],
is in a monotone order with one previous phrase           (Charniak and Johnson, 2005; Huang, 2008), the
and in a discontinuous order with two subsequent          fractional count of (o, bp0 , bp) is
phrases (see Figure 2(b)).
                                                                                         α(bp0 ) · β(bp)
2.3 Estimation of Reordering Probabilities                        Count(o, bp0 , bp) =                      (3)
                                                                                             β(bs )
We estimate the reordering probabilities from re-         where the numerator indicates the number of paths
ordering graphs. Given a parallel sentence pair,          containing the reordering example (o, bp0 , bp) and
there are many translation derivations correspond-        the denominator is the total number of paths in the
ing to different paths in its reordering graph. As-       reordering graph. Continuing with the reordering
suming all derivations have a uniform probability,        example described above, we obtain its fractional
the fractional counts of bilingual phrases for orien-     count using the formula (3): Count(M, bp1 , bp2 ) =
tations can be calculated by utilizing an algorithm in    (1 × 2)/9 = 2/9.
the inside-outside fashion.                                  Then, the fractional count of bp in the orientation
   Given a phrase pair bp in the reordering graph,        o is calculated as described below:
we denote the number of paths from bs to bp with                                   X
α(bp). ItPcan be computed in an iterative way                    Count(o, bp) =        Count(o, bp0 , bp) (4)
α(bp) = bp0 α(bp0 ), where bp0 is one of the pre-                                  bp0
vious bilingual phrases of bp and α(bs )=1. In a sim-
ilar way, the number of paths from                        For example, we compute the fractional count of
                                 P be to bp, 00
                                                notated
                                                          bp2 in the monotone orientation by the formula (4):
as β(bp), is simply β(bp) =         bp00
                                         β(bp ), where
bp00 is one of the subsequent bilingual phrases of bp     Count(M, bp2 ) = 2/9.
and β(be )=1. Here, we show the α and β values of            As described in the lexicalized reordering model
all bilingual phrases of Figure 2 in Table 1. Espe-       (Section 2.1), we apply the formula (2) to calculate
cially, for the reordering example consisting of the      the final reordering probabilities.
bilingual phrases bp1 =hjiang juxing, will holdi and
                                                          3   Experiments
bp2 =hzhengshi huitan, formal meetingsi, marked in
the gray color in Figure 2, the α and β values can be     We conduct experiments to investigate the effec-
calculated: α(bp1 ) = 1, β(bp2 ) = 1+1 = 2, β(bs ) =      tiveness of our method on the msd-fe reorder-
8+1 = 9.                                                  ing model and the msd-bidirectional-fe reordering
   Inspired by the parsing literature on pruning          model. These two models are widely applied in




                                                     14


phrase-based system (Koehn et al., 2007). The msd-                    model    method     MT-03      MT-04      MT-05
fe reordering model has three features, which rep-                             baseline   29.62      32.12      28.88
                                                                       m-f
                                                                                 RG       30.51∗∗    32.78∗∗    29.50∗
resent the probabilities of bilingual phrases in three
                                                                               baseline   29.38      32.00      28.64
orientations: monotone, swap, or discontinuous. If a                  m-b-f
                                                                                 RG       30.49∗∗    32.73∗∗    29.24∗
msd-bidirectional-fe model is used, then the number
of features doubles: one for each direction.                      Table 2: Experimental results with the small-scale cor-
                                                                  pus. m-f: msd-fe reordering model. m-b-f: msd-
3.1 Experiment Setup                                              bidirectional-fe reordering model. RG: probabilities esti-
                                                                  mation based on Reordering Graph. * or **: significantly
Two different sizes of training corpora are used in
                                                                  better than baseline (p < 0 .05 or p < 0 .01 ).
our experiments: one is a small-scale corpus that
comes from FBIS corpus consisting of 239K bilin-                      model    method     MT-03      MT-04      MT-05
gual sentence pairs, the other is a large-scale corpus                         baseline   31.58      32.39      31.49
                                                                       m-f
that includes 1.55M bilingual sentence pairs from                                RG       32.44∗∗    33.24∗∗    31.64
LDC. The 2002 NIST MT evaluation test data is                         m-b-f
                                                                               baseline   32.43      33.07      31.69
used as the development set and the 2003, 2004,                                  RG       33.29∗∗    34.49∗∗    32.79∗∗
2005 NIST MT test data are the test sets. We
                                                                  Table 3: Experimental results with the large-scale cor-
choose the MOSES1 (Koehn et al., 2007) as the ex-                 pus.
perimental decoder. GIZA++ (Och and Ney, 2003)
and the heuristics “grow-diag-final-and” are used to
generate a word-aligned corpus, where we extract                     Table 3 shows the results of experiments with
bilingual phrases with maximum length 7. We use                   the large training corpus. In the experiments of
SRILM Toolkits (Stolcke, 2002) to train a 4-gram                  the msd-fe model, in exception to the MT-05 test
language model on the Xinhua portion of Gigaword                  set, our method is superior to the baseline method.
corpus.                                                           The BLEU scores by our method are 32.44, 33.24
   In exception to the reordering probabilities, we               and 31.64, which obtain 0.86, 0.85 and 0.15 gains
use the same features in the comparative experi-                  on three test set, respectively. For the msd-
ments. During decoding, we set ttable-limit = 20,                 bidirectional-fe model, the BLEU scores produced
stack = 100, and perform minimum-error-rate train-                by our approach are 33.29, 34.49 and 32.79 on the
ing (Och, 2003) to tune various feature weights. The              three test sets, with 0.86, 1.42 and 1.1 points higher
translation quality is evaluated by case-insensitive              than the baseline method, respectively.
BLEU-4 metric (Papineni et al., 2002). Finally, we
conduct paired bootstrap sampling (Koehn, 2004) to                4   Conclusion and Future Work
test the significance in BLEU scores differences.
                                                                  In this paper, we propose a method to improve the
3.2 Experimental Results                                          reordering model by considering the effect of the
Table 2 shows the results of experiments with the                 number of adjacent bilingual phrases on the reorder-
small training corpus. For the msd-fe model, the                  ing probabilities estimation. Experimental results on
BLEU scores by our method are 30.51 32.78 and                     NIST Chinese-to-English tasks demonstrate the ef-
29.50, achieving absolute improvements of 0.89,                   fectiveness of our method.
0.66 and 0.62 on the three test sets, respectively. For              Our method is also general to other lexicalized
the msd-bidirectional-fe model, our method obtains                reordering models. We plan to apply our method
BLEU scores of 30.49 32.73 and 29.24, with abso-                  to the complex lexicalized reordering models, for
lute improvements of 1.11, 0.73 and 0.60 over the                 example, the hierarchical reordering model (Galley
baseline method.                                                  and Manning, 2008) and the MEBTG reordering
   1
                                                                  model (Xiong et al., 2006). In addition, how to fur-
     The phrase-based lexical reordering model (Tillmann,
2004) is also closely related to our model. However, due to
                                                                  ther improve the reordering model by distinguishing
the limit of time and space, we only use Moses-style reordering   the derivations with different probabilities will be-
model (Koehn et al., 2007) as our baseline.                       come another study emphasis in further research.




                                                             15


Acknowledgement                                              Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
                                                               mum entropy based phrase reordering model for statis-
The authors were supported by National Natural Sci-            tical machine translation. In Proc. of ACL 2006, pages
ence Foundation of China, Contracts 60873167 and               521–528.
60903138. We thank the anonymous reviewers for               Richard Zens and Hermann Ney. 2006. Discriminvative
their insightful comments. We are also grateful to             reordering models for statistical machine translation.
Hongmei Zhao and Shu Cai for their helpful feed-               In Proc. of Workshop on Statistical Machine Transla-
back.                                                          tion 2006, pages 521–528.



References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
  fine n-best parsing and maxent discriminative rerank-
  ing. In Proc. of ACL 2005, pages 173–180.
Michel Galley and Christopher D. Manning. 2008. A
  simple and effective hierarchical phrase reordering
  model. In Proc. of EMNLP 2008, pages 848–856.
Liang Huang. 2008. Forest reranking: Discriminative
  parsing with non-local features. In Proc. of ACL 2008,
  pages 586–594.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
  2003. Statistical phrase-based translation. In Proc.
  of HLT-NAACL 2003, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
  Callison-Burch, Marcello Federico, Nicola Bertoldi,
  Brooke Cowan, Wade Shen, Christine Moran, Richard
  Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
  stantin, and Evan Herbst. 2007. Moses: Open source
  toolkit for statistical machine translation. In Proc. of
  ACL 2007, Demonstration Session, pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
  machine translation evaluation. In Proc. of EMNLP
  2004, pages 388–395.
Franz Josef Och and Hermann Ney. 2003. A system-
  atic comparison of various statistical alignment mod-
  els. Computational Linguistics, 29(1):19–51.
Franz Joseph Och and Hermann Ney. 2004. The align-
  ment template approach to statistical machine transla-
  tion. Computational Linguistics, pages 417–449.
Franz Josef Och. 2003. Minimum error rate training in
  statistical machine translation. In Proc. of ACL 2003,
  pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
  Jing Zhu. 2002. Bleu: a method for automatic eval-
  uation of machine translation. In Proc. of ACL 2002,
  pages 311–318.
Andreas Stolcke. 2002. Srilm - an extensible language
  modeling toolkit. In Proc. of ICSLP 2002, pages 901–
  904.
Christoph Tillmann. 2004. A unigram orientation model
  for statistical machine translation. In Proc. of HLT-
  ACL 2004, Short Papers, pages 101–104.




                                                        16

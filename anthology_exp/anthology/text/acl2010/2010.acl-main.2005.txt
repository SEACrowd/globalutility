      Diversify and Combine: Improving Word Alignment for Machine
                  Translation on Low-Resource Languages


                          Bing Xiang, Yonggang Deng, and Bowen Zhou
                                IBM T. J. Watson Research Center
                                  Yorktown Heights, NY 10598
                            {bxiang,ydeng,zhou}@us.ibm.com




                     Abstract                                   Most of the research on alignment combination
                                                             in the past has focused on how to combine the
    We present a novel method to improve                     alignments from two different directions, source-
    word alignment quality and eventually the                to-target and target-to-source. Usually people start
    translation performance by producing and                 from the intersection of two sets of alignments,
    combining complementary word align-                      and gradually add links in the union based on
    ments for low-resource languages. Instead                certain heuristics, as in (Koehn et al., 2003), to
    of focusing on the improvement of a single               achieve a better balance compared to using either
    set of word alignments, we generate mul-                 intersection (high precision) or union (high recall).
    tiple sets of diversified alignments based               In (Ayan and Dorr, 2006) a maximum entropy ap-
    on different motivations, such as linguis-               proach was proposed to combine multiple align-
    tic knowledge, morphology and heuris-                    ments based on a set of linguistic and alignment
    tics. We demonstrate this approach on an                 features. A different approach was presented in
    English-to-Pashto translation task by com-               (Deng and Zhou, 2009), which again concentrated
    bining the alignments obtained from syn-                 on the combination of two sets of alignments, but
    tactic reordering, stemming, and partial                 with a different criterion. It tries to maximize the
    words. The combined alignment outper-                    number of phrases that can be extracted in the
    forms the baseline alignment, with signif-               combined alignments. A greedy search method
    icantly higher F-scores and better transla-              was utilized and it achieved higher translation per-
    tion performance.                                        formance than the baseline.
                                                                More recently, an alignment selection approach
1 Introduction
                                                             was proposed in (Huang, 2009), which com-
Word alignment usually serves as the starting                putes confidence scores for each link and prunes
point and foundation for a statistical machine               the links from multiple sets of alignments using
translation (SMT) system. It has received a signif-          a hand-picked threshold. The alignments used
icant amount of research over the years, notably in          in that work were generated from different align-
(Brown et al., 1993; Ittycheriah and Roukos, 2005;           ers (HMM, block model, and maximum entropy
Fraser and Marcu, 2007; Hermjakob, 2009). They               model). In this work, we use soft voting with
all focused on the improvement of word alignment             weighted confidence scores, where the weights
models. In this work, we leverage existing align-            can be tuned with a specific objective function.
ers and generate multiple sets of word alignments            There is no need for a pre-determined threshold
based on complementary information, then com-                as used in (Huang, 2009). Also, we utilize var-
bine them to get the final alignment for phrase              ious knowledge sources to enrich the alignments
training. The resource required for this approach            instead of using different aligners. Our strategy is
is little, compared to what is needed to build a rea-        to diversify and then combine in order to catch any
sonable discriminative alignment model, for ex-              complementary information captured in the word
ample. This makes the approach especially ap-                alignments for low-resource languages.
pealing for SMT on low-resource languages.                      The rest of the paper is organized as follows.


                                                        22
                        Proceedings of the ACL 2010 Conference Short Papers, pages 22–26,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                                                                                S
We present three different sets of alignments in
Section 2 for an English-to-Pashto MT task. In                                        S                    CC                        S

Section 3, we propose the alignment combination                 NP                        VP                        NP                    VP

algorithm. The experimental results are reported                PRP     VBP               NP                                   VBP         NP      ADVP

                                                                               PRP$                  NNS                                   PRP     RB
in Section 4. We conclude the paper in Section 5.
                                                        E:      they     are     your employees and you                         know     them    well

2 Diversified Word Alignments                           P:    hQvy     stAsO     kArvAl         dy          Av       tAsO        hQvy     smh    pOZnB


We take an English-to-Pashto MT task as an exam-        E’: they       your    employees        are         and          you     them     well    know

ple and create three sets of additional alignments
on top of the baseline alignment.
                                                            Figure 1: Alignment before/after VP-based re-
2.1   Syntactic Reordering
                                                            ordering.
Pashto is a subject-object-verb (SOV) language,
which puts verbs after objects. People have pro-
posed different syntactic rules to pre-reorder SOV          1980), a widely applied algorithm to remove the
languages, either based on a constituent parse tree         common morphological and inflexional endings
(Drábek and Yarowsky, 2004; Wang et al., 2007)             from words in English. For Pashto, we utilize
or dependency parse tree (Xu et al., 2009). In              a morphological decompostion algorithm that has
this work, we apply syntactic reordering for verb           been shown to be effective for Arabic speech
phrases (VP) based on the English constituent               recognition (Xiang et al., 2006). We start from a
parse. The VP-based reordering rule we apply in             fixed set of affixes with 8 prefixes and 21 suffixes.
the work is:                                                The prefixes and suffixes are stripped off from
                                                            the Pashto words under the two constraints:(1)
  • V P (V B∗, ∗) → V P (∗, V B∗)                           Longest matched affixes first; (2) Remaining stem
                                                            must be at least two characters long.
where V B∗ represents V B, V BD, V BG, V BN ,
V BP and V BZ.                                              2.3 Partial Word
   In Figure 1, we show the reference alignment             For low-resource languages, we usually suffer
between an English sentence and the correspond-             from the data sparsity issue. Recently, a simple
ing Pashto translation, where E is the original En-         method was presented in (Chiang et al., 2009),
glish sentence, P is the Pashto sentence (in ro-            which keeps partial English and Urdu words in the
manized text), and E ′ is the English sentence after        training data for alignment training. This is similar
reordering. As we can see, after the VP-based re-           to the stemming method, but is more heuristics-
ordering, the alignment between the two sentences           based, and does not rely on a set of available af-
becomes monotone, which makes it easier for the             fixes. With the same motivation, we keep the first
aligner to get the alignment correct. During the            4 characters of each English and Pashto word to
reordering of English sentences, we store the in-           generate one more alternative for the word align-
dex changes for the English words. After getting            ment.
the alignment trained on the reordered English and
original Pashto sentence pairs, we map the English          3 Confidence-Based Alignment
words back to the original order, along with the              Combination
learned alignment links. In this way, the align-
ment is ready to be combined with the baseline              Now we describe the algorithm to combine mul-
alignment and any other alternatives.                       tiple sets of word alignments based on weighted
                                                            confidence scores. Suppose aijk is an alignment
2.2   Stemming                                              link in the i-th set of alignments between the j-th
Pashto is one of the morphologically rich lan-              source word and k-th target word in sentence pair
guages. In addition to the linguistic knowledge ap-         (S,T ). Similar to (Huang, 2009), we define the
plied in the syntactic reordering described above,          confidence of aijk as
we also utilize morphological analysis by applying                                        q
stemming on both the English and Pashto sides.               c(aijk |S, T ) =                  qs2t (aijk |S, T )qt2s (aijk |T, S),
For English, we use Porter stemming (Porter,                                                                                     (1)


                                                       23


where the source-to-target link posterior probabil-               apply grow-diagonal-final (gdf). The decoding
ity                                                               weights are optimized with minimum error rate
                                                                  training (MERT) (Och, 2003) to maximize BLEU
                                pi (tk |sj )                      scores (Papineni et al., 2002). There are 2028 sen-
       qs2t (aijk |S, T ) = PK                     ,   (2)
                             k ′ =1 pi (tk ′ |sj )                tences in the tuning set and 1019 sentences in the
                                                                  test set, both with one reference. We use another
and the target-to-source link posterior probability
                                                                  150 sentence pairs as a heldout hand-aligned set
qt2s (aijk |T, S) is defined similarly. pi (tk |sj ) is
                                                                  to measure the word alignment quality. The three
the lexical translation probability between source
                                                                  sets of alignments described in Section 2 are gen-
word sj and target word tk in the i-th set of align-
                                                                  erated on the same training data separately with
ments.
                                                                  GIZA++ and enhanced by gdf as for the baseline
   Our alignment combination algorithm is as fol-
                                                                  alignment. The English parse tree used for the
lows.
                                                                  syntactic reordering was produced by a maximum
  1. Each candidate link ajk gets soft votes from                 entropy based parser (Ratnaparkhi, 1997).
     N sets of alignments via weighted confidence
     scores:                                                      4.2 Improvement in Word Alignment
                          N
                          X                                       In Table 1 we show the precision, recall and F-
        v(ajk |S, T ) =         wi ∗ c(aijk |S, T ),   (3)
                                                                  score of each set of word alignments for the 150-
                          i=1
                                                                  sentence set. Using partial word provides the high-
      where the weight wi for each set of alignment               est F-score among all individual alignments. The
      can be optimized under various criteria. In                 F-score is 5% higher than for the baseline align-
      this work, we tune it on a hand-aligned de-                 ment. The VP-based reordering itself does not im-
      velopment set to maximize the alignment F-                  prove the F-score, which could be due to the parse
      score.                                                      errors on the conversational training data. We ex-
                                                                  periment with three options (c0 , c1 , c2 ) when com-
  2. All candidates are sorted by soft votes in de-
                                                                  bining the baseline and reordering-based align-
     scending order and evaluated sequentially. A
                                                                  ments. In c0 , the weights wi and confidence scores
     candidate link ajk is included if one of the
                                                                  c(aijk |S, T ) in Eq. (3) are all set to 1. In c1 ,
     following is true:
                                                                  we set confidence scores to 1, while tuning the
        • Neither sj nor tk is aligned so far;                    weights with hill climbing to maximize the F-
        • sj is not aligned and its left or right                 score on a hand-aligned tuning set. In c2 , we com-
          neighboring word is aligned to tk so far;               pute the confidence scores as in Eq. (1) and tune
        • tk is not aligned and its left or right                 the weights as in c1 . The numbers in Table 1 show
          neighboring word is aligned to sj so far.               the effectiveness of having both weights and con-
                                                                  fidence scores during the combination.
  3. Repeat scanning all candidate links until no                    Similarly, we combine the baseline with each
     more links can be added.                                     of the other sets of alignments using c2 . They
                                                                  all result in significantly higher F-scores. We
   In this way, those alignment links with higher
                                                                  also generate alignments on VP-reordered partial
confidence scores have higher priority to be in-
                                                                  words (X in Table 1) and compared B + X and
cluded in the combined alignment.
                                                                  B + V + P . The better results with B + V + P
4 Experiments                                                     show the benefit of keeping the alignments as di-
                                                                  versified as possible before the combination. Fi-
4.1   Baseline                                                    nally, we compare the proposed alignment combi-
Our training data contains around 70K English-                    nation c2 with the heuristics-based method (gdf),
Pashto sentence pairs released under the DARPA                    where the latter starts from the intersection of all 4
TRANSTAC project, with about 900K words on                        sets of alignments and then applies grow-diagonal-
the English side. The baseline is a phrase-based                  final (Koehn et al., 2003) based on the links in
MT system similar to (Koehn et al., 2003). We                     the union. The proposed combination approach on
use GIZA++ (Och and Ney, 2000) to generate                        B + V + S + P results in close to 7% higher F-
the baseline alignment for each direction and then                scores than the baseline and also 2% higher than


                                                             24


gdf. We also notice that its higher F-score is                 Alignment      Comb       Links     Phrase     BLEU
mainly due to the higher precision, which should               Baseline                   963K      565K      12.67
result from the consideration of confidence scores.            V                          965K      624K      12.82
                                                               S                          915K      692K      13.04
 Alignment      Comb         P         R          F
                                                               P                          906K      716K      13.30
 Baseline                 0.6923    0.6414     0.6659          X                          911K      689K      13.00
 V                        0.6934    0.6388     0.6650          B+V              c0        870K      890K      13.20
 S                        0.7376    0.6495     0.6907          B+V              c1        865K      899K      13.32
 P                        0.7665    0.6643     0.7118          B+V              c2        874K      879K      13.60
 X                        0.7615    0.6641     0.7095          B+S              c2        864K      948K      13.41
 B+V              c0      0.7639    0.6312     0.6913          B+P              c2        863K      942K      13.40
 B+V              c1      0.7645    0.6373     0.6951          B+X              c2        871K      905K      13.37
 B+V              c2      0.7895    0.6505     0.7133          B+V+P            c2        880K      914K      13.60
 B+S              c2      0.7942    0.6553     0.7181          B+V+S+P          cat      3749K     1258K      13.01
 B+P              c2      0.8006    0.6612     0.7242          B+V+S+P          gdf      1021K      653K      13.14
 B+X              c2      0.7827    0.6670     0.7202          B+V+S+P          c2        907K      771K      13.73
 B+V+P            c2      0.7912    0.6755     0.7288
 B+V+S+P          gdf     0.7238    0.7042     0.7138         Table 2: Improvement in BLEU scores (B: base-
 B+V+S+P          c2      0.7906    0.6852     0.7342         line; V: VP-based reordering; S: stemming; P: par-
                                                              tial word; X: VP-reordered partial word).
Table 1: Alignment precision, recall and F-score
(B: baseline; V: VP-based reordering; S: stem-
ming; P: partial word; X: VP-reordered partial                both higher F-score and higher BLEU score. The
word).                                                        combination approach itself is not limited to any
                                                              specific alignment. It provides a general frame-
                                                              work that can take advantage of as many align-
4.3   Improvement in MT Performance                           ments as possible, which could differ in prepro-
In Table 2, we show the corresponding BLEU                    cessing, alignment modeling, or any other aspect.
scores on the test set for the systems built on each
set of word alignment in Table 1. Similar to the              Acknowledgments
observation from Table 1, c2 outperforms c0 and               This work was supported by the DARPA
c1 , and B + V + S + P with c2 outperforms                    TRANSTAC program. We would like to thank
B + V + S + P with gdf. We also ran one ex-                   Upendra Chaudhari, Sameer Maskey and Xiao-
periment in which we concatenated all 4 sets of               qiang Luo for providing useful resources and the
alignments into one big set (shown as cat). Over-             anonymous reviewers for their constructive com-
all, the BLEU score with confidence-based com-                ments.
bination was increased by 1 point compared to the
baseline, 0.6 compared to gdf, and 0.7 compared
to cat. All results are statistically significant with        References
p < 0.05 using the sign-test described in (Collins            Necip Fazil Ayan and Bonnie J. Dorr. 2006. A max-
et al., 2005).                                                  imum entropy approach to combining word align-
                                                                ments. In Proc. HLT/NAACL, June.
5 Conclusions
                                                              Peter Brown, Vincent Della Pietra, Stephen Della
In this work, we have presented a word alignment                Pietra, and Robert Mercer. 1993. The mathematics
combination method that improves both the align-                of statistical machine translation: parameter estima-
ment quality and the translation performance. We                tion. Computational Linguistics, 19(2):263–311.
generated multiple sets of diversified alignments             David Chiang, Kevin Knight, Samad Echihabi, et al.
based on linguistics, morphology, and heuris-                   2009. Isi/language weaver nist 2009 systems. In
tics, and demonstrated the effectiveness of com-                Presentation at NIST MT 2009 Workshop, August.
bination on the English-to-Pashto translation task.           Michael Collins, Philipp Koehn, and Ivona Kučerová.
We showed that the combined alignment signif-                   2005. Clause restructuring for statistical machine
icantly outperforms the baseline alignment with                 translation. In Proc. of ACL, pages 531–540.


                                                         25


Yonggang Deng and Bowen Zhou. 2009. Optimizing
  word alignment combination for phrase table train-
  ing. In Proc. ACL, pages 229–232, August.
Elliott Franco Drábek and David Yarowsky. 2004. Im-
   proving bitext word alignments via syntax-based re-
   ordering of english. In Proc. ACL.
Alexander Fraser and Daniel Marcu. 2007. Getting the
  structure right for word alignment: Leaf. In Proc. of
  EMNLP, pages 51–60, June.
Ulf Hermjakob. 2009. Improved word alignment with
  statistics and linguistic heuristics. In Proc. EMNLP,
  pages 229–237, August.
Fei Huang. 2009. Confidence measure for word align-
  ment. In Proc. ACL, pages 932–940, August.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
  imum entropy word aligner for arabic-english ma-
  chine translation. In Proc. of HLT/EMNLP, pages
  89–96, October.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
  Statistical phrase-based translation.  In Proc.
  NAACL/HLT.
Franz Josef Och and Hermann Ney. 2000. Improved
  statistical alignment models. In Proc. of ACL, pages
  440–447, Hong Kong, China, October.
Franz Josef Och. 2003. Minimum error rate training
  in statistical machine translation. In Proc. of ACL,
  pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
  jing Zhu. 2002. Bleu: a method for automatic evalu-
  ation of machine translation. In Proc. of ACL, pages
  311–318.
Martin Porter. 1980. An algorithm for suffix stripping.
 In Program, volume 14, pages 130–137.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
  tistical parser based on maximum entropy models.
  In Proc. of EMNLP, pages 1–10.
Chao Wang, Michael Collins, and Philipp Koehn.
  2007. Chinese syntactic reordering for statistical
  machine translation. In Proc. EMNLP, pages 737–
  745.
Bing Xiang, Kham Nguyen, Long Nguyen, Richard
  Schwartz, and John Makhoul. 2006. Morphological
  decomposition for arabic broadcast news transcrip-
  tion. In Proc. ICASSP.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
  Och. 2009. Using a dependency parser to improve
  smt for subject-object-verb languages. In Proc.
  NAACL/HLT, pages 245–253, June.




                                                          26

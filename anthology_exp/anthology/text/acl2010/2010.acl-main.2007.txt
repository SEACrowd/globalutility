                       The Same-head Heuristic for Coreference


                           Micha Elsner and Eugene Charniak
               Brown Laboratory for Linguistic Information Processing (BLLIP)
                                      Brown University
                                    Providence, RI 02912
                            {melsner,ec}@cs.brown.edu




                    Abstract                                resolve (variant MUC score .82 on MUC-6) and
                                                            while those with partial matches are quite a bit
    We investigate coreference relationships                harder (.53), by far the worst performance is on
    between NPs with the same head noun.                    those without any match at all (.27). This effect
    It is relatively common in unsupervised                 is magnified by most popular metrics for coref-
    work to assume that such pairs are                      erence, which reward finding links within large
    coreferent– but this is not always true, es-            clusters more than they punish proposing spu-
    pecially if realistic mention detection is              rious links, making it hard to improve perfor-
    used. We describe the distribution of non-              mance by linking conservatively. Systems that
    coreferent same-head pairs in news text,                use gold mention boundaries (the locations of NPs
    and present an unsupervised generative                  marked by annotators)1 have even less need to
    model which learns not to link some same-               worry about same-head relationships, since most
    head NPs using syntactic features, improv-              NPs which disobey the conventional assumption
    ing precision.                                          are not marked as mentions.
                                                               In this paper, we count how often same-head
1   Introduction                                            pairs fail to corefer in the MUC-6 corpus, show-
Full NP coreference, the task of discovering which          ing that gold mention detection hides most such
non-pronominal NPs in a discourse refer to the              pairs, but more realistic detection finds large num-
same entity, is widely known to be challenging.             bers. We also present an unsupervised genera-
In practice, however, most work focuses on the              tive model which learns to make certain same-
subtask of linking NPs with different head words.           head pairs non-coreferent. The model is based
Decisions involving NPs with the same head word             on the idea that pronoun referents are likely to
have not attracted nearly as much attention, and            be salient noun phrases in the discourse, so we
many systems, especially unsupervised ones, op-             can learn about NP antecedents using pronom-
erate under the assumption that all same-head               inal antecedents as a starting point. Pronoun
pairs corefer. This is by no means always the case–         anaphora, in turn, is learnable from raw data
there are several systematic exceptions to the rule.        (Cherry and Bergsma, 2005; Charniak and Elsner,
In this paper, we show that these exceptions are            2009). Since our model links fewer NPs than the
fairly common, and describe an unsupervised sys-            baseline, it improves precision but decreases re-
tem which learns to distinguish them from coref-            call. This tradeoff is favorable for CEAF, but not
erent same-head pairs.                                      for b3 .
   There are several reasons why relatively little
                                                            2       Related work
attention has been paid to same-head pairs. Pri-
marily, this is because they are a comparatively            Unsupervised systems specify the assumption of
easy subtask in a notoriously difficult area; Stoy-         same-head coreference in several ways: by as-
anov et al. (2009) shows that, among NPs headed                 1
                                                                 Gold mention detection means something slightly differ-
by common nouns, those which have an exact                  ent in the ACE corpus, where the system input contains every
match earlier in the document are the easiest to            NP annotated with an entity type.


                                                       33
                        Proceedings of the ACL 2010 Conference Short Papers, pages 33–37,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


sumption (Haghighi and Klein, 2009), using                             tite matching between gold and proposed clusters,
a head-prediction clause (Poon and Domingos,                           then gives the percentage of entities whose gold
2008), and using a sparse Dirichlet prior on word                      label and proposed label match. b3 gives more
emissions (Haghighi and Klein, 2007). (These                           weight to errors involving larger clusters (since
three systems, perhaps not coincidentally, use gold                    these lower scores for several mentions at once);
mentions.) An exception is Ng (2008), who points                       for mention CEAF, all mentions are weighted
out that head identity is not an entirely reliable cue                 equally.
and instead uses exact string match (minus deter-                         We annotate the data with the self-trained Char-
miners) for common NPs and an alias detection                          niak parser (McClosky et al., 2006), then extract
system for proper NPs. This work uses mentions                         mentions using three different methods. The gold
extracted with an NP chunker. No specific results                      mentions method takes only mentions marked by
are reported for same-head NPs. However, while                         annotators. The nps method takes all base noun
using exact string match raises precision, many                        phrases detected by the parser. Finally, the nouns
non-matching phrases are still coreferent, so this                     method takes all nouns, even those that do not
approach cannot be considered a full solution to                       head NPs; this method maximizes recall, since it
the problem.                                                           does not exclude prenominals in phrases like “a
   Supervised systems do better on the task, but                       Bush spokesman”. (High-precision models of the
not perfectly. Recent work (Stoyanov et al., 2009)                     internal structure of flat Penn Treebank-style NPs
attempts to determine the contributions of various                     were investigated by Vadas and Curran (2007).)
categories of NP to coreference scores, and shows                      For each experimental setting, we show the num-
(as stated above) that common NPs which partially                      ber of mentions detected, and how many of them
match an earlier mention are not well resolved by                      are linked to some antecedent by the system.
the state-of-the-art RECONCILE system, which                              The data is shown in Table 1. b3 shows a large
uses pairwise classification. They also show that                      drop in precision when all same-head pairs are
using gold mention boundaries makes the corefer-                       linked; in fact, in the nps and nouns settings, only
ence task substantially easier, and argue that this                    about half the same-headed NPs are actually coref-
experimental setting is “rather unrealistic”.                          erent (864 real links, 1592 pairs for nps). This
                                                                       demonstrates that non-coreferent same-head pairs
3       Descriptive study: MUC-6                                       not only occur, but are actually rather common in
We begin by examining how often non-same-head                          the dataset. The drop in precision is much less
pairs appear in the MUC-6 coreference dataset.                         obvious in the gold mentions setting, however;
To do so, we compare two artificial coreference                        most unlinked same-head pairs are not annotated
systems: the link-all strategy links all, and only,                    as mentions in the gold data, which is one reason
full (non-pronominal) NP pairs with the same head                      why systems run in this experimental setting can
which occur within 10 sentences of one another.                        afford to ignore them.
The oracle strategy links NP pairs with the same                          Improperly linking same-head pairs causes a
head which occur within 10 sentences, but only if                      loss in precision, but scores are dominated by re-
they are actually coreferent (according to the gold                    call3 . Thus, reporting b3 helps to mask the impact
annotation)2 The link-all system, in other words,                      of these pairs when examining the final f-score.
does what most existing unsupervised systems do                           We roughly characterize what sort of same-
on the same-head subset of NPs, while the oracle                       headed NPs are non-coreferent by hand-
system performs perfectly.                                             examining 100 randomly selected pairs. 39
   We compare our results to the gold standard us-                     pairs denoted different entities (“recent employ-
ing two metrics. b3 (Bagga and Baldwin, 1998)                          ees” vs “employees who have worked for longer”)
is a standard metric which calculates a precision                      disambiguated by modifiers or sometimes by
and recall for each mention. The mention CEAF                          discourse position. The next largest group (24)
(Luo, 2005) constructs a maximum-weight bipar-                         consists of time and measure phrases like “ten
                                                                       miles”. 12 pairs refer to parts or quantities
    2
     The choice of 10 sentences as the window size captures
                                                                           3
most, but not all, of the available recall. Using nouns mention              This bias is exaggerated for systems which only link
detection, it misses 117 possible same-head links, or about            same-head pairs, but continues to apply to real systems; for
10%. However, precision drops further as the window size               instance (Haghighi and Klein, 2009) has a b3 precision of 84
increases.                                                             and recall of 67.


                                                                  34


                Mentions    Linked b3 pr rec            F           mention CEAF
                               Gold mentions
    Oracle       1929        1164    100 32.3          48.8               54.4
    Link all     1929        1182    80.6 31.7         45.5               53.8
    Alignment    1929        495     93.7 22.1         35.8               40.5
                                   NPs
    Oracle       3993         864    100 30.6          46.9               73.4
    Link all     3993        1592    67.2 29.5         41.0               62.2
    Alignment    3993        518     87.2 24.7         38.5               67.0
                                  Nouns
    Oracle       5435        1127    100 41.5          58.6               83.5
    Link all     5435        2541    56.6 40.9         45.7               67.0
    Alignment    5435        935     83.0 32.8         47.1               74.4

Table 1: Oracle, system and baseline scores on MUC-6 test data. Gold mentions leave little room
for improvement between baseline and oracle; detecting more mentions widens the gap between
them. With realistic mention detection, precision and CEAF scores improve over baselines, while recall
and f-scores drop.


(“members of...”), and 12 contained a generic               relationship to a potential generator gj . These fea-
(“In a corporate campaign, a union tries...”). 9            tures, which we denote f (ni , gj , D), may depend
contained an annotator error. The remaining 4               on their relative position in the document D, and
were mistakes involving proper noun phrases                 on any features of gj , since we have already gener-
headed by Inc. and other abbreviations; this case           ated its tree. However, we cannot extract features
is easy to handle, but apparently not the primary           from the subtree under ni , since we have yet to
cause of errors.                                            generate it!
                                                                As usual for IBM models, we learn using EM,
4     System                                                and we need to start our alignment function off
                                                            with a good initial set of parameters. Since an-
Our system is a version of the popular IBM model            tecedents of NPs and pronouns (both salient NPs)
2 for machine translation. To define our generative         often occur in similar syntactic environments, we
model, we assume that the parse trees for the en-           use an alignment function for pronoun corefer-
tire document D are given, except for the subtrees          ence as a starting point. This alignment can be
with root nonterminal NP, denoted ni , which our            learned from raw data, making our approach un-
system will generate. These subtrees are related            supervised.
by a hidden set of alignments, ai , which link each             We take the pronoun model of Charniak and El-
NP to another NP (which we call a generator) ap-            sner (2009)4 as our starting point. We re-express
pearing somewhere before it in the document, or             it in the IBM framework, using a log-linear model
to a null antecedent. The set of potential genera-          for our alignment. Then our alignment (parame-
tors G (which plays the same role as the source-            terized by feature weights w) is:
language text in MT) is taken to be all the NPs
occurring within 10 sentences of the target, plus a                p(ai = j|G, D) ∝ exp(f (ni , gj , D) • w)
special null antecedent which plays the same role
as the null word in machine translation– it serves             The weights w are learned by gradient descent
as a dummy generator for NPs which are unrelated            on the log-likelihood. To use this model within
to any real NP in G.                                        EM, we alternate an E-step where we calculate
    The generative process fills in all the NP nodes        the expected alignments E[ai = j], then an M-
in order, from left to right. This process ensures          step where we run gradient descent. (We have also
that, when generating node ni , we have already             had some success with stepwise EM as in (Liang
filled in all the NPs in the set G (since these all         and Klein, 2009), but this requires some tuning to
precede ni ). When deciding on a generator for              work properly.)
                                                               4
NP ni , we can extract features characterizing its                 Downloaded from http://bllip.cs.brown.edu.


                                                       35


   As features, we take the same features as Char-           ator (the largest term in either of the sums) is from
niak and Elsner (2009): sentence and word-count              pT and is not the null antecedent are marked as
distance between ni and gj , sentence position of            coreferent to the generator. Other NPs are marked
each, syntactic role of each, and head type of gj            not coreferent.
(proper, common or pronoun). We add binary fea-
tures for the nonterminal directly over gj (NP, VP,          5   Results
PP, any S type, or other), the type of phrases mod-
ifying gj (proper nouns, phrasals (except QP and             Our results on the MUC-6 formal test set are
PP), QP, PP-of, PP-other, other modifiers, or noth-          shown in Table 1. In all experimental settings,
ing), and the type of determiner of gj (possessive,          the model improves precision over the baseline
definite, indefinite, deictic, other, or nothing). We        while decreasing recall– that is, it misses some le-
designed this feature set to distinguish prominent           gitimate coreferent pairs while correctly exclud-
NPs in the discourse, and also to be able to detect          ing many of the spurious ones. Because of the
abstract or partitive phrases by examining modi-             precision-recall tradeoff at which the systems op-
fiers and determiners.                                       erate, this results in reduced b3 and link F. How-
   To produce full NPs and learn same-head coref-            ever, for the nps and nouns settings, where the
erence, we focus on learning a good alignment                parser is responsible for finding mentions, the
using the pronoun model as a starting point. For             tradeoff is positive for the CEAF metrics. For in-
translation, we use a trivial model, p(ni |gai ) = 1         stance, in the nps setting, it improves over baseline
if the two have the same head, and 0 otherwise,              by 57%.
except for the null antecedent, which draws heads               As expected, the model does poorly in the gold
from a multinomial distribution over words.                  mentions setting, doing worse than baseline on
   While we could learn an alignment and then                both metrics. Although it is possible to get very
treat all generators as antecedents, so that only            high precision in this setting, the model is far too
NPs aligned to the null antecedent were not la-              conservative, linking less than half of the available
beled coreferent, in practice this model would               mentions to anything, when in fact about 60% of
align nearly all the same-head pairs. This is                them are coreferent. As we explain above, this ex-
true because many words are “bursty”; the prob-              perimental setting makes it mostly unnecessary to
ability of a second occurrence given the first is            worry about non-coreferent same-head pairs be-
higher than the a priori probability of occurrence           cause the MUC-6 annotators don’t often mark
(Church, 2000). Therefore, our model is actually a           them.
mixture of two IBM models, pC and pN , where pC
produces NPs with antecedents and pN produces                6   Conclusions
pairs that share a head, but are not coreferent. To
break the symmetry, we allow pC to use any pa-               While same-head pairs are easier to resolve than
rameters w, while pN uses a uniform alignment,               same-other pairs, they are still non-trivial and de-
w ≡ ~0. We interpolate between these two models              serve further attention in coreference research. To
with a constant λ, the single manually set parame-           effectively measure their effect on performance,
ter of our system, which we fixed at .9.                     researchers should report multiple metrics, since
   The full model, therefore, is:                            under b3 the link-all heuristic is extremely diffi-
                                                             cult to beat. It is also important to report results
                                                             using a realistic mention detector as well as gold
 p(ni |G, D) =λpT (ni |G, D)                                 mentions.
                + (1 − λ)pN (ni |G, D)
                1 X                                          Acknowledgements
pT (ni |G, D) =      exp(f (ni , gj , D) • w)
                Z
                   j∈G                                       We thank Jean Carletta for the S WITCHBOARD
                × I{head(ni ) = head(j)}                     annotations, and Dan Jurafsky and eight anony-
                X 1                                          mous reviewers for their comments and sugges-
pT (ni |G, D) =         I{head(ni ) = head(gj )}             tions. This work was funded by a Google graduate
                    |G|
                 j∈G
                                                             fellowship.
  NPs for which the maximum-likelihood gener-


                                                        36


References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
 scoring coreference chains. In LREC Workshop on
 Linguistics Coreference, pages 563–566.
Eugene Charniak and Micha Elsner. 2009. EM works
  for pronoun anaphora resolution. In Proceedings of
  EACL, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
  tion Maximization approach to pronoun resolution.
  In Proceedings of CoNLL, pages 88–95, Ann Arbor,
  Michigan.
Kenneth W. Church. 2000. Empirical estimates of
  adaptation: the chance of two Noriegas is closer to
  p/2 than p2 . In Proceedings of ACL, pages 180–186.
Aria Haghighi and Dan Klein. 2007. Unsupervised
  coreference resolution in a nonparametric Bayesian
  model. In Proceedings of ACL, pages 848–855.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
  ence resolution with rich syntactic and semantic fea-
  tures. In Proceedings of EMNLP, pages 1152–1161.
Percy Liang and Dan Klein. 2009. Online EM for un-
  supervised models. In HLT-NAACL.

Xiaoqiang Luo. 2005. On coreference resolution per-
  formance metrics. In Proceedings of HLT-EMNLP,
  pages 25–32, Morristown, NJ, USA. Association for
  Computational Linguistics.

David McClosky, Eugene Charniak, and Mark John-
  son. 2006. Effective self-training for parsing. In
  Proceedings of HLT-NAACL, pages 152–159.

Vincent Ng. 2008. Unsupervised models for corefer-
  ence resolution. In Proceedings of EMNLP, pages
  640–649, Honolulu, Hawaii. Association for Com-
  putational Linguistics.

Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
  pervised coreference resolution with Markov Logic.
  In Proceedings of EMNLP, pages 650–659, Hon-
  olulu, Hawaii, October. Association for Computa-
  tional Linguistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
  Ellen Riloff. 2009. Conundrums in noun phrase
  coreference resolution: Making sense of the state-
  of-the-art. In Proceedings of ACL-IJCNLP, pages
  656–664, Suntec, Singapore, August. Association
  for Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
  phrase structure to the penn treebank. In Proceed-
  ings of ACL, pages 240–247, Prague, Czech Repub-
  lic, June. Association for Computational Linguis-
  tics.




                                                          37

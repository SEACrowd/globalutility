    Authorship Attribution Using Probabilistic Context-Free Grammars
                  Sindhu Raghavan Adriana Kovashka Raymond Mooney
                              Department of Computer Science
                              The University of Texas at Austin
                                1 University Station C0500
                               Austin, TX 78712-0233, USA
                      {sindhu,adriana,mooney}@cs.utexas.edu


                     Abstract                                (2008) use a combination of word-level statistics
    In this paper, we present a novel approach               and part-of-speech counts or n-grams. Baayen et
    for authorship attribution, the task of iden-            al. (1996) demonstrate that the use of syntactic
    tifying the author of a document, using                  features from parse trees can improve the accu-
    probabilistic context-free grammars. Our                 racy of authorship attribution. While there have
    approach involves building a probabilistic               been several approaches proposed for authorship
    context-free grammar for each author and                 attribution, it is not clear if the performance of one
    using this grammar as a language model                   is better than the other. Further, it is difficult to
    for classification. We evaluate the perfor-              compare the performance of these algorithms be-
    mance of our method on a wide range of                   cause they were primarily evaluated on different
    datasets to demonstrate its efficacy.                    datasets. For more information on the current state
                                                             of the art for authorship attribution, we refer the
1   Introduction                                             reader to a detailed survey by Stamatatos (2009).
Natural language processing allows us to build                  We further investigate the use of syntactic infor-
language models, and these models can be used                mation by building complete models of each au-
to distinguish between languages. In the con-                thor’s syntax to distinguish between authors. Our
text of written text, such as newspaper articles or          approach involves building a probabilistic context-
short stories, the author’s style could be consid-           free grammar (PCFG) for each author and using
ered a distinct “language.” Authorship attribution,          this grammar as a language model for classifica-
also referred to as authorship identification or pre-        tion. Experiments on a variety of corpora includ-
diction, studies strategies for discriminating be-           ing poetry and newspaper articles on a number of
tween the styles of different authors. These strate-         topics demonstrate that our PCFG approach per-
gies have numerous applications, including set-              forms fairly well, but it only outperforms a bi-
tling disputes regarding the authorship of old and           gram language model on a couple of datasets (e.g.
historically important documents (Mosteller and              poetry). However, combining our approach with
Wallace, 1984), automatic plagiarism detection,              other methods results in an ensemble that performs
determination of document authenticity in court              the best on most datasets.
(Juola and Sofko, 2004), cyber crime investiga-
                                                             2   Authorship Attribution using PCFG
tion (Zheng et al., 2009), and forensics (Luyckx
and Daelemans, 2008).                                        We now describe our approach to authorship at-
   The general approach to authorship attribution            tribution. Given a training set of documents from
is to extract a number of style markers from the             different authors, we build a PCFG for each author
text and use these style markers as features to train        based on the documents they have written. Given
a classifier (Burrows, 1987; Binongo and Smith,              a test document, we parse it using each author’s
1999; Diederich et al., 2000; Holmes and Forsyth,            grammar and assign it to the author whose PCFG
1995; Joachims, 1998; Mosteller and Wallace,                 produced the highest likelihood for the document.
1984). These style markers could include the                    In order to build a PCFG, a standard statistical
frequencies of certain characters, function words,           parser takes a corpus of parse trees of sentences
phrases or sentences. Peng et al. (2003) build a             as training input. Since we do not have access to
character-level n-gram model for each author. Sta-           authors’ documents annotated with parse trees,
matatos et al. (1999) and Luyckx and Daelemans               we use a statistical parser trained on a generic


                                                        38
                        Proceedings of the ACL 2010 Conference Short Papers, pages 38–42,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


corpus like the Wall Street Journal (WSJ) or                 //www.cricinfo.com). In addition, we col-
Brown corpus from the Penn Treebank (http:                   lected poems from the Project Gutenberg web-
//www.cis.upenn.edu/˜treebank/)                              site (http://www.gutenberg.org/wiki/
to automatically annotate (i.e. treebank) the                Main_Page). We attempted to collect sets of
training documents for each author. In our                   documents on a shared topic written by multiple
experiments, we used the Stanford Parser (Klein              authors. This was done to ensure that the datasets
and Manning, 2003b; Klein and Manning,                       truly tested authorship attribution as opposed to
2003a) and the OpenNLP sentence segmenter                    topic identification. However, since it is very dif-
(http://opennlp.sourceforge.net/).                           ficult to find authors that write literary works on
Our approach is summarized below:                            the same topic, the Poetry dataset exhibits higher
                                                             topic variability than our news datasets. We had
  Input – A training set of documents labeled                5 different datasets in total – Football, Business,
with author names and a test set of documents with           Travel, Cricket, and Poetry. The number of au-
unknown authors.                                             thors in our datasets ranged from 3 to 6.
                                                                For each dataset, we split the documents into
    1. Train a statistical parser on a generic corpus        training and test sets. Previous studies (Stamatatos
       like the WSJ or Brown corpus.                         et al., 1999) have observed that having unequal
                                                             number of words per author in the training set
    2. Treebank each training document using the             leads to poor performance for the authors with
       parser trained in Step 1.                             fewer words. Therefore, we ensured that, in the
                                                             training set, the total number of words per author
    3. Train a PCFG Gi for each author Ai using the
                                                             was roughly the same. We would like to note that
       treebanked documents for that author.
                                                             we could have also selected the training set such
                                                             that the total number of sentences per author was
    4. For each test document, compute its likeli-
                                                             roughly the same. However, since we would like
       hood for each grammar Gi by multiplying the
                                                             to compare the performance of the PCFG-based
       probability of the top PCFG parse for each
                                                             approach with a bag-of-words baseline, we de-
       sentence.
                                                             cided to normalize the training set based on the
    5. For each test document, find the author Ai            number of words, rather than sentences. For test-
       whose grammar Gi results in the highest like-         ing, we used 15 documents per author for datasets
       lihood score.                                         with news articles and 5 or 10 documents per au-
                                                             thor for the Poetry dataset. More details about the
Output – A label (author name) for each docu-                datasets can be found in Table 1.
ment in the test set.
                                                                   Dataset   # authors   # words/auth   # docs/auth   # sent/auth

3     Experimental Comparison                                 Football          3        14374.67       17.3          786.3
                                                              Business          6        11215.5        14.16         543.6
This section describes experiments evaluating our              Travel           4        23765.75        28            1086
approach on several real-world datasets.                      Cricket           4        23357.25       24.5          1189.5
                                                               Poetry           6        7261.83        24.16          329
3.1    Data
                                                             Table 1: Statistics for the training datasets used in
We collected a variety of documents with known
                                                             our experiments. The numbers in columns 3, 4 and
authors including news articles on a wide range of
                                                             5 are averages.
topics and literary works like poetry. We down-
loaded all texts from the Internet and manually re-
moved extraneous information as well as titles, au-
thor names, and chapter headings. We collected               3.2      Methodology
several news articles from the New York Times                We evaluated our approach to authorship predic-
online journal (http://global.nytimes.                       tion on the five datasets described above. For news
com/) on topics related to business, travel, and             articles, we used the first 10 sections of the WSJ
football. We also collected news articles on                 corpus, which consists of annotated news articles
cricket from the ESPN cricinfo website (http:                on finance, to build the initial statistical parser in


                                                        39


Step 1. For Poetry, we used 7 sections of the                 writing style at the syntactic level, it may not accu-
Brown corpus which consists of annotated docu-                rately capture lexical information. Since both syn-
ments from different areas of literature.                     tactic and lexical information is presumably useful
   In the basic approach, we trained a PCFG model             in capturing the author’s overall writing style, we
for each author based solely on the documents                 also developed an ensemble using a PCFG model,
written by that author. However, since the num-               the bag-of-words MaxEnt classifier, and an n-
ber of documents per author is relatively low, this           gram language model. We linearly combined the
leads to very sparse training data. Therefore, we             confidence scores assigned by each model to each
also augmented the training data by adding one,               author, and used the combined score for the final
two or three sections of the WSJ or Brown corpus              classification. We refer to this model as “PCFG-
to each training set, and up-sampling (replicating)           E”, where E stands for ensemble. We also de-
the data from the original author. We refer to this           veloped another ensemble based on MaxEnt and
model as “PCFG-I”, where I stands for interpo-                n-gram language models to demonstrate the con-
lation since this effectively exploits linear interpo-        tribution of the PCFG model to the overall per-
lation with the base corpus to smooth parameters.             formance of PCFG-E. For each dataset, we report
Based on our preliminary experiments, we repli-               accuracy, the fraction of the test documents whose
cated the original data three or four times.                  authors were correctly identified.
   We compared the performance of our approach
to bag-of-words classification and n-gram lan-                3.3   Results and Discussion
guage models. When using bag-of-words, one
                                                              Table 2 shows the accuracy of authorship predic-
generally removes commonly occurring “stop
                                                              tion on different datasets. For the n-gram mod-
words.” However, for the task of authorship pre-
                                                              els, we only report the results for the bigram
diction, we hypothesized that the frequency of
                                                              model with smoothing (Bigram-I) as it was the
specific stop words could provide useful infor-
                                                              best performing model for most datasets (except
mation about the author’s writing style. Prelim-
                                                              for Cricket and Poetry). For the Cricket dataset,
inary experiments verified that eliminating stop
                                                              the trigram-I model was the best performing n-
words degraded performance; therefore, we did
                                                              gram model with an accuracy of 98.34%. Gener-
not remove them. We used the Maximum Entropy
                                                              ally, a higher order n-gram model (n = 3 or higher)
(MaxEnt) and Naive Bayes classifiers in the MAL-
                                                              performs poorly as it requires a fair amount of
LET software package (McCallum, 2002) as ini-
                                                              smoothing due to the exponential increase in all
tial baselines. We surmised that a discriminative
                                                              possible n-gram combinations. Hence, the supe-
classifier like MaxEnt might perform better than
                                                              rior performance of the trigram-I model on the
a generative classifier like Naive Bayes. How-
                                                              Cricket dataset was a surprising result. For the
ever, when sufficient training data is not available,
                                                              Poetry dataset, the unigram-I model performed
generative models are known to perform better
                                                              best among the smoothed n-gram models at 81.8%
than discriminative models (Ng and Jordan, 2001).
                                                              accuracy. This is unsurprising because as men-
Hence, we chose to compare our method to both
                                                              tioned above, topic information is strongest in
Naive Bayes and MaxEnt.
                                                              the Poetry dataset, and it is captured well in the
   We also compared the performance of the                    unigram model. For bag-of-words methods, we
PCFG approach against n-gram language models.                 find that the generatively trained Naive Bayes
Specifically, we tried unigram, bigram and trigram            model (unigram language model) performs bet-
models. We used the same background corpus                    ter than or equal to the discriminatively trained
mixing method used for the PCFG-I model to ef-                MaxEnt model on most datasets (except for Busi-
fectively smooth the n-gram models. Since a gen-              ness). This result is not suprising since our
erative model like Naive Bayes that uses n-gram               datasets are limited in size, and generative models
frequencies is equivalent to an n-gram language               tend to perform better than discriminative meth-
model, we also used the Naive Bayes classifier in             ods when there is very little training data available.
MALLET to implement the n-gram models. Note                   Amongst the different baseline models (MaxEnt,
that a Naive-Bayes bag-of-words model is equiva-              Naive Bayes, Bigram-I), we find Bigram-I to be
lent to a unigram language model.                             the best performing model (except for Cricket and
  While the PCFG model captures the author’s                  Poetry). For both Cricket and Poetry, Naive Bayes


                                                         40


         Dataset    MaxEnt    Naive Bayes    Bigram-I     PCFG       PCFG-I    PCFG-E     MaxEnt+Bigram-I
        Football     84.45       86.67        86.67       93.34        80       91.11           86.67
        Business     83.34       77.78        90.00       77.78       85.56     91.11           92.22
         Travel      83.34       83.34        91.67       81.67       86.67     91.67           90.00
        Cricket      91.67       95.00        91.67       86.67       91.67     95.00           93.34
         Poetry      56.36       78.18        70.90       78.18       83.63     87.27           76.36

Table 2: Accuracy in % for authorship prediction on different datasets. Bigram-I refers to the bigram
language model with smoothing. PCFG-E refers to the ensemble based on MaxEnt, Bigram-I, and
PCFG-I. MaxEnt+Bigram-I refers to the ensemble based on MaxEnt and Bigram-I.


is the best performing baseline model. While dis-             is better than the best constituent model. Further-
cussing the performance of the PCFG model and                 more, for the Football, Cricket and Poetry datasets
its variants, we consider the best performing base-           this improvement is quite substantial. We now
line model.                                                   find that the performance of some variant of PCFG
   We observe that the basic PCFG model and the               is always better than or equal to that of the best
PCFG-I model do not usually outperform the best               baseline. While the basic PCFG model outper-
baseline method (except for Football and Poetry,              forms the baseline for the Football dataset, PCFG-
as discussed below). For Football, the basic PCFG             E outperforms the best baseline for the Poetry
model outperforms the best baseline, while for                and Business datasets. For the Cricket and Travel
Poetry, the PCFG-I model outperforms the best                 datasets, the performance of the PCFG-E model
baseline. Further, the performance of the basic               equals that of the best baseline. In order to as-
PCFG model is inferior to that of PCFG-I for most             sess the statistical significance of any performance
datasets, likely due to the insufficient training data        difference between the best PCFG model and the
used in the basic model. Ideally one would use                best baseline, we performed the McNemar’s test,
more training documents, but in many domains                  a non-parametric test for binomial variables (Ros-
it is impossible to obtain a large corpus of doc-             ner, 2005). We found that the difference in the
uments written by a single author. For example,               performance of the two methods was not statisti-
as Luyckx and Daelemans (2008) argue, in foren-               cally significant at .05 significance level for any of
sics one would like to identify the authorship of             the datasets, probably due to the small number of
documents based on a limited number of docu-                  test samples.
ments written by the author. Hence, we investi-                  The performance of PCFG and PCFG-I is par-
gated smoothing techniques to improve the perfor-             ticularly impressive on the Football and Poetry
mance of the basic PCFG model. We found that                  datasets. For the Football dataset, the basic PCFG
the interpolation approach resulted in a substan-             model is the best performing PCFG model and it
tial improvement in the performance of the PCFG               performs much better than other methods. It is sur-
model for all but the Football dataset (discussed             prising that smoothing using PCFG-I actually re-
below). However, for some datasets, even this                 sults in a drop in performance on this dataset. We
improvement was not sufficient to outperform the              hypothesize that the authors in the Football dataset
best baseline.                                                may have very different syntactic writing styles
   The results for PCFG and PCFG-I demon-                     that are effectively captured by the basic PCFG
strate that syntactic information alone is gener-             model. Smoothing the data apparently weakens
ally a bit less accurate than using n-grams. In or-           this signal, hence causing a drop in performance.
der to utilize both syntactic and lexical informa-            For Poetry, PCFG-I achieves much higher accu-
tion, we developed PCFG-E as described above.                 racy than the baselines. This is impressive given
We combined the best n-gram model (Bigram-I)                  the much looser syntactic structure of poetry com-
and PCFG model (PCFG-I) with MaxEnt to build                  pared to news articles, and it indicates the value of
PCFG-E. For the Travel dataset, we find that the              syntactic information for distinguishing between
performance of the PCFG-E model is equal to that              literary authors.
of the best constituent model (Bigram-I). For the                Finally, we consider the specific contribution of
remaining datasets, the performance of PCFG-E                 the PCFG-I model towards the performance of


                                                         41


the PCFG-E ensemble. Based on comparing the                     Conference on Machine Learning (ECML), pages
results for PCFG-E and MaxEnt+Bigram-I, we                      137–142, Berlin, Heidelberg. Springer-Verlag.
find that there is a drop in performance for most             Patrick Juola and John Sofko. 2004. Proving and
datasets when removing PCFG-I from the ensem-                   Improving Authorship Attribution Technologies. In
ble. This drop is quite substantial for the Football            Proceedings of Canadian Symposium for Text Anal-
                                                                ysis (CaSTA).
and Poetry datasets. This indicates that PCFG-I
is contributing substantially to the performance of           Dan Klein and Christopher D. Manning. 2003a. Ac-
PCFG-E. Thus, it further illustrates the impor-                 curate unlexicalized parsing. In Proceedings of the
                                                                41st Annual Meeting on Association for Computa-
tance of broader syntactic information for the task             tional Linguistics (ACL), pages 423–430, Morris-
of authorship attribution.                                      town, NJ, USA. Association for Computational Lin-
                                                                guistics.
4   Future Work and Conclusions
                                                              Dan Klein and Christopher D. Manning. 2003b. Fast
In this paper, we have presented our ongoing work               Exact Inference with a Factored Model for Natural
on authorship attribution, describing a novel ap-               Language Parsing. In Advances in Neural Infor-
                                                                mation Processing Systems 15 (NIPS), pages 3–10.
proach that uses probabilistic context-free gram-               MIT Press.
mars. We have demonstrated that both syntac-
tic and lexical information are useful in effec-              Kim Luyckx and Walter Daelemans. 2008. Author-
                                                                ship Attribution and Verification with Many Authors
tively capturing authors’ overall writing style. To             and Limited Data. In Proceedings of the 22nd Inter-
this end, we have developed an ensemble ap-                     national Conference on Computational Linguistics
proach that performs better than the baseline mod-              (COLING), pages 513–520, August.
els on several datasets. An interesting extension             Andrew Kachites McCallum.      2002.    MAL-
of our current approach is to consider discrimina-              LET: A Machine Learning for Language Toolkit.
tive training of PCFGs for each author. Finally,                http://mallet.cs.umass.edu.
we would like to compare the performance of our               Frederick Mosteller and David L. Wallace. 1984. Ap-
method to other state-of-the-art approaches to au-              plied Bayesian and Classical Inference: The Case of
thorship prediction.                                            the Federalist Papers. Springer-Verlag.
                                                              Andrew Y. Ng and Michael I. Jordan. 2001. On Dis-
Acknowledgments                                                 criminative vs. Generative classifiers: A compari-
Experiments were run on the Mastodon Cluster,                   son of logistic regression and naive Bayes. In Ad-
                                                                vances in Neural Information Processing Systems 14
provided by NSF Grant EIA-0303609.                              (NIPS), pages 841–848.
                                                              Fuchun Peng, Dale Schuurmans, Viado Keselj, and
References                                                      Shaojun Wang. 2003. Language Independent
                                                                Authorship Attribution using Character Level Lan-
H. Baayen, H. van Halteren, and F. Tweedie. 1996.               guage Models. In Proceedings of the 10th Confer-
  Outside the cave of shadows: using syntactic annota-          ence of the European Chapter of the Association for
  tion to enhance authorship attribution. Literary and          Computational Linguistics (EACL).
  Linguistic Computing, 11(3):121–132, September.
                                                              Bernard Rosner. 2005. Fundamentals of Biostatistics.
Binongo and Smith. 1999. A Study of Oscar Wilde’s               Duxbury Press.
  Writings. Journal of Applied Statistics, 26:781.
                                                              E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 1999.
J Burrows. 1987. Word-patterns and Story-shapes:                 Automatic Authorship Attribution. In Proceedings
  The Statistical Analysis of Narrative Style.                   of the 9th Conference of the European Chapter of the
                                                                 Association for Computational Linguistics (EACL),
Joachim Diederich, Jörg Kindermann, Edda Leopold,
                                                                 pages 158–164, Morristown, NJ, USA. Association
  and Gerhard Paass. 2000. Authorship Attribu-
                                                                 for Computational Linguistics.
  tion with Support Vector Machines. Applied Intel-
  ligence, 19:2003.                                           E. Stamatatos. 2009. A Survey of Modern Author-
                                                                 ship Attribution Methods. Journal of the Ameri-
D. I. Holmes and R. S. Forsyth. 1995. The Federal-               can Society for Information Science and Technology,
  ist Revisited: New Directions in Authorship Attri-             60(3):538–556.
  bution. Literary and Linguistic Computing, 10:111–
  127.                                                        Rong Zheng, Yi Qin, Zan Huang, and Hsinchun
                                                                Chen. 2009. Authorship Analysis in Cybercrime
Thorsten Joachims. 1998. Text categorization with               Investigation. Lecture Notes in Computer Science,
  Support Vector Machines: Learning with many rel-              2665/2009:959.
  evant features. In Proceedings of the 10th European


                                                         42

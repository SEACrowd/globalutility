                   Evaluating Machine Translations using mNCD
           Marcus Dobrinkat and Tero Tapiovaara and Jaakko Väyrynen
                        Adaptive Informatics Research Centre
                 Aalto University School of Science and Technology
                      P.O. Box 15400, FI-00076 Aalto, Finland
    {marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fi

                                       Kimmo Kettunen
                            Kymenlaakso University of Applied Sciences
                               P.O. Box 9, FI-48401 Kotka, Finland
                                kimmo.kettunen@kyamk.fi

                    Abstract                                method. BADGER scores were directly compared
                                                            against the scores of METEOR and word error
    This paper introduces mNCD, a method                    rate (WER). The correlation between BADGER
    for automatic evaluation of machine trans-              and METEOR were low and correlations between
    lations. The measure is based on nor-                   BADGER and WER high. Kettunen (2009) uses
    malized compression distance (NCD), a                   the NCD directly as an MT evaluation measure.
    general information theoretic measure of                He showed with a small corpus of three language
    string similarity, and flexible word match-             pairs that NCD and METEOR 0.6 correlated for
    ing provided by stemming and synonyms.                  translations of 10–12 MT systems. NCD was not
    The mNCD measure outperforms NCD in                     compared to human assessments of translations,
    system-level correlation to human judg-                 but correlations of NCD and METEOR scores
    ments in English.                                       were very high for all the three language pairs.
                                                               Väyrynen et al. (2010) have extended the work
1   Introduction
                                                            by including NCD in the ACL WMT08 evaluation
Automatic evaluation of machine translation (MT)            framework and showing that NCD is correlated
systems requires automated procedures to en-                to human judgments. The NCD measure did not
sure consistency and efficient handling of large            match the performance of the state-of-the-art MT
amounts of data. In statistical MT systems, au-             evaluation measures in English, but it presented a
tomatic evaluation of translations is essential for         viable alternative to de facto standard BLEU (Pa-
parameter optimization and system development.              pineni et al., 2001), which is simple and effective
Human evaluation is too labor intensive, time con-          but has been shown to have a number of drawbacks
suming and expensive for daily evaluations. How-            (Callison-Burch et al., 2006).
ever, manual evaluation is important in the com-               Some recent advances in automatic MT evalu-
parison of different MT systems and for the valida-         ation have included non-binary matching between
tion and development of automatic MT evaluation             compared items (Banerjee and Lavie, 2005; Agar-
measures, which try to model human assessments              wal and Lavie, 2008; Chan and Ng, 2009), which
of translations as closely as possible. Furthermore,        is implicitly present in the string-based NCD mea-
the ideal evaluation method would be language in-           sure. Our motivation is to investigate whether in-
dependent, fast to compute and simple.                      cluding additional language dependent resources
   Recently, normalized compression distance                would improve the NCD measure. We experiment
(NCD) has been applied to the evaluation of                 with relaxed word matching using stemming and
machine translations. NCD is a general in-                  a lexical database to allow lexical changes. These
formation theoretic measure of string similar-              additional modules attempt to make the reference
ity, whereas most MT evaluation measures, e.g.,             sentences more similar to the evaluated transla-
BLEU and METEOR, are specifically constructed               tions on the string level. We report an experiment
for the task. Parker (2008) introduced BAD-                 showing that document-level NCD and aggregated
GER, an MT evaluation measure that uses NCD                 NCD scores for individual sentences produce very
and a language independent word normalization               similar correlations to human judgments.


                                                       80
                        Proceedings of the ACL 2010 Conference Short Papers, pages 80–85,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                                Variation in language leads to several accept-
                                                             able translations for each source sentence, which
                                                             is why multiple reference translations are pre-
                                                             ferred in evaluation. Unfortunately, it is typical
Figure 1: An example showing the compressed                  to have only one reference translation. Paraphras-
sizes of two strings separately and concatenated.            ing techniques can produce additional translation
                                                             variants (Russo-Lassner et al., 2005; Kauchak and
                                                             Barzilay, 2006). These can be seen as new refer-
2    Normalized Compression Distance                         ence translations, similar to pseudo references (Ma
Normalized compression distance (NCD) is a sim-              et al., 2007).
ilarity measure based on the idea that a string x is            The proposed method, mNCD, works analo-
similar to another string y when both share sub-             gously to M-BLEU and M-TER, which use the
strings. The description of y can reference shared           flexible word matching modules from METEOR
substrings in the known x without repetition, in-            to find relaxed word-to-word alignments (Agar-
dicating shared information. Figure 1 shows an               wal and Lavie, 2008). The modules are able to
example in which the compression of the concate-             align words even if they do not share the same
nation of x and y results in a shorter output than           surface form, but instead have a common stem or
individual compressions of x and y.                          are synonyms of each other. A similarized transla-
   The normalized compression distance, as de-               tion reference is generated by replacing words in
fined by Cilibrasi and Vitanyi (2005), is given in           the reference with their aligned counterparts from
Equation 1, with C(x) as length of the compres-              the translation hypothesis. The NCD score is com-
sion of x and C(x, y) as the length of the com-              puted between the translations and the similarized
pression of the concatenation of x and y.                    references to get the mNCD score.
                                                                Table 1 shows some hand-picked German–
                   C(x, y) − min {C(x), C(y)}                English candidate translations along with a) the
    N CD(x, y) =
                         max {C(x), C(y)}                    reference translations including the 1-NCD score
                                                  (1)
                                                             to easily compare with METEOR and b) the simi-
NCD computes the distance as a score closer to
                                                             larized references including the mNCD score. For
one for very different strings and closer to zero for
                                                             comparison, the corresponding METEOR scores
more similar strings.
                                                             without implicit relaxed matching are shown.
   NCD is an approximation of the uncomputable
normalized information distance (NID), a general             4     Experiments
measure for the similarity of two objects. NID
is based on the notion of Kolmogorov complex-                The proposed mNCD and the basic NCD measure
ity K(x), a theoretical measure for the informa-             were evaluated by computing correlation to hu-
tion content of a string x, defined as the shortest          man judgments of translations. A high correlation
universal Turing machine that prints x and stops             value between an MT evaluation measure and hu-
(Solomonoff, 1964). NCD approximates NID by                  man judgments indicates that the measure is able
the use of a compressor C(x) that is an upper                to evaluate translations in a more similar way to
bound of the Kolmogorov complexity K(x).                     humans.
                                                                Relaxed alignments with the METEOR mod-
3    mNCD                                                    ules exact, stem and synonym were created
Normalized compression distance was not con-                 for English for the computation of the mNCD
ceived with MT evaluation in mind, but rather it             score. The synonym module was not available
is a general measure of string similarity. Implicit          with other target languages.
non-binary matching with NCD is indicated by
preliminary experiments which show that NCD is               4.1    Evaluation Data
less sensitive to random changes on the character            The 2008 ACL Workshop on Statistical Machine
level than, for instance, BLEU, which only counts            Translation (Callison-Burch et al., 2008) shared
the exact matches between word n-grams. Thus                 task data includes translations from a total of 30
comparison of sentences at the character level               MT systems between English and five European
could account better for morphological changes.              languages, as well as automatic and human trans-


                                                        81


            Candidate C/ Reference R/ Similarized Reference S                                         1-NCD     METEOR
      C     There is no effective means to stop a Tratsch, which was already included in the world.
      R     There is no good way to halt gossip that has already begun to spread.                     .41       .31
      S     There is no effective means to stop gossip that has already begun to spread.              .56       .55
      C     Crisis, not only in America
      R     A Crisis Not Only in the U.S.                                                             .51       .44
      S     A Crisis not only in the America                                                          .72       .56
      C     Influence on the whole economy should not have this crisis.
      R     Nevertheless, the crisis should not have influenced the entire economy.                   .60       .37
      S     Nevertheless, the crisis should not have Influence the entire economy.                    .62       .44
      C     Or the lost tight meeting will be discovered at the hands of a gentlemen?
      R     Perhaps you see the pen you thought you lost lying on your colleague’s desk.              .42       .09
      S     Perhaps you meeting the pen you thought you lost lying on your colleague’s desk.          .40       .13


Table 1: Example German–English translations showing the effect of relaxed matching in the 1-mNCD
score (for rows S) compared with METEOR using the exact module only, since the modules stem
and synonym are already used in the similarized reference. Replaced words are emphasized.


lation evaluations for the translations. There are                  where for each system i, di is the difference be-
several tasks, defined by the language pair and the                 tween the rank derived from annotators’ input and
domain of translated text.                                          the rank obtained from the measure. From the an-
   The human judgments include three different                      notators’ input, the n systems were ranked based
categories. The R ANK category has human quality                    on the number of times each system’s output was
rankings of five translations for one sentence from                 selected as the best translation divided by the num-
different MT systems. The C ONST category con-                      ber of times each system was part of a judgment.
tains rankings for short phrases (constituents), and                   We computed system-level correlations for
the Y ES /N O category contains binary answers if a                 tasks with English, French, Spanish and German
short phrase is an acceptable translation or not.                   as the target language1 .
   For the translation tasks into English, the re-
laxed alignment using a stem module and the                         5     Results
synonym module affected 7.5 % of all words,
                                                                    We compare mNCD against NCD and relate their
whereas only 5.1 % of the words were changed in
                                                                    performance to other MT evaluation measures.
the tasks from English into the other languages.
   The data was preprocessed in two different                       5.1    Block size effect on NCD scores
ways. For NCD we kept the data as is, which we
                                                                    Väyrynen et al. (2010) computed NCD between a
called real casing (rc). Since the used METEOR
                                                                    set of candidate translations and references at the
align module lowercases all text, we restored the
                                                                    same time regardless of the sentence alignments,
case information in mNCD by copying the correct
                                                                    analogously to document comparison. We experi-
case from the reference translation to the similar-
                                                                    mented with segmentation of the candidate trans-
ized reference, based on METEOR’s alignment.
                                                                    lations into smaller blocks, which were individ-
The other way was to lowercase all data (lc).
                                                                    ually evaluated with NCD and aggregated into a
4.2       System-level correlation                                  single value with arithmetic mean. The resulting
                                                                    system-level correlations between NCD and hu-
We follow the same evaluation methodology as in
                                                                    man judgments are shown in Figure 2 as a function
Callison-Burch et al. (2008), which allows us to
                                                                    of the block size. The correlations are very simi-
measure how well MT evaluation measures corre-
                                                                    lar with all block sizes, except for Spanish, where
late with human judgments on the system level.
                                                                    smaller block size produces higher correlation. An
   Spearman’s rank correlation coefficient ρ was
                                                                    experiment with geometric mean produced similar
calculated between each MT evaluation measure
                                                                    results. The reported results with mNCD use max-
and human judgment category using the simplified
                                                                    imum block size, similar to Väyrynen et al. (2010).
equation                   P
                         6 i di                                        1
                                                                         The English-Spanish news task was left out as most mea-
               ρ=1−                           (2)                   sures had negative correlation with human judgments.
                        n(n2 − 1)

                                                               82


                                                                                                                                                                    Target Lang Corr

                                                   1.0
                                                                                                                                         Method     Parameters    EN     DE    FR    ES
  system level correlation with human judgements




                                                                                                                                         mNCD       PPMZ     rc   .69    .37   .82   .38
                                                   0.8




                                                                                                                                          NCD       PPMZ     rc   .60    .37   .84   .39
                                                                                                                                         mNCD        bz2     rc   .64    .32   .75   .25
                                                   0.6




                                                                                                                                          NCD        bz2     rc   .57    .34   .85   .42
                                                   0.4




                                                                                                                                         mNCD       PPMZ     lc   .66    .33   .79   .23
                                                                                                                                          NCD       PPMZ     lc   .56    .37   .77   .21
                                                   0.2




                                                                                                              into en
                                                                                                              into de
                                                                                                              into fr
                                                                                                                                         mNCD        bz2     lc   .59    .25   .78   .16
                                                                                                              into es                     NCD        bz2     lc   .54    .26   .77   .15
                                                   0.0




                                                         2    5   10   20        50   100           500       2000 5000
                                                                                                                                       Table 3: mNCD versus NCD system correlation
                                                                            block size in lines
                                                                                                                                       R ANK results with different parameters (the same
                                                                                                                                       as in Table 2) for each target language. Higher
Figure 2: The block size has very little effect on                                                                                     values are emphasized. Target languages D E, F R
the correlation between NCD and human judg-                                                                                            and E S use only the stem module.
ments. The right side corresponds to document
comparison and the left side to aggregated NCD
scores for sentences.                                                                                                                  lowercased C ONST category.
                                                                                                                                          Table 2 shows that real casing improves R ANK
                                                                                                                                       correlation slightly throughout NCD and mNCD
5.2                                                 mNCD against NCD                                                                   variants, whereas it reduces correlation in the cat-
Table 2 shows the average system level correlation                                                                                     egories C ONST, Y ES /N O as well as the mean.
of different NCD and mNCD variants for trans-                                                                                             The best mNCD (PPMZ rc) improves the best
lations into English. The two compressors that                                                                                         NCD (PPMZ rc) method by 15% in the R ANK
worked best in our experiments were PPMZ and                                                                                           category. In the C ONST category the best mNCD
bz2. PPMZ is slower to compute but performs                                                                                            (bz2 lc) improves the best NCD (bz2 lc) by 3.7%.
slightly better compared to bz2, except for the                                                                                        For the total average, the best mNCD (PPMZ rc)
                                                                                                                                       improves the the best NCD (bz2 lc) by 7.2%.
                                                                                                                                          Table 3 shows the correlation results for the
                                                                                                               Y ES /N O




                                                                                                                                       R ANK category by target language. As shown al-
                                                                                                     C ONST
                                                                                            R ANK




                                                                                                                           Mean




                                                                                                                                       ready in Table 2, mNCD clearly outperforms NCD
 Method                                                      Parameters                                                                for English. Correlations for other languages show
 mNCD                                                        PPMZ           rc              .69      .74       .80         .74         mixed results and on average, mNCD gives lower
  NCD                                                        PPMZ           rc              .60      .66       .71         .66         correlations than NCD.

 mNCD                                                         bz2           rc              .64      .73       .73         .70         5.3   mNCD versus other methods
  NCD                                                         bz2           rc              .57      .64       .69         .64         Table 4 presents the results for the selected mNCD
 mNCD                                                        PPMZ           lc              .66      .80       .79         .75         (PPMZ rc) and NCD (bz2 rc) variants along with
  NCD                                                        PPMZ           lc              .56      .79       .75         .70         the correlations for other MT evaluation methods
                                                                                                                                       from the WMT’08 data, based on the results in
 mNCD                                                         bz2           lc              .59      .85       .74         .73
                                                                                                                                       Callison-Burch et al. (2008). The results are av-
  NCD                                                         bz2           lc              .54      .82       .71         .69
                                                                                                                                       erages over language pairs into English, sorted
                                                                                                                                       by R ANK, which we consider the most signifi-
Table 2: Mean system level correlations over
                                                                                                                                       cant category. Although mNCD correlation with
all translation tasks into English for variants of
                                                                                                                                       human evaluations improved over NCD, the rank-
mNCD and NCD. Higher values are emphasized.
                                                                                                                                       ing among other measures was not affected. Lan-
Parameters are the compressor PPMZ or bz2 and
                                                                                                                                       guage and task specific results not shown here, re-
the preprocessing choice lowercasing (lc) or real
                                                                                                                                       veal very low mNCD and NCD correlations in the
casing (rc).
                                                                                                                                       Spanish-English news task, which significantly


                                                                                                                                  83


                                          Y ES /N O
                                                                                              Target Lang Corr




                                 C ONST
                                                                          Method




                         R ANK




                                                      Mean
                                                                                          DE     FR    ES    Mean
         Method
                                                                             posbleu      .75    .80   .75   .75
                 DP      .81     .66      .74         .73             posF4gram-am        .74    .82   .79   .74
              ULCh       .80     .68      .78         .75             posF4gram-gm        .74    .82   .79   .74
                 DR      .79     .53      .65         .66                       bleu      .47    .83   .80   .68
      meteor-ranking     .78     .55      .63         .65              NCD (bz2 rc)       .34    .85   .42   .66
                ULC      .77     .72      .81         .76                  svm-rank       .44    .80   .80   .66
             posbleu     .75     .69      .78         .74                     mbleu       .39    .77   .83   .63
                 SR      .75     .66      .76         .72           mNCD (PPMZ rc)        .37    .82   .38   .63
      posF4gram-gm       .74     .60      .71         .68            meteor-baseline      .43    .61   .84   .58
     meteor-baseline     .74     .60      .63         .66             meteor-ranking      .26    .70   .83   .55
      posF4gram-am       .74     .58      .69         .67                       mter      .26    .69   .73   .52
    mNCD (PPMZ rc)       .69     .74      .80         .74
     NCD (PPMZ rc)       .60     .66      .71         .66                         Mean    .47    .77   .72   .65
               mbleu     .50     .76      .70         .65
                                                                  Table 5: Average system-level correlations for the
                bleu     .50     .72      .74         .65
                                                                  R ANK category from English for NCD, mNCD
                mter     .38     .74      .68         .60
                                                                  and other MT evaluation measures.
           svm-rank      .37     .10      .23         .23
                 Mean    .67     .62      .69         .66
                                                                  similarized references. We believe there is poten-
Table 4: Average system-level correlations over                   tial for improvement in other languages as well if
translation tasks into English for NCD, mNCD                      synonym lexicons are available.
and other MT evaluations measures                                    We have also extended the basic NCD measure
                                                                  to scale between a document comparison mea-
degrades the averages. Considering the mean of                    sure and aggregated sentence-level measure. The
the categories instead, mNCD’s correlation of .74                 rather surprising result is that NCD produces quite
is third best together with ’posbleu’.                            similar scores with all block sizes. The different
   Table 5 shows the results from English. The ta-                result with Spanish may be caused by differences
ble is shorter since many of the better MT mea-                   in the data or problems in the calculations.
sures use language specific linguistic resources                     After using the same evaluation methodology as
that are not easily available for languages other                 in Callison-Burch et al. (2008), we have doubts
than English. mNCD performs competitively only                    whether it presents the most effective method ex-
for French, otherwise it falls behind NCD and                     ploiting all the given human evaluations in the best
other methods as already shown earlier.                           way. The system-level correlation measure only
                                                                  awards the winner of the ranking of five differ-
6   Discussion                                                    ent systems. If a system always scored second,
We have introduced a new MT evaluation mea-                       it would never be awarded and therefore be overly
sure, mNCD, which is based on normalized com-                     penalized. In addition, the human knowledge that
pression distance and METEOR’s relaxed align-                     gave the lower rankings is not exploited.
ment modules. The mNCD measure outperforms                           In future work with mNCD as an MT evalu-
NCD in English with all tested parameter com-                     ation measure, we are planning to evaluate syn-
binations, whereas results with other target lan-                 onym dictionaries for other languages than En-
guages are unclear. The improved correlations                     glish. The synonym module for English does
with mNCD did not change the position in the                      not distinguish between different senses of words.
R ANK category of the MT evaluation measures in                   Therefore, synonym lexicons found with statis-
the 2008 ACL WMT shared task.                                     tical methods might provide a viable alternative
   The improvement in English was expected on                     for manually constructed lexicons (Kauchak and
the grounds of the synonym module, and indicated                  Barzilay, 2006).
also by the larger number of affected words in the


                                                             84


References                                                    Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
                                                                2005. A paraphrase-based approach to machine
Abhaya Agarwal and Alon Lavie. 2008. METEOR,                    translation evaluation. Technical Report LAMP-
  M-BLEU and M-TER: evaluation metrics for high-                TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
  correlation with human rankings of machine trans-             versity of Maryland, College Park.
  lation output. In StatMT ’08: Proceedings of the
  Third Workshop on Statistical Machine Translation,          Ray Solomonoff. 1964. Formal theory of inductive
  pages 115–118, Morristown, NJ, USA. Association               inference. Part I. Information and Control,, 7(1):1–
  for Computational Linguistics.                                22.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:              Jaakko J. Väyrynen, Tero Tapiovaara, Kimmo Ket-
  An automatic metric for MT evaluation with im-                 tunen, and Marcus Dobrinkat. 2010. Normalized
  proved correlation with human judgments. In Pro-               compression distance as an automatic MT evalua-
  ceedings of the ACL Workshop on Intrinsic and Ex-              tion metric. In Proceedings of MT 25 years on. To
  trinsic Evaluation Measures for Machine Transla-               appear.
  tion and/or Summarization, pages 65–72, Ann Ar-
  bor, Michigan, June. Association for Computational
  Linguistics.

Chris Callison-Burch, Miles Osborne, and Philipp
  Koehn. 2006. Re-evaluating the role of BLEU
  in machine translation research. In Proceedings of
  EACL-2006, pages 249–256.

Chris Callison-Burch, Cameron Fordyce, Philipp
  Koehn, Christoph Monz, and Josh Schroeder. 2008.
  Further meta-evalutation of machine translation.
  ACL Workshop on Statistical Machine Translation.

Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim:
  performance and effects of translation fluency. Ma-
  chine Translation, 23(2-3):157–168.

Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering
  by compression. IEEE Transactions on Information
  Theory, 51:1523–1545.

David Kauchak and Regina Barzilay. 2006. Para-
  phrasing for automatic evaluation. In Proceedings
  of the main conference on Human Language Tech-
  nology Conference of the North American Chap-
  ter of the Association of Computational Linguistics,
  pages 455–462, Morristown, NJ, USA. Association
  for Computational Linguistics.

Kimmo Kettunen. 2009. Packing it all up in search for
  a language independent MT quality measure tool. In
  In Proceedings of LTC-09, 4th Language and Tech-
  nology Conference, pages 280–284, Poznan.

Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
  Bootstrapping word alignment via word packing. In
  Proceedings of the 45th Annual Meeting of the As-
  sociation of Computational Linguistics, pages 304–
  311, Prague, Czech Republic, June. Association for
  Computational Linguistics.

K. Papineni, S. Roukos, T. Ward, and W. Zhu.
  2001. BLEU: a method for automatic evaluation
  of machine translation. Technical Report RC22176
  (W0109-022), IBM Research Division, Thomas J.
  Watson Research Center.

Steven Parker. 2008. BADGER: A new machine trans-
   lation metric. In Metrics for Machine Translation
   Challenge 2008, Waikiki, Hawai’i, October. AMTA.


                                                         85

        Tackling Sparse Data Issue in Machine Translation Evaluation ∗

                       Ondřej Bojar, Kamil Kos, and David Mareček
            Charles University in Prague, Institute of Formal and Applied Linguistics
           {bojar,marecek}@ufal.mff.cuni.cz, kamilkos@email.cz




                       Abstract                                   Rank                pctrans eurotranxp             google
                                                                                                 b b                         b
                                                                                                                                   b
                                                                      0.6
    We illustrate and explain problems of                                                                                cu-bojar
                                                                                                                         b
                                                                                b
                                                                                    cu-tectomt                   uedin
    n-grams-based machine translation (MT)
    metrics (e.g. BLEU) when applied to                               0.4
                                                                         0.06       0.08               0.10   0.12               0.14 BLEU
    morphologically rich languages such as
    Czech. A novel metric SemPOS based
    on the deep-syntactic representation of the                   Figure 1: BLEU and human ranks of systems par-
    sentence tackles the issue and retains the                    ticipating in the English-to-Czech WMT09 shared
    performance for translation to English as                     task.
    well.
                                                                     Section 3 introduces and evaluates some new
1   Introduction                                                  variations of SemPOS (Kos and Bojar, 2009), a
                                                                  metric based on the deep syntactic representation
Automatic metrics of machine translation (MT)                     of the sentence performing very well for Czech as
quality are vital for research progress at a fast                 the target language. Aside from including depen-
pace. Many automatic metrics of MT quality have                   dency and n-gram relations in the scoring, we also
been proposed and evaluated in terms of correla-                  apply and evaluate SemPOS for English.
tion with human judgments while various tech-
niques of manual judging are being examined as                    2     Problems of BLEU
well, see e.g. MetricsMATR08 (Przybocki et al.,
2008)1 , WMT08 and WMT09 (Callison-Burch et                       BLEU (Papineni et al., 2002) is an established
al., 2008; Callison-Burch et al., 2009)2 .                        language-independent MT metric. Its correlation
                                                                  to human judgments was originally deemed high
   The contribution of this paper is twofold. Sec-
                                                                  (for English) but better correlating metrics (esp.
tion 2 illustrates and explains severe problems of a
                                                                  for other languages) were found later, usually em-
widely used BLEU metric (Papineni et al., 2002)
                                                                  ploying language-specific tools, see e.g. Przy-
when applied to Czech as a representative of lan-
                                                                  bocki et al. (2008) or Callison-Burch et al. (2009).
guages with rich morphology. We see this as an
                                                                  The unbeaten advantage of BLEU is its simplicity.
instance of the sparse data problem well known
                                                                     Figure 1 illustrates a very low correlation to hu-
for MT itself: too much detail in the formal repre-
                                                                  man judgments when translating to Czech. We
sentation leading to low coverage of e.g. a transla-
                                                                  plot the official BLEU score against the rank es-
tion dictionary. In MT evaluation, too much detail
                                                                  tablished as the percentage of sentences where a
leads to the lack of comparable parts of the hy-
                                                                  system ranked no worse than all its competitors
pothesis and the reference.
                                                                  (Callison-Burch et al., 2009). The systems devel-
      ∗
        This work has been supported by the grants EuroMa-        oped at Charles University (cu-) are described in
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003             Bojar et al. (2009), uedin is a vanilla configuration
of the Czech Republic), FP7-ICT-2009-4-247762 (Faust),
GA201/09/H057, GAUK 1163/2010, and MSM 0021620838.                of Moses (Koehn et al., 2007) and the remaining
We are grateful to the anonymous reviewers for further re-        ones are commercial MT systems.
search suggestions.
    1                                                                In a manual analysis, we identified the reasons
      http://nist.gov/speech/tests
                    /metricsmatr/2008/results/                    for the low correlation: BLEU is overly sensitive
    2
      http://www.statmt.org/wmt08 and wmt09                       to sequences and forms in the hypothesis matching


                                                             86
                          Proceedings of the ACL 2010 Conference Short Papers, pages 86–91,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


 Con-      Error                                                       firmed by the reference. This amounts to 34% of
 firmed Flags      1-grams    2-grams     3-grams     4-grams
 Yes       Yes       6.34%      1.58%       0.55%       0.29%          running unigrams, giving enough space to differ in
 Yes       No      36.93%     13.68%        5.87%       2.69%          human judgments and still remain unscored.
 No        Yes     22.33%     41.83%      54.64%      63.88%
 No        No      34.40%     42.91%      38.94%      33.14%              Figure 3 documents the issue across languages:
 Total n-grams      35,531     33,891      32,251      30,611          the lower the BLEU score itself (i.e. fewer con-
                                                                       firmed n-grams), the lower the correlation to hu-
Table 1: n-grams confirmed by the reference and                        man judgments regardless of the target language
containing error flags.                                                (WMT09 shared task, 2025 sentences per lan-
                                                                       guage).
the reference translation. This focus goes directly                       Figure 4 illustrates the overestimation of scores
against the properties of Czech: relatively free                       caused by too much attention to sequences of to-
word order allows many permutations of words                           kens. A phrase-based system like Moses (cu-
and rich morphology renders many valid word                            bojar) can sometimes produce a long sequence of
forms not confirmed by the reference.3 These                           tokens exactly as required by the reference, lead-
problems are to some extent mitigated if several                       ing to a high BLEU score. The framed words
reference translations are available, but this is of-                  in the illustration are not confirmed by the refer-
ten not the case.                                                      ence, but the actual error in these words is very
   Figure 2 illustrates the problem of “sparse data”                   severe for comprehension: nouns were used twice
in the reference. Due to the lexical and morpho-                       instead of finite verbs, and a misleading transla-
logical variance of Czech, only a single word in                       tion of a preposition was chosen. The output by
each hypothesis matches a word in the reference.                       pctrans preserves the meaning much better despite
In the case of pctrans, the match is even a false                      not scoring in either of the finite verbs and produc-
positive, “do” (to) is a preposition that should be                    ing far shorter confirmed sequences.
used for the “minus” phrase and not for the “end
of the day” phrase. In terms of BLEU, both hy-
                                                                       3       Extensions of SemPOS
potheses are equally poor but 90% of their tokens
were not evaluated.
                                                                       SemPOS (Kos and Bojar, 2009) is inspired by met-
   Table 1 estimates the overall magnitude of this
                                                                       rics based on overlapping of linguistic features in
issue: For 1-grams to 4-grams in 1640 instances
                                                                       the reference and in the translation (Giménez and
(different MT outputs and different annotators) of
                                                                       Márquez, 2007). It operates on so-called “tec-
200 sentences with manually flagged errors4 , we
                                                                       togrammatical” (deep syntactic) representation of
count how often the n-gram is confirmed by the
                                                                       the sentence (Sgall et al., 1986; Hajič et al., 2006),
reference and how often it contains an error flag.
                                                                       formally a dependency tree that includes only au-
The suspicious cases are n-grams confirmed by
                                                                       tosemantic (content-bearing) words.5 SemPOS as
the reference but still containing a flag (false posi-
                                                                       defined in Kos and Bojar (2009) disregards the
tives) and n-grams not confirmed despite contain-
                                                                       syntactic structure and uses the semantic part of
ing no error flag (false negatives).
                                                                       speech of the words (noun, verb, etc.). There are
   Fortunately, there are relatively few false posi-
                                                                       19 fine-grained parts of speech. For each semantic
tives in n-gram based metrics: 6.3% of unigrams
                                                                       part of speech t, the overlapping O(t) is set to zero
and far fewer higher n-grams.
                                                                       if the part of speech does not occur in the reference
   The issue of false negatives is more serious and
                                                                       or the candidate set and otherwise it is computed
confirms the problem of sparse data if only one
                                                                       as given in Equation 1 below.
reference is available. 30 to 40% of n-grams do
not contain any error and yet they are not con-
                                                                           5
                                                                            We use TectoMT (Žabokrtský and Bojar, 2008),
   3
      Condon et al. (2009) identify similar issues when eval-          http://ufal.mff.cuni.cz/tectomt/, for the lin-
uating translation to Arabic and employ rule-based normal-             guistic pre-processing. While both our implementation of
ization of MT output to improve the correlation. It is beyond          SemPOS as well as TectoMT are in principle freely avail-
the scope of this paper to describe the rather different nature        able, a stable public version has yet to be released. Our plans
of morphological richness in Czech, Arabic and also other              include experiments with approximating the deep syntactic
languages, e.g. German or Finnish.                                     analysis with a simple tagger, which would also decrease the
    4
      The dataset with manually flagged errors is available at         installation burden and computation costs, at the expense of
http://ufal.mff.cuni.cz/euromatrixplus/                                accuracy.


                                                                  87


                            SRC          Prague Stock Market falls to minus by the end of the trading day
                            REF          pražská burza se ke konci obchodovánı́ propadla do minusu
                            cu-bojar     praha stock market klesne k minus na konci obchodnı́ho dne
                            pctrans      praha trh cenných papı́rů padá minus do konce obchodnı́ho dne


Figure 2: Sparse data in BLEU evaluation: Large chunks of hypotheses are not compared at all. Only a
single unigram in each hypothesis is confirmed in the reference.


                    1
                                                                                 cs-en
                  0.8                                                                      en-fr
                                                                                            en-es   fr-en
   Correlation




                  0.6                                                      de-en         es-en
                  0.4
                  0.2                        en-cs hu-en
                    0                                en-de
                 -0.2
                     0.05              0.1                   0.15                  0.2              0.25    0.3
                                                                    BLEU score

Figure 3: BLEU correlates with its correlation to human judgments. BLEU scores around 0.1 predict
little about translation quality.


                                                           similarly as semantic roles do. There are 67
         X X                                               functor types in total.
                       min(cnt(w, t, ri ), cnt(w, t, ci ))    Using Functor instead of SemPOS increases the
          i∈I w∈ri ∩ci                                     number of word classes that independently require
O(t) = X X
                       max(cnt(w, t, ri ), cnt(w, t, ci )) a high overlap. For a contrast we also completely
         i∈I w∈ri ∪ci                                      remove the classification and use only one global
                                                   (1)     class (Void).
   The semantic part of speech is denoted t; ci               Deep Syntactic Relations in SemPOS. In
and ri are the candidate and reference translations        SemPOS, an autosemantic word of a class is con-
of sentence i, and cnt(w, t, rc) is the number of          firmed if its lemma matches the reference. We uti-
words w with type t in rc (the reference or the can-       lize the dependency relations at the tectogrammat-
didate). The matching is performed on the level of         ical layer to validate valence by refining the over-
lemmas, i.e. no morphological information is pre-          lap and requiring also the lemma of 1) the parent
served in ws. See Figure 5 for an example; the             (denoted “par”), or 2) all the children regardless of
sentence is the same as in Figure 4.                       their order (denoted “sons”) to match.
   The final SemPOS score is obtained by macro-               Combining BLEU and SemPOS. One of the
averaging over all parts of speech:                        major drawbacks of SemPOS is that it completely
                            1   X                          ignores word order. This is too coarse even for
              SemPOS =              O(t)           (2)     languages with relatively free word order like
                           |T |
                                t∈T                        Czech. Another issue is that it operates on lemmas
   where T is the set of all possible semantic parts       and it completely disregards correct word forms.
of speech types. (The degenerate case of blank             Thus, a weighted linear combination of SemPOS
candidate and reference has SemPOS zero.)                  and BLEU (computed on the surface representa-
                                                           tion of the sentence) should compensate for this.
3.1 Variations of SemPOS                                   For the purposes of the combination, we compute
This section describes our modifications of Sem-           BLEU only on unigrams up to fourgrams (denoted
POS. All methods are evaluated in Section 3.2.             BLEU1 , . . . , BLEU4 ) but including the brevity
   Different Classification of Autosemantic                penalty as usual. Here we try only a few weight
Words. SemPOS uses semantic parts of speech                settings in the linear combination but given a held-
to classify autosemantic words. The tectogram-             out dataset, one could optimize the weights for the
matical layer offers also a feature called Functor         best performance.
describing the relation of a word to its governor


                                                                    88


               SRC        Congress yields: US government can pump 700 billion dollars into banks
               REF        kongres ustoupil : vláda usa může do bank napumpovat 700 miliard dolarů
               cu-bojar   kongres výnosy : vláda usa může čerpadlo 700 miliard dolarů v bankách
               pctrans    kongres vynášı́ : us vláda může čerpat 700 miliardu dolarů do bank


Figure 4: Too much focus on sequences in BLEU: pctrans’ output is better but does not score well.
BLEU gave credit to cu-bojar for 1, 3, 5 and 8 fourgrams, trigrams, bigrams and unigrams, resp., but
only for 0, 0, 1 and 8 n-grams produced by pctrans. Confirmed sequences of tokens are underlined and
important errors (not considered by BLEU) are framed.
           REF        kongres/n ustoupit/v :/n vláda/n usa/n banka/n napumpovat/v 700/n miliarda/n dolar/n
           cu-bojar   kongres/n výnos/n :/n vláda/n usa/n moci/v čerpadlo/n 700/n miliarda/n dolar/n banka/n
           pctrans    kongres/n vynášet/v :/n us/n vláda/n čerpat/v 700/n miliarda/n dolar/n banka/n


Figure 5: SemPOS evaluates the overlap of lemmas of autosemantic words given their semantic part of
speech (n, v, . . . ). Underlined words are confirmed by the reference.


   SemPOS for English. The tectogrammatical                     To English: MetricsMATR08 (cn+ar:           1652),
layer is being adapted for English (Cinková et al.,                WMT08 News Articles (de: 199, fr: 251),
2004; Hajič et al., 2009) and we are able to use the               WMT08 Europarl (es: 190, fr: 183), WMT09
available tools to obtain all SemPOS features for                   (cz: 320, de: 749, es: 484, fr: 786, hu: 287)
English sentences as well.
                                                                To Czech: WMT08 News Articles (en: 267),
3.2   Evaluation of SemPOS and Friends                              WMT08 Commentary (en: 243), WMT09
                                                                    (en: 1425)
We measured the metric performance on data used
in MetricsMATR08, WMT09 and WMT08. For                              The MetricsMATR08 testset contained 4 refer-
the evaluation of metric correlation with human                 ence translations for each sentence whereas the re-
judgments at the system level, we used the Pearson              maining testsets only one reference.
correlation coefficient ρ applied to ranks. In case                 Correlation coefficients for English are shown
of a tie, the systems were assigned the average po-             in Table 2. The best metric is Voidpar closely fol-
sition. For example if three systems achieved the               lowed by Voidsons . The explanation is that Void
same highest score (thus occupying the positions                compared to SemPOS or Functor does not lose
1, 2 and 3 when sorted by score), each of them                  points by an erroneous assignment of the POS or
would obtain the average rank of 2 = 1+2+3       3  .           the functor, and that Voidpar profits from check-
When correlating ranks (instead of exact scores)                ing the dependency relations between autoseman-
and with this handling of ties, the Pearson coeffi-             tic words. The combination of BLEU and Sem-
cient is equivalent to Spearman’s rank correlation              POS6 outperforms both individual metrics, but in
coefficient.                                                    case of SemPOS only by a minimal difference.
   The MetricsMATR08 human judgments include                    Additionally, we confirm that 4-grams alone have
preferences for pairs of MT systems saying which                little discriminative power both when used as a
one of the two systems is better, while the WMT08               metric of their own (BLEU4 ) as well as in a lin-
and WMT09 data contain system scores (for up to                 ear combination with SemPOS.
5 systems) on the scale 1 to 5 for a given sentence.                The best metric for Czech (see Table 3) is a lin-
We assigned a human ranking to the systems based                ear combination of SemPOS and 4-gram BLEU
on the percent of time that their translations were             closely followed by other SemPOS and BLEUn
judged to be better than or equal to the translations           combinations. We assume this is because BLEU4
of any other system in the manual evaluation. We                can capture correctly translated fixed phrases,
converted automatic metric scores to ranks.                     which is positively reflected in human judgments.
   Metrics’ performance for translation to English              Including BLEU1 in the combination favors trans-
and Czech was measured on the following test-                   lations with word forms as expected by the refer-
sets (the number of human judgments for a given                     6
                                                                      For each n ∈ {1, 2, 3, 4}, we show only the best weight
source language in brackets):                                   setting for SemPOS and BLEUn .


                                                           89


      Metric                Avg    Best   Worst                    Metric                 Avg    Best   Worst
      Voidpar               0.75   0.89    0.60                    3·SemPOS+1·BLEU4       0.55   0.83    0.14
      Voidsons              0.75   0.90    0.54                    2·SemPOS+1·BLEU2       0.55   0.83    0.14
      Void                  0.72   0.91    0.59                    2·SemPOS+1·BLEU1       0.53   0.83    0.09
      Functorsons           0.72   1.00    0.43                    4·SemPOS+1·BLEU3       0.53   0.83    0.09
      GTM                   0.71   0.90    0.54                    SemPOS                 0.53   0.83    0.09
      4·SemPOS+1·BLEU2      0.70   0.93    0.43                    BLEU2                  0.43   0.83    0.09
      SemPOSpar             0.70   0.93    0.30                    SemPOSpar              0.37   0.53    0.14
      1·SemPOS+4·BLEU3      0.70   0.91    0.26                    Functorsons            0.36   0.53    0.14
      4·SemPOS+1·BLEU1      0.69   0.93    0.43                    GTM                    0.35   0.53    0.14
      NIST                  0.69   0.90    0.53                    BLEU4                  0.33   0.53    0.09
      SemPOSsons            0.69   0.94    0.40                    Void                   0.33   0.53    0.09
      SemPOS                0.69   0.95    0.30                    NIST                   0.33   0.53    0.09
      2·SemPOS+1·BLEU4      0.68   0.91    0.09                    Voidsons               0.33   0.53    0.09
      BLEU1                 0.68   0.87    0.43                    BLEU                   0.33   0.53    0.09
      BLEU2                 0.68   0.90    0.26                    BLEU3                  0.33   0.53    0.09
      BLEU3                 0.66   0.90    0.14                    BLEU1                  0.29   0.53   -0.03
      BLEU                  0.66   0.91    0.20                    SemPOSsons             0.28   0.42    0.03
      TER                   0.63   0.87    0.29                    Functorpar             0.23   0.40    0.14
      PER                   0.63   0.88    0.32                    Functor                0.21   0.40    0.09
      BLEU4                 0.61   0.90   -0.31                    Voidpar                0.16   0.53   -0.08
      Functorpar            0.57   0.83   -0.03                    PER                    0.12   0.53   -0.09
      Functor               0.55   0.82   -0.09                    TER                    0.07   0.53   -0.23

Table 2: Average, best and worst system-level cor-           Table 3: System-level correlation coefficients for
relation coefficients for translation to English from        English-to-Czech translation evaluated on 3 differ-
various source languages evaluated on 10 different           ent testsets.
testsets.
                                                             the sparse data issue. SemPOS was evaluated on
ence, thus allowing to spot bad word forms. In               translation to Czech and to English, scoring better
all cases, the linear combination puts more weight           than or comparable to many established metrics.
on SemPOS. Given the negligible difference be-
tween SemPOS alone and the linear combinations,
we see that word forms are not the major issue for           References
humans interpreting the translation—most likely              Ondřej Bojar, David Mareček, Václav Novák, Mar-
because the systems so far often make more im-                 tin Popel, Jan Ptáček, Jan Rouš, and Zdeněk
portant errors. This is also confirmed by the obser-           Žabokrtský. 2009. English-Czech MT in 2008. In
                                                               Proceedings of the Fourth Workshop on Statistical
vation that using BLEU alone is rather unreliable              Machine Translation, Athens, Greece, March. Asso-
for Czech and BLEU-1 (which judges unigrams                    ciation for Computational Linguistics.
only) is even worse. Surprisingly BLEU-2 per-
                                                             Chris Callison-Burch, Cameron Fordyce, Philipp
formed better than any other n-grams for reasons
                                                               Koehn, Christof Monz, and Josh Schroeder. 2008.
that have yet to be examined. The error metrics                Further meta-evaluation of machine translation. In
PER and TER showed the lowest correlation with                 Proceedings of the Third Workshop on Statisti-
human judgments for translation to Czech.                      cal Machine Translation, pages 70–106, Columbus,
                                                               Ohio, June. Association for Computational Linguis-
                                                               tics.
4   Conclusion
                                                             Chris Callison-Burch, Philipp Koehn, Christof Monz,
This paper documented problems of single-                      and Josh Schroeder. 2009. Findings of the 2009
reference BLEU when applied to morphologically                 workshop on statistical machine translation. In Pro-
rich languages such as Czech. BLEU suffers from                ceedings of the Fourth Workshop on Statistical Ma-
                                                               chine Translation, Athens, Greece. Association for
a sparse data problem, unable to judge the quality
                                                               Computational Linguistics.
of tokens not confirmed by the reference. This is
confirmed for other languages as well: the lower             Silvie Cinková, Jan Hajič, Marie Mikulová, Lu-
the BLEU score the lower the correlation to hu-                 cie Mladová, Anja Nedolužko, Petr Pajas, Jarmila
man judgments.                                                  Panevová, Jiřı́ Semecký, Jana Šindlerová, Josef
                                                                Toman, Zdeňka Urešová, and Zdeněk Žabokrtský.
   We introduced a refinement of SemPOS, an                     2004. Annotation of English on the tectogram-
automatic metric of MT quality based on deep-                   matical level.      Technical Report TR-2006-35,
syntactic representation of the sentence tackling               ÚFAL/CKL, Prague, Czech Republic, December.


                                                        90


Sherri Condon, Gregory A. Sanders, Dan Parvaz, Alan             Zdeněk Žabokrtský and Ondřej Bojar. 2008. TectoMT,
  Rubenstein, Christy Doran, John Aberdeen, and                   Developer’s Guide. Technical Report TR-2008-39,
  Beatrice Oshika. 2009. Normalization for Auto-                  Institute of Formal and Applied Linguistics, Faculty
  mated Metrics: English and Arabic Speech Transla-               of Mathematics and Physics, Charles University in
  tion. In MT Summit XII.                                         Prague, December.

Jesús Giménez and Lluı́s Márquez. 2007. Linguis-
   tic Features for Automatic Evaluation of Heteroge-
   nous MT Systems. In Proceedings of the Second
   Workshop on Statistical Machine Translation, pages
   256–264, Prague, June. Association for Computa-
   tional Linguistics.

Jan Hajič, Silvie Cinková, Kristýna Čermáková, Lu-
   cie Mladová, Anja Nedolužko, Petr Pajas, Jiřı́ Se-
   mecký, Jana Šindlerová, Josef Toman, Kristýna
   Tomšů, Matěj Korvas, Magdaléna Rysová, Kateřina
   Veselovská, and Zdeněk Žabokrtský. 2009. Prague
   English Dependency Treebank 1.0. Institute of For-
   mal and Applied Linguistics, Charles University in
   Prague, ISBN 978-80-904175-0-2, January.

Jan Hajič, Jarmila Panevová, Eva Hajičová, Petr
   Sgall, Petr Pajas, Jan Štěpánek, Jiřı́ Havelka,
   Marie Mikulová, Zdeněk Žabokrtský, and Magda
   Ševčı́ková Razı́mová. 2006. Prague Dependency
   Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-
   4.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
  Callison-Burch, Marcello Federico, Nicola Bertoldi,
  Brooke Cowan, Wade Shen, Christine Moran,
  Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
  Constantin, and Evan Herbst. 2007. Moses: Open
  Source Toolkit for Statistical Machine Translation.
  In ACL 2007, Proceedings of the 45th Annual Meet-
  ing of the Association for Computational Linguis-
  tics Companion Volume Proceedings of the Demo
  and Poster Sessions, pages 177–180, Prague, Czech
  Republic, June. Association for Computational Lin-
  guistics.

Kamil Kos and Ondřej Bojar. 2009. Evaluation of Ma-
  chine Translation Metrics for Czech as the Target
  Language. Prague Bulletin of Mathematical Lin-
  guistics, 92.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
  Jing Zhu. 2002. BLEU: a Method for Automatic
  Evaluation of Machine Translation. In ACL 2002,
  Proceedings of the 40th Annual Meeting of the As-
  sociation for Computational Linguistics, pages 311–
  318, Philadelphia, Pennsylvania.

M. Przybocki, K. Peterson, and S. Bronsart. 2008. Of-
  ficial results of the NIST 2008 ”Metrics for MA-
  chine TRanslation” Challenge (MetricsMATR08).

Petr Sgall, Eva Hajičová, and Jarmila Panevová. 1986.
  The Meaning of the Sentence and Its Semantic
  and Pragmatic Aspects. Academia/Reidel Publish-
  ing Company, Prague, Czech Republic/Dordrecht,
  Netherlands.


                                                           91

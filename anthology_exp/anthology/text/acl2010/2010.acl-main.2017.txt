              Exemplar-Based Models for Word Meaning In Context

                   Katrin Erk                           Sebastian Padó
            Department of Linguistics    Institut für maschinelle Sprachverarbeitung
           University of Texas at Austin               Stuttgart University
        katrin.erk@mail.utexas.edu pado@ims.uni-stuttgart.de



                     Abstract                                top 20 features for coach, we get match and team
                                                             (for the “trainer” sense) as well as driver and car
    This paper describes ongoing work on dis-                (for the “bus” sense). This problem has typically
    tributional models for word meaning in                   been approached by modifying the type vector for
    context. We abandon the usual one-vector-                a target to better match a given context (Mitchell
    per-word paradigm in favor of an exemplar                and Lapata, 2008; Erk and Padó, 2008; Thater et
    model that activates only relevant occur-                al., 2009).
    rences. On a paraphrasing task, we find                     In the terms of research on human concept rep-
    that a simple exemplar model outperforms                 resentation, which often employs feature vector
    more complex state-of-the-art models.                    representations, the use of type vectors can be un-
                                                             derstood as a prototype-based approach, which uses
1   Introduction                                             a single vector per category. From this angle, com-
Distributional models are a popular framework                puting prototypes throws away much interesting
for representing word meaning. They describe                 distributional information. A rival class of mod-
a lemma through a high-dimensional vector that               els is that of exemplar models, which memorize
records co-occurrence with context features over a           each seen instance of a category and perform cat-
large corpus. Distributional models have been used           egorization by comparing a new stimulus to each
in many NLP analysis tasks (Salton et al., 1975;             remembered exemplar vector.
McCarthy and Carroll, 2003; Salton et al., 1975), as            We can address the polysemy issue through an
well as for cognitive modeling (Baroni and Lenci,            exemplar model by simply removing all exem-
2009; Landauer and Dumais, 1997; McDonald and                plars that are “not relevant” for the present con-
Ramscar, 2001). Among their attractive properties            text, or conversely activating only the relevant
are their simplicity and versatility, as well as the         ones. For the coach example, in the context of
fact that they can be acquired from corpora in an            a text about motorways, presumably an instance
unsupervised manner.                                         like “The coach drove a steady 45 mph” would be
   Distributional models are also attractive as a            activated, while “The team lost all games since the
model of word meaning in context, since they do              new coach arrived” would not.
not have to rely on fixed sets of dictionary sense              In this paper, we present an exemplar-based dis-
with their well-known problems (Kilgarriff, 1997;            tributional model for modeling word meaning in
McCarthy and Navigli, 2009). Also, they can                  context, applying the model to the task of decid-
be used directly for testing paraphrase applicabil-          ing paraphrase applicability. With a very simple
ity (Szpektor et al., 2008), a task that has recently        vector representation and just using activation, we
become prominent in the context of textual entail-           outperform the state-of-the-art prototype models.
ment (Bar-Haim et al., 2007). However, polysemy              We perform an in-depth error analysis to identify
is a fundamental problem for distributional models.          stable parameters for this class of models.
Typically, distributional models compute a single
                                                             2   Related Work
“type” vector for a target word, which contains co-
occurrence counts for all the occurrences of the             Among distributional models of word, there are
target in a large corpus. If the target is polyse-           some approaches that address polysemy, either
mous, this vector mixes contextual features for all          by inducing a fixed clustering of contexts into
the senses of the target. For example, among the             senses (Schütze, 1998) or by dynamically modi-


                                                        92
                        Proceedings of the ACL 2010 Conference Short Papers, pages 92–97,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


fying a word’s type vector according to each given             Sentential context                           Paraphrase
                                                               After a fire extinguisher is used, it must   bring back (3),
sentence context (Landauer and Dumais, 1997;                   always be returned for recharging and        take back (2),
Mitchell and Lapata, 2008; Erk and Padó, 2008;                its use recorded.                            send back (1),
Thater et al., 2009). Polysemy-aware approaches                                                             give back (1)
                                                               We return to the young woman who is          come back (3),
also differ in their notion of context. Some use a             reading the Wrigley’s wrapping paper.        revert (1), revisit
bag-of-words representation of words in the cur-                                                            (1), go (1)
rent sentence (Schütze, 1998; Landauer and Du-
mais, 1997), some make use of syntactic con-                  Table 1: The Lexical Substitution (LexSub) dataset.
text (Mitchell and Lapata, 2008; Erk and Padó,
2008; Thater et al., 2009). The approach that we              letters for sets of exemplars.
present in the current paper computes a representa-              We model polysemy by activating relevant ex-
tion dynamically for each sentence context, using             emplars of a lemma E in a given sentence context
a simple bag-of-words representation of context.              s. (Note that we use E to refer to both a lemma
   In cognitive science, prototype models predict             and its exemplar set, and that s can be viewed as
degree of category membership through similar-                just another exemplar vector.) In general, we define
ity to a single prototype, while exemplar theory              activation of a set E by exemplar s as
represents a concept as a collection of all previ-
ously seen exemplars (Murphy, 2002). Griffiths et                 act(E, s) = {e ∈ E | sim(e, s) > θ(E, s)}
al. (2007) found that the benefit of exemplars over           where E is an exemplar set, s is the “point of com-
prototypes grows with the number of available ex-             parison”, sim is some similarity measure such as
emplars. The problem of representing meaning in               Cosine or Jaccard, and θ(E, s) is a threshold. Ex-
context, which we consider in this paper, is closely          emplars belong to the activated set if their similarity
related to the problem of concept combination in              to s exceeds θ(E, s).1 We explore two variants of
cognitive science, i.e., the derivation of representa-        activation. In kNN activation, the k most simi-
tions for complex concepts (such as “metal spoon”)            lar exemplars to s are activated by setting θ to the
given the representations of base concepts (“metal”           similarity of the k-th most similar exemplar. In
and “spoon”). While most approaches to concept                q-percentage activation, we activate the top q%
combination are based on prototype models, Voor-              of E by setting θ to the (100-q)-th percentile of the
spoels et al. (2009) show superior results for an             sim(e, s) distribution. Note that, while in the kNN
exemplar model based on exemplar activation.                  activation scheme the number of activated exem-
   In NLP, exemplar-based (memory-based) mod-                 plars is the same for every lemma, this is not the
els have been applied to many problems (Daele-                case for percentage activation: There, a more fre-
mans et al., 1999). In the current paper, we use an           quent lemma (i.e., a lemma with more exemplars)
exemplar model for computing distributional repre-            will have more exemplars activated.
sentations for word meaning in context, using the
context to activate relevant exemplars. Comparing             Exemplar activation for paraphrasing. A para-
representations of context, bag-of-words (BOW)                phrases is typically only applicable to a particular
representations are more informative and noisier,             sense of a target word. Table 1 illustrates this on
while syntax-based representations deliver sparser            two examples from the Lexical Substitution (Lex-
and less noisy information. Following the hypothe-            Sub) dataset (McCarthy and Navigli, 2009), both
sis that richer, topical information is more suitable         featuring the target return. The right column lists
for exemplar activation, we use BOW representa-               appropriate paraphrases of return in each context
tions of sentential context in the current paper.             (given by human annotators). 2 We apply the ex-
                                                              emplar activation model to the task of predicting
3   Exemplar Activation Models                                paraphrase felicity: Given a target lemma T in a
                                                              particular sentential context s, and given a list of
We now present an exemplar-based model for                       1
                                                                    In principle, activation could be treated not just as binary
meaning in context. It assumes that each target               inclusion/exclusion, but also as a graded weighting scheme.
lemma is represented by a set of exemplars, where             However, weighting schemes introduce a large number of
an exemplar is a sentence in which the target occurs,         parameters, which we wanted to avoid.
                                                                  2
                                                                    Each annotator was allowed to give up to three para-
represented as a vector. We use lowercase letters             phrases per target in context. As a consequence, the number
for individual exemplars (vectors), and uppercase             of gold paraphrases per target sentence varies.


                                                         93


potential paraphrases of T , the task is to predict               para-                actT      actP
which of the paraphrases are applicable in s.                     meter            kNN perc. kNN perc.
   Previous approaches (Mitchell and Lapata, 2008;                10               36.1 35.5 36.5 38.6
Erk and Padó, 2008; Erk and Padó, 2009; Thater                  20               36.2 35.2 36.2 37.9
et al., 2009) have performed this task by modify-                 30               36.1 35.3 35.8 37.8
ing the type vector for T to the context s and then               40               36.0 35.3 35.8 37.7
comparing the resulting vector T 0 to the type vec-               50               35.9 35.1 35.9 37.5
tor of a paraphrase candidate P . In our exemplar                 60               36.0 35.0 36.1 37.5
setting, we select a contextually adequate subset                 70               35.9 34.8 36.1 37.5
of contexts in which T has been observed, using                   80               36.0 34.7 36.0 37.4
T 0 = act(T, s) as a generalized representation of                90               35.9 34.5 35.9 37.3
meaning of target T in the context of s.                          no act.              34.6      35.7
   Previous approaches used all of P as a repre-                  random BL                 28.5
sentation for a paraphrase candidate P . However,
                                                              Table 2: Activation of T or P individually on the
P includes also irrelevant exemplars, while for a
                                                              full LexSub dataset (GAP evaluation)
paraphrase to be judged as good, it is sufficient that
one plausible reading exists. Therefore, we use
P 0 = act(P, s) to represent the paraphrase.                  sion (GAP), which interpolates the precision values
                                                              of top-n prediction lists for increasing n. Let G =
4   Experimental Evaluation                                   hq1 , . . . , qm i be the list of gold paraphrases with
                                                              gold weights hy1 , . . . , ym i. Let P = hp1 , . . . , pn i
Data. We evaluate our model on predicting para-
                                                              be the list of model predictions as ranked by the
phrases from the Lexical Substitution (LexSub)
                                                              model, and let hx1 , . . . , xn i be the gold weights
dataset (McCarthy and Navigli, 2009). This dataset
                                                              associated with them (assume xi = 0 if pi 6∈ G),
consists of 2000 instances of 200 target words in
                                                              where G ⊆ P . Let I(xi ) = 1P        if pi ∈ G, and zero
sentential contexts, with paraphrases for each tar-
                                                              otherwise. We write xi = 1i ik=1 xk for the av-
get word instance generated by up to 6 participants.
                                                              erage gold weight of the first i model predictions,
Paraphrases are ranked by the number of annota-
                                                              and analogously yi . Then
tors that chose them (cf. Table 1). Following Erk
and Padó (2008), we take the list of paraphrase can-                                                n
                                                                                            1        X
didates for a target as given (computed by pooling                GAP (P, G) = Pm                      I(xi )xi
                                                                                        j=1 I(yj )yj  i=1
all paraphrases that LexSub annotators proposed
for the target) and use the models to rank them for           Since the model may rank multiple paraphrases the
any given sentence context.                                   same, we average over 10 random permutations of
   As exemplars, we create bag-of-words co-                   equally ranked paraphrases. We report mean GAP
occurrence vectors from the BNC. These vectors                over all items in the dataset.
represent instances of a target word by the other
                                                              Results and Discussion. We first computed two
words in the same sentence, lemmatized and POS-
                                                              models that activate either the paraphrase or the
tagged, minus stop words. E.g., if the lemma
                                                              target, but not both. Model 1, actT, activates only
gnurge occurs twice in the BNC, once in the sen-
                                                              the target, using the complete P as paraphrase, and
tence “The dog will gnurge the other dog”, and
                                                              ranking paraphrases by sim(P, act(T, s)). Model
once in “The old windows gnurged”, the exemplar
                                                              2, actP, activates only the paraphrase, using s as
set for gnurge contains the vectors [dog-n: 2, other-
                                                              the target word, ranking by sim(act(P, s), s).
a:1] and [old-a: 1, window-n: 1]. For exemplar
                                                                 The results for these models are shown in Ta-
similarity, we use the standard Cosine similarity,
                                                              ble 2, with both kNN and percentage activation:
and for the similarity of two exemplar sets, the
                                                              kNN activation with a parameter of 10 means that
Cosine of their centroids.
                                                              the 10 closest neighbors were activated, while per-
Evaluation. The model’s prediction for an item                centage with a parameter of 10 means that the clos-
is a list of paraphrases ranked by their predicted            est 10% of the exemplars were used. Note first
goodness of fit. To evaluate them against a                   that we computed a random baseline (last row)
weighted list of gold paraphrases, we follow Thater           with a GAP of 28.5. The second-to-last row (“no
et al. (2009) in using Generalized Average Preci-             activation”) shows two more informed baselines.


                                                         94


The actT “no act” result (34.6) corresponds to a                 P activation (%) ⇒       10      20      30
prototype-based model that ranks paraphrase can-                T activation (kNN) ⇓
didates by the distance between their type vectors                        5               38.2    38.1    38.1
and the target’s type vector. Virtually all exem-                         10              37.6    37.8    37.7
plar models outperform this prototype model. Note                         20              37.3    37.4    37.3
also that both actT and actP show the best results                        40              37.2    37.2    36.1
for small values of the activation parameter. This
indicates paraphrases can be judged on the basis             Table 3: Joint activation of P and T on the full
of a rather small number of exemplars. Neverthe-             LexSub dataset (GAP evaluation)
less, actT and actP differ with regard to the details
of their optimal activation. For actT, a small ab-           we fix the actP activation level, we find compara-
solute number of activated exemplars (here, 20)              tively large performance differences between the
works best , while actP yields the best results for          T activation settings k=5 and k=10 (highly signif-
a small percentage of paraphrase exemplars. This             icant for 10% actP, and significant for 20% and
can be explained by the different functions played           30% actP). On the other hand, when we fix the
by actT and actP (cf. Section 3): Activation of the          actT activation level, changes in actP activation
paraphrase must allow a guess about whether there            generally have an insignificant impact.
is reasonable interpretation of P in the context s.             Somewhat disappointingly, we are not able to
This appears to require a reasonably-sized sample            surpass the best result for actP alone. This indicates
from P . In contrast, target activation merely has to        that – at least in the current vector space – the
counteract the sparsity of s, and activation of too          sparsity of s is less of a problem than the “dilution”
many exemplars from T leads to oversmoothing.                of s that we face when we representing the target
   We obtained significances by computing 95%                word by exemplars of T close to s. Note, however,
and 99% confidence intervals with bootstrap re-              that the numerically worse performance of the best
sampling. As a rule of thumb, we find that 0.4%              actTP model is still not significantly different from
difference in GAP corresponds to a significant dif-          the best actP model.
ference at the 95% level, and 0.7% difference in             Influence of POS and frequency. An analysis
GAP to significance at the 99% level. The four               of the results by target part-of-speech showed that
activation methods (i.e., columns in Table 2) are            the globally optimal parameters also yield the best
significantly different from each other, with the ex-        results for individual POS, even though there are
ception of the pair actT/kNN and actP/kNN (n.s.),            substantial differences among POS. For actT, the
so that we get the following order:                          best results emerge for all POS with kNN activation
                                                             with k between 10 and 30. For k=20, we obtain a
actP/perc > actP/kNN ≈ actT/kNN > actT/perc                  GAP of 35.3 (verbs), 38.2 (nouns), and 35.1 (adjec-
                                                             tives). For actP, the best parameter for all POS was
where > means “significantly outperforms”. In par-
                                                             activation of 10%, with GAPs of 36.9 (verbs), 41.4
ticular, the best method (actT/kNN) outperforms
                                                             (nouns), and 37.5 (adjectives). Interestingly, the
all other methods at p<0.01. Here, the best param-
                                                             results for actTP (verbs: 38.4, nouns: 40.6, adjec-
eter setting (10% activation) is also significantly
                                                             tives: 36.9) are better than actP for verbs, but worse
better than the next-one one (20% activation). With
                                                             for nouns and adjectives, which indicates that the
the exception of actT/perc, all activation methods
                                                             sparsity problem might be more prominent than for
significantly outperform the best baseline (actP, no
                                                             the other POS. In all three models, we found a clear
activation).
                                                             effect of target and paraphrase frequency, with de-
   Based on these observations, we computed a
                                                             teriorating performance for the highest-frequency
third model, actTP, that activates both T (by kNN)
                                                             targets as well as for the lemmas with the highest
and P (by percentage), ranking paraphrases by
                                                             average paraphrase frequency.
sim(act(P, s), act(T, s)). Table 3 shows the re-
sults. We find the overall best model at a similar           Comparison to other models. Many of the
location in parameter space as for actT and actP             other models are syntax-based and are therefore
(cf. Table 2), namely by setting the activation pa-          only applicable to a subset of the LexSub data.
rameters to small values. The sensitivity of the             We have re-evaluated our exemplar models on the
parameters changes considerably, though. When                subsets we used in Erk and Padó (2008, EP08, 367


                                                        95


                                   Models                             sult from activating a low absolute number of exem-
                         EP08      EP09       TDP09                   plars. Paraphrase representations are best activated
      EP08 dataset       27.4      NA         NA                      with a percentage-based threshold. Overall, we
      EP09 dataset       NA        32.2       36.5                    found that paraphrase activation had a much larger
                         actT      actP       actTP                   impact on performance than target activation, and
      EP08 dataset       36.5      38.0       39.9                    that drawing on target exemplars other than s to
      EP09 dataset       39.1      39.9       39.6                    represent the target meaning in context improved
                                                                      over using s itself only for verbs (Tab. 3). This sug-
Table 4: Comparison to other models on two sub-                       gests the possibility of considering T ’s activated
sets of LexSub (GAP evaluation)                                       paraphrase candidates as the representation of T in
                                                                      the context s, rather than some vector of T itself,
datapoints) and Erk and Padó (2009, EP09, 100 dat-                   in the spirit of Kintsch (2001).
apoints). The second set was also used by Thater et                      While it is encouraging that the best parameter
al. (2009, TDP09). The results in Table 4 compare                     settings involved the activation of only few exem-
these models against our best previous exemplar                       plars, computation with exemplar models still re-
models and show that our models outperform these                      quires the management of large numbers of vectors.
models across the board. 3 Due to the small sizes                     The computational overhead can be reduced by us-
of these datasets, statistical significance is more                   ing data structures that cut down on the number
difficult to attain. On EP09, the differences among                   of vector comparisons, or by decreasing vector di-
our models are not significant, but the difference                    mensionality (Gorman and Curran, 2006). We will
between them and the original EP09 model is.4 On                      experiment with those methods to determine the
EP08, all differences are significant except for actP                 tradeoff of runtime and accuracy for this task.
vs. actTP.                                                               Another area of future work is to move beyond
   We note that both the EP08 and the EP09                            bag-of-words context: It is known from WSD
datasets appear to be simpler to model than the                       that syntactic and bag-of-words contexts provide
complete Lexical Substitution dataset, at least by                    complementary information (Florian et al., 2002;
our exemplar-based models. This underscores an                        Szpektor et al., 2008), and we hope that they can be
old insight: namely, that direct syntactic neighbors,                 integrated in a more sophisticated exemplar model.
such as arguments and modifiers, provide strong                          Finally, we will to explore task-based evalua-
clues as to word sense.                                               tions. Relation extraction and textual entailment
                                                                      in particular are tasks where similar models have
5    Conclusions and Outlook                                          been used before (Szpektor et al., 2008).
                                                                         Acknowledgements. This work was supported
This paper reports on work in progress on an ex-                      in part by National Science Foundation grant IIS-
emplar activation model as an alternative to one-                     0845925, and by a Morris Memorial Grant from
vector-per-word approaches to word meaning in                         the New York Community Trust.
context. Exemplar activation is very effective in
handling polysemy, even with a very simple (and
sparse) bag-of-words vector representation. On                        References
both the EP08 and EP09 datasets, our models sur-                      R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
pass more complex prototype-based approaches                            2007. Semantic inference at the lexical-syntactic
(Tab. 4). It is also noteworthy that the exemplar                       level. In Proceedings of AAAI, pages 871–876, Van-
                                                                        couver, BC.
activation models work best when few exemplars
are used, which bodes well for their efficiency.                      M. Baroni and A. Lenci. 2009. One distributional
   We found that the best target representations re-                    memory, many semantic spaces. In Proceedings of
                                                                        the EACL Workshop on Geometrical Models of Nat-
    3
      Since our models had the advantage of being tuned on              ural Language Semantics, Athens, Greece.
the dataset, we also report the range of results across the
parameters we tested. On the EP08 dataset, we obtained 33.1–          W. Daelemans, A. van den Bosch, and J. Zavrel. 1999.
36.5 for actT; 33.3–38.0 for actP; 37.7-39.9 for actTP. On the          Forgetting exceptions is harmful in language learn-
EP09 dataset, the numbers were 35.8–39.1 for actT; 38.1–39.9            ing. Machine Learning, 34(1/3):11–43. Special Is-
for actP; 37.2–39.8 for actTP.                                          sue on Natural Language Learning.
    4
      We did not have access to the TDP09 predictions to do
significance testing.                                                 K. Erk and S. Padó. 2008. A structured vector space


                                                                 96


  model for word meaning in context. In Proceedings             I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
  of EMNLP, pages 897–906, Honolulu, HI.                           2008. Contextual preferences. In Proceedings of
                                                                   ACL, pages 683–691, Columbus, OH.
K. Erk and S. Padó. 2009. Paraphrase assessment in
  structured vector space: Exploring parameters and             S. Thater, G. Dinu, and M. Pinkal. 2009. Ranking
  datasets. In Proceedings of the EACL Workshop on                 paraphrases in context. In Proceedings of the ACL
  Geometrical Models of Natural Language Seman-                    Workshop on Applied Textual Inference, pages 44–
  tics, Athens, Greece.                                            47, Singapore.

R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.           W. Voorspoels, W. Vanpaemel, and G. Storms. 2009.
   2002. Combining classifiers for word sense disam-              The role of extensional information in conceptual
   biguation. Journal of Natural Language Engineer-               combination. In Proceedings of CogSci.
   ing, 8(4):327–341.

J. Gorman and J. R. Curran. 2006. Scaling distribu-
   tional similarity to large corpora. In Proceedings of
   ACL, pages 361–368, Sydney.

T. Griffiths, K. Canini, A. Sanborn, and D. J. Navarro.
   2007. Unifying rational models of categorization
   via the hierarchical Dirichlet process. In Proceed-
   ings of CogSci, pages 323–328, Nashville, TN.

A. Kilgarriff. 1997. I don’t believe in word senses.
  Computers and the Humanities, 31(2):91–113.

W. Kintsch. 2001. Predication. Cognitive Science,
  25:173–202.

T. Landauer and S. Dumais. 1997. A solution to Platos
   problem: the latent semantic analysis theory of ac-
   quisition, induction, and representation of knowl-
   edge. Psychological Review, 104(2):211–240.

D. McCarthy and J. Carroll. 2003. Disambiguating
  nouns, verbs, and adjectives using automatically ac-
  quired selectional preferences. Computational Lin-
  guistics, 29(4):639–654.

D. McCarthy and R. Navigli. 2009. The English lexi-
  cal substitution task. Language Resources and Eval-
  uation, 43(2):139–159. Special Issue on Compu-
  tational Semantic Analysis of Language: SemEval-
  2007 and Beyond.

S. McDonald and M. Ramscar. 2001. Testing the dis-
   tributional hypothesis: The influence of context on
   judgements of semantic similarity. In Proceedings
   of CogSci, pages 611–616.

J. Mitchell and M. Lapata. 2008. Vector-based models
   of semantic composition. In Proceedings of ACL,
   pages 236–244, Columbus, OH.

G. L. Murphy. 2002. The Big Book of Concepts. MIT
  Press.

G Salton, A Wang, and C Yang. 1975. A vector-
  space model for information retrieval. Journal of the
  American Society for Information Science, 18:613–
  620.

H. Schütze. 1998. Automatic word sense discrimina-
  tion. Computational Linguistics, 24(1):97–124.


                                                           97

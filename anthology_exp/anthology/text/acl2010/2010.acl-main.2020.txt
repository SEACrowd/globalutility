         Collocation Extraction beyond the Independence Assumption
                                             Gerlof Bouma
                              Universität Potsdam, Department Linguistik
                                      Campus Golm, Haus 24/35
                                    Karl-Liebknecht-Straße 24–25
                                       14476 Potsdam, Germany
                               gerlof.bouma@uni-potsdam.de


                        Abstract                               Two aspects of (P)MI are worth highlighting.
                                                            First, the observed occurrence probability pobs is
    In this paper we start to explore two-part              compared to the expected occurrence probability
    collocation extraction association measures             pexp . Secondly, the independence assumption un-
    that do not estimate expected probabili-                derlies the estimation of pexp .
    ties on the basis of the independence as-                  The first aspect is motivated by the observa-
    sumption. We propose two new measures                   tion that interesting combinations are often those
    based upon the well-known measures of                   that are unexpectedly frequent. For instance, the
    mutual information and pointwise mutual                 bigramme of the is uninteresting from a colloca-
    information. Expected probabilities are de-             tion extraction perspective, although it probably is
    rived from automatically trained Aggregate              amongst the most frequent bigrammes for any En-
    Markov Models. On three collocation gold                glish corpus. However, we can expect to frequently
    standards, we find the new association mea-             observe the combination by mere chance, simply
    sures vary in their effectiveness.                      because its parts are so frequent. Looking at pobs
1   Introduction                                            and pexp together allows us to recognize these cases
                                                            (Manning and Schütze (1999) and Evert (2007) for
Collocation extraction typically proceeds by scor-          more discussion).
ing collocation candidates with an association mea-            The second aspect, the independence assump-
sure, where high scores are taken to indicate likely        tion in the estimation of pexp , is more problem-
collocationhood. Two well-known such measures               atic, however, even in the context of collocation
are pointwise mutual information (PMI) and mu-              extraction. As Evert (2007, p42) notes, the assump-
tual information (MI). In terms of observing a com-         tion of “independence is extremely unrealistic,” be-
bination of words w1 , w2 , these are:                      cause it ignores “a variety of syntactic, semantic
                           p(w1 , w2 )                      and lexical restrictions.” Consider an estimate for
       i (w1 , w2 ) = log               ,           (1)     pexp (the the). Under independence, this estimate
                          p(w1 ) p(w2 )
                                                            will be high, as the itself is very frequent. However,
                          X
       I (w1 , w2 ) =        p(x, y) i(x, y).       (2)     with our knowledge of English syntax, we would
                                                            say pexp (the the) is low. The independence assump-
                     x∈{w1 ,¬w1 }
                     y∈{w2 ,¬w2 }                           tion leads to overestimated expectation and the the
                                                            will need to be very frequent for it to show up as a
PMI (1) is the logged ratio of the observed bi-             likely collocation. A less contrived example of how
gramme probability and the expected bigramme                the independence assumption might mislead collo-
probability under independence of the two words             cation extraction is when bigramme distribution is
in the combination. MI (2) is the expected outcome          influenced by compositional, non-collocational, se-
of PMI, and measures how much information of the            mantic dependencies. Investigating adjective-noun
distribution of one word is contained in the distribu-      combinations in a corpus, we might find that beige
tion of the other. PMI was introduced into the collo-       cloth gets a high PMI, whereas beige thought does
cation extraction field by Church and Hanks (1990).         not. This does not make the former a collocation or
Dunning (1993) proposed the use of the likelihood-          multiword unit. Rather, what we would measure is
ratio test statistic, which is equivalent to MI up to       the tendency to use colours with visible things and
a constant factor.                                          not with abstract objects. Syntactic and semantic


                                                          109
                       Proceedings of the ACL 2010 Conference Short Papers, pages 109–114,
                 Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


associations between words are real dependencies,                      according to:
but they need not be collocational in nature. Be-                                       P
                                                                                             n(w1 , w)p(c|w1 , w)
cause of the independence assumption, PMI and                                p(c|w1 ) ← P w               0
                                                                                                                     ,       (4)
MI measure these syntactic and semantic associa-                                         w,c0 n(w1 , w)p(c |w1 , w)
                                                                                        P
tions just as much as they measure collocational                                            n(w, w2 )p(c|w, w2 )
                                                                             p(w2 |c) ← P w           0          0
                                                                                                                   ,         (5)
association. In this paper, we therefore experimen-                                      w,w0 n(w, w )p(c|w, w )
tally investigate the use of a more informed pexp in
the context of collocation extraction.                             where n(w1 , w2 ) are bigramme counts and the pos-
                                                                   terior probability of a hidden category c is esti-
2       Aggregate Markov Models                                    mated by:

To replace pexp under independence, one might                                                      p(c|w1 )p(w2 |c)
                                                                               p(c|w1 , w2 ) = P         0            0
                                                                                                                         .   (6)
consider models with explicit linguistic infor-                                                    c0 p(c |w1 )p(w2 |c )
mation, such as a POS-tag bigramme model.
                                                                       Successive updates converge to a local maximum
This would for instance give us a more realistic
                                                                       of the AMM’s log-likelihood.
pexp (the the). However, lexical semantic informa-
                                                                          The definition of the counterparts to (P)MI with-
tion is harder to incorporate. We might not know
                                                                       out the independence assumption, the AMM-ratio
exactly what factors are needed to estimate pexp
                                                                       and AMM-divergence, is now straightforward:
and even if we do, we might lack the resources
to train the resulting models. The only thing we                                                    p(w1 , w2 )
know about estimating pexp is that we need more                            ramm (w1 , w2 ) = log                     , (7)
                                                                                               p(w1 ) pamm (w2 |w1 )
information than a unigramme model but less than                                               X
a bigramme model (as this would make pobs /pexp                            damm (w1 , w2 ) =      p(x, y) ramm (x, y). (8)
uninformative). Therefore, we propose to use Ag-                                           x∈{w1 ,¬w1 }
gregate Markov Models (Saul and Pereira, 1997;                                             y∈{w2 ,¬w2 }
Hofmann and Puzicha, 1998; Rooth et al., 1999;
Blitzer et al., 2005)1 for the task of estimating pexp .           The free parameter in these association measures is
In an AMM, bigramme probability is not directly                    the number of hidden classes in the AMM, that is,
modeled, but mediated by a hidden class variable c:                the amount of dependency between the bigramme
                                                                   parts used to estimate pexp . Note that AMM-ratio
                                                                   and AMM-divergence with one hidden class are
                             X
         pamm (w2 |w1 ) =         p(c|w1 )p(w2 |c).       (3)
                              c                                    equivalent to PMI and MI, respectively. It can be
                                                                   expected that in different corpora and for differ-
The number of classes in an AMM determines the                     ent types of collocation, different settings of this
amount of dependency that can be captured. In the                  parameter are suitable.
case of just one class, AMM is equivalent to a uni-
gramme model. AMMs become equivalent to the                            3     Evaluation
full bigramme model when the number of classes                         3.1    Data and procedure
equals the size of the smallest of the vocabular-
ies of the parts of the combination. Between these                 We apply AMM-ratio and AMM-divergence to
two extremes, AMMs can capture syntactic, lexical,                 three collocation gold standards. The effectiveness
semantic and even pragmatic dependencies.                          of association measures in collocation extraction is
                                                                   measured by ranking collocation candidates after
   AMMs can be trained with EM, using no more
                                                                   the scores defined by the measures, and calculat-
information than one would need for ML bigramme
                                                                   ing average precision of these lists against the gold
probability estimates. Specifications of the E- and
                                                                   standard annotation. We consider the newly pro-
M-steps can be found in any of the four papers cited
                                                                   posed AMM-based measures for a varying number
above – here we follow Saul and Pereira (1997). At
                                                                   of hidden categories. The new measures are com-
each iteration, the model components are updated
                                                                   pared against two baselines: ranking by frequency
    1
     These authors use very similar models, but with differing     (pobs ) and random ordering. Because AMM-ratio
terminology and with different goals. The term AMM is used         and -divergence with one hidden class boil down
in the first and fourth paper. In the second paper, the models
are referred to as Separable Mixture Models. Their use in          to PMI and MI (and thus log-likelihood ratio), the
collocation extraction is to our knowledge novel.                  evaluation contains an implicit comparison with


                                                                 110


these canonical measures, too. However, the re-          AMMs become rich enough to not only capture
sults will not be state-of-the-art: for the datasets     the broadly applicative distributional influences of
investigated below, there are more effective extrac-     syntax and semantics, but also provide accurate
tion methods based on supervised machine learning        pexp s for individual, distributionally deviant combi-
(Pecina, 2008).                                          nations – like collocations. An accurate pexp results
   The first gold standard used is the German            in a low association score.
adjective-noun dataset (Evert, 2008). It contains            One way of inspecting what kind of dependen-
1212 A-N pairs taken from a German newspaper             cies the AMMs pick up is to cluster the data with
corpus. We consider three subtasks, depending on         them. Following Blitzer et al. (2005), we take the
how strict we define true positives. We used the         200 most frequent adjectives and assign them to
bigramme frequency data included in the resource.        the category that maximizes p(c|w1 ); likewise for
We assigned all types with a token count ≤5 to one       nouns and p(w2 |c). Four selected clusters (out of
type, resulting in AMM training data of 10k As,          16) are given in Table 2.2 The esoteric class 1 con-
20k Ns and 446k A-N pair types.                          tains ordinal numbers and nouns that one typically
   The second gold standard consists of 5102 Ger-        uses those with, including references to temporal
man PP-verb combinations, also sampled from              concepts. Class 2 and 3 appear more semantically
newspaper texts (Krenn, 2008). The data con-             motivated, roughly containing human and collec-
tains annotation for support verb constructions          tive denoting nouns, respectively. Class 4 shows
(FVGs) and figurative expressions. This resource         a group of adjectives denoting colours and/or po-
also comes with its own frequency data. After fre-       litical affiliations and a less coherent set of nouns,
quency thresholding, AMMs are trained on 46k             although the noun cluster can be understood if we
PPs, 7.6k Vs, and 890k PP-V pair types.                  consider individual adjectives that are associated
   Third and last is the English verb-particle con-      with this class. Our informal impression from look-
struction (VPC) gold standard (Baldwin, 2008),           ing at clusters is that this is a common situation: as
consisting of 3078 verb-particle pairs and annota-       a whole, a cluster cannot be easily characterized,
tion for transitive and intransitive idiomatic VPCs.     although for subsets or individual pairs, one can
We extract frequency data from the BNC, follow-          get an intuition for why they are in the same class.
ing the methods described in Baldwin (2005). This        Unfortunately, we also see that some actual collo-
results in two slightly different datasets for the two   cations are clustered in class 4, such as gelbe Karte
types of VPC. For the intransitive VPCs, we train        ‘warning’ (lit.: ‘yellow card’) and dickes Auto ‘big
AMMs on 4.5k Vs, 35 particles, and 43k pair types.       (lit.: fat) car’.
For the transitive VPCs, we have 5k Vs, 35 parti-
                                                         German PP-Verb collocations The second slice
cles and 54k pair types.
                                                         in Table 1 shows that, for both subtypes of PP-V
   All our EM runs start with randomly initialized
                                                         collocation, better pexp -estimates lead to decreased
model vectors. In Section 3.3 we discuss the impact
                                                         average precision. The most effective AMM-ratio
of model variation due to this random factor.
                                                         and -distance measures are those equivalent to
                                                         (P)MI. Apparently, the better pexp s are unfortunate
3.2   Results
                                                         for the extraction of the type of collocations in this
German A-N collocations The top slice in Ta-             dataset.
ble 1 shows results for the three subtasks of the           The poor performance of PMI on these data –
A-N dataset. We see that using AMM-based pexp            clearly below frequency – has been noticed before
initially improves average precision, for each task      by Krenn and Evert (2001). A possible explanation
and for both the ratio and the divergence measure.       for the lack of improvement in the AMMs lies in
At their maxima, the informed measures outper-           the relatively high performing frequency baselines.
form both baselines as well as PMI and MI/log-           The frequency baseline for FVGs is five times the
likelihood ratio (# classes=1). The AMM-ratio per-          2
forms best for 16-class AMMs, the optimum for                 An anonymous reviewer rightly warns against sketching
                                                         an overly positive picture of the knowledge captured in the
AMM-divergence varies slightly.                          AMMs by only presenting a few clusters. However, the clus-
   It is likely that the drop in performance for the     tering performed here is only secondary to our main goal
                                                         of improving collocation extraction. The model inspection
larger AMM-based measures is due to the AMMs             should thus not be taken as an evaluation of the quality of the
learning the collocations themselves. That is, the       models as clustering models.


                                                     111


                                                                        # classes
                                1         2        4        8          16        32      64      128      256      512      Rnd      Frq
A-N
 category 1         ramm       45.6     46.4     47.6      47.3       48.3      48.0    47.0     46.1     44.7     41.9    30.1     32.2
                    damm       42.3     42.9     44.4      45.2       46.1      46.5    45.0     46.3     45.5     45.5
 category 1–2       ramm       55.7     56.3     57.4      57.5       58.1      58.1    57.7     56.9     55.7     52.8    43.1     47.0
                    damm       56.3     57.0     58.1      58.4       59.8      60.1    59.3     60.6     59.2     59.3
 category 1–3       ramm       62.3     62.8     63.9      64.0       64.4      62.2    62.2     62.7     62.4     60.0    52.7     56.4
                    damm       64.3     64.7     65.9      66.6       66.7      66.3    66.3     65.4     66.0     64.7
PP-V
 figurative         ramm        7.5      6.1      6.4       6.0        5.6       5.4     4.5      4.2      3.8      3.5      3.3    10.5
                    damm       14.4     13.0     13.3      13.1       12.2      11.2     9.0      7.7      6.9      5.7
 FVG                ramm        4.1      3.4      3.4       3.0        2.9       2.7     2.2      2.1      2.0      2.0      3.0    14.7
                    damm       15.3     12.7     12.6      10.7        9.0       7.7     3.4      3.2      2.5      2.3
VPC
 intransitive       ramm        9.3      9.2      9.0       8.3        5.5       5.3                                         4.8    14.7
                    damm       12.2     12.2     14.0      16.3        6.9       5.8
 transitive         ramm       16.4     14.8     15.2      14.5       11.3      10.0                                       10.1     20.1
                    damm       19.6     17.3     20.7      23.8       12.8      10.1

     Table 1: Average precision for AMM-based association measures and baselines on three datasets.

Cl                            Adjective                                                                 Noun
1 dritt ‘third’, erst ‘first’, fünft ‘fifth’, halb ‘half’, kommend     Jahr ‘year’, Klasse ‘class’, Linie ‘line’, Mal ‘time’, Monat
  ‘next’, laufend ‘current’, letzt ‘last’, nah ‘near’, paar ‘pair’,     ‘month’, Platz ‘place’, Rang ‘grade’, Runde ‘round’, Saison
   vergangen ‘last’, viert ‘fourth’, wenig ‘few’, zweit ‘sec-           ‘season’, Satz ‘sentence’, Schritt ‘step’, Sitzung ‘session’, Son-
   ond’                                                                  ntag ‘Sunday’, Spiel ‘game’, Stunde ‘hour’, Tag ‘day’, Woche
                                                                        ‘week’, Wochenende ‘weekend’
2 aktiv ‘active’, alt ‘old’, ausländisch ‘foreign’, betroffen          Besucher ‘visitor’, Bürger ‘citizens’, Deutsche ‘German’, Frau
  ‘concerned’, jung ‘young’, lebend ‘alive’, meist ‘most’,              ‘woman’, Gast ‘guest’, Jugendliche ‘youth’, Kind ‘child’, Leute
  unbekannt ‘unknown’, viel ‘many’                                      ‘people’, Mädchen ‘girl’, Mann ‘man’, Mensch ‘human’, Mit-
                                                                         glied ‘member’
3 deutsch ‘German’, europäisch ‘European’, ganz ‘whole’,               Betrieb ‘company’, Familie ‘family’, Firma ‘firm’, Gebiet
   gesamt ‘whole’, international ‘international’, national ‘na-         ‘area’, Gesellschaft ‘society’, Land ‘country’, Mannschaft
   tional’, örtlich ‘local’, ostdeutsch ‘East-German’, privat          ‘team’, Markt ‘market’, Organisation ‘organisation’, Staat
  ‘private’, rein ‘pure’, sogenannt ‘so-called’, sonstig ‘other’,       ‘state’, Stadtteil ‘city district’, System ‘system’, Team ‘team’,
   westlich ‘western’                                                   Unternehmen ‘enterprise’, Verein ‘club’, Welt ‘world’
4 blau ‘blue’, dick ‘fat’, gelb ‘yellow’, grün ‘green’, linke          Auge ‘eye’, Auto ‘car’, Haar ‘hair’, Hand ‘hand’, Karte ‘card’,
  ‘left’, recht ‘right’, rot ‘red’, schwarz ‘black’, white ‘weiß’       Stimme ‘voice/vote’

                           Table 2: Selected adjective-noun clusters from a 16-class AMM.


random baseline, and MI does not outperform it by                           average precision here approaches the random base-
much. Since the AMMs provide a better fit for the                           line. The VPC extraction task shows a difference
more frequent pairs in the training data, they might                        between the two AMM-based measures: AMM-
end up providing too good pexp -estimates for the                           ratio does not improve at all, remaining below the
true collocations from the beginning.                                       frequency baseline. AMM-divergence, however,
   Further investigation is needed to find out                              shows a slight decrease in precision first, but ends
whether this situation can be ameliorated and, if                           up performing above the frequency baseline for the
not, whether we can systematically identify for                             8-class AMMs in both subtasks.
what kind of collocation extraction tasks using bet-                           Table 3 shows four clusters of verbs and par-
ter pexp s is simply not a good idea.                                       ticles. The large first cluster contains verbs that
                                                                            involve motion/displacement of the subject or ob-
English Verb-Particle constructions The last                                ject and associated particles, for instance walk
gold standard is the English VPC dataset, shown                             about or push away. Interestingly, the description
in the bottom slice of Table 1. We have only used                           of the gold standard gives exactly such cases as
class-sizes up to 32, as there are only 35 particle                         negatives, since they constitute compositional verb-
types. We can clearly see the effect of the largest                         particle constructions (Baldwin, 2008). Classes 2
AMMs approaching the full bigramme model as                                 and 3 show syntactic dependencies, which helps


                                                                      112


Cl                                    Verb                                                           Particle
1 break, bring, come, cut, drive, fall, get, go, lay, look, move, pass, push,   across, ahead, along, around, away, back, back-
  put, run, sit, throw, turn, voice, walk                                       ward, down, forward, into, over, through, together
2 accord, add, apply, give, happen, lead, listen, offer, pay, present, refer,   astray, to
  relate, return, rise, say, sell, send, speak, write
3 know, talk, tell, think                                                       about
4 accompany, achieve, affect, cause, create, follow, hit, increase, issue,      by
  mean, produce, replace, require, sign, support

               Table 3: Selected verb-particle clusters from an 8-class AMM on transitive data.


collocation extraction by decreasing the impact of                     in Table 4 shows this combined estimator leads to
verb-preposition associations that are due to PP-                      good extraction results.
selecting verbs. Class 4 shows a third type of distri-
butional generalization: the verbs in this class are                   4   Conclusions
all frequently used in the passive.                                  In this paper, we have started to explore collocation
                                                                     extraction beyond the assumption of independence.
3.3      Variation due to local optima
                                                                     We have introduced two new association measures
We start each EM run with a random initializa-                       that do away with this assumption in the estima-
tion of the model parameters. Since EM finds local                   tion of expected probabilities. The success of using
rather than global optima, each run may lead to                      these association measures varies. It remains to be
different AMMs, which in turn will affect AMM-                       investigated whether they can be improved more.
based collocation extraction. To gain insight into                      A possible obstacle in the adoption of AMMs in
this variation, we have trained 40 16-class AMMs                     collocation extraction is that we have not provided
on the A-N dataset. Table 4 gives five point sum-                    any heuristic for setting the number of classes for
maries of the average precision of the resulting                     the AMMs. We hope to be able to look into this
40 ‘association measures’. Performance varies con-                   question in future research. Luckily, for the AN and
siderably, spanning 2–3 percentage points in each                    VPC data, the best models are not that large (in the
case. The models consistently outperform (P)MI in                    order of 8–32 classes), which means that model fit-
Table 1, though.                                                     ting is fast enough to experiment with different set-
   Several techniques might help to address this                     tings. In general, considering these smaller models
variation. One might try to find a good fixed way of                 might suffice for tasks that have a fairly restricted
initializing EM or to use EM variants that reduce                    definition of collocation candidate, like the tasks
the impact of the initial state (Smith and Eisner,                   in our evaluation do. Because AMM fitting is un-
2004, a.o.), so that a run with the same data and                    supervised, selecting a class size is in this respect
the same number of classes will always learn (al-                    no different from selecting a suitable association
most) the same model. On the assumption that an                      measure from the canon of existing measures.
average over several runs will vary less than indi-                     Future research into association measures that
vidual runs, we have also constructed a combined                     are not based on the independence assumption will
pexp by averaging over 40 pexp s. The last column                    also include considering different EM variants and
                                                                     other automatically learnable models besides the
                                                                     AMMs used in this paper. Finally, the idea of us-
                        Variation in avg precision                   ing an informed estimate of expected probability
                       min q1 med q3               max    Comb       in an association measure need not be confined
A-N                                                                  to (P)MI, as there are many other measures that
 cat 1       ramm      46.5   47.3   47.9   48.4   49.1   48.4       employ expected probabilities.
             damm      44.4   45.4   45.8   46.1   47.1   46.4
 cat 1–2     ramm      56.7   57.2   57.9   58.2   59.0   58.2
             damm      58.1   58.8   59.2   59.4   60.4   60.0         Acknowledgements
 cat 1–3     ramm      63.0   63.7   64.2   64.6   65.3   64.6
             damm      65.2   66.0   66.4   66.6   67.6   66.9
                                                                     This research was carried out in the context of
                                                                     the SFB 632 Information Structure, subproject D4:
Table 4: Variation on A-N data over 40 EM runs                       Methoden zur interaktiven linguistischen Korpus-
and result of combining pexp s.                                      analyse von Informationsstruktur.


                                                                 113


References                                                   Lawrence Saul and Fernando Pereira. 1997. Aggre-
                                                               gate and mixed-order markov models for statistical
Timothy Baldwin. 2005. The deep lexical acquisition            language processing. In Proceedings of the Second
  of english verb-particle constructions. Computer             Conference on Empirical Methods in Natural Lan-
  Speech and Language, Special Issue on Multiword              guage Processing, pages 81–89.
  Expressions, 19(4):398–414.
Timothy Baldwin. 2008. A resource for evaluating the         Noah A. Smith and Jason Eisner. 2004. Anneal-
  deep lexical acquisition of English verb-particle con-       ing techniques for unsupervised statistical language
  structions. In Proceedings of the LREC 2008 Work-            learning. In Proceedings of the 42nd Annual Meet-
  shop Towards a Shared Task for Multiword Expres-             ing of the Association for Computational Linguis-
  sions (MWE 2008), pages 1–2, Marrakech.                      tics.

John Blitzer, Amir Globerson, and Fernando Pereira.
  2005. Distributed latent variable models of lexical
  co-occurrences. In Tenth International Workshop on
  Artificial Intelligence and Statistics.
Kenneth W. Church and Patrick Hanks. 1990. Word
  association norms, mutual information, and lexicog-
  raphy. Computational Linguistics, 16(1):22–29.
Ted Dunning. 1993. Accurate methods for the statis-
  tics of surprise and coincidence. Computational Lin-
  guistics, 19(1):61–74.
Stefan Evert. 2007. Corpora and collocations. Ex-
   tended Manuscript of Chapter 58 of A. Lüdeling and
   M. Kytö, 2008, Corpus Linguistics. An International
   Handbook, Mouton de Gruyter, Berlin.
Stefan Evert. 2008. A lexicographic evaluation of Ger-
   man adjective-noun collocations. In Proceedings of
   the LREC 2008 Workshop Towards a Shared Task
   for Multiword Expressions (MWE 2008), pages 3–6,
   Marrakech.
Thomas Hofmann and Jan Puzicha. 1998. Statisti-
  cal models for co-occurrence data. Technical report,
  MIT. AI Memo 1625, CBCL Memo 159.
Brigitte Krenn and Stefan Evert. 2001. Can we do
  better than frequency? a case study on extracting PP-
  verb collocations. In Proceedings of the ACL Work-
  shop on Collocations, Toulouse.
Brigitte Krenn. 2008. Description of evaluation re-
  source – German PP-verb data. In Proceedings of
  the LREC 2008 Workshop Towards a Shared Task
  for Multiword Expressions (MWE 2008), pages 7–
  10, Marrakech.
Chris Manning and Hinrich Schütze. 1999. Foun-
  dations of Statistical Natural Language Processing.
  MIT Press, Cambridge, MA.
Pavel Pecina. 2008. A machine learning approach to
  multiword expression extraction. In Proceedings of
  the LREC 2008 Workshop Towards a Shared Task
  for Multiword Expressions (MWE 2008), pages 54–
  57, Marrakech.
Mats Rooth, Stefan Riester, Detlef Prescher, Glenn Car-
 rol, and Franz Beil. 1999. Inducing a semantically
 annotated lexicon via em-based clustering. In Pro-
 ceedings of the 37th Annual Meeting of the Associ-
 ation for Computational Linguistics, College Park,
 MD.


                                                       114

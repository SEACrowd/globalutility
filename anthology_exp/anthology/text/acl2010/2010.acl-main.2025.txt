                   Word Alignment with Synonym Regularization

                    Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
                      NTT Communication Science Laboratories, NTT Corp.
                    2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
                     {shindo,a.fujino}@cslab.kecl.ntt.co.jp
                            nagata.masaaki@lab.ntt.co.jp


                    Abstract                                 information works as a constraint in word align-
                                                             ment models and improves word alignment qual-
    We present a novel framework for word                    ity.
    alignment that incorporates synonym                         A large number of monolingual lexical seman-
    knowledge collected from monolingual                     tic resources such as WordNet (Miller, 1995) have
    linguistic resources in a bilingual proba-               been constructed in more than fifty languages
    bilistic model. Synonym information is                   (Sagot and Fiser, 2008). They include word-
    helpful for word alignment because we                    level relations such as synonyms, hypernyms and
    can expect a synonym to correspond to                    hyponyms. Synonym information is particularly
    the same word in a different language.                   helpful for word alignment because we can ex-
    We design a generative model for word                    pect a synonym to correspond to the same word
    alignment that uses synonym information                  in a different language. In this paper, we explore a
    as a regularization term. The experimental               method for using synonym information effectively
    results show that our proposed method                    to improve word alignment quality.
    significantly improves word alignment                       In general, synonym relations are defined in
    quality.                                                 terms of word sense, not in terms of word form. In
                                                             other words, synonym relations are usually con-
1 Introduction
                                                             text or domain dependent. For instance, ‘head’
Word alignment is an essential step in most phrase           and ‘chief’ are synonyms in contexts referring to
and syntax based statistical machine translation             working environment, while ‘head’ and ‘forefront’
(SMT). It is an inference problem of word cor-               are synonyms in contexts referring to physical po-
respondences between different languages given               sitions. It is difficult, however, to imagine a con-
parallel sentence pairs. Accurate word alignment             text where ‘chief’ and ‘forefront’ are synonyms.
can induce high quality phrase detection and trans-          Therefore, it is easy to imagine that simply replac-
lation probability, which leads to a significant im-         ing all occurrences of ‘chief’ and ‘forefront’ with
provement in SMT performance. Many word                      ‘head’ do sometimes harm with word alignment
alignment approaches based on generative mod-                accuracy, and we have to model either the context
els have been proposed and they learn from bilin-            or senses of words.
gual sentences in an unsupervised manner (Vo-                   We propose a novel method that incorporates
gel et al., 1996; Och and Ney, 2003; Fraser and              synonyms from monolingual resources in a bilin-
Marcu, 2007).                                                gual word alignment model. We formulate a syn-
   One way to improve word alignment quality                 onym pair generative model with a topic variable
is to add linguistic knowledge derived from a                and use this model as a regularization term with a
monolingual corpus. This monolingual knowl-                  bilingual word alignment model. The topic vari-
edge makes it easier to determine corresponding              able in our synonym model is helpful for disam-
words correctly. For instance, functional words              biguating the meanings of synonyms. We extend
in one language tend to correspond to functional             HM-BiTAM, which is a HMM-based word align-
words in another language (Deng and Gao, 2007),              ment model with a latent topic, with a novel syn-
and the syntactic dependency of words in each lan-           onym pair generative model. We applied the pro-
guage can help the alignment process (Ma et al.,             posed method to an English-French word align-
2008). It has been shown that such grammatical               ment task and successfully improved the word


                                                       137
                      Proceedings of the ACL 2010 Conference Short Papers, pages 137–141,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                                        translation probability from {e to }f under the kth
                                                                        topic: p (f |e, z = k ). T = Ti,i′ is a state tran-
                                                                        sition probability of a first order Markov process.
                                                                        Fig. 1 shows a graphical model of HM-BiTAM.
                                                                           The total likelihood of bilingual sentence pairs
                                                                        {E, F } can be obtained by marginalizing out la-
                                                                        tent variables z, a and θ,

                                                                                              ∑∑
                                                                           p (F, E; Ψ) =                       p (F, E, z, a, θ; Ψ) dθ,   (1)
                                                                                                   z       a


                                                                           where Ψ = {α, β, T, B} is a parameter set. In
      Figure 1: Graphical model of HM-BiTAM                             this model, we can infer word alignment a by max-
                                                                        imizing the likelihood above.
alignment quality.
                                                                        3 Proposed Method
2     Bilingual Word Alignment Model                                    3.1 Synonym Pair Generative Model
In this section, we review a conventional gener-                        We design a generative model for synonym pairs
ative word alignment model, HM-BiTAM (Zhao                              {f, f ′ } in language F , which assumes that the
and Xing, 2008).                                                        synonyms are collected from monolingual linguis-
   HM-BiTAM is a bilingual generative model                             tic resources. We assume that each synonym pair
with topic z, alignment a and topic weight vec-                         (f, f ′ ) is generated independently given the same
tor θ as latent variables. Topic variables such                         ‘sense’ s. Under this assumption, the probability
as ‘science’ or ‘economy’ assigned to individual                        of synonym pair (f, f ′ ) can be formulated as,
sentences help to disambiguate the meanings of                                      (      ) ∑            (      )
                                                                                   p f, f ′ ∝  p (f |s ) p f ′ |s p (s) .                 (2)
words. HM-BiTAM assumes that the nth bilin-                                                            s
gual sentence pair, (En , Fn ), is generated under a
                                                                           We define a pair (e, k) as a representation of
given latent topic zn ∈ {1, . . . , k, . . . , K}, where
                                                                        the sense s, where e and k are a word in a dif-
K is the number of latent topics. Let N be the
                                                                        ferent language E and a latent topic, respectively.
number of sentence pairs, and In and Jn be the
                                                                        It has been shown that a word e in a different
lengths of En and Fn , respectively. In this frame-
                                                                        language is an appropriate representation of s in
work, all of the bilingual sentence pairs {E, F } =
                                                                        synonym modeling (Bannard and Callison-Burch,
{(En , Fn )}N
            n=1 are generated as follows.                               2005). We assume that adding a latent topic k for
    1. θ ∼ Dirichlet (α): sample topic-weight vector                    the sense is very useful for disambiguating word
                                                                        meaning, and thus that (e, k) gives us a good ap-
    2. For each sentence pair (En , Fn )
                                                                        proximation of s. Under this assumption, the syn-
        (a) zn ∼ M ultinomial (θ): sample the topic                     onym pair generative model can be defined as fol-
        (b) en,i:In |zn ∼ p (En |zn ; β ): sample English               lows.
              words from a monolingual unigram model given
              topic zn
                                                                                   ({  } )
        (c)   For each position jn = 1, . . . , Jn                             p          e
                                                                                 f, f ′ ; Ψ
                i. ajn ∼ p (ajn |ajn −1 ; T ): sample an align-                  ∏ ∑
                   ment link ajn from a first order Markov pro-                ∝                     e
                                                                                          p(f |e, k; Ψ)p(f ′        e
                                                                                                             |e, k; Ψ)p(e,    e
                                                                                                                           k; Ψ),(3)
                   cess                                                             (f,f ′ ) e,k
               ii. fjn ∼ p (fjn |En , ajn , zn ; B ): sample a
                   target word fjn given an aligned source                      e is the parameter set of our model.
                                                                          where Ψ
                   word and topic

where alignment ajn = i denotes source word ei                          3.2 Word Alignment with Synonym
and target word fjn are aligned. α is a parame-                             Regularization
ter over the topic weight vector θ, β = {βk,e } is                      In this section, we extend the bilingual genera-
the source word probability given the kth topic:                        tive model (HM-BiTAM) with our synonym pair
p (e |z = k ). B = {Bf,e,k } represents the word                        model. Our expectation is that synonym pairs


                                                                  138


                                                                    4 Experiments
                                                                    4.1 Experimental Setting
                                                                    For an empirical evaluation of the proposed
                                                                    method, we used a bilingual parallel corpus of
                                                                    English-French Hansards (Mihalcea and Pedersen,
                                                                    2003). The corpus consists of over 1 million sen-
                                                                    tence pairs, which include 447 manually word-
                                                                    aligned sentences. We selected 100 sentence pairs
                                                                    randomly from the manually word-aligned sen-
Figure 2: Graphical model of synonym pair gen-
                                                                    tences as development data for tuning the regu-
erative process
                                                                    larization weight ζ, and used the 347 remaining
                                                                    sentence pairs as evaluation data. We also ran-
correspond to the same word in a different lan-                     domly selected 10k, 50k, and 100k sized sentence
guage, thus they make it easy to infer accurate                     pairs from the corpus as additional training data.
word alignment. HM-BiTAM and the synonym                            We ran the unsupervised training of our proposed
model share parameters in order to incorporate                      word alignment model on the additional training
monolingual synonym information into the bilin-                     data and the 347 sentence pairs of the evaluation
gual word alignment model. This can be achieved                     data. Note that manual word alignment of the
                     e in eq. 3 as,
via reparameterizing Ψ                                              347 sentence pairs was not used for the unsuper-
                                                                    vised training. After the unsupervised training, we
        (          )                                                evaluated the word alignment performance of our
                 e
       p f e, k; Ψ       ≡    p (f |e, k; B ) ,         (4)
          (        )
                                                                    proposed method by comparing the manual word
                 e
         p e, k; Ψ       ≡    p (e |k; β ) p (k; α) .   (5)         alignment of the 347 sentence pairs with the pre-
                                                                    diction provided by the trained model.
  Overall, we re-define the synonym pair model                         We collected English and French synonym pairs
with the HM-BiTAM parameter set Ψ,                                  from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4
                                                                    (Sagot and Fiser, 2008), respectively. WOLF is a
          {      }                                                  semantic resource constructed from the Princeton
        p( f, f ′ ; Ψ)
                 1     ∏ ∑                                          WordNet and various multilingual resources. We
          ∝ ∑              αk βk,e Bf,e,k Bf ′ ,e,k . (6)           selected synonym pairs where both words were in-
                   α
                k′ k
                     ′
                        ′
                       (f,f ) k,e
                                                                    cluded in the bilingual training set.
   Fig. 2 shows a graphical model of the synonym                       We compared the word alignment performance
pair generative process. We estimate the param-                     of our model with that of GIZA++ 1.03 1 (Vo-
eter values to maximize the likelihood of HM-                       gel et al., 1996; Och and Ney, 2003), and HM-
BiTAM with respect to bilingual sentences and                       BiTAM (Zhao and Xing, 2008) implemented by
that of the synonym model with respect to syn-                      us. GIZA++ is an implementation of IBM-model
onym pairs collected from monolingual resources.                    4 and HMM, and HM-BiTAM corresponds to ζ =
Namely, the parameter estimate, Ψ̂, is computed                     0 in eq. 7. We adopted K = 3 topics, following
as                                                                  the setting in (Zhao and Xing, 2006).
                                                                       We trained the word alignment in two direc-
             {                         {      }    }                tions: English to French, and French to English.
 Ψ̂ = arg max log p(F, E; Ψ) + ζ log p( f, f ′ ; Ψ) ,
         Ψ                                                          The alignment results for both directions were re-
                                                        (7)
                                                                    fined with ‘GROW’ heuristics to yield high preci-
   where ζ is a regularization weight that should                   sion and high recall in accordance with previous
be set for training. We can expect that the second                  work (Och and Ney, 2003; Zhao and Xing, 2006).
term of eq. 7 to constrain parameter set Ψ and                      We evaluated these results for precision, recall, F-
avoid overfitting for the bilingual word alignment                  measure and alignment error rate (AER), which
model. We resort to the variational EM approach                     are standard metrics for word alignment accuracy
(Bernardo et al., 2003) to infer Ψ̂ following HM-                   (Och and Ney, 2000).
BiTAM. We omit the parameter update equation
                                                                       1
due to lack of space.                                                      http://fjoch.com/GIZA++.html


                                                              139


       10k        Precision Recall F-measure AER                   # vocabularies        10k       50k     100k
 GIZA++ standard    0.856 0.718 0.781 0.207                     English    standard      8578     16924    22817
          with SRH 0.874 0.720 0.789 0.198                                with SRH       5435     7235     13978
                                                                French     standard     10791     21872    30294
HM-BiTAM standard   0.869 0.788 0.826 0.169
                                                                          with SRH       9737     20077    27970
          with SRH 0.884 0.790 0.834 0.160
     Proposed       0.941 0.808 0.870 0.123
                         (a)                                  Table 2: The number of vocabularies in the 10k,
                                                              50k and 100k data sets.
       50k        Precision Recall F-measure AER
 GIZA++ standard    0.905 0.770 0.832 0.156
          with SRH 0.903 0.759 0.825 0.164                    were replaced with the word ‘sick’. As shown in
HM-BiTAM standard   0.901 0.814 0.855 0.140                   Table 2, the number of vocabularies in the English
          with SRH 0.899 0.808 0.853 0.145
                                                              and French data sets decreased as a result of em-
     Proposed       0.947 0.824 0.881 0.112
                                                              ploying the SRH.
                         (b)
                                                                  We show the performance of GIZA++ and HM-
       100k       Precision Recall F-measure AER              BiTAM with the SRH in the lines entitled “with
 GIZA++ standard    0.925 0.791 0.853 0.136                   SRH” in Table 1. The GIZA++ and HM-BiTAM
          with SRH 0.934 0.803 0.864 0.126                    with the SRH slightly outperformed the standard
HM-BiTAM standard   0.898 0.851 0.874 0.124                   GIZA++ and HM-BiTAM for the 10k and 100k
          with SRH 0.909 0.860 0.879 0.114
     Proposed       0.927 0.862 0.893 0.103
                                                              data sets, but underperformed with the 50k data
                                                              set. We assume that the SRH mitigated the over-
                         (c)
                                                              fitting of these models into low-frequency word
                                                              pairs in bilingual sentences, and then improved the
Table 1: Comparison of word alignment accuracy.
                                                              word alignment performance. The SRH regards
The best results are indicated in bold type. The
                                                              all of the different words coupled with the same
additional data set sizes are (a) 10k, (b) 50k, (c)
                                                              word in the synonym pairs as synonyms. For in-
100k.
                                                              stance, the words ‘head’, ‘chief’ and ‘forefront’ in
                                                              the bilingual sentences are replaced with ‘chief’,
4.2 Results and Discussion                                    since (‘head’, ‘chief’) and (‘head’, ‘forefront’) are
                                                              synonyms. Obviously, (‘chief’, ‘forefront’) are
Table 1 shows the word alignment accuracy of the              not synonyms, which is detrimented to word align-
three methods trained with 10k, 50k, and 100k ad-             ment.
ditional sentence pairs. For all settings, our pro-               The proposed method consistently outper-
posed method outperformed other conventional                  formed GIZA++ and HM-BiTAM with the SRH
methods. This result shows that synonym infor-                in 10k, 50k and 100k data sets in F-measure.
mation is effective for improving word alignment              The synonym pair model in our proposed method
quality as we expected.                                       can automatically learn that (‘head’, ‘chief’) and
   As mentioned in Sections 1 and 3.1, the main               (‘head’, ‘forefront’) are individual synonyms with
idea of our proposed method is to introduce la-               different meanings by assigning these pairs to dif-
tent topics for modeling synonym pairs, and then              ferent topics. By sharing latent topics between
to utilize the synonym pair model for the regu-               the synonym pair model and the word alignment
larization of word alignment models. We expect                model, the synonym information incorporated in
the latent topics to be useful for modeling poly-             the synonym pair model is used directly for train-
semous words included in synonym pairs and to                 ing word alignment model. The experimental re-
enable us to incorporate synonym information ef-              sults show that our proposed method was effec-
fectively into word alignment models. To con-                 tive in improving the performance of the word
firm the effect of the synonym pair model with                alignment model by using synonym pairs includ-
latent topics, we also tested GIZA++ and HM-                  ing such ambiguous synonym words.
BiTAM with what we call Synonym Replacement                       Finally, we discuss the data set size used for un-
Heuristics (SRH), where all of the synonym pairs              supervised training. As shown in Table 1, using
in the bilingual training sentences were simply re-           a large number of additional sentence pairs im-
placed with a representative word. For instance,              proved the performance of all the models. In all
the words ‘sick’ and ‘ill’ in the bilingual sentences         our experimental settings, all the additional sen-


                                                        140


tence pairs and the evaluation data were selected               Y. Deng and Y. Gao. 2007. Guiding statistical word
from the Hansards data set. These experimental                     alignment models with prior knowledge. In Pro-
                                                                   ceedings of the 45th Annual Meeting of the As-
results show that a larger number of sentence pairs
                                                                   sociation of Computational Linguistics, pages 1–8,
was more effective in improving word alignment                     Prague, Czech Republic, June. Association for Com-
performance when the sentence pairs were col-                      putational Linguistics.
lected from a homogeneous data source. However,
                                                                A. Fraser and D. Marcu. 2007. Getting the struc-
in practice, it might be difficult to collect a large             ture right for word alignment: LEAF. In Pro-
number of such homogeneous sentence pairs for                     ceedings of the 2007 Joint Conference on Empirical
a specific target domain and language pair. One                   Methods in Natural Language Processing and Com-
direction for future work is to confirm the effect                putational Natural Language Learning (EMNLP-
                                                                  CoNLL), pages 51–60, Prague, Czech Republic,
of the proposed method when training the word                     June. Association for Computational Linguistics.
alignment model by using a large number of sen-
tence pairs collected from various data sources in-             Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008.
                                                                   Improving word alignment using syntactic depen-
cluding many topics for a specific language pair.
                                                                   dencies. In Proceedings of the ACL-08: HLT Sec-
                                                                   ond Workshop on Syntax and Structure in Statisti-
5   Conclusions and Future Work                                    cal Translation (SSST-2), pages 69–77, Columbus,
                                                                   Ohio, June. Association for Computational Linguis-
We proposed a novel framework that incorpo-                        tics.
rates synonyms from monolingual linguistic re-                  R. Mihalcea and T. Pedersen. 2003. An evaluation
sources in a word alignment generative model.                     exercise for word alignment. In Proceedings of the
This approach utilizes both bilingual and mono-                   HLT-NAACL 2003 Workshop on building and using
lingual synonym resources effectively for word                    parallel texts: data driven machine translation and
                                                                  beyond-Volume 3, page 10. Association for Compu-
alignment. Our proposed method uses a latent
                                                                  tational Linguistics.
topic for bilingual sentences and monolingual syn-
onym pairs, which is helpful in terms of word                   G. A. Miller. 1995. WordNet: a lexical database for
sense disambiguation. Our proposed method im-                     English. Communications of the ACM, 38(11):41.
proved word alignment quality with both small                   F. J. Och and H. Ney. 2000. Improved statistical align-
and large data sets. Future work will involve ex-                  ment models. In Proceedings of the 38th Annual
amining the proposed method for different lan-                     Meeting on Association for Computational Linguis-
                                                                   tics, pages 440–447. Association for Computational
guage pairs such as English-Chinese and English-                   Linguistics Morristown, NJ, USA.
Japanese and evaluating the impact of our pro-
posed method on SMT performance. We will also                   F. J. Och and H. Ney. 2003. A systematic comparison
apply our proposed method to a larger data sets                    of various statistical alignment models. Computa-
                                                                   tional Linguistics, 29(1):19–51.
of multiple domains since we can expect a fur-
ther improvement in word alignment accuracy if                  B. Sagot and D. Fiser. 2008. Building a free French
we use more bilingual sentences and more mono-                     wordnet from multilingual resources. In Proceed-
                                                                   ings of Ontolex.
lingual knowledge.
                                                                S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
                                                                   based word alignment in statistical translation. In
References                                                         Proceedings of the 16th Conference on Computa-
                                                                   tional Linguistics-Volume 2, pages 836–841. Asso-
C. Bannard and C. Callison-Burch. 2005. Paraphras-                 ciation for Computational Linguistics Morristown,
   ing with bilingual parallel corpora. In Proceed-                NJ, USA.
   ings of the 43rd Annual Meeting on Association for
                                                                B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual
   Computational Linguistics, pages 597–604. Asso-
                                                                  topic admixture models for word alignment. In Pro-
   ciation for Computational Linguistics Morristown,
                                                                  ceedings of the COLING/ACL on Main Conference
   NJ, USA.
                                                                  Poster Sessions, page 976. Association for Compu-
                                                                  tational Linguistics.
J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P.
   Dawid, D. Heckerman, A. F. M. Smith, and M. West.            B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual
   2003. The variational bayesian EM algorithm for in-             topic exploration, word alignment, and translation.
   complete data: with application to scoring graphical            In Advances in Neural Information Processing Sys-
   model structures. In Bayesian Statistics 7: Proceed-            tems 20, pages 1689–1696, Cambridge, MA. MIT
   ings of the 7th Valencia International Meeting, June            Press.
   2-6, 2002, page 453. Oxford University Press, USA.


                                                          141

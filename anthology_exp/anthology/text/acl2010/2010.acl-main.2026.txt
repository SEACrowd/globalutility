         Better Filtration and Augmentation for Hierarchical Phrase-Based
                                 Translation Rules
             Zhiyang Wang †            Yajuan Lü †           Qun Liu †                Young-Sook Hwang ‡
†                                                     ‡
    Key Lab. of Intelligent Information Processing      HILab Convergence Technology Center
         Institute of Computing Technology                          C&I Business
           Chinese Academy of Sciences                               SKTelecom
       P.O. Box 2704, Beijing 100190, China        11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea
           wangzhiyang@ict.ac.cn                           yshwang@sktelecom.com

                         Abstract                                              f             f’
                                                                                                          Source

        This paper presents a novel filtration cri-
        terion to restrict the rule extraction for                                 e
                                                                                                          Target

        the hierarchical phrase-based translation
        model, where a bilingual but relaxed well-                Figure 1: Solid wire reveals the dependency rela-
        formed dependency restriction is used to                  tion pointing from the child to the parent. Target
        filter out bad rules. Furthermore, a new                  word e is triggered by the source word f and it’s
        feature which describes the regularity that               head word f ′ , p(e|f → f ′ ).
        the source/target dependency edge trig-
        gers the target/source word is also pro-
        posed. Experimental results show that, the                   Based on the relaxed-well-formed dependency
        new criteria weeds out about 40% rules                    structure, we also introduce a new linguistic fea-
        while with translation performance im-                    ture to enhance translation performance. In the
        provement, and the new feature brings an-                 traditional phrase-based SMT model, there are
        other improvement to the baseline system,                 always lexical translation probabilities based on
        especially on larger corpus.                              IBM model 1 (Brown et al., 1993), i.e. p(e|f ),
                                                                  namely, the target word e is triggered by the source
    1 Introduction                                                word f . Intuitively, however, the generation of e
                                                                  is not only involved with f , sometimes may also
    Hierarchical phrase-based (HPB) model (Chiang,
                                                                  be triggered by other context words in the source
    2005) is the state-of-the-art statistical machine
                                                                  side. Here we assume that the dependency edge
    translation (SMT) model. By looking for phrases
                                                                  (f → f ′ ) of word f generates target word e (we
    that contain other phrases and replacing the sub-
                                                                  call it head word trigger in Section 4). Therefore,
    phrases with nonterminal symbols, it gets hierar-
                                                                  two words in one language trigger one word in
    chical rules. Hierarchical rules are more powerful
                                                                  another, which provides a more sophisticated and
    than conventional phrases since they have better
                                                                  better choice for the target word, i.e. Figure 1.
    generalization capability and could capture long
                                                                  Similarly, the dependency feature works well in
    distance reordering. However, when the train-
                                                                  Chinese-to-English translation task, especially on
    ing corpus becomes larger, the number of rules
                                                                  large corpus.
    will grow exponentially, which inevitably results
    in slow and memory-consuming decoding.                        2 Related Work
       In this paper, we address the problem of reduc-
    ing the hierarchical translation rule table resorting         In the past, a significant number of techniques
    to the dependency information of bilingual lan-               have been presented to reduce the hierarchical rule
    guages. We only keep rules that both sides are                table. He et al. (2009) just used the key phrases
    relaxed-well-formed (RWF) dependency structure                of source side to filter the rule table without taking
    (see the definition in Section 3), and discard others         advantage of any linguistic information. Iglesias
    which do not satisfy this constraint. In this way,            et al. (2009) put rules into syntactic classes based
    about 40% bad rules are weeded out from the orig-             on the number of non-terminals and patterns, and
    inal rule table. However, the performance is even             applied various filtration strategies to improve the
    better than the traditional HPB translation system.           rule table quality. Shen et al. (2008) discarded


                                                            142
                          Proceedings of the ACL 2010 Conference Short Papers, pages 142–146,
                    Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                  found                             ure 2 shows an example of a dependency tree. In
                                                                    this example, the word found is the root of the tree.
                                                                        Shen et al. (2008) propose the well-formed de-
                       girl                           house
                                                                    pendency structure to filter the hierarchical rule ta-
                                                                    ble. A well-formed dependency structure could be
      The     lovely          a           beautiful
                                                                    either a single-rooted dependency tree or a set of
                                                                    sibling trees. Although most rules are discarded
Figure 2: An example of dependency tree. The                        with the constraint that the target side should be
corresponding plain sentence is The lovely girl                     well-formed, this filtration leads to degradation in
found a beautiful house.                                            translation performance.
                                                                       As an extension of the work of (Shen et
                                                                    al., 2008), we introduce the so-called relaxed-
most entries of the rule table by using the con-
                                                                    well-formed dependency structure to filter the hi-
straint that rules of the target-side are well-formed
                                                                    erarchical rule table. Given a sentence S =
(WF) dependency structure, but this filtering led to
                                                                    w1 w2 ...wn . Let d1 d2 ...dn represent the position of
degradation in translation performance. They ob-
                                                                    parent word for each word. For example, d3 = 4
tained improvements by adding an additional de-
                                                                    means that w3 depends on w4 . If wi is a root, we
pendency language model. The basic difference
                                                                    define di = −1.
of our method from (Shen et al., 2008) is that we
                                                                       Definition A dependency structure wi ...wj is
keep rules that both sides should be relaxed-well-
                                                                    a relaxed-well-formed structure, where there is
formed dependency structure, not just the target
                                                                    h ∈ / [i, j], all the words wi ...wj are directly or
side. Besides, our system complexity is not in-
                                                                    indirectly depended on wh or -1 (here we define
creased because no additional language model is
                                                                    h = −1). If and only if it satisfies the following
introduced.
                                                                    conditions
   The feature of head word trigger which we ap-
ply to the log-linear model is motivated by the                       • dh ∈
                                                                           / [i, j]
trigger-based approach (Hasan and Ney, 2009).
Hasan and Ney (2009) introduced a second word                         • ∀k ∈ [i, j], dk ∈ [i, j] or dk = h
to trigger the target word without considering any
linguistic information. Furthermore, since the sec-                    From the definition above, we can see that
ond word can come from any part of the sentence,                    the relaxed-well-formed structure obviously cov-
there may be a prohibitively large number of pa-                    ers the well-formed one. In this structure, we
rameters involved. Besides, He et al. (2008) built                  don’t constrain that all the children of the sub-root
a maximum entropy model which combines rich                         should be complete. Let’s review the dependency
context information for selecting translation rules                 tree in Figure 2 as an example. Except for the well-
during decoding. However, as the size of the cor-                   formed structure, we could also extract girl found
pus increases, the maximum entropy model will                       a beautiful house. Therefore, if the modifier The
become larger. Similarly, In (Shen et al., 2009),                   lovely changes to The cute, this rule also works.
context language model is proposed for better rule
selection. Taking the dependency edge as condi-                     4 Head Word Trigger
tion, our approach is very different from previous                   (Koehn et al., 2003) introduced the concept of
approaches of exploring context information.                        lexical weighting to check how well words of
                                                                    the phrase translate to each other. Source word
3 Relaxed-well-formed Dependency
                                                                    f aligns with target word e, according to the
  Structure
                                                                    IBM model 1, the lexical translation probability
Dependency models have recently gained consid-                      is p(e|f ). However, in the sense of dependency
erable interest in SMT (Ding and Palmer, 2005;                      relationship, we believe that the generation of the
Quirk et al., 2005; Shen et al., 2008). Depen-                      target word e, is not only triggered by the aligned
dency tree can represent richer structural infor-                   source word f , but also associated with f ’s head
mation. It reveals long-distance relation between                   word f ′ . Therefore, the lexical translation prob-
words and directly models the semantic structure                    ability becomes p(e|f → f ′ ), which of course
of a sentence without any constituent labels. Fig-                  allows for a more fine-grained lexical choice of


                                                              143


the target word. More specifically, the probabil-               For language model, we use the SRI Language
ity could be estimated by the maximum likelihood             Modeling Toolkit (Stolcke, 2002) to train a 4-
(MLE) approach,                                              gram model on the first 1/3 of the Xinhua portion
                                                             of GIGAWORD corpus. And we use the NIST
                                                             2002 MT evaluation test set as our development
                          count(e, f → f ′ )
    p(e|f → f ) = P
                  ′
                                       ′        ′
                                                       (1)   set, and NIST 2004, 2005 test sets as our blind
                          e′ count(e , f → f )               test sets. We evaluate the translation quality us-
                                                             ing case-insensitive BLEU metric (Papineni et
   Given a phrase pair f , e and word alignment              al., 2002) without dropping OOV words, and the
a, and the dependent relation of the source sen-             feature weights are tuned by minimum error rate
tence dJ1 (J is the length of the source sentence,           training (Och, 2003).
I is the length of the target sentence). Therefore,             In order to get the dependency relation of the
given the lexical translation probability distribu-          training corpus, we re-implement a beam-search
tion p(e|f → f ′ ), we compute the feature score of          style monolingual dependency parser according
a phrase pair (f , e) as                                     to (Nivre and Scholz, 2004). Then we use the
                                                             same method suggested in (Chiang, 2005) to
           J
 p(e|f , d1 , a)                                             extract SCFG grammar rules within dependency
                                X                            constraint on both sides except that unaligned
       |e|           1                                   (2)
  = Πi=1                               p(ei |fj → fdj )      words are allowed at the edge of phrases. Pa-
            |{j|(j, i) ∈ a}|                                 rameters of head word trigger are estimated as de-
                              ∀(j,i)∈a
                                                             scribed in Section 4. As a default, the maximum
   Now we get p(e|f , dJ1 , a), we could obtain              initial phrase length is set to 10 and the maximum
p(f |e, dI1 , a) (dI1 represents dependent relation of       rule length of the source side is set to 5. Besides,
the target side) in the similar way. This new fea-           we also re-implement the decoder of Hiero (Chi-
ture can be easily integrated into the log-linear            ang, 2007) as our baseline. In fact, we just exploit
model as lexical weighting does.                             the dependency structure during the rule extrac-
                                                             tion phase. Therefore, we don’t need to change
5 Experiments                                                the main decoding algorithm of the SMT system.
In this section, we describe the experimental set-          5.2 Results on FBIS Corpus
ting used in this work, and verify the effect of
                                                            A series of experiments was done on the FBIS cor-
the relaxed-well-formed structure filtering and the
                                                            pus. We first parse the bilingual languages with
new feature, head word trigger.
                                                            monolingual dependency parser respectively, and
5.1    Experimental Setup                                   then only retain the rules that both sides are in line
                                                            with the constraint of dependency structure. In
Experiments are carried out on the NIST1                    Table 1, the relaxed-well-formed structure filtered
Chinese-English translation task with two differ-           out 35% of the rule table and the well-formed dis-
ent size of training corpora.                               carded 74%. RWF extracts additional 39% com-
                                                            pared to WF, which can be seen as some kind
  • FBIS: We use the FBIS corpus as the first
                                                            of evidence that the rules we additional get seem
    training corpus, which contains 239K sen-
                                                            common in the sense of linguistics. Compared to
    tence pairs with 6.9M Chinese words and
                                                            (Shen et al., 2008), we just use the dependency
    8.9M English words.
                                                            structure to constrain rules, not to maintain the tree
  • GQ: This is manually selected from the                  structures to guide decoding.
    LDC2 corpora. GQ contains 1.5M sentence                    Table 2 shows the translation result on FBIS.
    pairs with 41M Chinese words and 48M En-                We can see that the RWF structure constraint can
    glish words. In fact, FBIS is the subset of             improve translation quality substantially both at
    GQ.                                                     development set and different test sets. On the
                                                            Test04 task, it gains +0.86% BLEU, and +0.84%
   1
    www.nist.gov/speech/tests/mt                            on Test05. Besides, we also used Shen et al.
   2
    It consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part           (2008)’s WF structure to filter both sides. Al-
of LDC2004T07, LDC2004T08, LDC2005T06.                      though it discard about 74% of the rule table, the


                                                      144


             System    Rule table size                       feature works well on two different test sets. The
              HPB       30,152,090                           gain is +2.21% BLEU on Test04, and +1.33% on
              RWF       19,610,255                           Test05. Compared to the result of the baseline,
              WF         7,742,031                           only using the RWF structure to filter performs the
                                                             same as the baseline on Test05, and +0.99% gains
Table 1: Rule table size with different con-                 on Test04.
straint on FBIS. Here HPB refers to the base-
line hierarchal phrase-based system, RWF means               6 Conclusions
relaxed-well-formed constraint and WF represents             This paper proposes a simple strategy to filter the
the well-formed structure.                                   hierarchal rule table, and introduces a new feature
       System    Dev02      Test04       Test05              to enhance the translation performance. We em-
        HPB      0.3285     0.3284       0.2965              ploy the relaxed-well-formed dependency struc-
        WF       0.3125     0.3218       0.2887              ture to constrain both sides of the rule, and about
        RWF      0.3326    0.3370**      0.3050              40% of rules are discarded with improvement of
      RWF+Tri    0.3281        /         0.2965              the translation performance. In order to make full
                                                             use of the dependency information, we assume
                                                             that the target word e is triggered by dependency
Table 2: Results of FBIS corpus. Here Tri means
                                                             edge of the corresponding source word f . And
the feature of head word trigger on both sides. And
                                                             this feature works well on large parallel training
we don’t test the new feature on Test04 because of
                                                             corpus.
the bad performance on development set. * or **
= significantly better than baseline (p < 0.05 or               How to estimate the probability of head word
                                                             trigger is very important. Here we only get the pa-
0.01, respectively).
                                                             rameters in a generative way. In the future, we we
                                                             are plan to exploit some discriminative approach
over-all BLEU is decreased by 0.66%-0.78% on                 to train parameters of this feature, such as EM al-
the test sets.                                               gorithm (Hasan et al., 2008) or maximum entropy
   As for the feature of head word trigger, it seems         (He et al., 2008).
not work on the FBIS corpus. On Test05, it gets                 Besides, the quality of the parser is another ef-
the same score with the baseline, but lower than             fect for this method. As the next step, we will
RWF filtering. This may be caused by the data                try to exploit bilingual knowledge to improve the
sparseness problem, which results in inaccurate              monolingual parser, i.e. (Huang et al., 2009).
parameter estimation of the new feature.
                                                             Acknowledgments
5.3    Result on GQ Corpus
In this part, we increased the size of the training          This work was partly supported by National
corpus to check whether the feature of head word             Natural Science Foundation of China Contract
trigger works on large corpus.                               60873167. It was also funded by SK Telecom,
   We get 152M rule entries from the GQ corpus               Korea under the contract 4360002953. We show
according to (Chiang, 2007)’s extraction method.             our special thanks to Wenbin Jiang and Shu Cai
If we use the RWF structure to constrain both                for their valuable suggestions. We also thank
sides, the number of rules is 87M, about 43% of              the anonymous reviewers for their insightful com-
rule entries are discarded. From Table 3, the new            ments.


       System   Dev02      Test04         Test05             References
        HPB     0.3473     0.3386         0.3206
                                                             Peter F. Brown, Vincent J. Della Pietra, Stephen
        RWF     0.3539    0.3485**        0.3228
                                                               A. Della Pietra, and Robert L. Mercer. 1993. The
      RWF+Tri   0.3540    0.3607**       0.3339*               mathematics of statistical machine translation: pa-
                                                               rameter estimation. Comput. Linguist., 19(2):263–
Table 3: Results of GQ corpus. * or ** = sig-                  311.
nificantly better than baseline (p < 0.05 or 0.01,           David Chiang. 2005. A hierarchical phrase-based
respectively).                                                 model for statistical machine translation. In ACL


                                                       145


  ’05: Proceedings of the 43rd Annual Meeting on As-         Franz Josef Och. 2003. Minimum error rate training
  sociation for Computational Linguistics, pages 263–          in statistical machine translation. In ACL ’03: Pro-
  270.                                                         ceedings of the 41st Annual Meeting on Association
                                                               for Computational Linguistics, pages 160–167.
David Chiang. 2007. Hierarchical phrase-based trans-
  lation. Comput. Linguist., 33(2):201–228.                  Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
                                                               Jing Zhu. 2002. Bleu: a method for automatic eval-
Yuan Ding and Martha Palmer. 2005. Machine trans-              uation of machine translation. In ACL ’02: Proceed-
  lation using probabilistic synchronous dependency            ings of the 40th Annual Meeting on Association for
  insertion grammars. In ACL ’05: Proceedings of the           Computational Linguistics, pages 311–318.
  43rd Annual Meeting on Association for Computa-
  tional Linguistics, pages 541–548.                         Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
                                                               Dependency treelet translation: syntactically in-
Saša Hasan and Hermann Ney. 2009. Comparison of               formed phrasal smt. In ACL ’05: Proceedings of
  extended lexicon models in search and rescoring for          the 43rd Annual Meeting on Association for Com-
  smt. In NAACL ’09: Proceedings of Human Lan-                 putational Linguistics, pages 271–279.
  guage Technologies: The 2009 Annual Conference
                                                             Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
  of the North American Chapter of the Association
                                                               new string-to-dependency machine translation algo-
  for Computational Linguistics, Companion Volume:
                                                               rithm with a target dependency language model. In
  Short Papers, pages 17–20.
                                                               Proceedings of ACL-08: HLT, pages 577–585.
Saša Hasan, Juri Ganitkevitch, Hermann Ney, and             Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
  Jesús Andrés-Ferrer. 2008. Triplet lexicon models          and Ralph Weischedel. 2009. Effective use of lin-
  for statistical machine translation. In EMNLP ’08:           guistic and contextual information for statistical ma-
  Proceedings of the Conference on Empirical Meth-             chine translation. In EMNLP ’09: Proceedings of
  ods in Natural Language Processing, pages 372–               the 2009 Conference on Empirical Methods in Nat-
  381.                                                         ural Language Processing, pages 72–80.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-             Andreas Stolcke. 2002. Srilman extensible language
  proving statistical machine translation using lexical-       modeling toolkit. In In Proceedings of the 7th Inter-
  ized rule selection. In COLING ’08: Proceedings              national Conference on Spoken Language Process-
  of the 22nd International Conference on Computa-             ing (ICSLP 2002), pages 901–904.
  tional Linguistics, pages 321–328.

Zhongjun He, Yao Meng, Yajuan Lü, Hao Yu, and Qun
  Liu. 2009. Reducing smt rule table with monolin-
  gual key phrase. In ACL-IJCNLP ’09: Proceedings
  of the ACL-IJCNLP 2009 Conference Short Papers,
  pages 121–124.

Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
  Bilingually-constrained (monolingual) shift-reduce
  parsing. In EMNLP ’09: Proceedings of the 2009
  Conference on Empirical Methods in Natural Lan-
  guage Processing, pages 1222–1231.

Gonzalo Iglesias, Adrià de Gispert, Eduardo R. Banga,
  and William Byrne. 2009. Rule filtering by pattern
  for efficient hierarchical translation. In EACL ’09:
  Proceedings of the 12th Conference of the European
  Chapter of the Association for Computational Lin-
  guistics, pages 380–388.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
  2003. Statistical phrase-based translation. In
  NAACL ’03: Proceedings of the 2003 Conference
  of the North American Chapter of the Association
  for Computational Linguistics on Human Language
  Technology, pages 48–54.

Joakim Nivre and Mario Scholz. 2004. Determinis-
  tic dependency parsing of english text. In COLING
  ’04: Proceedings of the 20th international confer-
  ence on Computational Linguistics, pages 64–70.


                                                       146

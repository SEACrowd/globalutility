                       Fixed Length Word Suffix for Factored
                           Statistical Machine Translation

           Narges Sharif Razavian                                            Stephan Vogel
           School of Computer Science                               School of Computer Science
           Carnegie Mellon Universiy                                 Carnegie Mellon Universiy
                Pittsburgh, USA                                          Pittsburgh, USA
           nsharifr@cs.cmu.edu                                    stephan.vogel@cs.cmu.edu



                                                             Previous work in factored translation modeling
                    Abstract                              have reported consistent improvements from Part
                                                          of Speech(POS) tags, morphology, gender, and
    Factored Statistical Machine Translation ex-          case factors (Koehn et. a. 2007). In another work,
    tends the Phrase Based SMT model by al-               Birch et. al. 2007 have achieved improvement
    lowing each word to be a vector of factors.           using Combinational Categorial Grammar (CCG)
    Experiments have shown effectiveness of               super-tag factors. Creating the factors is done as
    many factors, including the Part of Speech
                                                          a preprocessing step, and so far, most of the ex-
    tags in improving the grammaticality of the
    output. However, high quality part of                 periments have assumed existence of external
    speech taggers are not available in open              tools for the creation of these factors (i. e. Part of
    domain for many languages. In this paper              speech taggers, CCG parsers, etc.). Unfortunately
    we used fixed length word suffix as a new             high quality language processing tools, especial-
    factor in the Factored SMT, and were able             ly for the open domain, are not available for most
    to achieve significant improvements in three          languages.
    set of experiments: large NIST Arabic to                 While linguistically identifiable representations
    English system, medium WMT Spanish to                 (i.e. POS tags, CCG supertags, etc) have been
    English system, and small TRANSTAC                    very frequently used as factors in many applica-
    English to Iraqi system.
                                                          tions including MT, simpler representations have
                                                          also been effective in achieving the same result
1    Introduction                                         in other application areas. Grzymala-Busse and
Statistical Machine Translation(SMT) is current-          Old 1997, DINCER et.al. 2008, were able to use
ly the state of the art solution to the machine           fixed length suffixes as features for training a
translation. Phrase based SMT is also among the           POS tagging. In another work Saberi and Perrot
top performing approaches available as of today.          1999 showed that reversing middle chunks of the
This approach is a purely lexical approach, using         words while keeping the first and last part intact,
surface forms of the words in the parallel corpus         does not decrease listeners’ recognition ability.
to generate the translations and estimate proba-          This result is very relevant to Machine Transla-
bilities. It is possible to incorporate syntactical       tion, suggesting that inaccurate context which is
information into this framework through differ-           usually modeled with n-gram language models,
ent ways. Source side syntax based re-ordering            can still be as effective as accurate surface forms.
as preprocessing step, dependency based reorder-          Another research (Rawlinson 1997) confirms this
ing models, cohesive decoding features are                finding; this time in textual domain, observing
among many available successful attempts for              that randomization of letters in the middle of
the integration of syntax into the translation            words has little or no effect on the ability of
model. Factored translation modeling is another           skilled readers to understand the text. These re-
way to achieve this goal. These models allow              sults suggest that the inexpensive representation-
each word to be represented as a vector of factors        al factors which do not need unavailable tools
rather than a single surface form. Factors can            might also be worth investigating.
represent richer expression power on each word.              These results encouraged us to introduce lan-
Any factors such as word stems, gender, part of           guage independent simple factors for machine
speech, tense, etc. can be easily used in this            translation. In this paper, following the work of
framework.                                                Grzymala-Busse et. al. we used fixed length suf-


                                                       147
                      Proceedings of the ACL 2010 Conference Short Papers, pages 147–150,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


fix as word factor, to lower the perplexity of the
language model, and have the factors roughly
function as part of speech tags, thus increasing
the grammaticality of the translation results. We
were able to obtain consistent, significant im-
provements over our baseline in 3 different expe-
riments, large NIST Arabic to English system,
medium WMT Spanish to English system, and
small TRANSTAC English to Iraqi system.
  The rest of this paper is as follows. Section 2       Figure 1: An example of a Factored Translation and
briefly reviews the Factored Translation Models.        Generation Model
In section 3 we will introduce our model, and
section 4 will contain the experiments and the            In this particular model, words on both source
analysis of the results, and finally, we will con-      and target side are represented as a vector of four
clude this paper in section 5.                          factors: surface form, lemma, part of speech
                                                        (POS) and the morphology. The target phrase is
2    Factored Translation Model                         generated as follows: Source word lemma gene-
                                                        rates target word lemma. Source word's Part of
Statistical Machine Translation uses the log li-        speech and morphology together generate the
near combination of a number of features, to            target word's part of speech and morphology, and
compute the highest probable hypothesis as the          from its lemma, part of speech and morphology
translation.                                            the surface form of the target word is finally gen-
                                                        erated. This model has been able to result in
e = argmaxe p(e|f) = argmaxe p exp Σi=1n λi hi(e,f)     higher translation BLEU score as well as gram-
                                                        matical coherency for English to German, Eng-
  In phrase based SMT, assuming the source and          lish to Spanish, English to Czech, English to
target phrase segmentation as {(fi,ei)}, the most       Chinese, Chinese to English and German to Eng-
important features include: the Language Model          lish.
feature hlm(e,f) = plm(e); the phrase translation
feature ht(e,f) defined as product of translation       3    Fixed Length Suffix Factors for Fac-
probabilities, lexical probabilities and phrase pe-          tored Translation Modeling
nalty; and the reordering probability, hd(e,f),
usually defined as πi=1n d(starti,endi-1) over the         Part of speech tagging, constituent and depen-
source phrase reordering events.                        dency parsing, combinatory categorical grammar
  Factored Translation Model, recently intro-           super tagging are used extensively in most appli-
duced by (Koehn et. al. 2007), allow words to           cations when syntactic representations are
have a vector representation. The model can then        needed. However training these tools require
extend the definition of each of the features from      medium size treebanks and tagged data, which
a uni-dimensional value to an arbitrary joint and       for most languages will not be available for a
conditional combination of features. Phrase             while. On the other hand, many simple words
based SMT is in fact a special case of Factored         features, such as their character n-grams, have in
SMT.                                                    fact proven to be comparably as effective in
  The factored features are defined as an exten-        many applications.
sion of phrase translation features. The function          (Keikha et. al. 2008) did an experiment on text
τ(fj,ej), which was defined for a phrase pair be-       classification on noisy data, and compared sever-
fore, can now be extended as a log linear combi-        al word representations. They compared surface
nation Σf τf(fjf,ejf). The model also allows for a      form, stemmed words, character n-grams, and
generation feature, defining the relationship be-       semantic relationships, and found that for noisy
tween final surface form and target factors. Other      and open domain text, character-ngrams outper-
features include additional language model fea-         form other representations when used for text
tures over individual factors, and factored reor-       classification. In another work (Dincer et al
dering features.                                        2009) showed that using fixed length word end-
  Figure 1 shows an example of a possible fac-          ing outperforms whole word representation for
tored model.                                            training a part of speech tagger for Turkish lan-
                                                        guage.


                                                      148


   Based on this result, we proposed a suffix fac-                  about 8K distinct words. Table 1 shows the result
tored model for translation, which is shown in                      (BLEU Score) of the system compared to the
Figure 2.                                                           baseline.

       Source    Target              LM                                 System          Tune on   Test on        Test on
                                                                                        Set-      Set-           Set-
Word
                          Word           Word Language Model                           July07    June08         Nov08
                                                                        Baseline        27.74     21.73          15.62
                          Suffix         Suffix Language Model
                                                                        Factored        28.83     22.84          16.41
   Figure 2: Suffix Factored model: Source word de-                     Improvement     1.09      1.11           0.79
termines factor vectors (target word, target word suf-
fix) and each factor will be associated with its                    Table 1: BLEU score, English to Iraqi Transtac sys-
language model.                                                     tem, comparing Factored and Baseline systems.

   Based on this model, the final probability of                       As you can see, this improvement is consistent
the translation hypothesis will be the log linear                   over multiple unseen datasets. Arabic cases and
combination of phrase probabilities, reordering                     numbers show up as the word suffix. Also, verb
model probabilities, and each of the language                       numbers usually appear partly as word suffix and
models’ probabilities.                                              in some cases as word prefix. Defining a lan-
                                                                    guage model over the word endings increases the
    P(e|f) ~ plm-word(eword)* plm-suffix(esuffix)                   probability of sequences which have this case
    * Σi=1n p(eword-j & esuffix-j|fj)                               and number agreement, favoring correct agree-
    * Σi=1n p(fj | eword-j & esuffix-j)                             ments over the incorrect ones.

   Where plm-word is the n-gram language model                      4.2      Medium System on Travel Domain:
probability over the word surface sequence, with                             Spanish to English
the language model built from the surface forms.
Similarly, plm-suffix(esuffix) is the language model                This system is the WMT08 system, on a corpus
probability over suffix sequences. p(eword-j &                      of 1.2 million sentence pairs with average sen-
esuffix-j|fj) and p(fj | eword-j & esuffix-j) are translation       tence length 27.9 words. Like the previous expe-
probabilities for each phrase pair i , used in by                   riment, we defined the 3 character suffix of the
the decoder. This probability is estimated after                    words as the second factor, and built the lan-
the phrase extraction step which is based on                        guage model and reordering model on the joint
grow-diag heuristic at this stage.                                  event of (surface, suffix) pairs. We built 5-gram
                                                                    language models for each factor. The system had
4       Experiments and Results                                     about 97K distinct vocabulary in the surface lan-
                                                                    guage model, which was reduced to 8K using the
We used Moses implementation of the factored                        suffix corpus. Having defined the baseline, the
model for training the feature weights, and SRI                     system results are as follows.
toolkit for building n-gram language models. The
baseline for all systems included the moses sys-
tem with lexicalized re-ordering, SRI 5-gram                              System         Tune-       Test set-
language models.                                                                         WMT06       WMT08
                                                                          Baseline       33.34       32.53
4.1       Small System from Dialog Domain:                                Factored       33.60       32.84
          English to Iraqi
                                                                          Improvement    0.26        0.32
This system was TRANSTAC system, which                              Table 2: BLEU score, Spanish to English WMT sys-
was built on about 650K sentence pairs with the                     tem, comparing Factored and Baseline systems.
average sentence length of 5.9 words. After
choosing length 3 for suffixes, we built a new                         Here, we see improvement with the suffix fac-
parallel corpus, and SRI 5-gram language models                     tors compared to the baseline system. Word end-
for each factor. Vocabulary size for the surface                    ings in English language are major indicators of
form was 110K whereas the word suffixes had                         word’s part of speech in the sentence. In fact


                                                                  149


most common stemming algorithm, Porter’s              improvements over the baseline. This result, ob-
Stemmer, works by removing word’s suffix.             tained from the language independent and inex-
Having a language model on these suffixes push-       pensive factor, shows promising new
es the common patterns of these suffixes to the       opportunities for all language pairs.
top, making the more grammatically coherent
sentences to achieve a better probability.            References
                                                      Birch, A., Osborne, M., and Koehn, P. CCG supertags
4.3    Large NIST 2009 System: Arabic to                 in factored statistical machine translation. Proceed-
       English                                           ings of the Second Workshop on Statistical Ma-
                                                         chine Translation, pages 9–16, Prague, Czech
We used NIST2009 system as our baseline in               Republic. Association for Computational Linguis-
this experiment. The corpus had about 3.8 Mil-           tics, 2007.
lion sentence pairs, with average sentence length     Dincer T., Karaoglan B. and Kisla T., A Suffix Based
of 33.4 words. The baseline defined the lexica-          Part-Of-Speech Tagger For Turkish, Fifth Interna-
lized reordering model. As before we defined 3           tional Conference on Information Technology:
character long word endings, and built 5-gram            New Generations, 2008.
SRI language models for each factor. The result       Grzymala-Busse J.W., Old L.J. A machine learning
of this experiment is shown in table 3.                  experiment to determine part of speech from word-
                                                         endings, Lecture Notes in Computer Science,
                                                         Communications Session 6B Learning and Discov-
 System     Tune     Test on   Test on   Test
                                                         ery Systems, 1997.
            on       Dev07     Dev07     on
                                                      Keikha M., Sharif Razavian N, Oroumchian F., and
            MT06     News      Weblog    MT08
                                                         Seyed Razi H., Document Representation and
                     Wire
                                                         Quality of Text: An Analysis, Chapter 12, Survey
 Baseline   43.06    48.87     37.84     41.70           of Text Mining II, Springer London, 2008.
                                                      Koehn Ph., Hoang H., Factored Translation Models,
 Factored   44.20    50.39     39.93     42.74           Proceedings of 45th Annual Meeting of the Asso-
                                                         ciation for Computational Linguistics (ACL), 2007.
 Improve    1.14     1.52      2.09      1.04
                                                      Rawlinson G. E., The significance of letter position in
 ment
                                                         word recognition, PhD Thesis, Psychology De-
                                                         partment, University of Nottingham, Nottingham
 Table 3: BLEU score, Arabic to English NIST 2009
                                                         UK, 1976.
 system, comparing Factored and Baseline systems.
                                                      Saberi K and Perrot D R, Cognitive restoration of
                                                         reversed speech, Nature (London) 1999.
   This result confirms the positive effect of the
suffix factors even on large systems. As men-
tioned before we believe that this result is due to
the ability of the suffix to reduce the word into a
very simple but rough grammatical representa-
tion. Defining language models for this factor
forces the decoder to prefer sentences with more
probable suffix sequences, which is believed to
increase the grammaticality of the result. Future
error analysis will show us more insight of the
exact effect of this factor on the outcome.

5     Conclusion
In this paper we introduced a simple yet very
effective factor: fixed length word suffix, to use
in Factored Translation Models. This simple fac-
tor has been shown to be effective as a rough
replacement for part of speech. We tested our
factors in three experiments in a small, English to
Iraqi system, a medium sized system of Spanish
to English, and a large system, NIST09 Arabic to
English. We observed consistent and significant


                                                    150

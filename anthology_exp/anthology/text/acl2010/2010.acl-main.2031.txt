 Improving Chinese Semantic Role Labeling with Rich Syntactic Features


                                           Weiwei Sun∗
                    Department of Computational Linguistics, Saarland University
                     German Research Center for Artificial Intelligence (DFKI)
                                  D-66123, Saarbrücken, Germany
                               wsun@coli.uni-saarland.de




                       Abstract                                     features, some of which are designed to better cap-
                                                                    ture structural information of sub-trees in a given
    Developing features has been shown cru-                         parse. With help of these new features, our sys-
    cial to advancing the state-of-the-art in Se-                   tem achieves 93.49 F-measure with hand-crafted
    mantic Role Labeling (SRL). To improve                          parses. Comparison with the best reported results,
    Chinese SRL, we propose a set of ad-                            92.0 (Xue, 2008), shows that these features yield a
    ditional features, some of which are de-                        significant improvement of the state-of-the-art.
    signed to better capture structural infor-                         We further analyze the effect of syntactic pars-
    mation. Our system achieves 93.49 F-                            ing in Chinese SRL. The main effect of parsing
    measure, a significant improvement over                         in SRL is two-fold. First, grouping words into
    the best reported performance 92.0. We                          constituents, parsing helps to find argument candi-
    are further concerned with the effect                           dates. Second, parsers provide semantic classifiers
    of parsing in Chinese SRL. We empiri-                           plenty of syntactic information, not to only recog-
    cally analyze the two-fold effect, grouping                     nize arguments from all candidate constituents but
    words into constituents and providing syn-                      also to classify their detailed semantic types. We
    tactic information. We also give some pre-                      empirically analyze each effect in turn. We also
    liminary linguistic explanations.                               give some preliminary linguistic explanations for
                                                                    the phenomena.
1   Introduction
Previous work on Chinese Semantic Role La-
                                                                    2   Chinese SRL
beling (SRL) mainly focused on how to imple-                        The Chinese PropBank (CPB) is a semantic anno-
ment SRL methods which are successful on En-                        tation for the syntactic trees of the Chinese Tree-
glish. Similar to English, parsing is a standard                    Bank (CTB). The arguments of a predicate are la-
pre-processing for Chinese SRL. Many features                       beled with a contiguous sequence of integers, in
are extracted to represent constituents in the input                the form of AN (N is a natural number); the ad-
parses (Sun and Jurafsky, 2004; Xue, 2008; Ding                     juncts are annotated as such with the label AM
and Chang, 2008). By using these features, se-                      followed by a secondary tag that represents the se-
mantic classifiers are trained to predict whether a                 mantic classification of the adjunct. The assign-
constituent fills a semantic role. Developing fea-                  ment of semantic roles is illustrated in Figure 1,
tures that capture the right kind of information en-                where the predicate is the verb “调查/investigate”.
coded in the input parses has been shown crucial                    E.g., the NP “事故原因/the cause of the accident”
to advancing the state-of-the-art. Though there                     is labeled as A1, meaning that it is the Patient.
has been some work on feature design in Chinese                        In previous research, SRL methods that are suc-
SRL, information encoded in the syntactic trees is                  cessful on English are adopted to resolve Chinese
not fully exploited and requires more research ef-                  SRL (Sun and Jurafsky, 2004; Xue, 2008; Ding
fort. In this paper, we propose a set of additional                 and Chang, 2008, 2009; Sun et al., 2009; Sun,
     ∗
     The work was partially completed while this author was         2010). Xue (2008) produced complete and sys-
at Peking University.                                               tematic research on full parsing based methods.


                                                              168
                        Proceedings of the ACL 2010 Conference Short Papers, pages 168–172,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                      IP
                              bbbbbbbbb                                        3     Features
            bbbbbbbbbbbbbbbbbb
      bbbbbb
  A0                                      VP
                                           d                                   A majority of features used in our system are a
                                 dddddididii
                        dddddididiiiii
                 ddddddd    i                                                  combination of features described in (Xue, 2008;
  NP         AM-TMP AM-MNR                VP
                                           ZZ  ZZZZZZZ
                                                      ZZZZZZZ
                                                                               Ding and Chang, 2008) as well as the word for-
                                                             ZZZZZZ
                                                                               mation and coarse frame features introduced in
  NN          ADVP        ADVP           Rel                      A1
                                                                               (Sun et al., 2009), the c-command thread fea-
 警方             AD          AD           VV                       NP           tures proposed in (Sun et al., 2008). We give
                                                                iiii
 police                                                     iiii               a brief description of features used in previous
                                                        iiii
               正在          详细         调查              NN          NN           work, but explain new features in details. For
               now      thoroughly investigate
                                                                               more information, readers can refer to relevant
                                                      事故         原因
                                                    accident     cause
                                                                               papers and our source codes2 that are well com-
                                                                               mented. To conveniently illustrate, we denote
Figure 1: An example sentence: The police are                                  a candidate constituent ck with a fixed context
thoroughly investigating the cause of the accident.                            wi−1 [ck wi ...wh ...wj ]wj+1 , where wh is the head
                                                                               word of ck , and denote predicate in focus with
                                                                               a context w−2  v w v w v w v w v , where w v is the
                                                                                                   −1     +1 +2
Their method divided SRL into three sub-tasks: 1)
                                                                               predicate in focus.
pruning with a heuristic rule, 2) Argument Identi-
fication (AI) to recognize arguments, and 3) Se-                               3.1    Baseline Features
mantic Role Classification (SRC) to predict se-
                                                                               The following features are introduced in previous
mantic types. The main two sub-tasks, AI and
                                                                               Chinese SRL systems. We use them as baseline.
SRC, are formulated as two classification prob-
                                                                                  Word content of wv , wh , wi , wj and wi +wj ;
lems. Ding and Chang (2008) divided SRC into
                                                                               POS tag of wv , wh . subcategorization frame, verb
two sub-tasks in sequence: Each argument should
                                                                               class of wv ; position, phrase type ck , path from ck
first be determined whether it is a core argument or
                                                                               to wv (from (Xue, 2008; Ding and Chang, 2008))
an adjunct, and then be classified into fine-grained
                                                                                  First character, last character and word length
categories. However, delicately designed features
                                                                               of wv , first character+length, last character+word
are more important and our experiments suggest
                                                                               length, first character+position, last charac-
that by using rich features, a better SRC solver
                                                                               ter+position, coarse frame, frame+wv , frame+left
can be directly trained without using hierarchical
                                                                               character, frame+verb class, frame+ck (from (Sun
architecture. There are also some attempts at re-
                                                                               et al., 2009)).
laxing the necessity of using full syntactic parses,
                                                                                  Head word POS, head word of PP phrases, cat-
and semantic chunking methods have been intro-
                                                                               egory of ck ’s lift and right siblings, CFG rewrite
duced by (Sun et al., 2009; Sun, 2010; Ding and
                                                                               rule that expands ck and ck ’s parent (from (Ding
Chang, 2009).
                                                                               and Chang, 2008)).
2.1       Our System                                                           3.2    New Word Features
We implement a three-stage (i.e. pruning, AI and                               We introduce some new features which can be
SRC) SRL system. In the pruning step, our sys-                                 extracted without syntactic structure. We denote
tem keeps all constituents (except punctuations)                               them as word features. They include:
that c-command1 current predicate in focus as ar-                                 Word content of w−1 v , wv , w
                                                                                                           +1     i−1 and wj+1 ;
gument candidates. In the AI step, a lot of syntac-                                          v      v
                                                                               POS tag of w−1 , w+1 , w−2v , wv , w
                                                                                                               +2    i−1 , wi , wj ,
tic features are extracted to distinguish argument                             wj+1 , wi+2 and wj−2 .
and non-argument. In other words, a binary classi-                                Length of ck : how many words are there in ck .
fier is trained to classify each argument candidate                               Word before “LC”: If the POS of wj is “LC”
as either an argument or not. Finally, a multi-class                           (localizer), we use wj−1 and its POS tag as two
classifier is trained to label each argument recog-                            new features.
nized in the former stage with a specific semantic                                NT: Does ck contain a word with POS “NT”
role label. In both AI and SRC, the main job is to                             (temporal noun)?
select strong syntactic features.
                                                                                 2
                                                                                   Available    at   http://code.google.com/p/
   1
       See (Sun et al., 2008) for detailed definition.                         csrler/.


                                                                         169


  Combination features: wi ’s POS+wj ’s POS,                   et al., 2009; Sun, 2010). All parsing and SRL ex-
wv +Position                                                   periments use this data setting. To resolve clas-
                                                               sification problems, we use a linear SVM classi-
3.3    New Syntactic Features                                  fier SVMlin 3 , along with One-Vs-All approach for
Taking complex syntax trees as inputs, the clas-               multi-class classification. To evaluate SRL with
sifiers should characterize their structural proper-           automatic parsing, we use a state-of-the-art parser,
ties. We put forward a number of new features to               Bikel parser4 (Bikel, 2004). We use gold segmen-
encode the structural information.                             tation and POS as input to the Bikel parser and
   Category of ck ’s parent; head word and POS of              use it parsing results as input to our SRL system.
head word of parent, left sibling and right sibling            The overall LP/LR/F performance of Bikel parser
of ck .                                                        is 79.98%/82.95%/81.43.
   Lexicalized Rewrite rules: Conjuction of
                                                               4.2   Overall Performance
rewrite rule and head word of its corresponding
RHS. These features of candidate (lrw-c) and its               Table 1 summarizes precision, recall and F-
parent (lrw-p) are used. For example, this lrw-                measure of AI, SRC and the whole task (AI+SRC)
c feature of the NP “事 故 原 因” in Figure 1 is                   of our system respectively. The forth line is
N P → N N + N N (原因).                                          the best published SRC performance reported in
   Partial Path: Path from the ck or wv to the low-            (Ding and Chang, 2008), and the sixth line is the
est common ancestor of ck and wv . One path fea-               best SRL performance reported in (Xue, 2008).
ture, hence, is divided into left path and right path.         Other lines show the performance of our system.
   Clustered Path: We use the manually created                 These results indicate a significant improvement
clusters (see (Sun and Sui, 2009)) of categories of            over previous systems due to the new features.
all nodes in the path (cpath) and right path.
                                                                 Test                      P(%)    R(%)     F/A
   C-commander thread between ck and wv (cct):
(proposed by (Sun et al., 2008)). For example, this              AI                        98.56   97.91   98.24
                                                                 SRC                        --       --    95.04
feature of the NP “警方” in Figure 1 is N P +
                                                                 (Ding and Chang, 2008)     --       --    94.68
ADV P + ADV P + V V .                                            AI + SRC                  93.80   93.18   93.49
   Head Trace: The sequential container of the                   (Xue, 2008)               93.0     91.0   92.0
head down upon the phrase (from (Sun and Sui,
2009)). We design two kinds of traces (htr-p, htr-             Table 1: SRL performance on the test data with
w): one uses POS of the head word; the other uses              gold standard parses.
the head word word itself. E.g., the head word of
事故原因 is “原因” therefore these feature of this
NP are NP↓NN and NP↓原因.                                        4.3   Two-fold Effect of Parsing in SRL
   Combination features: verb class+ck , wh +wv ,              The effect of parsing in SRL is two-fold. On the
wh +Position,       wh +wv +Position,       path+wv ,          one hand, SRL systems should group words as ar-
wh +right path, w +left path, frame+wv +wh ,
                      v
                                                               gument candidates, which are also constituents in
and wv +cct.                                                   a given sentence. Full parsing provides bound-
                                                               ary information of all constituents. As arguments
4     Experiments and Analysis                                 should c-command the predicate, a full parser can
                                                               further prune a majority of useless constituents. In
4.1    Experimental Setting
                                                               other words, parsing can effectively supply SRL
To facilitate comparison with previous work, we                with argument candidates. Unfortunately, it is
use CPB 1.0 and CTB 5.0, the same data set-                    very hard to rightly produce full parses for Chi-
ting with (Xue, 2008). The data is divided into                nese text. On the other hand, given a constituent,
three parts: files from 081 to 899 are used as                 SRL systems should identify whether it is an argu-
training set; files from 041 to 080 as develop-                ment and further predict detailed semantic types if
ment set; files from 001 to 040, and 900 to 931
                                                                 3
as test set. Nearly all previous research on con-                  http://people.cs.uchicago.edu/
                                                               ˜vikass/svmlin.html
stituency based SRL evaluation use this setting,                 4
                                                                   http://www.cis.upenn.edu/˜dbikel/
also including (Ding and Chang, 2008, 2009; Sun                software.html


                                                         170


 Task   Parser   Bracket    Feat       P(%)    R(%)     F/A          4.3.2      The Effect of Parsing in SRC
 AI     --       Gold       W          82.44   86.78   84.55
        CTB      Gold       W+S        98.69   98.11   98.40         The second block in Table 2 summarizes the SRC
        Bikel    Bikel      W+S        77.54   71.62   74.46         performance with gold argument boundaries. Line
 SRC    --       Gold       W           --      --     93.93         5 is the accuracy when word features are used;
        CTB      Gold       W+S         --      --     95.80         Line 6 is the accuracy when additional syntactic
        Bikel    Gold       W+S         --      --     92.62
                                                                     features are added; The last row is the accuracy
Table 2: Classification perfromance on develop-                      when syntactic features used are extracted from
ment data. In the Feat column, W means word                          automatic parses (Bikel+Gold). We can see that
features; W+S means word and syntactic feautres.                     different from AI, word features only can train
                                                                     reasonable good semantic classifiers. The com-
                                                                     parison between Line 5 and 7 suggests that with
it is an argument. For the two classification prob-                  parsing errors, automatic parsed syntactic features
lems, parsing can provide complex syntactic infor-                   cause noise to the semantic role classifiers.
mation such as path features.
4.3.1   The Effect of Parsing in AI                                  4.4      Why Word Features Are Effective for
                                                                              SRC?
In AI, full parsing is very important for both
grouping words and classification. Table 2 sum-
marizes relative experimental results. Line 2 is the                  Rank        Feature                    Rank   Feature
                                                                      1           ‡ frame+w              v          ‡w      v
                                                                                                  h +w       2         h +w +position
AI performance when gold candidate boundaries                                     ‡w          v                       v
                                                                      3                h +w                  4      w +cct
and word features are used; Line 3 is the perfor-                     5           lrw-p                      6      † w +w
                                                                                                                       i   j
mance with additional syntactic features. Line 4                      7           lrw-c                      8      ‡ w +Postion
                                                                                                                       h
shows the performance by using automatic parses                       9           † frame+w v                10     htr-p
generated by Bikel parser. We can see that: 1)
word features only cannot train good classifiers to                         Table 4: Top 10 useful features for SRC.
identify arguments; 2) it is very easy to recognize
arguments with good enough syntactic parses; 3)                         Table 4 shows the ten most useful features in
there is a severe performance decline when auto-                     SRC. We can see that two of these ten features
matic parses are used. The third observation is a                    are word features (denoted by †). Namely, word
similar conclusion in English SRL. However this                      features play a more important role in SRC than
problem in Chinese is much more serious due to                       in AI. Though the other eight features are based
the state-of-the-art of Chinese parsing.                             on full parsing, four of them (denoted by ‡) use
   Information theoretic criteria are popular cri-                   the head word which can be well approximated
teria in variable selection (Guyon and Elisse-                       by word features, according to some language spe-
eff, 2003). This paper uses empirical mutual                         cific properties. The head rules described in (Sun
information between
                  P each variable andp(x,y) the tar-                 and Jurafsky, 2004) are very popular in Chinese
get, I(X, Y ) = x∈X,y∈Y p(x, y) log p(x)p(y)     , to                parsing research, such as in (Duan et al., 2007;
roughly rank the importance of features. Table 3                     Zhang and Clark, 2008). From these head rules,
shows the ten most useful features in AI. We can                     we can see that head words of most phrases in
see that the most important features all based on                    Chinese are located at the first or the last position.
full parsing information. Nine of these top 10 use-                  We implement these rules on Chinese Tree Bank
ful features are our new features.                                   and find that 84.12% 5 nodes realize their heads as
                                                                     either their first or last word. Head position sug-
    Rank   Feature         Rank    Feature
    1      wv cct          2       ‡ wh +wv +Position
                                                                     gests that boundary words are good approximation
    3      htr-w           4       htr-p                             of head word features. If head words have good
    5      path            6       ‡ w +w v
                                       h
                                                                     approximation word features, then it is not strange
    7      cpath           8       cct                               that the four features denoted by ‡ can be effec-
    9      path+wv         10      lrw-p                             tively represented by word features. Similar with
                                                                     feature effect in AI, most of most useful features
Table 3: Top 10 useful features for AI. ‡ means                      in SRC are our new features.
word features.
                                                                        5
                                                                            This statistics excludes all empty categories in CTB.


                                                               171


5   Conclusion                                               Honglin Sun and Daniel Jurafsky. 2004. Shallow
                                                               semantc parsing of Chinese. In Daniel Marcu
This paper proposes an additional set of features              Susan Dumais and Salim Roukos, editors, HLT-
to improve Chinese SRL. These new features yield               NAACL 2004: Main Proceedings.
a significant improvement over the best published
performance. We further analyze the effect of                Weiwei Sun. 2010. Semantics-driven shallow
parsing in Chinese SRL, and linguistically explain            parsing for Chinese semantic role labeling. In
some phenomena. We found that (1) full syntactic              Proceedings of the ACL 2010.
information playes an essential role only in AI and          Weiwei Sun and Zhifang Sui. 2009. Chinese func-
that (2) due to the head word position distribution,          tion tag labeling. In Proceedings of the 23rd
SRC is easy to resolve in Chinese SRL.                        Pacific Asia Conference on Language, Informa-
                                                              tion and Computation. Hong Kong.
Acknowledgments                                              Weiwei Sun, Zhifang Sui, and Haifeng Wang.
The author is funded both by German Academic                  2008. Prediction of maximal projection for se-
Exchange Service (DAAD) and German Research                   mantic role labeling. In Proceedings of the
Center for Artificial Intelligence (DFKI).                    22nd International Conference on Computa-
  The author would like to thank the anonymous                tional Linguistics.
reviewers for their helpful comments.                        Weiwei Sun, Zhifang Sui, Meng Wang, and Xin
                                                              Wang. 2009. Chinese semantic role labeling
References                                                    with shallow parsing. In Proceedings of the
Daniel M. Bikel. 2004. A distributional analysis              2009 Conference on Empirical Methods in Nat-
  of a lexicalized statistical parsing model. In              ural Language Processing, pages 1475–1483.
  Dekang Lin and Dekai Wu, editors, Proceed-                  Association for Computational Linguistics, Sin-
  ings of EMNLP 2004, pages 182–189. Associa-                 gapore.
  tion for Computational Linguistics, Barcelona,             Nianwen Xue. 2008. Labeling Chinese predi-
  Spain.                                                       cates with semantic roles. Comput. Linguist.,
Weiwei Ding and Baobao Chang. 2008. Improv-                    34(2):225–255.
 ing Chinese semantic role classification with hi-           Yue Zhang and Stephen Clark. 2008. A tale of two
 erarchical feature selection strategy. In Pro-                parsers: Investigating and combining graph-
 ceedings of the EMNLP 2008, pages 324–                        based and transition-based dependency parsing.
 333. Association for Computational Linguis-                   In Proceedings of the 2008 Conference on Em-
 tics, Honolulu, Hawaii.                                       pirical Methods in Natural Language Process-
                                                               ing, pages 562–571. Association for Computa-
Weiwei Ding and Baobao Chang. 2009. Fast se-
                                                               tional Linguistics, Honolulu, Hawaii.
 mantic role labeling for Chinese based on se-
 mantic chunking. In ICCPOL ’09: Proceed-
 ings of the 22nd International Conference on
 Computer Processing of Oriental Languages.
 Language Technology for the Knowledge-
 based Economy, pages 79–90. Springer-Verlag,
 Berlin, Heidelberg.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007.
  Probabilistic models for action-based Chinese
  dependency parsing. In ECML ’07: Pro-
  ceedings of the 18th European conference on
  Machine Learning, pages 559–566. Springer-
  Verlag, Berlin, Heidelberg.
Isabelle Guyon and André Elisseeff. 2003. An
   introduction to variable and feature selec-
   tion. Journal of Machine Learning Research,
   3:1157–1182.


                                                       172

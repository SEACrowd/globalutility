         Improving Arabic-to-English Statistical Machine Translation
             by Reordering Post-verbal Subjects for Alignment
                      Marine Carpuat Yuval Marton Nizar Habash
                                     Columbia University
                         Center for Computational Learning Systems
                         475 Riverside Drive, New York, NY 10115
                   {marine,ymarton,habash}@ccls.columbia.edu


                     Abstract                                 pre-verbal subject languages (SVO) such as En-
                                                              glish.
    We study the challenges raised by Ara-                       These issues are particularly problematic in
    bic verb and subject detection and re-                    phrase-based SMT (Koehn et al., 2003). Standard
    ordering in Statistical Machine Transla-                  phrase-based SMT systems memorize phrasal
    tion (SMT). We show that post-verbal sub-                 translation of verb and subject constructions as ob-
    ject (VS) constructions are hard to trans-                served in the training bitext. They do not cap-
    late because they have highly ambiguous                   ture any generalizations between occurrences in
    reordering patterns when translated to En-                VS and SV orders, even for the same verbs. In
    glish. In addition, implementing reorder-                 addition, their distance-based reordering models
    ing is difficult because the boundaries of                are not well suited to handling complex reorder-
    VS constructions are hard to detect accu-                 ing operations which can include long distance
    rately, even with a state-of-the-art Arabic               dependencies, and may vary by context. Despite
    dependency parser. We therefore propose                   these limitations, phrase-based SMT systems have
    to reorder VS constructions into SV or-                   achieved competitive results in Arabic-to-English
    der for SMT word alignment only. This                     benchmark evaluations.1 However, error analysis
    strategy significantly improves BLEU and                  shows that verbs are still often dropped or incor-
    TER scores, even on a strong large-scale                  rectly translated, and subjects are split or garbled
    baseline and despite noisy parses.                        in translation. This suggests that better syntactic
                                                              modeling should further improve SMT.
1   Introduction
                                                                 We attempt to get a better understanding of
Modern Standard Arabic (MSA) is a morpho-                     translation patterns for Arabic verb constructions,
syntactically complex language, with different                particularly VS constructions, by studying their
phenomena from English, a fact that raises many               occurrence and reordering patterns in a hand-
interesting issues for natural language processing            aligned Arabic-English parallel treebank. Our
and Arabic-to-English statistical machine transla-            analysis shows that VS reordering rules are not
tion (SMT). While comprehensive Arabic prepro-                straightforward and that SMT should therefore
cessing schemes have been widely adopted for                  benefit from direct modeling of Arabic verb sub-
handling Arabic morphology in SMT (e.g., Sa-                  ject translation. In order to detect VS construc-
dat and Habash (2006), Zollmann et al. (2006),                tions, we use our state-of-the-art Arabic depen-
Lee (2004)), syntactic issues have not received               dency parser, which is essentially the CATIB E X
as much attention by comparison (Green et                     baseline in our subsequent parsing work in Mar-
al. (2009), Crego and Habash (2008), Habash                   ton et al. (2010), and is further described there. We
(2007)). Arabic verbal constructions are par-                 show that VS subjects and their exact boundaries
ticularly challenging since subjects can occur in             are hard to identify accurately. Given the noise
pre-verbal (SV), post-verbal (VS) or pro-dropped              in VS detection, existing strategies for source-side
(“null subject”) constructions. As a result, training         reordering (e.g., Xia and McCord (2004), Collins
data for learning verbal construction translations            et al. (2005), Wang et al. (2007)) or using de-
is split between the different constructions and                 1
                                                                 http://www.itl.nist.gov/iad/
their patterns; and complex reordering schemas                mig/tests/mt/2009/ResultsRelease/
are needed in order to translate them into primarily          currentArabic.html


                                                        178
                      Proceedings of the ACL 2010 Conference Short Papers, pages 178–183,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                            (see Section 3). We then check whether the En-
Table 1: How are Arabic SV and VS translated in
                                                            glish translations of the Arabic verb and the Ara-
the manually word-aligned Arabic-English paral-
                                                            bic subject occur in the same order as in Arabic
lel treebank? We check whether V and S are trans-
                                                            (monotone) or not (inverted). Table 1 summa-
lated in a “monotone” or “inverted” order for all
                                                            rizes the reordering patterns for each category. As
VS and SV constructions. “Overlap” represents
                                                            expected, 98% of Arabic SV are translated in a
instances where translations of the Arabic verb
                                                            monotone order in English. For VS constructions,
and subject have some English words in common,
                                                            the picture is surprisingly more complex. The
and are not monotone nor inverted.
                                                            monotone VS translations are mostly explained
            gold reordering all verbs    %                  by changes to passive voice or to non-verbal con-
      SV monotone               2588 98.2                   structions (such as nominalization) in the English
      SV inverted                  15   0.5                 translation.
      SV overlap                   35   1.3                    In addition, Table 1 shows that verb subjects oc-
      SV total                  2638 100                    cur more frequently in VS order (70%) than in SV
      VS monotone               1700 27.3                   order (30%). These numbers do not include pro-
      VS inverted               4033 64.7                   dropped (“null subject”) constructions.
      VS overlap                 502    8.0
      VS total                  6235 100                    3    Arabic VS construction detection
                                                            Even if the SMT system had perfect knowledge
                                                            of VS reordering, it has to accurately detect VS
pendency parses as cohesion constraints in decod-           constructions and their spans in order to apply
ing (e.g., Cherry (2008); Bach et al. (2009)) are           the reordering correctly. For that purpose, we
not effective at this stage. While these approaches         use our state-of-ther-art parsing model, which is
have been successful for language pairs such as             essentially the CATIB E X baseline model in Mar-
German-English for which syntactic parsers are              ton et al. (2010), and whose details we summa-
more developed and relevant reordering patterns             rize next. We train a syntactic dependency parser,
might be less ambiguous, their impact potential on          MaltParser v1.3 with the Nivre “eager” algorithm
Arabic-English translation is still unclear.                (Nivre, 2003; Nivre et al., 2006; Nivre, 2008) on
    In this work, we focus on VS constructions              the training portion of the Penn Arabic Treebank
only, and propose a new strategy in order to bene-          part 3 v3.1, hereafter PATB3 (Maamouri et al.,
fit from their noisy detection: for the word align-         2008; Maamouri et al., 2009). The training / de-
ment stage only, we reorder phrases detected as             velopment split is the same as in Zitouni et al.
VS constructions into an SV order. Then, for                (2006). We convert the PATB3 representation into
phrase extraction, weight optimization and decod-           the succinct CATiB format, with 8 dependency
ing, we use the original (non-reordered) text. This         relations and 6 POS tags, which we then extend
approach significantly improves both BLEU and               to a set of 44 tags using regular expressions of
TER on top of strong medium and large-scale                 the basic POS and the normalized surface word
phrase-based SMT baselines.                                 form, similarly to Marton et al. (2010), following
                                                            Habash and Roth (2009). We normalize Alif Maq-
2   VS reordering in gold Arabic-English                    sura to Ya, and Hamzated Alifs to bare Alif, as is
    translation                                             commonly done in Arabic SMT.
                                                               For analysis purposes, we evaluate our subject
We use the manually word-aligned parallel
                                                            and verb detection on the development part of
Arabic-English Treebank (LDC2009E82) to study
                                                            PATB3 using gold POS tags. There are various
how Arabic VS constructions are translated into
                                                            ways to go about it. We argue that combined de-
English by humans. Given the gold Arabic syn-
                                                            tection statistics of constructions of verbs and their
tactic parses and the manual Arabic-English word
                                                            subjects (VATS), for which we achieve an F-score
alignments, we can determine the gold reorder-
                                                            of 74%, are more telling for the task at hand.2
ings for SV and VS constructions. We extract VS
                                                                2
representations from the gold constituent parses                  We divert from the CATiB representation in that a non-
                                                            matrix subject of a pseudo verb (An and her sisters) is treated
by deterministic conversion to a simplified depen-          as a subject of the verb that is under the same pseudo verb.
dency structure, CATiB (Habash and Roth, 2009)              This treatment of said subjects is comparable to the PATB’s.


                                                      179


These scores take into account the spans of both                 coding are performed on the original Arabic word
the subject and the specific verb it belongs to, and             order. Preliminary experiments on an earlier ver-
potentially reorder with. We also provide statistics             sion of the large-scale SMT system described in
of VS detection separately (F-score 63%), since                  Section 6 showed that forcing reordering of all
we only handle VS here. This low score can be                    VS constructions at training and test time does
explained by the difficulty in detecting the post-               not have a consistent impact on translation qual-
verbal subject’s end boundary, and the correct verb              ity: for instance, on the NIST MT08-NW test set,
the subject belongs to. The SV construction scores               TER slightly improved from 44.34 to 44.03, while
are higher, presumably since the pre-verbal sub-                 BLEU score decreased from 49.21 to 49.09.
ject’s end is bounded by the verb it belongs to. See                Limiting reordering to alignment allows the sys-
Table 2.                                                         tem to be more robust and recover from incorrect
   Although not directly comparable, our VS                      changes introduced either by incorrect VS detec-
scores are similar to those of Green et al. (2009).              tion, or by incorrect reordering of a correctly de-
Their VS detection technique with conditional                    tected VS. Given a parallel sentence (a, e), we
random fields (CRF) is different from ours in by-                proceed as follows:
passing full syntactic parsing, and in only detect-
                                                                     1. automatically tag VS constructions in a
ing maximal (non-nested) subjects of verb-initial
clauses. Additionally, they use a different train-                   2. generate new sentence a0 = reorder(a) by
ing / test split of the PATB data (parts 1, 2 and 3).                   reordering Arabic VS into SV
They report 65.9% precision and 61.3% F-score.                       3. get word alignment wa0 on new sentence pair
Note that a closer score comparison should take                         (a0 , e)
into account their reported verb detection accuracy                  4. using mapping from a to a0 , get correspond-
of 98.1%.                                                               ing word alignment wa = unreorder(wa0 )
                                                                        for the original sentence pair (a, e)
Table 2: Precision, Recall and F-scores for con-
structions of Arabic verbs and their subjects, eval-             5     Experiment set-up
uated on our development part of PATB3.                          We use the open-source Moses toolkit (Koehn et
    construction                     P       R       F           al., 2007) to build two phrase-based SMT systems
    VATS (verbs & their subj.)     73.84   74.37   74.11         trained on two different data conditions:
    VS                             66.62   59.41   62.81             • medium-scale the bitext consists of 12M
    SV                             86.75   61.07   71.68               words on the Arabic side (LDC2007E103).
    VNS (verbs w/ null subj.)      76.32   92.04   83.45               The language model is trained on the English
    verbal subj. exc. null subj.   72.46   60.18   65.75               side of the large bitext.
    verbal subj. inc. null subj.   73.97   74.50   74.23
                                                                     • large-scale the bitext consists of several
    verbs with non-null subj.      91.94   76.17   83.31
                                                                       newswire LDC corpora, and has 64M words
    SV or VS                       72.19   59.95   65.50
                                                                       on the Arabic side. The language model is
                                                                       trained on the English side of the bitext aug-
                                                                       mented with Gigaword data.
4        Reordering Arabic VS for SMT word
         alignment                                                  Except from this difference in training data, the
                                                                 two systems are identical. They use a standard
Based on these analyses, we propose a new                        phrase-based architecture. The parallel corpus is
method to help phrase-based SMT systems deal                     word-aligned using the GIZA++ (Och and Ney,
with Arabic-English word order differences due to                2003), which sequentially learns word alignments
VS constructions. As in related work on syntactic                for the IBM1, HMM, IBM3 and IBM4 models.
reordering by preprocessing, our method attempts                 The resulting alignments in both translation di-
to make Arabic and English word order closer to                  rections are intersected and augmented using the
each other by reordering Arabic VS constructions                 grow-diag-final-and heuristic (Koehn et al., 2007).
into SV. However, unlike in previous work, the re-               Phrase translations of up to 10 words are extracted
ordered Arabic sentences are used only for word                  in the Moses phrase-table. We apply statistical
alignment. Phrase translation extraction and de-                 significance tests to prune unreliable phrase-pairs


                                                           180


and score remaining phrase-table entries (Chen et
                                                             Table 3: Evaluation on all test sets: on the total
al., 2009). We use a 5-gram language model with
                                                             of 4432 test sentences, improvements are statisti-
modified Kneser-Ney smoothing. Feature weights
                                                             cally significant at the 99% level using bootstrap
are tuned to maximize BLEU on the NIST MT06
                                                             resampling (Koehn, 2004)
test set.
   For all systems, the English data is tokenized                  system        BLEU r4n4 (%)     TER (%)
using simple punctuation-based rules. The Arabic              medium baseline    44.35             48.34
side is segmented according to the Arabic Tree-               + VS reordering    44.65 (+0.30)     47.78 (-0.56)
bank (PATB3) tokenization scheme (Maamouri et                  large baseline    51.45             42.45
al., 2009) using the MADA+TOKAN morpholog-                    + VS reordering    51.70 (+0.25)     42.21 (-0.24)
ical analyzer and tokenizer (Habash and Rambow,
2005). MADA-produced Arabic lemmas are used
for word alignment.                                          ages a phrase-based SMT decoder to use phrasal
                                                             translations that do not break subject boundaries.
6   Results                                                     Syntactically motivated reordering for phrase-
                                                             based SMT has been more successful on language
We evaluate translation quality using both BLEU
                                                             pairs other than Arabic-English, perhaps due to
(Papineni et al., 2002) and TER (Snover et al.,
                                                             more accurate parsers and less ambiguous reorder-
2006) scores on three standard evaluation test
                                                             ing patterns than for Arabic VS. For instance,
sets from the NIST evaluations, which yield more
                                                             Collins et al. (2005) apply six manually defined
than 4400 test sentences with 4 reference transla-
                                                             transformations to German parse trees which im-
tions. On this large data set, our VS reordering
                                                             prove German-English translation by 0.4 BLEU
method remarkably yields statistically significant
                                                             on the Europarl task. Xia and McCord (2004)
improvements in BLEU and TER on the medium
                                                             learn reordering rules for French to English trans-
and large SMT systems at the 99% confidence
                                                             lations, which arguably presents less syntactic dis-
level (Table 3).
                                                             tortion than Arabic-English. Zhang et al. (2007)
   Results per test set are reported in Table 4. TER
                                                             limit reordering to decoding for Chinese-English
scores are improved in all 10 test configurations,
                                                             SMT using a lattice representation. Cherry (2008)
and BLEU scores are improved in 8 out of the 10
                                                             uses dependency parses as cohesion constraints in
configurations. Results on the MT08 test set show
                                                             decoding for French-English SMT.
that improvements are obtained both on newswire
                                                                For Arabic-English phrase-based SMT, the im-
and on web text as measured by TER (but not
                                                             pact of syntactic reordering as preprocessing is
BLEU score on the web section.) It is worth noting
                                                             less clear. Habash (2007) proposes to learn syntac-
that consistent improvements are obtained even on
                                                             tic reordering rules targeting Arabic-English word
the large-scale system, and that both baselines are
                                                             order differences and integrates them as deter-
full-fledged systems, which include lexicalized re-
                                                             ministic preprocessing. He reports improvements
ordering and large 5-gram language models.
                                                             in BLEU compared to phrase-based SMT limited
   Analysis shows that our VS reordering tech-
                                                             to monotonic decoding, but these improvements
nique improves word alignment coverage (yield-
                                                             do not hold with distortion. Instead of apply-
ing 48k and 330k additional links on the medium
                                                             ing reordering rules deterministically, Crego and
and large scale systems respectively). This results
                                                             Habash (2008) use a lattice input to represent alter-
in larger phrase-tables which improve translation
                                                             nate word orders which improves a ngram-based
quality.
                                                             SMT system. But they do not model VS construc-
                                                             tions explicitly.
7   Related work
                                                                Most previous syntax-aware word alignment
To the best of our knowledge, the only other ap-             models were specifically designed for syntax-
proach to detecting and using Arabic verb-subject            based SMT systems. These models are often
constructions for SMT is that of Green et al.                bootstrapped from existing word alignments, and
(2009) (see Section 3), which failed to improve              could therefore benefit from our VS reordering ap-
Arabic-English SMT. In contrast with our reorder-            proach. For instance, Fossum et al. (2008) report
ing approach, they integrate subject span informa-           improvements ranging from 0.1 to 0.5 BLEU on
tion as a log-linear model feature which encour-             Arabic translation by learning to delete alignment


                                                       181


Table 4: VS reordering improves BLEU and TER scores in almost all test conditions on 5 test sets, 2
metrics, and 2 MT systems
                                                  BLEU r4n4 (%)
         test set         MT03               MT04           MT05                          MT08nw              MT08wb
    medium baseline       45.95              44.94          48.05                         44.86               32.05
    + VS reordering       46.33 (+0.38)      45.03 (+0.09) 48.69 (+0.64)                  45.06 (+0.20)       31.96 (-0.09)
     large baseline       52.3               52.45          54.66                         52.60               39.22
    + VS reordering       52.63 (+0.33)      52.34 (-0.11) 55.29 (+0.63)                  52.85 (+0.25)       39.87 (+0.65)
                                                      TER (%)
         test set         MT03               MT04           MT05                          MT08nw              MT08wb
    medium baseline       48.77              46.45          45.00                         47.74               58.02
    + VS reordering       48.31 (-0.46)      46.10 (-0.35) 44.29 (-0.71)                  47.11 (-0.63)       57.30 (-0.72)
     large baseline       43.33              40.42          39.15                         41.81               52.05
    + VS reordering       42.95 (-0.38)      40.40 (-0.02) 38.75 (-0.40)                  41.51 (-0.30)       51.86 (-0.19)


links if they degrade their syntax-based translation                0110. Any opinions, findings and conclusions or recommen-
system. Departing from commonly-used align-                         dations expressed in this material are those of the authors and
                                                                    do not necessarily reflect the views of DARPA.
ment models, Hermjakob (2009) aligns Arabic and
English content words using pointwise mutual in-
formation, and in this process indirectly uses En-                  References
glish sentences reordered into VS order to collect                  Nguyen Bach, Stephan Vogel, and Colin Cherry. 2009. Co-
cooccurrence counts. The approach outperforms                         hesive constraints in a beam search phrase-based decoder.
                                                                      In Proceedings of the 10th Meeting of the North American
GIZA++ on a small-scale translation task, but the                     Chapter of the Association for Computational Linguistics,
impact of reordering alone is not evaluated.                          Companion Volume: Short Papers, pages 1–4.

                                                                    Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Re-
8    Conclusion and future work                                       ordering matrix post-verbal subjects for arabic-to-english
                                                                      smt. In Proceedings of the Conference Traitement Au-
We presented a novel method for improving over-                       tomatique des Langues Naturelles (TALN).
all SMT quality using a noisy syntactic parser: we
                                                                    Boxing Chen, George Foster, and Roland Kuhn. 2009.
use these parses to reorder VS constructions into                     Phrase translation model enhanced with association based
SV for word alignment only. This approach in-                         features. In Proceedings of MT-Summit XII, Ottawa, On-
creases word alignment coverage and significantly                     tario, September.
improves BLEU and TER scores on two strong                          Colin Cherry. 2008. Cohesive phrase-based decoding for
SMT baselines.                                                        statistical machine translation. In Proceedings of the 46th
                                                                      Annual Meeting of the Association for Computational Lin-
   In subsequent work, we show that matrix (main-                     guistics (ACL), pages 72–80, Columbus, Ohio, June.
clause) VS constructions are reordered much more
frequently than non-matrix VS, and that limit-                      Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
                                                                      Clause restructuring for statistical machine translation. In
ing reordering to matrix VS constructions for                         Proceedings of the 43rd Annual Meeting of the Associa-
word alignment further improves translation qual-                     tion for Computational Linguistics (ACL), pages 531–540,
ity (Carpuat et al., 2010). In the future, we plan to                 Ann Arbor, MI, June.
improve robustness to parsing errors by using not                   Josep M. Crego and Nizar Habash. 2008. Using shallow syn-
just one, but multiple subject boundary hypothe-                       tax information to improve word alignment and reordering
                                                                       for SMT. In Proceedings of the Third Workshop on Statis-
ses. We will also investigate the integration of VS                    tical Machine Translation, pages 53–61, June.
reordering in SMT decoding.
                                                                    Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Acknowledgements                                                       Using syntax to improve word alignment precision for
                                                                       syntax-based machine translation. In Proceedings of the
The authors would like to thank Mona Diab, Owen Ram-                   Third Workshop on Statistical Machine Translation, pages
bow, Ryan Roth, Kristen Parton and Joakim Nivre for help-              44–52.
ful discussions and assistance. This material is based upon
work supported by the Defense Advanced Research Projects            Spence Green, Conal Sathi, and Christopher D. Manning.
Agency (DARPA) under GALE Contract No HR0011-08-C-                    2009. NP subject detection in verb-initial Arabic clauses.


                                                              182


   In Proceedings of the Third Workshop on Computational              Joakim Nivre. 2003. An efficient algorithm for projective
   Approaches to Arabic Script-based Languages (CAASL3).                 dependency parsing. In Proceedings of the 8th Interna-
                                                                         tional Conference on Parsing Technologies (IWPT), pages
Nizar Habash and Owen Rambow. 2005. Arabic Tokeniza-                     149–160, Nancy, France.
  tion, Part-of-Speech Tagging and Morphological Disam-
  biguation in One Fell Swoop. In Proceedings of the 43rd             Joakim Nivre. 2008. Algorithms for Deterministic Incre-
  Annual Meeting of the Association for Computational Lin-               mental Dependency Parsing. Computational Linguistics,
  guistics (ACL’05), pages 573–580, Ann Arbor, Michigan,                 34(4).
  June.
                                                                      Franz Josef Och and Hermann Ney. 2003. A systematic com-
Nizar Habash and Ryan Roth. 2009. CATiB: The Columbia                    parison of various statistical alignment models. Computa-
  Arabic treebank. In Proceedings of the ACL-IJCNLP 2009                 tional Linguistics, 29(1):19–52.
  Conference Short Papers, pages 221–224, Suntec, Singa-
  pore, August. Association for Computational Linguistics.            Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
                                                                         Zhu. 2002. BLEU: a method for automatic evaluation of
Nizar Habash. 2007. Syntactic preprocessing for statisti-                machine translation. In Proceedings of the 40th Annual
  cal machine translation. In Proceedings of the Machine                 Meeting of the Association for Computational Linguistics.
  Translation Summit (MT-Summit), Copenhagen.
                                                                      Fatiha Sadat and Nizar Habash. 2006. Combination of arabic
                                                                         preprocessing schemes for statistical machine translation.
Ulf Hermjakob. 2009. Improved word alignment with statis-
                                                                         In Proceedings of the 21st International Conference on
   tics and linguistic heuristics. In Proceedings of the 2009
                                                                         Computational Linguistics and the 44th annual meeting of
   Conference on Empirical Methods in Natural Language
                                                                         the Association for Computational Linguistics, pages 1–8,
   Processing, pages 229–237, Singapore, August.
                                                                         Morristown, NJ, USA.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.                     Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
   Statistical phrase-based translation. In Proceedings of              Micciulla, and John Makhoul. 2006. A study of trans-
   HLT/NAACL-2003, Edmonton, Canada, May.                               lation edit rate with targeted human annotation. In Pro-
                                                                        ceedings of AMTA, pages 223–231, Boston, MA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
   Callison-Burch, Marcello Federico, Nicola Bertoldi,                Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chi-
   Brooke Cowan, Wade Shen, Christine Moran, Richard                    nese syntactic reordering for statistical machine transla-
   Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,                tion. In Proceedings of the 2007 Joint Conference on
   and Evan Herbst. 2007. Moses: Open source toolkit for                Empirical Methods in Natural Language Processing and
   statistical machine translation. In Annual Meeting of the            Computational Natural Language Learning (EMNLP-
   Association for Computational Linguistics (ACL), demon-              CoNLL), pages 737–745.
   stration session, Prague, Czech Republic, June.
                                                                      Fei Xia and Michael McCord. 2004. Improving a statistical
Philipp Koehn. 2004. Statistical significance tests for ma-              mt system with automatically learned rewrite patterns. In
   chine translation evaluation. In Proceedings of the 2004              Proceedings of COLING 2004, pages 508–514, Geneva,
   Conference on Empirical Methods in Natural Language                   Switzerland, August.
   Processing (EMNLP-2004), Barcelona, Spain, July.
                                                                      Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Chunk-
Young-Suk Lee. 2004. Morphological analysis for statistical             level reordering of source language sentences with auto-
  machine translation. In Proceedings of the Human Lan-                 matically learned rules for statistical machine translation.
  guage Technology Conference of the NAACL, pages 57–                   In Human Language Technology Conf. / North American
  60, Boston, MA.                                                       Chapter of the Assoc. for Computational Linguistics An-
                                                                        nual Meeting, Rochester, NY, April.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
  Enhancing the arabic treebank: a collaborative effort to-           Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006.
  ward new annotation guidelines. In Proceedings of the                 Maximum Entropy Based Restoration of Arabic Diacrit-
  Sixth International Language Resources and Evaluation                 ics. In Proceedings of COLING-ACL, the joint conference
  (LREC’08), Marrakech, Morocco.                                        of the International Committee on Computational Linguis-
                                                                        tics and the Association for Computational Linguistics,
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-                     pages 577–584, Sydney, Australia.
  deche, Wigdan Mekki, Sondos Krouna, and Basma
  Bouziri. 2009. The penn arabic treebank part 3 version              Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.
  3.1. Linguistic Data Consortium LDC2008E22.                           2006. Bridging the inflection morphology gap for ara-
                                                                        bic statistical machine translation. In Proceedings of the
Yuval Marton, Nizar Habash, and Owen Rambow. 2010. Im-                  Human Language Technology Conference of the NAACL,
  proving arabic dependency parsing with lexical and in-                Companion Volume: Short Papers, pages 201–204, New
  flectional morphological features. In Proceedings of the              York City, USA.
  11th Meeting of the North American Chapter of the Asso-
  ciation for Computational Linguistics (NAACL) workshop
  on Statistical Parsing of Morphologically Rich Languages
  (SPMRL), Los Angeles.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
   Parser: A Data-Driven Parser-Generator for Dependency
   Parsing. In Proceedings of the Conference on Language
   Resources and Evaluation (LREC).


                                                                183

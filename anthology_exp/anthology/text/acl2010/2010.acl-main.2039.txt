     Efficient Optimization of an MDL-Inspired Objective Function for
                   Unsupervised Part-of-Speech Tagging
        Ashish Vaswani1                         Adam Pauls2                                David Chiang1
            1                                                       2
           Information Sciences Institute                         Computer Science Division
          University of Southern California                   University of California at Berkeley
          4676 Admiralty Way, Suite 1001                                   Soda Hall
            Marina del Rey, CA 90292                                 Berkeley, CA 94720
          {avaswani,chiang}@isi.edu                            adpauls@eecs.berkeley.edu

                    Abstract                                 cently, Ravi and Knight (2009) alternately mini-
                                                             mize the model using an integer linear program
    The Minimum Description Length (MDL)                     and maximize likelihood using EM to achieve the
    principle is a method for model selection                highest accuracies on the task so far. However, in
    that trades off between the explanation of               the latter approach, because there is no single ob-
    the data by the model and the complexity                 jective function to optimize, it is not entirely clear
    of the model itself. Inspired by the MDL                 how to generalize this technique to other prob-
    principle, we develop an objective func-                 lems. In this paper, inspired by the MDL princi-
    tion for generative models that captures                 ple, we develop an objective function for genera-
    the description of the data by the model                 tive models that captures both the description of
    (log-likelihood) and the description of the              the data by the model (log-likelihood) and the de-
    model (model size). We also develop a ef-                scription of the model (model size). By using a
    ficient general search algorithm based on                simple prior that encourages sparsity, we cast our
    the MAP-EM framework to optimize this                    problem as a search for the maximum a poste-
    function. Since recent work has shown that               riori (MAP) hypothesis and present a variant of
    minimizing the model size in a Hidden                    EM to approximately search for the minimum-
    Markov Model for part-of-speech (POS)                    description-length model. Applying our approach
    tagging leads to higher accuracies, we test              to the POS tagging problem, we obtain higher ac-
    our approach by applying it to this prob-                curacies than both EM and Bayesian inference as
    lem. The search algorithm involves a sim-                reported by Goldwater and Griffiths (2007). On a
    ple change to EM and achieves high POS                   Italian POS tagging task, we obtain even larger
    tagging accuracies on both English and                   improvements. We find that our objective function
    Italian data sets.                                       correlates well with accuracy, suggesting that this
                                                             technique might be useful for other problems.
1   Introduction
The Minimum Description Length (MDL) princi-                 2     MAP EM with Sparse Priors
ple is a method for model selection that provides a          2.1    Objective function
generic solution to the overfitting problem (Barron
et al., 1998). A formalization of Ockham’s Razor,            In the unsupervised POS tagging task, we are
it says that the parameters are to be chosen that            given a word sequence w = w1 , . . . , wN and want
minimize the description length of the data given            to find the best tagging t = t1 , . . . , tN , where
the model plus the description length of the model           ti ∈ T , the tag vocabulary. We adopt the problem
itself.                                                      formulation of Merialdo (1994), in which we are
   It has been successfully shown that minimizing            given a dictionary of possible tags for each word
the model size in a Hidden Markov Model (HMM)                type.
for part-of-speech (POS) tagging leads to higher                We define a bigram HMM
accuracies than simply running the Expectation-
                                                                                   N
Maximization (EM) algorithm (Dempster et al.,
                                                                                   Y
                                                                   P(w, t | θ) =         P(w, t | θ) · P(ti | ti−1 )   (1)
1977). Goldwater and Griffiths (2007) employ a                                     i=1
Bayesian approach to POS tagging and use sparse
Dirichlet priors to minimize model size. More re-            In maximum likelihood estimation, the goal is to


                                                       209
                      Proceedings of the ACL 2010 Conference Short Papers, pages 209–214,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


find parameter estimates
                                                                                                  1
                                                                                                                                                       β=0.005
          θ̂ = arg max log P(w | θ)                        (2)                                                                                          β=0.05
                   θ                                                                             0.8                                                     β=0.5
                           X                                                                                                                            1-||θi||0




                                                                           Function Values
             = arg max log    P(w, t | θ)                  (3)                                   0.6
                    θ             t
                                                                                                 0.4
The EM algorithm can be used to find a solution.
However, we would like to maximize likelihood                                                    0.2

and minimize the size of the model simultane-
                                                                                                  0
ously. We define the size of a model as the number                                                     0            0.2             0.4          0.6              0.8             1
                                                                                                                                            θi
of non-zero probabilities in its parameter vector.
Let θ1 , . . . , θn be the components of θ. We would
like to find
                                                                       Figure 1: Ideal model-size term and its approxima-
       θ̂ = arg min − log P(w | θ) + αkθk0
                                                       
                                                           (4)         tions.
                θ

where kθk0 , called the L0 norm of θ, simply counts
the number of non-zero parameters in θ. The                            a posterior parameter estimate, which we find us-
hyperparameter α controls the tradeoff between                         ing MAP-EM (Bishop, 2006):
likelihood maximization and model minimization.                                θ̂ = arg max log P(w, θ)                                                                               (9)
Note the similarity of this objective function with                                                        θ
MDL’s, where α would be the space (measured                                                  = arg max log P(w | θ) + log P(θ)
                                                                                                                                                                        
                                                                                                                                                                                  (10)
                                                                                                           θ
in nats) needed to describe one parameter of the
model.                                                                 Substituting (8) into (10) and ignoring the constant
   Unfortunately, minimization of the L0 norm                          term log Z, we get our objective function (6) again.
is known to be NP-hard (Hyder and Mahata,                                 We can exercise finer control over the sparsity
2009). It is not smooth, making it unamenable                          of the tag-bigram and channel probability distri-
to gradient-based optimization algorithms. There-                      butions by using a different α for each:
fore, we use a smoothed approximation,
                        X       −θi
                                                                        arg max log P(w | θ) +
                kθk0 ≈      1−e   β             (5)                                          θ
                                                                                                                                                                              !
                          i                                                                                         X             −P(w|t)          X              −P(t0 |t)
                                                                                                               αc             e     β       + αt              e     β             (11)
where 0 < β ≤ 1 (Mohimani et al., 2007). For                                                                         w,t                               t,t0
smaller values of β, this closely approximates the                     In our experiments, we set αc = 0 since previ-
desired function (Figure 1). Inverting signs and ig-                   ous work has shown that minimizing the number
noring constant terms, our objective function is                       of tag n-gram parameters is more important (Ravi
now:                                                                   and Knight, 2009; Goldwater and Griffiths, 2007).
                   
                                     X −θi 
                                                                         A common method for preferring smaller mod-
    θ̂ = arg max log P(w | θ) + α
                   
                                          β
                                        e    (6)
                                                                                                         P
                                                                       els is minimizing the L1 norm, i |θi |. However,
            θ                                      i                   for a model which is a product of multinomial dis-
   We can think of the approximate model size as                       tributions, the L1 norm is a constant.
                                                                             X          X
a kind of prior:                                                                |θi | =   θi
                                             −θi
                                                                                             i                 i
                         exp α
                                  P           β
                                                                                                                                           
                                        ie
                P(θ) =
                                                                                                             X X             X
                                                                                                           =        P(w | t) +
                                                                                                                                            
                                                           (7)                                                  
                                                                                                                                 0
                                                                                                                                 P(t | t)
                          Z
                         X −θi                                                                                 t          w                             t0
          log P(θ) = α ·   e β − log Z                     (8)                                             = 2|T |
                              i

                                  −θi
                                                                         Therefore, we cannot use the L1 norm as part of
where Z = dθ exp α i e
                R         P        β
                              is a normalization                       the size term as the result will be the same as the
constant. Then our goal is to find the maximum                         EM algorithm.


                                                                 210


2.2    Parameter optimization                                                         • Gradient of objective function:
To optimize (11), we use MAP EM, which is an it-                                             ∂F         E[C(t, t0 )] αt −P(tβ 0 |t)
erative search procedure. The E step is the same as                                                   =             − e                 (16)
                                                                                           ∂P(t0 | t)    P(t0 | t)   β
in standard EM, which is to calculate P(t | w, θt ),
where the θt are the parameters in the current iter-                                  • Gradient of equality constraints:
ation t. The M step in iteration (t + 1) looks like                                                         
                                                                                                ∂gt         1 if t = t0
                                                                                                            
                                                                                                          =
                                                                                                            
                                                                                                                                        (17)
                                                                                             ∂P(t00 | t0 ) 
                                                                                                            
 θt+1 = arg max E P(t|w,θt ) log P(w, t | θ) +                                                              0 otherwise
                                           
            θ
                   X −P(t0 |t) !               (12)
                                                                                      • Hessian of objective function, which is not
                αt      e    β
                                                                                        required but greatly speeds up the optimiza-
                            t,t0
                                                                                        tion:
Let C(t, w; t, w) count the number of times the                                                                                       −P(t0 |t)
word w is tagged as t in t, and C(t, t0 ; t) the number                                         ∂2 F              E[C(t, t0 )]      e β
                                                                                                              = −              + αt
of times the tag bigram (t, t0 ) appears in t. We can                                    ∂P(t0 | t)∂P(t0 | t)     P(t0 | t)2         β2
rewrite the M step as                                                                                                                 (18)
                                                                                        The other second-order partial derivatives are
                        XX                                                              all zero, as are those of the equality con-
  θt+1 = arg max                       E[C(t, w)] log P(w | t) +                        straints.
               θ        t          w
 XX                                              −P(t0 |t)
                                                                               We perform this optimization for each instance
            E[C(t, t )] log P(t | t) + αt e
                    0                  0            β
                                                                
                                                                   (13)         of (15). These optimizations could easily be per-
  t    t0                                                                         formed in parallel for greater scalability.
subject to the constraints w P(w | t) = 1 and
                             P
                                                                                  3     Experiments
  t0 P(t | t) = 1. Note that we can optimize each
P       0

term of both summations over t separately. For                                    We carried out POS tagging experiments on En-
each t, the term                                                                  glish and Italian.
              X
                 E[C(t, w)] log P(w | t)     (14)                                 3.1    English POS tagging
                w                                                                 To set the hyperparameters αt and β, we prepared
is easily optimized as in EM: just let P(w | t) ∝                                 three held-out sets H1 , H2 , and H3 from the Penn
E[C(t, w)]. But the term                                                          Treebank. Each Hi comprised about 24, 000 words
                                                                                  annotated with POS tags. We ran MAP-EM for
    X                                    −P(t0 |t)
                                                    
         E[C(t, t0 )] log P(t0 | t) + αt e β          (15)                        100 iterations, with uniform probability initializa-
      t0                                                                          tion, for a suite of hyperparameters and averaged
                                                                                  their tagging accuracies over the three held-out
is trickier. This is a non-convex optimization prob-                              sets. The results are presented in Table 2. We then
lem for which we invoke a publicly available                                      picked the hyperparameter setting with the highest
constrained optimization tool, ALGENCAN (An-                                      average accuracy. These were αt = 80, β = 0.05.
dreani et al., 2007). To carry out its optimization,                              We then ran MAP-EM again on the test data with
ALGENCAN requires computation of the follow-                                      these hyperparameters and achieved a tagging ac-
ing in every iteration:                                                           curacy of 87.4% (see Table 1). This is higher than
                                                                                  the 85.2% that Goldwater and Griffiths (2007) ob-
   • Objective function, defined in equation (15).
                                                                                  tain using Bayesian methods for inferring both
     This is calculated in polynomial time using
                                                                                  POS tags and hyperparameters. It is much higher
     dynamic programming.
                                                                                  than the 82.4% that standard EM achieves on the
   • Constraints: gt = t0 P(t0 | t) − 1 = 0 for                                   test set when run for 100 iterations.
                           P
     each tag t ∈ T . Also, we constrain P(t0 | t) to                                Using αt = 80, β = 0.05, we ran multiple ran-
     the interval [, 1].1                                                        dom restarts on the test set (see Figure 2). We find
                                                                                  that the objective function correlates well with ac-
    1
      We must have  > 0 because of the log P(t0 | t) term
in equation (15). It seems reasonable to set   N1 ; in our                      curacy, and picking the point with the highest ob-
experiments, we set  = 10−7 .                                                    jective function value achieves 87.1% accuracy.


                                                                            211


                                                                 β
                  αt
                          0.75    0.5        0.25    0.075    0.05                0.025            0.0075        0.005        0.0025
                 10       82.81   82.78      83.10   83.50    83.76               83.70            84.07         83.95        83.75
                 20       82.78   82.82      83.26   83.60    83.89               84.88            83.74         84.12        83.46
                 30       82.78   83.06      83.26   83.29    84.50               84.82            84.54         83.93        83.47
                 40       82.81   83.13      83.50   83.98    84.23               85.31            85.05         83.84        83.46
                 50       82.84   83.24      83.15   84.08    82.53               84.90            84.73         83.69        82.70
                 60       83.05   83.14      83.26   83.30    82.08               85.23            85.06         83.26        82.96
                 70       83.09   83.10      82.97   82.37    83.30               86.32            83.98         83.55        82.97
                 80       83.13   83.15      82.71   83.00    86.47               86.24            83.94         83.26        82.93
                 90       83.20   83.18      82.53   84.20    86.32               84.87            83.49         83.62        82.03
                100       83.19   83.51      82.84   84.60    86.13               85.94            83.26         83.67        82.06
                110       83.18   83.53      83.29   84.40    86.19               85.18            80.76         83.32        82.05
                120       83.08   83.65      83.71   84.11    86.03               85.39            80.66         82.98        82.20
                130       83.10   83.19      83.52   84.02    85.79               85.65            80.08         82.04        81.76
                140       83.11   83.17      83.34   85.26    85.86               85.84            79.09         82.51        81.64
                150       83.14   83.20      83.40   85.33    85.54               85.18            78.90         81.99        81.88

                       Table 2: Average accuracies over three held-out sets for English.

 system                                   accuracy (%)
 Standard EM                              82.4                                                             αt=80,β=0.05,Test Set 24115 Words
   + random restarts                      84.5                                             0.89
                                                                                           0.88
 (Goldwater and Griffiths, 2007)          85.2                                             0.87

 our approach                             87.4                                             0.86
                                                                        Tagging accuracy

                                                                                           0.85
   + random restarts                      87.1                                             0.84
                                                                                           0.83
                                                                                           0.82
Table 1: MAP-EM with a L0 norm achieves higher                                             0.81
tagging accuracy on English than (2007) and much                                            0.8
                                                                                           0.79
higher than standard EM.
                                                                                           0.78
                                                                                             -53200 -53000 -52800 -52600 -52400 -52200 -52000 -51800 -51600 -51400
                                                                                                                     objective function value
 system                    zero parameters     bigram types
 maximum possible          1389                –
 EM, 100 iterations        444                 924
 MAP-EM, 100 iterations    695                 648                  Figure 2: Tagging accuracy vs. objective func-
                                                                    tion for 1152 random restarts of MAP-EM with
Table 3: MAP-EM with a smoothed L0 norm                             smoothed L0 norm.
yields much smaller models than standard EM.

                                                                    sity Treebank (Bos et al., 2009). This test set com-
We also carried out the same experiment with stan-
                                                                    prises 21, 878 words annotated with POS tags and
dard EM (Figure 3), where picking the point with
                                                                    a dictionary for each word type. Since this is all
the highest corpus probability achieves 84.5% ac-
                                                                    the available data, we could not tune the hyperpa-
curacy.
                                                                    rameters on a held-out data set. Using the hyper-
   We also measured the minimization effect of the
                                                                    parameters tuned on English (αt = 80, β = 0.05),
sparse prior against that of standard EM. Since our
                                                                    we obtained 89.7% tagging accuracy (see Table 4),
method lower-bounds all the parameters by , we
                                                                    which was a large improvement over 81.2% that
consider a parameter θi as a zero if θi ≤ . We
                                                                    standard EM achieved. When we tuned the hyper-
also measured the number of unique tag bigram
                                                                    parameters on the test set, the best setting (αt =
types in the Viterbi tagging of the word sequence.
                                                                    120, β = 0.05 gave an accuracy of 90.28%.
Table 3 shows that our method produces much
smaller models than EM, and produces Viterbi
                                                                    4     Conclusion
taggings with many fewer tag-bigram types.
                                                                    A variety of other techniques in the literature have
3.2   Italian POS tagging
                                                                    been applied to this unsupervised POS tagging
We also carried out POS tagging experiments on                      task. Smith and Eisner (2005) use conditional ran-
an Italian corpus from the Italian Turin Univer-                    dom fields with contrastive estimation to achieve


                                                              212


                                                                                                                             β
                                               αt
                                                         0.75             0.5              0.25             0.075         0.05       0.025   0.0075   0.005   0.0025
                                             10          81.62            81.67            81.63            82.47         82.70      84.64   84.82    84.96   84.90
                                             20          81.67            81.63            81.76            82.75         84.28      84.79   85.85    88.49   85.30
                                             30          81.66            81.63            82.29            83.43         85.08      88.10   86.16    88.70   88.34
                                             40          81.64            81.79            82.30            85.00         86.10      88.86   89.28    88.76   88.80
                                             50          81.71            81.71            78.86            85.93         86.16      88.98   88.98    89.11   88.01
                                             60          81.65            82.22            78.95            86.11         87.16      89.35   88.97    88.59   88.00
                                             70          81.69            82.25            79.55            86.32         89.79      89.37   88.91    85.63   87.89
                                             80          81.74            82.23            80.78            86.34         89.70      89.58   88.87    88.32   88.56
                                             90          81.70            81.85            81.00            86.35         90.08      89.40   89.09    88.09   88.50
                                            100          81.70            82.27            82.24            86.53         90.07      88.93   89.09    88.30   88.72
                                            110          82.19            82.49            82.22            86.77         90.12      89.22   88.87    88.48   87.91
                                            120          82.23            78.60            82.76            86.77         90.28      89.05   88.75    88.83   88.53
                                            130          82.20            78.60            83.33            87.48         90.12      89.15   89.30    87.81   88.66
                                            140          82.24            78.64            83.34            87.48         90.12      89.01   88.87    88.99   88.85
                                            150          82.28            78.69            83.32            87.75         90.25      87.81   88.50    89.07   88.41

                                                                               Table 4: Accuracies on test set for Italian.

                                                                                                                                racy supporting the MDL principle. Our approach
                       0.9
                                                    EM, Test Set 24115 Words                                                    performs quite well on POS tagging for both En-
                                                                                                                                glish and Italian. We believe that, like EM, our
                      0.88
                                                                                                                                method can benefit from more unlabeled data, and
                      0.86
                                                                                                                                there is reason to hope that the success of these
   Tagging accuracy




                      0.84
                                                                                                                                experiments will carry over to other tasks as well.
                      0.82


                       0.8
                                                                                                                                Acknowledgements
                      0.78                                                                                                      We would like to thank Sujith Ravi, Kevin Knight
                      0.76
                                                                                                                                and Steve DeNeefe for their valuable input, and
                        -147500 -147400 -147300 -147200 -147100 -147000 -146900 -146800 -146700 -146600 -146500 -146400
                                                             objective function value                                           Jason Baldridge for directing us to the Italian
                                                                                                                                POS data. This research was supported in part by
                                                                                                                                DARPA contract HR0011-06-C-0022 under sub-
Figure 3: Tagging accuracy vs. likelihood for 1152                                                                              contract to BBN Technologies and DARPA con-
random restarts of standard EM.                                                                                                 tract HR0011-09-1-0028.


88.6% accuracy. Goldberg et al. (2008) provide                                                                                  References
a linguistically-informed starting point for EM to                                                                              R. Andreani, E. G. Birgin, J. M. Martnez, and M. L.
achieve 91.4% accuracy. More recently, Chiang et                                                                                  Schuverdt. 2007. On Augmented Lagrangian meth-
al. (2010) use GIbbs sampling for Bayesian in-                                                                                    ods with general lower-level constraints. SIAM
ference along with automatic run selection and                                                                                    Journal on Optimization, 18:1286–1309.
achieve 90.7%.                                                                                                                  A. Barron, J. Rissanen, and B. Yu. 1998. The min-
   In this paper, our goal has been to investi-                                                                                   imum description length principle in coding and
gate whether EM can be extended in a generic                                                                                      modeling. IEEE Transactions on Information The-
way to use an MDL-like objective function that                                                                                    ory, 44(6):2743–2760.
simultaneously maximizes likelihood and mini-                                                                                   C. Bishop. 2006. Pattern Recognition and Machine
mizes model size. We have presented an efficient                                                                                  Learning. Springer.
search procedure that optimizes this function for
                                                                                                                                J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
generative models and demonstrated that maxi-
                                                                                                                                   dependency treebank to a categorical grammar tree-
mizing this function leads to improvement in tag-                                                                                  bank for italian. In Eighth International Workshop
ging accuracy over standard EM. We infer the hy-                                                                                   on Treebanks and Linguistic Theories (TLT8).
perparameters of our model using held out data
                                                                                                                                D. Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi.
and achieve better accuracies than (Goldwater and                                                                                  2010. Bayesian inference for Finite-State transduc-
Griffiths, 2007). We have also shown that the ob-                                                                                  ers. In Proceedings of the North American Associa-
jective function correlates well with tagging accu-                                                                                tion of Computational Linguistics.


                                                                                                                          213


A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
  Maximum likelihood from incomplete data via the
  EM algorithm. Computational Linguistics, 39(4):1–
  38.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
   find pretty good HMM POS-taggers (when given a
   good start). In Proceedings of the ACL.
S. Goldwater and T. L. Griffiths. 2007. A fully
   Bayesian approach to unsupervised part-of-speech
   tagging. In Proceedings of the ACL.
M. Hyder and K. Mahata. 2009. An approximate L0
  norm minimization algorithm for compressed sens-
  ing. In Proceedings of the 2009 IEEE International
  Conference on Acoustics, Speech and Signal Pro-
  cessing.
B. Merialdo. 1994. Tagging English text with a
  probabilistic model. Computational Linguistics,
  20(2):155–171.
H. Mohimani, M. Babaie-Zadeh, and C. Jutten. 2007.
  Fast sparse representation based on smoothed L0
  norm. In Proceedings of the 7th International Con-
  ference on Independent Component Analysis and
  Signal Separation (ICA2007).
S. Ravi and K. Knight. 2009. Minimized models for
   unsupervised part-of-speech tagging. In Proceed-
   ings of ACL-IJCNLP.
N. Smith. and J. Eisner. 2005. Contrastive estima-
  tion: Training log-linear models on unlabeled data.
  In Proceedings of the ACL.




                                                        214

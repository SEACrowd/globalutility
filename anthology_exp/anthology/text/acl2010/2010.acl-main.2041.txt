              Intelligent Selection of Language Model Training Data

                               Robert C. Moore William Lewis
                                      Microsoft Research
                                  Redmond, WA 98052, USA
                           {bobmoore,wilewis}@microsoft.com



                    Abstract                                    The normal practice when using multiple lan-
    We address the problem of selecting non-                 guages models in machine translation seems to be
    domain-specific language model training                  to train models on as much data as feasible from
    data to build auxiliary language models                  each source, and to depend on feature weight opti-
    for use in tasks such as machine transla-                mization to down-weight the impact of data that is
    tion. Our approach is based on comparing                 less well-matched to the translation application. In
    the cross-entropy, according to domain-                  this paper, however, we show that for a data source
    specific and non-domain-specifc language                 that is not entirely in-domain, we can improve the
    models, for each sentence of the text                    match between the language model from that data
    source used to produce the latter language               source and the desired application output by intel-
    model. We show that this produces better                 ligently selecting a subset of the available data as
    language models, trained on less data, than              language model training data. This not only pro-
    both random data selection and two other                 duces a language model better matched to the do-
    previously proposed methods.                             main of interest (as measured in terms of perplex-
                                                             ity on held-out in-domain data), but it reduces the
1   Introduction                                             computational resources needed to exploit a large
Statistical N-gram language models are widely                amount of non-domain-specific data, since the re-
used in applications that produce natural-language           sources needed to filter a large amount of data are
text as output, particularly speech recognition and          much less (especially in terms of memory) than
machine translation. It seems to be a univer-                those required to build a language model from all
sal truth that output quality can always be im-              the data.
proved by using more language model training
                                                             2   Approaches to the Problem
data, but only if the training data is reasonably
well-matched to the desired output. This presents            Our approach to the problem assumes that we have
a problem, because in virtually any particular ap-           enough in-domain data to train a reasonable in-
plication the amount of in-domain data is limited.           domain language model, which we then use to
   Thus it has become standard practice to com-              help score text segments from other data sources,
bine in-domain data with other data, either by               and we select segments based on a score cutoff op-
combining N-gram counts from in-domain and                   timized on held-out in-domain data.
other data (usually weighting the counts in some                We are aware of two comparable previous ap-
way), or building separate language models from              proaches. Lin et al. (1997) and Gao et al. (2002)
different data sources, interpolating the language           both used a method similar to ours, in which the
model probabilities either linearly or log-linearly.         metric used to score text segments is their perplex-
Log-linear interpolation is particularly popular             ity according to the in-domain language model.
in statistical machine translation (e.g., Brants et          The candidate text segments with perplexity less
al., 2007), because the interpolation weights can            than some threshold are selected.
easily be discriminatively trained to optimize an               The second previous approach does not explic-
end-to-end translation objective function (such as           itly make use of an in-domain language model, but
B LEU) by making the log probability according to            is still applicable to our scenario. Klakow (2000)
each language model a separate feature function in           estimates a unigram language model from the
the overall translation model.                               entire non-domain-specific corpus to be selected


                                                       220
                      Proceedings of the ACL 2010 Conference Short Papers, pages 220–224,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


from, and scores each candidate text segment from
that corpus by the change in the log likelihood
                                                                                      P (s|NI , N )P (NI |N )
of the in-domain data according to the unigram                      P (NI |s, N ) =
model, if that segment were removed from the cor-                                           P (s|N )
pus used to estimate the unigram model. Those                    Since NI is a subset of N , P (s|NI , N ) =
segments whose removal would decrease the log                 P (s|NI ), and by our assumption about the rela-
likelihood of the in-domain data more than some               tionship of I and NI , P (s|NI ) = P (s|I). Hence,
threshold are selected.
   Our method is a fairly simple variant of scoring                                      P (s|I)P (NI |N )
                                                                       P (NI |s, N ) =
by perplexity according to an in-domain language                                              P (s|N )
model. First, note that selecting segments based
                                                              If we could estimate all the probabilities in the
on a perplexity threshold is equivalent to selecting
                                                              right-hand side of this equation, we could use it
based on a cross-entropy threshold. Perplexity and
                                                              to select text segments that have a high probability
cross-entropy are monotonically related, since the
                                                              of being in NI .
perplexity of a string s according to a model M is
                                                                  We can estimate P (s|I) and P (s|N ) by train-
simply bHM (s) , where HM (s) is the cross-entropy
                                                              ing language models on I and a sample of N , re-
of s according to M and b is the base with re-
                                                              spectively. That leaves us only P (NI |N ), to es-
spect to which the cross-entropy is measured (e.g.,
                                                              timate, but we really don’t care what P (NI |N )
bits or nats). However, instead of scoring text seg-
                                                              is, because knowing that would still leave us won-
ments by perplexity or cross-entropy according to
                                                              dering what threshold to set on P (NI |s, N ). We
the in-domain language model, we score them by
                                                              don’t care about classification accuracy; we care
the difference of the cross-entropy of a text seg-
                                                              only about the quality of the resulting language
ment according to the in-domain language model
                                                              model, so we might as well just attempt to find
and the cross-entropy of the text segment accord-
                                                              a threshold on P (s|I)/P (s|N ) that optimizes the
ing to a language model trained on a random sam-
                                                              fit of the resulting language model to held-out in-
ple of the data source from which the text segment
                                                              domain data.
is drawn.
                                                                  Equivalently, we can work in the log domain
   To state this formally, let I be an in-domain data         with the quantity log(P (s|I)) − log(P (s|N )).
set and N be a non-domain-specific (or otherwise              This gets us very close to working with the differ-
not entirely in-domain) data set. Let HI (s) be the           ence in cross-entropies, because HI (s)−HN (s) is
per-word cross-entropy, according to a language               just a length-normalized version of log(P (s|I)) −
model trained on I, of a text segment s drawn from            log(P (s|N )), with the sign reversed. The rea-
N . Let HN (s) be the per-word cross-entropy of s             son that we need to normalize for length is that
according to a language model trained on a ran-               the value of log(P (s|I)) − log(P (s|N )) tends to
dom sample of N . We partition N into text seg-               correlate very strongly with text segment length.
ments (e.g., sentences), and score the segments ac-           If the candidate text segments vary greatly in
cording to HI (s) − HN (s), selecting all text seg-           length—e.g., if we partition N into sentences—
ments whose score is less than a threshold T .                this correlation can be a serious problem.
   This method can be justified by reasoning sim-                 We estimated this effect on a 1000-sentence
liar to that used to derive methods for training              sample of our experimental data described be-
binary text classifiers without labeled negative              low, and found the correlation between sentence
examples (Denis et al., 2002; Elkin and Noto,                 log probability difference and sentence length to
2008). Let us imagine that our non-domain-                    be r = −0.92, while the cross-entropy differ-
specific corpus N contains an in-domain subcor-               ence was almost uncorrelated with sentence length
pus NI , drawn from the same distribution as our              (r = 0.04). Hence, using sentence probability ra-
in-domain corpus I. Since NI is statistically just            tios or log probability differences as our scoring
like our in-domain data I, it would seem to be a              function would result in selecting disproportion-
good candidate for the data that we want to extract           ately very short sentences. We tested this in an
from N . By a simple variant of Bayes rule, the               experiment not described here in detail, and found
probability P (NI |s, N ) of a text segment s, drawn          it not to be significantly better as a selection crite-
randomly from N , being in NI is given by                     rion than random selection.


                                                        221


    Corpus           Sentence count     Token count          discounted probability mass at the unigram level
    Gigaword           133,310,562    3,445,946,266          was added to the probability of <UNK>. A count
    Europarl train        1,651,392      48,230,859          cutoff of 2 occurrences was applied to the trigrams
    Europarl test             2,000          55,566          and 4-grams in estimating these models.
                                                                We computed the cross-entropy of each sen-
            Table 1: Corpus size statistics                  tence in the Gigaword corpus according to both
                                                             models, and scored each sentence by the differ-
                                                             ence in cross-entropy, HEp (s)−HGw (s). We then
3     Experiments                                            selected subsets of the Gigaword data correspond-
                                                             ing to 8 cutoff points in the cross-entropy differ-
We have empirically evaluated our proposed
                                                             ence scores, and trained 4-gram models (again us-
method for selecting data from a non-domain-
                                                             ing absolute discounting with a discount of 0.7) on
specific source to model text in a specific domain.
                                                             each of these subsets and on the full Gigaword cor-
For the in-domain corpus, we chose the English
                                                             pus. These language models were estimated with-
side of the English-French parallel text from re-
                                                             out restricting the vocabulary or applying count
lease v5 of the Europarl corpus (Koehn, 2005).
                                                             cutoffs, but the only parameters computed were
This consists of proceedings of the European Par-
                                                             those needed to determine the perplexity of the
liament from 1999 through 2009. We used the
                                                             held-out Europarl test set, which saves a substan-
text from 1999 through 2008 as in-domain train-
                                                             tial amount of computation in determining the op-
ing data, and we used the first 2000 sentences
                                                             timal selection threshold.
from January 2009 as test data. For the non-
                                                                We compared our selection method to three
domain-specific corpus, we used the LDC Eng-
                                                             other methods. As a baseline, we trained lan-
lish Gigaword Third Edition (LDC Catalog No.:
                                                             guage models on random subsets of the Gigaword
LDC2007T07).
                                                             corpus of approximately equal size to the data
   We used a simple tokenization scheme on all
                                                             sets produced by the cutoffs we selected for the
data, splitting on white space and on boundaries
                                                             cross-entropy difference scores. Next, we scored
between alphanumeric and nonalphanumeric (e.g.,
                                                             all the Gigaword sentences by the cross-entropy
punctuation) characters. With this tokenization,
                                                             according to the Europarl-trained model alone.
the sizes of our data sets in terms of sentences and
                                                             As we noted above, this is equivalent to the in-
tokens are shown in Table 1. The token counts in-
                                                             domain perplexity scoring method used by Lin et
clude added end-of-sentence tokens.
                                                             al. (1997) and Gao et al. (2002). Finally, we im-
   To implement our data selection method we re-             plemented Klakow’s (2000) method, scoring each
quired one language model trained on the Europarl            Gigaword sentence by removing it from the Giga-
training data and one trained on the Gigaword                word corpus and computing the difference in the
data. To make these language models comparable,              log likelihood of the Europarl corpus according to
and to show the feasibility of optimizing the fit to         unigram models trained on the Gigaword corpus
the in-domain data without training a model on the           with and without that sentence. With the latter two
entire Gigaword corpus, we trained the Gigaword              methods, we chose cutoff points in the resulting
language model for data selection on a random                scores to produce data sets approximately equal in
sample of the Gigaword corpus of a similar size to           size to those obtained using our selection method.
that of the Europarl training data: 1,874,051 sen-
tences, 48,459,945 tokens.                                   4   Results
   To further increase the comparability of these
Europarl and Gigaword language models, we re-                For all four selection methods, plots of test set per-
stricted the vocabulary of both models to the to-            plexity vs. the number of training data tokens se-
kens appearing at least twice in the Europarl train-         lected are displayed in Figure 1. (Note that the
ing data, treating all other tokens as instances of          training data token counts are displayed on a log-
<UNK>. With this vocabulary, 4-gram language                 arithmic scale.) The test set perplexity for the lan-
models were trained on both the Europarl training            guage model trained on the full Gigaword corpus
data and the Gigaword random sample using back-              is 135. As we might expect, reducing training
off absolute discounting (Ney et al. 1994), with a           data by random sampling always increases per-
discount of 0.7 used for all N-gram lengths. The             plexity. Selecting Gigaword sentences by their


                                                       222


                        240



                        220



                        200
  Test-set perplexity




                                                                                                          random selection
                        180
                                                                                                          in-domain cross-entropy scoring

                        160                                                                               Klakow's method

                                                                                                          cross-entropy difference scoring
                        140



                        120



                        100
                              0.01               0.1                      1                        10

                                              Billions of words of training data


                                                 Figure 1: Test set perplexity vs. training set size

                                     Selection Method                         Original LM PPL           Modified LM PPL
                                     in-domain cross-entropy scoring                124.4                    124.8
                                     Klakow’s method                                110.5                    110.8
                                     cross-entropy difference scoring               100.7                    101.9

                                                Table 2: Results adjusted for vocabulary coverage


cross-entropy according to the Europarl-trained                                     the training sets that appear to produce the lowest
model is effective in reducing both test set perplex-                               perplexity for each selection method, however, the
ity and training corpus size, with an optimum per-                                  spread of OOV counts is much narrower, ranging
plexity of 124, obtained with a model built from                                    53 (0.10%) for best training set based on cross-
36% of the Gigaword corpus. Klakow’s method                                         entropy difference scoring, to 20 (0.03%), for ran-
is even more effective, with an optimum perplex-                                    dom selection.
ity of 111, obtained with a model built from 21%                                       To control for the difference in vocabulary, we
of the Gigaword corpus. The cross-entropy differ-                                   estimated a modified 4-gram language model for
ence selection method, however, is yet more effec-                                  each selection method (other than random se-
tive, with an optimum perplexity of 101, obtained                                   lection) using the training set that appeared to
with a model built from less than 7% of the Giga-                                   produce the lowest perplexity for that selection
word corpus.                                                                        method in our initial experiments. In the modified
   The comparisons implied by Figure 1, how-                                        language models, the unigram model based on the
ever, are only approximate, because each perplex-                                   selected training set is smoothed by absolute dis-
ity (even along the same curve) is computed with                                    counting, and backed-off to an unsmoothed uni-
respect to a different vocabulary, resulting in a dif-                              gram model based on the full Gigaword corpus.
ferent out-of-vocabulary (OOV) rate. OOV tokens                                     This produces language models that are normal-
in the test data are excluded from the perplexity                                   ized over the same vocabulary as a model trained
computation, so the perplexity measurements are                                     on the full Gigaword corpus; thus the test set has
not strictly comparable.                                                            the same OOVs for each model.
   Out of the 55566 test set tokens, the number                                        Test set perplexity for each of these modifed
of OOV tokens ranges from 418 (0.75%), for the                                      language models is compared to that of the orig-
smallest training set based on in-domain cross-                                     inal version of the model in Table 2. It can be
entropy scoring, to 20 (0.03%), for training on                                     seen that adjusting the vocabulary in this way, so
the full Gigaword corpus. If we consider only                                       that all models are based on the same vocabulary,


                                                                              223


yields only very small changes in the measured                 ACM Transactions on Asian Language Informa-
test-set perplexity, and these differences are much            tion Processing, 1(1):3–33.
smaller than the differences between the different
selection methods, whichever way the vocabulary              Dietrich Klakow. 2000. Selecting articles from
of the language models is determined.                          the language model training corpus. In ICASSP
                                                               2000, June 5–9, Istanbul, Turkey, vol. 3, 1695–
5   Conclusions                                                1698.

The cross-entropy difference selection method in-            Philipp Koehn. 2005. Europarl: a parallel cor-
troduced here seems to produce language mod-                   pus for statistical machine translation. In MT
els that are both a better match to texts in a re-             Summit X, September 12–16, Phuket, Thailand,
stricted domain, and require less data for train-              79–86.
ing, than any of the other data selection methods
tested. This study is preliminary, however, in that          Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,
we have not yet shown improved end-to-end task                 Ker-Jiann Chen, and Lin-Shan Lee. 1997.
performance applying this approach, such as im-                Chinese language model adaptation based on
proved B LEU scores in a machine translation task.             document classification and multiple domain-
However, we believe there is reason to be opti-                specific language models. In EUROSPEECH-
mistic about this. When a language model trained               1997, 1463–1466.
on non-domain-specific data is used in a statisti-           Hermann Ney, Ute Essen, and Reinhard Kneser.
cal translation model as a separate feature func-              1994. On structuring dependencies in stochas-
tion (as is often the case), lower perplexity on in-           tic language modelling. Computer Speech and
domain target language test data derived from ref-             Language, 8:1–38.
erence translations corresponds directly to assign-
ing higher language model feature scores to those
reference translations, which should in turn lead to
translation system output that matches reference
translations better.

References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz
  J. Och, and Jeffrey Dean. 2007. Large language
  models in machine translation. In Proceedings
  of the Joint Conference on Empirical Methods
  in Natural Language Processing and Computa-
  tional Natural Language Learning, June 28–30,
  Prague, Czech Republic, 858–867.

François Denis, Remi Gilleron, and Marc Tom-
  masi. 2002. Text classification from positive
  and unlabeled examples. In The 9th Interna-
  tional Conference on Information Processing
  and Management of Uncertainty in Knowledge-
  Based Systems (IPMU 2002), 1927–1934.

Charles Elkin and Keith Noto. 2008. Learn-
  ing classifiers from only positive and unlabeled
  data. In KDD 2008, August 24–27, Las Vegas,
  Nevada, USA, 213–220.

Jianfeng Gao, Joshua Goodman, Mingjing Li, and
   Kai-Fu Lee. 2002. Toward a unified approach
   to statistical language modeling for Chinese.


                                                       224

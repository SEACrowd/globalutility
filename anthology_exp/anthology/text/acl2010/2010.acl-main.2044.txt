Optimizing Question Answering Accuracy by Maximizing Log-Likelihood

               Matthias H. Heie, Edward W. D. Whittaker and Sadaoki Furui
                             Department of Computer Science
                               Tokyo Institute of Technology
                                  Tokyo 152-8552, Japan
                   {heie,edw,furui}@furui.cs.titech.ac.jp


                    Abstract                                 of q-a pairs. This framework was used in several
                                                             TREC evaluations where it placed in the top 10
    In this paper we demonstrate that there                  of participating systems (Whittaker et al., 2006).
    is a strong correlation between the Ques-                In Section 3 we show that answer accuracy is
    tion Answering (QA) accuracy and the                     strongly correlated with the log-likelihood of the
    log-likelihood of the answer typing com-                 q-a pairs computed by this statistical model. In
    ponent of our statistical QA model. We                   Section 4 we propose an algorithm to cluster q-a
    exploit this observation in a clustering al-             pairs by maximizing the log-likelihood of a dis-
    gorithm which optimizes QA accuracy by                   joint set of q-a pairs. In Section 5 we evaluate the
    maximizing the log-likelihood of a set of                QA accuracy by training the QA system with the
    question-and-answer pairs. Experimental                  resulting clusters.
    results show that we achieve better QA ac-
    curacy using the resulting clusters than by              2 QA system
    using manually derived clusters.
                                                             In our QA framework we choose to model only
                                                             the probability of an answer A given a question Q,
1   Introduction
                                                             and assume that the answer A depends on two sets
Question Answering (QA) distinguishes itself                 of features: W = W (Q) and X = X(Q):
from other information retrieval tasks in that the
system tries to return accurate answers to queries                        P (A|Q) = P (A|W, X),              (1)
posed in natural language. Factoid QA limits it-             where W represents a set of |W | features describ-
self to questions that can usually be answered with          ing the question-type part of Q such as who, when,
a few words. Typically factoid QA systems em-                where, which, etc., and X is a set of features
ploy some form of question type analysis, so that            which describes the “information-bearing” part of
a question such as What is the capital of Japan?             Q, i.e. what the question is actually about and
will be answered with a geographical term. While             what it refers to. For example, in the questions
many QA systems use hand-crafted rules for this              Where is Mount Fuji? and How high is Mount
task, such an approach is time-consuming and                 Fuji?, the question type features W differ, while
doesn’t generalize well to other languages. Ma-              the information-bearing features X are identical.
chine learning methods have been proposed, such              Finding the best answer Â involves a search over
as question classification using support vector ma-          all A for the one which maximizes the probability
chines (Zhang and Lee, 2003) and language mod-               of the above model, i.e.:
eling (Merkel and Klakow, 2007). In these ap-
proaches, question categories are predefined and a                      Â = arg max P (A|W, X).             (2)
classifier is trained on manually labeled data. This                                 A
is an example of supervised learning. In this pa-               Given the correct probability distribution, this
per we present an unsupervised method, where we              will give us the optimal answer in a maximum
attempt to cluster question-and-answer (q-a) pairs           likelihood sense. Using Bayes’ rule, assuming
without any predefined question categories, hence            uniform P (A) and that W and X are indepen-
no manually class-labeled questions are used.                dent of each other given A, in addition to ignoring
   We use a statistical QA framework, described in           P (W, X) since it is independent of A, enables us
Section 2, where the system is trained with clusters         to rewrite Eq. (2) as


                                                       236
                      Proceedings of the ACL 2010 Conference Short Papers, pages 236–240,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


                                                                    P (W | ceW ) is implemented as trigram langu-
                                                                 age models with backoff smoothing using absolute
      Â = arg max P (A | X) · P (W | A).           (3)          discounting (Huang et al., 2001).
                A |    {z } | {z }
                          retrieval     f ilter                     Due to data sparsity, our set of example q-a
                           model        model                    pairs cannot be expected to cover all the possi-
                                                                 ble answers to questions that may ever be asked.
2.1 Retrieval Model
                                                                 We therefore employ answer class modeling rather
The retrieval model P (A|X) is essentially a lan-                than answer word modeling by expanding Eq. (4)
guage model which models the probability of an                   as follows:
answer sequence A given a set of information-
bearing features X = {x1 , . . . , x|X| }. This set
is constructed by extracting single-word features                                |C
                                                                                  P E|
                                                                   P (W | A) =           P (W | ceW )·
from Q that are not present in a stop-list of high-                               e=1
                                                                               |K
                                                                                                                  (5)
frequency words. The implementation of the re-                                  PA |
trieval model used for the experiments described                                     P (ceA   | ka )P (ka | A),
                                                                               a=1
in this paper, models the proximity of A to fea-
tures in X. It is not examined further here;               where ka is a concrete class in the set of |KA |
see (Whittaker et al., 2005) for more details.             answer classes KA . These classes are generated
                                                           using the Kneser-Ney clustering algorithm, com-
2.2 Filter Model                                           monly used for generating class definitions for
The question-type feature set W = {w1 , . . . , w|W | } class language models (Kneser and Ney, 1993).
is constructed by extracting n-tuples (n = 1, 2, . . .)       In this paper we restrict ourselves to single-
such as where, in what and when were from the              word answers; see (Whittaker et al., 2005) for the
input question Q. We limit ourselves to extracting         modeling of multi-word answers. We estimate
single-word features. The 2522 most frequent               P (ceA | kA ) as
words in a collection of example questions are
considered in-vocabulary words; all other words                                           f (kA , ceA )
                                                                      P (ceA | kA ) =                      ,   (6)
are out-of-vocabulary words, and substituted with                                      |C
                                                                                        P E|
                                                                                                       g
hUNKi.                                                                                        f (kA , cA )
                                                                                        g=1
   Modeling the complex relationship between
W and A directly is non-trivial. We there-                 where
fore introduce an intermediate variable CE =                                           P
                                                                                               δ(i ∈ kA )
{c1 , . . . , c|CE | }, representing a set of classes of                             ∀i:i∈c e
example q-a pairs. In order to construct these                       f (kA , ceA ) =        A
                                                                                                            ,  (7)
                                                                                              |ceA |
classes, given a set E = {t1 , . . . , t|E| } of ex-
ample q-a pairs, we define a mapping function              and δ(·) is a discrete indicator function which
f : E 7→ CE which maps each example q-a pair tj            equals 1 if its argument evaluates true and 0 if
for j = 1 . . . |E| into a particular class f (tj ) = ce . false.
Thus each class ce may be defined as the union of             P (ka | A) is estimated as
all component q-a features from each tj satisfy-
ing f (tj ) = ce . Hence each class ce constitutes a                                            1
                                                                     P (ka | A) = P                          . (8)
cluster of q-a pairs. Finally, to facilitate modeling                                           δ(A ∈ j)
we say that W is conditionally independent of A                                      ∀j:j∈Ka

given ce so that,
                                                                 3 The Relationship between Mean
                                                                   Reciprocal Rank and Log-Likelihood
                 |CE |
                 X                                               We use Mean Reciprocal Rank (M RR) as our
  P (W | A) =            P (W | ceW ) · P (ceA | A), (4)
                 e=1
                                                                 metric when evaluating the QA accuracy on a set
                                                                 of questions G = {g1 ...g|G| }:
where ceW and ceA refer to the subsets of question-
                                                                                           P|G|
type features and example answers for the class ce ,                                          i=1 1/Ri
respectively.                                                                 M RR =                     ,        (9)
                                                                                                |G|

                                                           237


            0.23                                                               init: c1 ∈ CE contains all training pairs |E|
                                                  ρ = 0.86
            0.22                                                               while improvement > threshold do
            0.21                                                                  best LLdev ← −∞
             0.2
                                                                                  for all j = 1...|E| do
                                                                                     original cluster = f (tj )
    MRR




            0.19
                                                                                     Take tj out of f (tj )
            0.18
                                                                                     for e = −1, 1...|CE |, |CE | + 1 do
            0.17
                                                                                       Put tj in ce
            0.16                                                                       Calculate LLdev
            0.15
                   -1.18        -1.16         -1.14     -1.12
                                                                                       if LLdev > best LLdev then
                                         LL                                               best LLdev ← LLdev
                                                                                          best cluster ← e
Figure 1: M RR vs. LL (average per q-a pair) for                                          best pair ← j
100 random cluster configurations.                                                     end if
                                                                                       Take tj out of ce
where Ri is the rank of the highest ranking correct                                  end for
candidate answer for gi .                                                            Put tj back in original cluster
   Given a set D = (d1 ...d|D| ) of q-a pairs disjoint                            end for
from the q-a pairs in CE , we can, using Eq. (5),                                 Take tbest pair out of f (tbest pair )
calculate the log-likelihood as                                                   Put tbest pair into cbest cluster
                                                                               end while
                   |D|                                                          In this algorithm, c−1 indicates the set of train-
                   X                                                         ing pairs outside the cluster configuration, thus ev-
    LL =                 log P (Wd |Ad )
                   d=1
                                                                             ery training pair will not necessarily be included
                   |D|
                   X           |CE |
                               X                                             in the final configuration. c|C|+1 refers to a new,
              =          log           P (Wd | ceW )·           (10)         empty cluster, hence this algorithm automatically
                   d=1         e=1                                           finds the optimal number of clusters as well as the
                    |K
                     PA |                                                    optimal configuration of them.
                            P (ceA | ka )P (ka | Ad ).
                    a=1
                                                                             5 Experiments
   To examine the relationship between M RR and
LL, we randomly generate configurations CE ,                                 5.1 Experimental Setup
with a fixed cluster size of 4, and plot the result-
                                                                             For our data sets, we restrict ourselves to questions
ing M RR and LL, computed on the same data set
                                                                             that start with who, when or where. Furthermore,
D, as data points in a scatter plot, as seen in Fig-
                                                                             we only use q-a pairs which can be answered with
ure 1. We find that LL and M RR are strongly
                                                                             a single word. As training data we use questions
correlated, with a correlation coefficient ρ = 0.86.
                                                                             and answers from the Knowledge-Master collec-
   This observation indicates that we should be
                                                                             tion1 . Development/evaluation questions are the
able to improve the answer accuracy of the QA
                                                                             questions from TREC QA evaluations from TREC
system by optimizing the LL of the filter model
                                                                             2002 to TREC 2006, the answers to which are to
in isolation, similar to how, in automatic speech
                                                                             be retrieved from the AQUAINT corpus. In total
recognition, the LL of the language model can
                                                                             we have 2016 q-a pairs for training and 568 ques-
be optimized in isolation to improve the speech
                                                                             tions for development/evaluation. We are able to
recognition accuracy (Huang et al., 2001).
                                                                             retrieve the correct answer for 317 of the devel-
4         Clustering algorithm                                               opment/evaluation questions, thus the theoretical
                                                                             upper bound for our experiments is an answer ac-
Using the observation that LL is correlated with                             curacy of M RR = 0.558.
M RR on the same data set, we expect that opti-                                 Accuracy is evaluated using 5-fold (rotating)
mizing LL on a development set (LLdev ) will also                            cross-validation, where in each fold the TREC
improve M RR on an evaluation set (M RReval ).                               QA data is partitioned into a development set of
Hence we propose the following greedy algorithm
to maximize LLdev :                                                             1
                                                                                    http://www.greatauk.com/


                                                                       238


 Configuration     LLeval     M RReval     #clusters                     0                                             0.32

    manual          -1.18      0.262           3                       -0.2                                            0.3

   all-in-one       -1.32      0.183           1                       -0.4                                            0.28
  one-in-each       -0.87      0.263         2016                      -0.6
                                                                                               LLdev                   0.26
                                                                                              MRRdev




                                                                                                                              MRR
   automatic        -0.24      0.281           4




                                                                  LL
                                                                                                                       0.24
                                                                       -0.8
                                                                                                                       0.22
Table 1: LLeval (average per q-a pair) and                              -1
                                                                                                                       0.2
M RReval (over all held-out TREC years), and                           -1.2
                                                                                                                       0.18
number of clusters (median of the cross-evaluation                     -1.4
folds) for the various configurations.                                         0    400   800 1200       1600   2000
                                                                                                                       0.16

                                                                                          # iterations

4 years’ data and an evaluation set of one year’s                             (a) Development set, 4 year’s TREC.
data. For each TREC question the top 50 doc-                             0                                             0.32
uments from the AQUAINT corpus are retrieved                           -0.2                                            0.3
using Lucene2 . We use the QA system described                                                                         0.28
                                                                       -0.4
in Section 2 for QA evaluation. Our evaluation                                                                         0.26
                                                                       -0.6
metric is M RReval , and LLdev is our optimiza-




                                                                                                                              MRR
                                                                  LL
                                                                                               LLeval                  0.24
tion criterion, as motivated in Section 3.                             -0.8                   MRReval
                                                                                                                       0.22
   Our baseline system uses manual clusters.                            -1
                                                                                                                       0.2
These clusters are obtained by putting all who q-a                     -1.2
                                                                                                                       0.18
pairs in one cluster, all when pairs in a second and                   -1.4
all where pairs in a third. We compare this baseline                           0    400   800 1200       1600   2000
                                                                                                                       0.16

with using clusters resulting from the algorithm                                          # iterations
described in Section 4. We run this algorithm until                            (b) Evaluation set, 1 year’s TREC.
there are no further improvements in LLdev . Two
other cluster configurations are also investigated:            Figure 2: M RR and LL (average per q-a pair)
all q-a pairs in one cluster (all-in-one), and each q-         vs. number of algorithm iterations for one cross-
a pair in its own cluster (one-in-each). The all-in-           validation fold.
one configuration is equivalent to not using the fil-
ter model, i.e. answer candidates are ranked solely
                                                               6 Discussion
by the retrieval model. The one-in-each configura-
tion was shown to perform well in the TREC 2006                Manual inspection of the automatically derived
QA evaluation (Whittaker et al., 2006), where it               clusters showed that the algorithm had constructed
ranked 9th among 27 participants on the factoid                configurations where typically who, when and
QA task.                                                       where q-a pairs were put in separate clusters, as in
                                                               the manual configuration. However, in some cases
5.2 Results                                                    both who and where q-a pairs occurred in the same
In Table 1, we see that the manual clusters (base-             cluster, so as to better answer questions like Who
line) achieves an M RReval of 0.262, while the                 won the World Cup?, where the answer could be a
clusters resulting from the clustering algorithm               country name.
give an M RReval of 0.281, which is a relative                    As can be seen from Table 1, there are only 4
improvement of 7%. This improvement is sta-                    clusters in the automatic configuration, compared
tistically significant at the 0.01 level using the             to 2016 in the one-in-each configuration. Since
Wilcoxon signed-rank test. The one-in-each clus-               the computational complexity of the filter model
ter configuration achieves an M RReval of 0.263,               described in Section 2.2 is linear in the number of
which is not a statistically significant improvement           clusters, a beneficial side effect of our clustering
over the baseline. The all-in-one cluster configura-           procedure is a significant reduction in the compu-
tion (i.e. no filter model) has the lowest accuracy,           tational requirement of the filter model.
with an M RReval of 0.183.                                        In Figure 2 we plot LL and M RR for one of
                                                               the cross-validation folds over multiple iterations
   2
       http://lucene.apache.org/                               (the while loop) of the clustering algorithm in Sec-


                                                         239


 tion 4. It can clearly be seen that the optimization
 of LLdev leads to improvement in M RReval , and
 that LLeval is also well correlated with M RReval .

 7   Conclusions and Future Work
 In this paper we have shown that the log-likelihood
 of our statistical model is strongly correlated with
 answer accuracy. Using this information, we have
 clustered training q-a pairs by maximizing log-
 likelihood on a disjoint development set of q-a
 pairs. The experiments show that with these clus-
 ters we achieve better QA accuracy than using
 manually clustered training q-a pairs.
    In future work we will extend the types of ques-
 tions that we consider, and also allow for multi-
 word answers.

 Acknowledgements
 The authors wish to thank Dietrich Klakow for his
 discussion at the concept stage of this work. The
 anonymous reviewers are also thanked for their
 constructive feedback.


 References
[Huang et al.2001] Xuedong Huang, Alex Acero and
   Hsiao-Wuen Hon. 2001. Spoken Language Pro-
   cessing. Prentice-Hall, Upper Saddle River, NJ,
   USA.

[Kneser and Ney1993] Reinhard Kneser and Hermann
   Ney. 1993. Improved Clustering Techniques for
   Class-based Statistical Language Modelling. Pro-
   ceedings of the European Conference on Speech
   Communication and Technology (EUROSPEECH).

[Merkel and Klakow2007] Andreas Merkel and Diet-
   rich Klakow. 2007. Language Model Based Query
   Classification. Proceedings of the European Confer-
   ence on Information Retrieval (ECIR).

[Whittaker et al.2005] Edward Whittaker, Sadaoki Fu-
   rui and Dietrich Klakow. 2005. A Statistical Clas-
   sification Approach to Question Answering using
   Web Data. Proceedings of the International Con-
   ference on Cyberworlds.

[Whittaker et al.2006] Edward Whittaker, Josef Novak,
   Pierre Chatain and Sadaoki Furui. 2006. TREC
   2006 Question Answering Experiments at Tokyo In-
   stitute of Technology. Proceedings of The Fifteenth
   Text REtrieval Conference (TREC).

[Zhang and Lee2003] Dell Zhang and Wee Sun Lee.
   2003. Question Classification using Support Vec-
   tor Machines. Proceedings of the Special Interest
   Group on Information Retrieval (SIGIR).



                                                         240

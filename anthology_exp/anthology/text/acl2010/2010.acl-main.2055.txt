 A Semi-Supervised Key Phrase Extraction Approach: Learning from
        Title Phrases through a Document Semantic Network

                Decong Li1, Sujian Li1, Wenjie Li2, Wei Wang1, Weiguang Qu3
                 1
                   Key Laboratory of Computational Linguistics, Peking University
               2
                 Department of Computing, The Hong Kong Polytechnic University
           3
             School of Computer Science and Technology, Nanjing Normal University
    {lidecong,lisujian, wwei }@pku.edu.cn cswjli@comp.polyu.edu.hk wgqu@njnu.edu.cn


                                                             of these two kinds of methods, we propose a
                      Abstract                               novel semi-supervised key phrase extraction ap-
                                                             proach in this paper, which explores title phrases
     It is a fundamental and important task to ex-           as the source of knowledge.
     tract key phrases from documents. Generally,                It is well agreed that the title has a similar role
     phrases in a document are not independent in            to the key phrases. They are both elaborated to
     delivering the content of the document. In or-          reflect the content of a document. Therefore,
     der to capture and make better use of their re-
                                                             phrases in the titles are often appropriate to be
     lationships in key phrase extraction, we sug-
     gest exploring the Wikipedia knowledge to               key phrases. That is why position has been a
     model a document as a semantic network,                 quite effective feature in the feature-based key
     where both n-ary and binary relationships               phrase extraction methods (Witten, 1999), i.e., if
     among phrases are formulated. Based on a                a phrase is located in the title, it is ranked higher.
     commonly accepted assumption that the title                 However, one can only include a couple of
     of a document is always elaborated to reflect           most important phrases in the title prudently due
     the content of a document and consequently              to the limitation of the title length, even though
     key phrases tend to have close semantics to the         many other key phrases are all pivotal to the un-
     title, we propose a novel semi-supervised key           derstanding of the document. For example, when
     phrase extraction approach in this paper by
                                                             we read the title “China Tightens Grip on the
     computing the phrase importance in the se-
     mantic network, through which the influence             Web”, we can only have a glimpse of what the
     of title phrases is propagated to the other             document says. On the other hand, the key
     phrases iteratively. Experimental results dem-          phrases, such as “China”, “Censorship”, “Web”,
     onstrate the remarkable performance of this             “Domain name”, “Internet”, and “CNNIC”, etc.
     approach.                                               can tell more details about the main topics of the
                                                             document. In this regard, title phrases are often
1     Introduction                                           good key phrases but they are far from enough.
                                                                 If we review the above example again, we will
Key phrases are defined as the phrases that ex-              find that the key phrase “Internet” can be in-
press the main content of a document. Guided by              ferred from the title phrase “Web”. As a matter
the given key phrases, people can easily under-              of fact, key phrases often have close semantics to
stand what a document describes, saving a great              title phrases. Then a question comes to our minds:
amount of time reading the whole text. Conse-                can we make use of these title phrases to infer
quently, automatic key phrase extraction is in               the other key phrases?
high demand. Meanwhile, it is also fundamental                   To provide a foundation of inference, a seman-
to many other natural language processing appli-             tic network that captures the relationships among
cations, such as information retrieval, text clus-           phrases is required. In the previous works (Tur-
tering and so on.                                            dakov and Velikhov, 2008), semantic networks
   Key phrase extraction can be normally cast as             are constructed based on the binary relations, and
a ranking problem solved by either supervised or             the semantic relatedness between a pair of phras-
unsupervised methods. Supervised learning re-                es is formulated by the weighted edges that con-
quires a large amount of expensive training data,            nects them. The deficiency of these approaches is
whereas unsupervised learning totally ignores                the incapability to capture the n-ary relations
human knowledge. To overcome the deficiencies                among multiple phrases. For example, a group of

                                                         296
                        Proceedings of the ACL 2010 Conference Short Papers, pages 296–300,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


phrases may collectively describe an entity or an        G=(V, E, W), where each vertex viV (1in)
event.                                                   represents a phrase, each hyper-edge ejE
   In this study, we propose to model a semantic         (1jm) is a subset of V, representing binary re-
network as a hyper-graph, where vertices                 lations or n-ary relations among phrases, and the
represent phrases and weighted hyper-edges               weight w(ej) measures the semantic relatedness
measure the semantic relatedness of both binary          of ej.
relations and n-ary relations among phrases. We             By applying the WSD technique proposed by
explore a universal knowledge base – Wikipedia           (Turdakov and Velikhov, 2008), each phrase is
– to compute the semantic relatedness. Yet our           assigned with a single Wikipedia article that de-
major contribution is to develop a novel semi-           scribes its meaning. Intuitively, if the fraction of
supervised key phrase extraction approach by             the links that the two articles have in common to
computing the phrase importance in the semantic          the total number of the links in both articles is
network, through which the influence of title            high, the two phrases corresponding to the two
phrases is propagated to the other phrases itera-        articles are more semantically related. Also, an
tively.                                                  article contains different types of links, which are
   The goal of the semi-supervised learning is to        relevant to the computation of semantic related-
design a function that is sufficiently smooth with       ness to different extent. Hence we adopt the
respect to the intrinsic structure revealed by title     weighted Dice metric proposed by (Turdakov
phrases and other phrases. Based on the assump-          2008) to compute the semantic relatedness of
tion that semantically related phrases are likely        each binary relation, resulting in the edge weight
to have similar scores, the function to be esti-         w(eij), where eij is an edge connecting the phrases
mated is required to assign title phrases a higher       vi and vj.
score and meanwhile locally smooth on the con-              To define the n-ary relations in the semantic
structed hyper-graph. Zhou et al.’s work (Zhou           network, a proper graph clustering technique is
2005) lays down a foundation for our semi-               needed. We adopt the weighted Girvan-Newman
supervised phrase ranking algorithm introduced           algorithm (Newman 2004) to cluster phrases (in-
in Section 3. Experimental results presented in          cluding title phrases) by computing their bet-
Section 4 demonstrate the effectiveness of this          weenness centrality. The advantage of this algo-
approach.                                                rithm is that it need not specify a pre-defined
                                                         number of clusters. Then the phrases, within
2      Wikipedia-based Semantic Network                  each cluster, are connected by a n-ary relation. n-
       Construction                                      ary relations among the phrases in the same clus-
Wikipedia1 is a free online encyclopedia, which          ter are then measured based on binary relations.
has unarguably become the world’s largest col-           The weight of a hyper-edge e is defined as:
                                                                           
lection of encyclopedic knowledge. Articles are
the basic entries in the Wikipedia, with each ar-
                                                                  w(e)           w(e )
                                                                           | e | eij e
                                                                                          ij              (1)
ticle explaining one Wikipedia term. Articles
                                                         where |e| is the number of the vertices in e, eij is
contain links pointing from one article to another.
                                                         an edge with two vertices included in e and  ≥ 0
Currently, there are over 3 million articles and 90
                                                         is a parameter balancing the relative importance
million links in English Wikipedia. In addition to
                                                         of n-ary hyper-edges compared with binary ones.
providing a large vocabulary, Wikipedia articles
also contain a rich body of lexical semantic in-         3    Semi-supervised Learning from Title
formation expressed via the extensive number of
links. During recent years, Wikipedia has been           Given the document semantic network
used as a powerful tool to compute semantic re-          represented as a phrase hyper-graph, one way to
latedness between terms in a good few of works           make better use of the semantic information is to
(Turdakov 2008).                                         rank phrases with a semi-supervised learning
   We consider a document composed of the                strategy, where the title phrases are regarded as
phrases that describe various aspects of entities        labeled samples, while the other phrases as unla-
or events with different semantic relationships.         beled ones. That is, the information we have at
We then model a document as a semantic net-              the beginning about how to rank phrases is that
work formulated by a weighted hyper-graph                the title phrases are the most important phrases.
                                                         Initially, the title phrases are assigned with a pos-
1
    www.wikipedia.org                                    itive score of 1 indicating its importance and oth-


                                                       297


er phrases are assigned zero. Then the impor-                          vergence threshold 
tance scores of the phrases are learned iteratively                    Output: The approximate phrase scores f
from the title phrases through the hyper-graph.                        Construct a document semantic network for all the
The key idea behind hyper-graph based semi-                            phrases {v1,v2,…,vn} using the method described in
supervised ranking is that the vertices which                          section 2.
usually belong to the same hyper-edges should                              Let   Dv 1/2 HWDe 1 H T Dv 1/2 ;
be assigned with similar scores. Then, we have                             Initialize the score vector y as yi  1,1  i  t , and
the following two constraints:                                             y j  0, t  j  n ;
   1. The phrases which have many incident hy-                           Let         , k = 0;
per-edges in common should be assigned similar                           REPEAT
scores.                                                                      f k 1   f k  (1   ) y ;
   2. The given initial scores of the title phrases
                                                                                                    ,              ;
should be changed as little as possible.
                                                                                         ;
   Given a weighted hyper-graph G, assume a                              UNTIL
ranking function f over V, which assigns each                          END
vertex v an importance score f(v). f can be
thought as a vector in Euclid space R|V|. For the                         Finally we rank phrases in descending order of
convenience of computation, we use an inci-                            the calculated importance scores and select those
dence matrix H to represent the hypergraph, de-                        highest ranked phrases as key phrases. Accord-
fined as:                                                              ing to the number of all the candidate phrases,
                                                                       we choose an appropriate proportion, i.e. 10%, of
                 0, if v  e
       h(v, e)                                               (2)     all the phrases as key phrases.
                 1, if v  e
   Based on the incidence matrix, we define the                        4    Evaluation
degrees of the vertex v and the hyper-edge e as
                                                                       4.1 Experiment Set-up
                                                 (3)
and                                                                    We first collect all the Wikipedia terms to com-
                                                 (4)                   pose of a dictionary. The word sequences that
   Then, to formulate the above-mentioned con-                         occur in the dictionary are identified as phrases.
straints, let denote the initial score vector, then                    Here we use a finite-state automaton to accom-
the importance scores of the phrases are learned                       plish this task to avoid the imprecision of pre-
iteratively by solving the following optimization                      processing by POS tagging or chunking. Then,
problem:                                                               we adopt the WSD technique proposed by (Tur-
                                                                       dakov and Velikhov 2008) to find the corres-
       arg min f R|V | {( f )   f  y }
                                                  2
                                                               (5)
                                                                       ponding Wikipedia article for each phrase. As
               1         1                   f (u)
                                                           2
                                                      f (v)  (6)
                                                                       mentioned in Section 2, a document semantic
    ( f ) 
               2
                  eE
                        (e)
                             {u ,v}e
                                       w(e) 
                                             d (u)
                                                             
                                                       d (v) 
                                                                       network in the form of a hyper-graph is con-
                                                                      structed, on which Algorithm 1 is applied to rank
   where > 0 is the parameter specifying the                          the phrases.
tradeoff between the two competitive items. Let                           To evaluate our proposed approach, we select
Dv and De denote the diagonal matrices contain-                        200 pieces of news from well-known English
ing the vertex and the hyper-edge degrees re-                          media. 5 to 10 key phrases are manually labeled
spectively, W denote the diagonal matrix con-                          in each news document and the average number
taining the hyper-edge weights, f* denote the so-                      of the key phrases is 7.2 per document. Due to
lution of (6). Zhou has given the solution (Zhou,                      the abbreviation and synonymy phenomena, we
2005) as.                                                              construct a thesaurus and convert all manual and
       f *  f *  (1   ) y               (7)                      automatic phrases into their canonical forms
where   Dv HWDe H Dv and   1/ (   1) .
              1/2    1 T   1/2                                      when evaluated. The traditional Recall, Precision
                                                                       and F1-measure metrics are adopted for evalua-
Using an approximation algorithm (e.g. Algo-
                                                                       tion. This section conducts two sets of experi-
rithm 1), we can finally get a vector f
                                                                       ment: (1) to examine the influence of two para-
representing the approximate phrase scores.
                                                                       meters:  and , on the key phrase extraction
Algorithm 1: PhraseRank(V, T, a, b)                                    performance; (2) to compare with other well
Input: Title phrase set = {v1,v2,…,vt},the set of other                known state-of-art key phrase extraction ap-
phrases ={vt+1,vt+2,…,vn}, parameters  and , con-                    proaches.


                                                                     298


4.2 Parameter tuning
The approach involves two parameters:  (0)
is a relation factor balancing the influence of n-
ary relations and binary relations;  (01) is a
learning factor tuning the influence from the title
phrases. It is hard to find a global optimized so-
lution for the combination of these two factors.
So we apply a gradient search strategy. At first,
the learning factor is set to =0.8. Different val-                 Figure 2. F1-measure with  in [0,1]
ues of  ranging from 0 to 3 are examined. Then,
                                                        4.3 Comparison with Other Approaches
given that  is set to the value with the best per-
formance, we conduct experiments to find an             Our approach aims at inferring important key
appropriate value for .                                phrases from title phrases through a semantic
                                                        network. Here we take a method of synonym
4.2.1   : Relation Factor                              expansion as the baseline, called WordNet ex-
                                                        pansion here. The WordNet 2 expansion approach
First, we fix the learning factor  as 0.8 random-
                                                        selects all the synonyms of the title phrases in the
ly and evaluate the performance by varying 
                                                        document as key phrases. Afterwards, our ap-
value from 0 to 3. When =0, it means that the
                                                        proach is evaluated against two existing ap-
weight of n-ary relations is zero and only binary       proaches, which rely on the conventional seman-
relations are considered. As we can see from
                                                        tic network and are able to capture binary rela-
Figure 1, the performance is improved in most           tions only. One approach combines the title in-
cases in terms of F1-measure and reaches a peak         formation into the Grineva’s community-based
at =1.8. This justifies the rational to incorpo-       method (Grineva et al., 2009), called title-
rate n-ary relations with binary relations in the       community approach. The title-community ap-
document semantic network.                              proach uses the Girvan-Newman algorithm to
                                                        cluster phrases into communities and selects
                                                        those phrases in the communities containing the
                                                        title phrases as key phrases. We do not limit the
                                                        number of key phrases selected. The other one is
                                                        based on topic-sensitive LexRank (Otterbacher et
                                                        al., 2005), called title-sensitive PageRank here.
                                                        The title-sensitive PageRank approach makes use
                                                        of title phrases to re-weight the transitions be-
        Figure 1. F1-measures with  in [0 3]
                                                        tween vertices and picks up 10% top-ranked
                                                        phrases as key phrases.
4.2.2   : Learning factor                                    Approach         Precision Recall         F1
   Next, we set the relation factor =1.8, we in-       Title-sensitive Pa-
                                                                                34.8%      39.5%     37.0%
spect the performance with the learning factor         geRank (d=0.15)
                                                        Title-community         29.8%      56.9%     39.1%
ranging from 0 to 1. =1 means that the ranking
                                                        Our approach
scores learn from the semantic network without                                  39.4%      44.6%     41.8%
                                                        (=1.8, =0.5)
any consideration of title phrases. As shown in         WordNet expansion
Figure 2, we find that the performance almost                                    7.9%      32.9%     12.5%
                                                        (baseline)
keep a smooth fluctuation as  increases from 0               Table 1. Comparison with other approaches
to 0.9, and then a diving when =1. This proves
that title phrases indeed provide valuable infor-          Table 1 summarizes the performance on the
mation for learning.                                    test data. The results presented in the table show
                                                        that our approach exhibits the best performance
                                                        among all the four approaches. It follows that the
                                                        key phrases inferred from a document semantic
                                                        network are not limited to the synonyms of title
                                                        phrases. As the title-sensitive PageRank ap-

                                                        2
                                                            http://wordnet.princeton.edu


                                                      299


proach totally ignores the n-ary relations, its per-     Jahna Otterbacher, Gunes Erkan and Dragomir R.
formance is the worst. Based on binary relations,           Radev. 2005. Using Random Walks for Ques-
the title-community approach clusters phrases               tion-focused Sentence Retrieval. In Proceedings
into communities and each community can be                  of HLT/EMNLP 2005, pp. 915-922, Vancouver,
considered as an n-ary relation. However, this              Canada.
approach lacks of an importance propagation              Maria Grineva, Maxim Grinev and Dmitry Lizorkin.
process. Consequently, it has the highest recall           2009. Extracting key terms from noisy and
value but the lowest precision. In contrast, our           multitheme documents, In Proceedings of the
approach achieves the highest precision, due to            18th international conference on World wide web,
its ability to infer many correct key phrases using        pp. 661-670, Madrid, Spain.
importance propagation among n-ary relations.            Michael Strube and Simone Paolo Ponzetto.
                                                           2006.WikiRelate! Computing Semantic Rela-
5     Conclusion                                           tedness using Wikipedia. In Proceedings of the
This work is based on the belief that key phrases          21st National Conference on Artiﬁcial Intelligence,
tend to have close semantics to the title phrases.         pp. 1419–1424, Boston, MA.
In order to make better use of phrase relations in       M. E. J. Newman. 2004. Analysis of Weighted Net-
key phrase extraction, we explore the Wikipedia            works. Physical Review E 70, 056131.
knowledge to model one document as a semantic
network in the form of hyper-graph, through
which the other phrases learned their importance
scores from the title phrases iteratively. Experi-
mental results demonstrate the effectiveness and
robustness of our approach.

Acknowledgments
The work described in this paper was partially
supported by NSFC programs (No: 60773173,
60875042 and 90920011), and Hong Kong RGC
Projects (No: PolyU5217/07E). We thank the
anonymous reviewers for their insightful com-
ments.

References
David Milne, Ian H. Witten. 2008. An Effective,
    Low-Cost Measure of Semantic Relatedness
    Obtained from Wikipedia Links. In Wikipedia
    and AI workshop at the AAAI-08 Conference,
    Chicago, US.
Dengyong Zhou, Jiayuan Huang and Bernhard
  Schölkopf. 2005. Beyond Pairwise Classifica-
  tion and Clustering Using Hypergraphs. MPI
  Technical Report, Tübingen, Germany.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
    relatedness metric for wikipedia concepts
    based on link analysis and its application to
    word sense disambiguation. In Colloquium on
    Databases and Information Systems (SYRCoDIS).
Ian H. Witten, Gordon W. Paynter, Eibe Frank , Carl
   Gutwin , Craig G. Nevill-Manning. 1999. KEA:
   practical automatic keyphrase extraction, In
   Proceedings of the fourth ACM conference on Dig-
   ital libraries, pp.254-255, California, USA.



                                                       300

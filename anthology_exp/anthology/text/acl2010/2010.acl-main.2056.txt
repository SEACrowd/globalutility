               Domain Adaptation of Maximum Entropy Language Models
                        Tanel Alumäe∗                                         Mikko Kurimo
             Adaptive Informatics Research Centre                    Adaptive Informatics Research Centre
              School of Science and Technology                        School of Science and Technology
                       Aalto University                                        Aalto University
                       Helsinki, Finland                                       Helsinki, Finland
                   tanel@cis.hut.fi                                      Mikko.Kurimo@tkk.fi


                           Abstract                                      paper is that we show how the suggested hierar-
                                                                         chical adaptation can be used with suitable pri-
      We investigate a recently proposed                                 ors and combined with the class-based speedup
      Bayesian adaptation method for building                            technique (Goodman, 2001) to adapt ME LMs
      style-adapted maximum entropy language                             in large-vocabulary speech recognition when the
      models for speech recognition, given a                             amount of target data is small. The results outper-
      large corpus of written language data                              form the conventional linear interpolation of back-
      and a small corpus of speech transcripts.                          ground and target models in both N -grams and
      Experiments show that the method con-                              ME models. It seems that with the adapted ME
      sistently outperforms linear interpolation                         models, the same recognition accuracy for the tar-
      which is typically used in such cases.                             get evaluation data can be obtained with 50% less
                                                                         adaptation data than in interpolated ME models.
1     Introduction
In large vocabulary speech recognition, a language                       2   Review of Conditional Maximum
model (LM) is typically estimated from large                                 Entropy Language Models
amounts of written text data. However, recogni-                          Maximum entropy (ME) modeling is a framework
tion is typically applied to speech that is stylisti-                    that has been used in a wide area of natural lan-
cally different from written language. For exam-                         guage processing (NLP) tasks. A conditional ME
ple, in an often-tried setting, speech recognition is                    model has the following form:
applied to broadcast news, that includes introduc-
tory segments, conversations and spontaneous in-
                                                                                                    P
                                                                                                   e   i λi fi (x,h)
terviews. To decrease the mismatch between train-                                   P (x|h) = P        P            0      (1)
                                                                                                       e  j λj fj (x ,h)
ing and test data, often a small amount of speech                                                 x0

data is human-transcribed. A LM is then built                            where x is an outcome (in case of a LM, a word),
by interpolating the models estimated from large                         h is a context (the word history), and x0 a set of all
corpus of written language and the small corpus                          possible outcomes (words). The functions fi are
of transcribed data. However, in practice, differ-                       (typically binary) feature functions. During ME
ent models might be of different importance de-                          training, the optimal weights λi corresponding to
pending on the word context. Global interpola-                           features fi (x, h) are learned. More precisely, find-
tion doesn’t take such variability into account and                      ing the ME model is equal to finding weights that
all predictions are weighted across models identi-                       maximize the log-likelihood L(X; Λ) of the train-
cally, regardless of the context.                                        ing data X. The weights are learned via improved
   In this paper we investigate a recently proposed                      iterative scaling algorithm or some of its modern
Bayesian adaptation approach (Daume III, 2007;                           fast counterparts (i.e., conjugate gradient descent).
Finkel and Manning, 2009) for adapting a con-                               Since LMs typically have a vocabulary of tens
ditional maximum entropy (ME) LM (Rosenfeld,                             of thousands of words, the use of a normalization
1996) to a new domain, given a large corpus of                           factor over all possible outcomes makes estimat-
out-of-domain training data and a small corpus                           ing a ME LM very memory and time consuming.
of in-domain data. The main contribution of this                         Goodman (2001) proposed a class-based method
      ∗
          Currently with Tallinn University of Technology, Esto-         that drastically reduces the resource requirements
nia                                                                      for training such models. The idea is to cluster


                                                                   301
                             Proceedings of the ACL 2010 Conference Short Papers, pages 301–306,
                       Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


words in the vocabulary into classes (e.g., based            third and often the best performing approach is to
on their distributional similarity). Then, we can            train separate models for each data source, apply
decompose the prediction of a word given its his-            them to test data and interpolate the results.
tory into prediction of its class given the history,            The hierarchical Bayesian adaptation method
and prediction of the word given the history and             is a generalization of the three approaches de-
its class :                                                  scribed above. The hierarchical model jointly
                                                             optimizes global and domain-specific parameters,
    P (w|h) = P (C(w)|h)· P (w|h, C(w))         (2)          using parameters built from pooled data as priors
                                                             for domain-specific parameters. In other words,
Using such decomposition, we can create two ME
                                                             instead of using smoothing to encourage param-
models: one corresponding to P (C(w)|h) and the
                                                             eters to be closer to zero, it encourages domain-
other corresponding to P (w|h, C(w)). It is easy to
                                                             specific model parameters to be closer to the
see that computing the normalization factor of the
                                                             corresponding global parameters, while a zero
first component model now requires only looping
                                                             mean Gaussian prior is still applied for global pa-
over all classes. It turns out that normalizing the
                                                             rameters. For processing test data during run-
second model is also easier: for a context h, C(w),
                                                             time, the domain-specific model is applied. Intu-
we only need to normalize over words that belong
                                                             itively, this approach can be described as follows:
to class C(w), since other words cannot occur in
                                                             the domain-specific parameters are largely deter-
this context. This decomposition can be further
                                                             mined by global data, unless there is good domain-
extended by using hierarchical classes.
                                                             specific evidence that they should be different.
   To avoid overfitting, ME models are usually
                                                             The key to this approach is that the global and
smoothed (regularized). The most widely used
                                                             domain-specific parameters are learned jointly, not
smoothing method for ME LMs is Gaussian pri-
                                                             hierarchically. This allows domain-specific pa-
ors (Chen and Rosenfeld, 2000): a zero-mean
                                                             rameters to influence the global parameters, and
prior with a given variance is added to all feature
                                                             vice versa. Formally, the joint optimization crite-
weights, and the model optimization criteria be-
                                                             ria becomes:
comes:
                              F                                Lhier (X; Λ) =
          0
                             X    λ2i
        L (X; Λ) = L(X; Λ) −                    (3)                                        F
                                                                                                                   !
                             i=1
                                 2σi2                           X                          X (λd,i − λ∗,i )2
                                                                      Lorig (Xd , Λd ) −
                                                                                           i=1
                                                                                                     2σd2
where F is the number of feature functions. Typi-                d
                                                                                                     F
cally, a fixed hyperparameter σi = σ is used for                                                     X λ2∗,i
all parameters. The optimal variance is usually                                                  −                     (4)
                                                                                                            2σ∗2
                                                                                                     i=1
estimated on a development set. Intuitively, this
method encourages feature weights to be smaller,             where Xd is data for domain d, λ∗,i the global
by penalizing weights with big absolute values.              parameters, λd,i the domain-specific parameters,
                                                             σ∗2 the global variance and σd2 the domain-specific
3   Domain Adaptation of Maximum
                                                             variances. The global and domain-specific vari-
    Entropy Models
                                                             ances are optimized on the heldout data. Usually,
Recently, a hierarchical Bayesian adaptation                 larger values are used for global parameters and
method was proposed that can be applied to a large           for domains with more data, while for domains
family of discriminative learning tasks (such as             with less data, the variance is typically set to be
ME models, SVMs) (Daume III, 2007; Finkel and                smaller, encouraging the domain-specific parame-
Manning, 2009). In NLP problems, data often                  ters to be closer to global values.
comes from different sources (e.g., newspapers,                 This adaptation scheme is very similar to the ap-
web, textbooks, speech transcriptions). There are            proaches proposed by (Chelba and Acero, 2006)
three classic approaches for building models from            and (Chen, 2009b): both use a model estimated
multiple sources. We can pool all training data and          from background data as a prior when learning
estimate a single model, and apply it for all tasks.         a model from in-domain data. The main differ-
The second approach is to “unpool” the data, i.e,            ence is the fact that in this method, the models are
only use training data from the test domain. The             estimated jointly while in the other works, back-


                                                       302


ground model has to be estimated before learning            three Estonian radio stations. Their format con-
the in-domain model.                                        sists of hosts and invited guests, spontaneously
                                                            discussing current affairs. There are 40 minutes
4     Experiments                                           of transcriptions, with 11 different speakers.
                                                               The acoustic models were trained on various
In this section, we look at experimental results            wideband Estonian speech corpora: the BABEL
over two speech recognition tasks.                          speech database (9h), transcriptions of Estonian
                                                            broadcast news (7.5h) and transcriptions of radio
4.1    Tasks
                                                            live talk programs (10h). The models are triphone
Task 1: English Broadcast News. This recog-                 HMMs, using MFCC features.
nition task consists of the English broadcast news             For training the LMs, two sources were used:
section of the 2003 NIST Rich Transcription Eval-           about 10M sentences from various Estonian news-
uation Data. The data includes six news record-             papers, and manual transcriptions of 10 hours of
ings from six different sources with a total length         live talk programs from three Estonian radio sta-
of 176 minutes.                                             tions. The latter is identical in style to the test data,
   As acoustic models, the CMU Sphinx open                  although it originates from a different time period
source triphone HUB4 models for wideband                    and covers a wider variety of programs, and was
(16kHz) speech1 were used. The models have                  treated as in-domain data.
been trained using 140 hours of speech.                        As Estonian is a highly inflective language,
   For training the LMs, two sources were used:             morphemes are used as basic units in the LM.
first 5M sentences from the Gigaword (2nd ed.)              We use a morphological analyzer (Kaalep and
corpus (99.5M words), and broadcast news tran-              Vaino, 2001) for splitting the words into mor-
scriptions from the TDT4 corpus (1.19M words).              phemes. After such processing, the newspaper
The latter was treated as in-domain data in the             corpus includes of 185M tokens, and the tran-
adaptation experiments. A vocabulary of 26K                 scribed data 104K tokens. A vocabulary of 30K
words was used. It is a subset of a bigger 60K              tokens was used for this task, with an OOV rate
vocabulary, and only includes words that occurred           of 1.7% against the test data. After recognition,
in the training data. The OOV rate against the test         morphemes were concatenated back to words.
set was 2.4%.                                                  As with English data, a three-pass recognition
   The audio used for testing was segmented                 strategy involving MLLR adaptation was applied.
into parts of up to 20 seconds in length.
Speaker diarization was applied using the                   4.2   Results
LIUM SpkDiarization toolkit (Deléglise et al.,
                                                            For both tasks, we rescored the N-best lists in
2005). The CMU Sphinx 3.7 was used for
                                                            two different ways: (1) using linear interpolation
decoding. A three-pass recognition strategy was
                                                            of source-specific ME models and (2) using hi-
applied: the first pass recognition hypotheses
                                                            erarchically domain-adapted ME model (as de-
were used for calculating MLLR-adapted models
                                                            scribed in previous chapter). The English ME
for each speaker. In the second pass, the adapted
                                                            models had a three-level and Estonian models a
acoustic models were used for generating a
                                                            four-level class hierarchy. The classes were de-
5000-best list of hypotheses for each segment. In
                                                            rived using the word exchange algorithm (Kneser
the third pass, the ME LM was used to re-rank the
                                                            and Ney, 1993). The number of classes at each
hypotheses and select the best one. During decod-
                                                            level was determined experimentally so as to op-
ing, a trigram LM model was used. The trigram
                                                            timize the resource requirements for training ME
model was an interpolation of source-specific
                                                            models (specifically, the number of classes was
models which were estimated using Kneser-Ney
                                                            150, 1000 and 5000 for the English models and
discounting.
                                                            20, 150, 1000 and 6000 for the Estonian models).
   Task 2: Estonian Broadcast Conversations.
                                                            We used unigram, bigram and trigram features that
The second recognition task consists of four
                                                            occurred at least twice in the training data. The
recordings from different live talk programs from
                                                            feature cut-off was applied in order to accommo-
  1
    http://www.speech.cs.cmu.edu/sphinx/                    date the memory requirements. The feature set
models/                                                     was identical for interpolated and adapted models.


                                                      303


             Interp. models    Adapted models               it’s not possible to simply normalize the variances.
 Adapta-       2
             σOD       2
                      σID      σ∗2 σOD
                                     2     2
                                          σID                  The experimental results are presented in Table
 tion data                                                  2. Perplexity and word error rate (WER) results of
 (No of                                                     the interpolated and adapted models are compared.
 words)                                                     For the Estonian task, letter error rate (LER) is
          English Broadcast News                            also reported, since it tends to be a more indicative
 147K      2e8     3e5     5e7 2e7           2e6            measure of speech recognition quality for highly
 292K      2e8     5e5     5e7 2e7           2e6            inflected languages. In all experiments, using the
 591K      2e8     1e6     5e7 2e7           2e6            adapted models resulted in lower perplexity and
 1119K     2e8     2e6     5e7 2e7           5e6            lower error rate. Improvements in the English ex-
      Estonian Broadcast Conversations                      periment were less evident than in the Estonian
 104K      5e8     3e5     5e7 1e7           2e6            system, with under 10% improvement in perplex-
                                                            ity and 1-3% in WER, against 15% and 4% for the
Table 1: The unnormalized values of Gaus-                   Estonian experiment. In most cases, there was a
sian prior variances for interpolated out-of-domain         significant improvement in WER when using the
(OD) and in-domain (ID) ME models, and hierar-              adapted ME model (according to the Wilcoxon
chically adapted global (*), out-of-odomain (OD)            test), with and exception of the English experi-
and in-domain (ID) models that were used in the             ments on the 292K and 591K data sets.
experiments.                                                   The comparison between N -gram models and
                                                            ME models is not entirely fair since ME models
                                                            are actually class-based. Such transformation in-
   For the English task, we also explored the ef-           troduces additional smoothing into the model and
ficiency of these two approaches with varying               can improve model perplexity, as also noticed by
size of adaptation data: we repeated the exper-             Goodman (2001).
iments when using one eighth, one quarter, half
and all of the TDT4 transcription data for interpo-         5   Discussion
lation/adaptation. The amount of used Gigaword
data was not changed. In all cases, interpolation           In this paper we have tested a hierarchical adapta-
weights were re-optimized and new Gaussian vari-            tion method (Daume III, 2007; Finkel and Man-
ance values were heuristically determined.                  ning, 2009) on building style-adapted LMs for
   The TADM toolkit2 was used for estimating ME             speech recognition. We showed that the method
models, utilizing its implementation of the conju-          achieves consistently lower error rates than when
gate gradient algorithm.                                    using linear interpolation which is typically used
   The models were regularized using Gaussian               in such scenarios.
priors. The variance parameters were chosen                    The tested method is ideally suited for language
heuristically based on light tuning on develop-             modeling in speech recognition: we almost always
ment set perplexity. For the source-specific ME             have access to large amounts of data from written
models, the variance was fixed on per-model ba-             sources but commonly the speech to be recognized
sis. For the adapted model, that jointly models             is stylistically noticeably different. The hierarchi-
global and domain-specific data, the Gaussian pri-          cal adaptation method enables to use even a small
ors were fixed for each hierarchy node (i.e., the           amount of in-domain data to modify the parame-
variance was fixed across global, out-of-domain,            ters estimated from out-of-domain data, if there is
and in-domain parameters). Table 1 lists values             enough evidence.
for the variances of Gaussian priors (as in equa-              As Finkel and Manning (2009) point out, the
tions 3 and 4) that we used in the experiments. In          hierarchical nature of the method makes it possi-
other publications, the variance values are often           ble to estimate highly specific models: we could
normalized to the size of the data. We chose not            draw style-specific models from general high-level
to normalize the values, since in the hierarchical          priors, and topic-and-style specific models from
adaptation scheme, also data from other domains             style-specific priors. Furthermore, the models
have impact on the learned model parameters, thus           don’t have to be hierarchical: it is easy to gen-
                                                            eralize the method to general multilevel approach
  2
      http://tadm.sourceforge.net/                          where a model is drawn from multiple priors. For


                                                      304


                      Perplexity                       WER                      LER
 Adaptation Pooled Interp.                    Interp.                  Interp.
                            Interp. Adapted            Interp. Adapted         Interp.                     Adapted
 data (No.   N-      N-                         N-                       N-
                             ME       ME                ME       ME             ME                          ME
 of words) gram gram                           gram                    gram
                           English Broadcast News
   147K      290    255      243      230      27.2     26.3     25.9
   292K      286    250      236      223      26.7     25.8     25.6
   591K      280    243      228      215      26.6     25.9     25.6
  1119K      272    232      217      204      26.2     25.6     24.9
                                Estonian Broadcast Conversations
   104K      237    197      200      169      40.5     38.9     37.4   17.7    17.3                        16.6

Table 2: Perplexity, WER and LER results comparing pooled and interpolated N -gram models and
interpolated and adapted ME models, with changing amount of available in-domain data.


instance, we could build a model for recognizing              guage Technology.
computer science lectures, given data from text-
books, including those about computer science,
and transcripts of lectures on various topics (which          References
don’t even need to include lectures about computer            Ciprian Chelba and Alex Acero. 2006. Adaptation of
science).                                                       maximum entropy capitalizer: Little data can help a
                                                                lot. Computer Speech & Language, 20(4):382–399,
   The method has some considerable shortcom-                   October.
ings from the practical perspective. First, train-
ing ME LMs in general has much higher resource                S. F. Chen and R. Rosenfeld. 2000. A survey of
                                                                 smoothing techniques for ME models. IEEE Trans-
requirements than training N -gram models which
                                                                 actions on Speech and Audio Processing, 8(1):37–
are typically used in speech recognition. More-                  50.
over, training hierarchical ME models requires
                                                              S. F. Chen. 2009a. Performance prediction for expo-
even more memory than training simple ME mod-
                                                                 nential language models. In Proceedings of HLT-
els, proportional to the number of nodes in the hi-              NAACL, pages 450–458, Boulder, Colorado.
erarchy. However, it should be possible to allevi-
ate this problem by profiting from the hierarchi-             Stanley F. Chen. 2009b. Shrinking exponential lan-
                                                                 guage models. In Proceedings of HLT-NAACL,
cal nature of n-gram features, as proposed in (Wu                pages 468–476, Boulder, Colorado.
and Khudanpur, 2002). It is also difficult to deter-
mine good variance values σi2 for the global and              H. Daume III. 2007. Frustratingly easy domain adap-
                                                                tation. In Proceedings of ACL, pages 256–263.
domain-specific priors. While good variance val-
ues for simple ME models can be chosen quite re-              P. Deléglise, Y. Estéve, S. Meignier, and T. Merlin.
liably based on the size of the training data (Chen,             2005. The LIUM speech transcription system: a
                                                                 CMU Sphinx III-based system for French broadcast
2009a), we have found that it is more demand-                    news. In Proceedings of Interspeech, Lisboa, Portu-
ing to find good hyperparameters for hierarchical                gal.
models since weights for the same feature in dif-
                                                              J. R. Finkel and Ch. Manning. 2009. Hierarchi-
ferent nodes in the hierarchy are all related to each            cal Bayesian domain adaptation. In Proceedings of
other. We plan to investigate this problem in the                HLT-NAACL, pages 602–610, Boulder, Colorado.
future since the choice of hyperparameters has a
                                                              J. Goodman. 2001. Classes for fast maximum entropy
strong impact on the performance of the model.
                                                                 training. In Proceedings of ICASSP, Utah, USA.
Acknowledgments                                               H.-J. Kaalep and T. Vaino. 2001. Complete morpho-
                                                                logical analysis in the linguist’s toolbox. In Con-
This research was partly funded by the Academy                  gressus Nonus Internationalis Fenno-Ugristarum
of Finland in the project Adaptive Informatics,                 Pars V, pages 9–16, Tartu, Estonia.
by the target-financed theme No. 0322709s06 of                R. Kneser and H. Ney. 1993. Improved clustering
the Estonian Ministry of Education and Research                 techniques for class-based statistical language mod-
and by the National Programme for Estonian Lan-                 elling. In Proceedings of the European Conference


                                                        305


  on Speech Communication and Technology, pages
  973–976.
R. Rosenfeld. 1996. A maximum entropy approach to
   adaptive statistical language modeling. Computer,
   Speech and Language, 10:187–228.
J. Wu and S. Khudanpur. 2002. Building a topic-
   dependent maximum entropy model for very large
   corpora.   In Proceedings of ICASSP, Orlando,
   Florida, USA.




                                                       306

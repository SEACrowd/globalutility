          Classification of Feedback Expressions in Multimodal Data

                Costanza Navarretta                     Patrizia Paggio
              University of Copenhagen            University of Copenhagen
       Centre for Language Technology (CST) Centre for Language Technology (CST)
        Njalsgade 140, 2300-DK Copenhagen Njalsgade 140, 2300-DK Copenhagen
             costanza@hum.ku.dk                     paggio@hum.ku.dk


                    Abstract                                 et al. (2007) study the relation between eye gaze,
                                                             facial expression, pauses and dialogue structure
    This paper addresses the issue of how lin-               in annotated English map-task dialogues (Ander-
    guistic feedback expressions, prosody and                son et al., 1991) and find correlations between the
    head gestures, i.e. head movements and                   various modalities both within and across speak-
    face expressions, relate to one another in               ers. Finally, feedback expressions (head nods and
    a collection of eight video-recorded Dan-                shakes) are successfully predicted from speech,
    ish map-task dialogues. The study shows                  prosody and eye gaze in interaction with Embod-
    that in these data, prosodic features and                ied Communication Agents as well as human com-
    head gestures significantly improve auto-                munication (Fujie et al., 2004; Morency et al.,
    matic classification of dialogue act labels              2005; Morency et al., 2007; Morency et al., 2009).
    for linguistic expressions of feedback.                     Our work is in line with these studies, all of
                                                             which focus on the relation between linguistic
1   Introduction
                                                             expressions, prosody, dialogue content and ges-
Several authors in communication studies have                tures. In this paper, we investigate how feedback
pointed out that head movements are relevant to              expressions can be classified into different dia-
feedback phenomena (see McClave (2000) for an                logue act categories based on prosodic and ges-
overview). Others have looked at the application             ture features. Our data are made up by a collec-
of machine learning algorithms to annotated mul-             tion of eight video-recorded map-task dialogues in
timodal corpora. For example, Jokinen and Ragni              Danish, which were annotated with phonetic and
(2007) and Jokinen et al. (2008) find that machine           prosodic information. We find that prosodic fea-
learning algorithms can be trained to recognise              tures improve the classification of dialogue acts
some of the functions of head movements, while               and that head gestures, where they occur, con-
Reidsma et al. (2009) show that there is a depen-            tribute to the semantic interpretation of feedback
dence between focus of attention and assignment              expressions. The results, which partly confirm
of dialogue act labels. Related are also the stud-           those obtained on a smaller dataset in Paggio and
ies by Rieks op den Akker and Schulz (2008) and              Navarretta (2010), must be seen in light of the
Murray and Renals (2008): both achieve promis-               fact that our gesture annotation scheme comprises
ing results in the automatic segmentation of dia-            more fine-grained categories than most of the stud-
logue acts using the annotations in a large multi-           ies mentioned earlier for both head movements
modal corpus.                                                and face expressions. The classification results
   Work has also been done on prosody and ges-               improve, however, if similar categories such as
tures in the specific domain of map-task dialogues,          head nods and jerks are collapsed into a more gen-
also targeted in this paper. Sridhar et al. (2009)           eral category.
obtain promising results in dialogue act tagging                In Section 2 we describe the multimodal Dan-
of the Switchboard-DAMSL corpus using lexical,               ish corpus. In Section 3, we describe how the
syntactic and prosodic cues, while Gravano and               prosody of feedback expressions is annotated, how
Hirschberg (2009) examine the relation between               their content is coded in terms of dialogue act, turn
particular acoustic and prosodic turn-yielding cues          and agreement labels, and we provide inter-coder
and turn taking in a large corpus of task-oriented           agreement measures. In Section 4 we account for
dialogues. Louwerse et al. (2006) and Louwerse               the annotation of head gestures, including inter-


                                                       318
                      Proceedings of the ACL 2010 Conference Short Papers, pages 318–324,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


coder agreements results. Section 5 contains a de-            of time restrictions, we limited the study to four
scription of the resulting datasets and a discussion          different subject pairs and two interactions per
of the results obtained in the classification experi-         pair, for a total of about an hour of video-recorded
ments. Section 6 is the conclusion.                           interaction.

2   The multimodal corpus
                                                              3    Annotation of feedback expressions
The Danish map-task dialogues from the Dan-
PASS corpus (Grønnum, 2006) are a collection
of dialogues in which 11 speaker pairs cooper-                As already mentioned, all words in DanPASS are
ate on a map task. The dialogue participants                  phonetically and prosodically annotated. In the
are seated in different rooms and cannot see each             subset of the corpus considered here, 82% of the
other. They talk through headsets, and one of them            feedback expressions bear stress or tone informa-
is recorded with a video camera. Each pair goes               tion, and 12% are unstressed; 7% of them are
through four different sets of maps, and changes              marked with onset or offset hesitation, or both.
roles each time, with one subject giving instruc-             For this study, we added semantic labels – includ-
tions and the other following them. The material              ing dialogue acts – and gesture annotation. Both
is transcribed orthographically with an indication            kinds of annotation were carried out using ANVIL
of stress, articulatory hesitations and pauses. In            (Kipp, 2004). To distinguish among the various
addition to this, the acoustic signals are segmented          functions that feedback expressions have in the di-
into words, syllables and prosodic phrases, and an-           alogues, we selected a subset of the categories de-
notated with POS-tags, phonological and phonetic              fined in the emerging ISO 24617-2 standard for
transcriptions, pitch and intonation contours.                semantic annotation of language resources. This
   Phonetic and prosodic segmentation and anno-               subset comprises the categories Accept, Decline,
tation were performed independently and in paral-             RepeatRephrase and Answer. Moreover, all feed-
lel by two annotators and then an agreed upon ver-            back expressions were annotated with an agree-
sion was produced with the supervision of an ex-              ment feature (Agree, NonAgree) where relevant.
pert annotator, for more information see Grønnum              Finally, the two turn management categories Turn-
(2006). The Praat tool was used (Boersma and                  Take and TurnElicit were also coded.
Weenink, 2009).                                                  It should be noted that the same expression may
   The feedback expressions we analyse here are               be annotated with a label for each of the three se-
Yes and No expressions, i.e. in Danish words like             mantic dimensions. For example, a yes can be an
ja (yes), jo (yes in a negative context), jamen (yes          Answer to a question, an Agree and a TurnElicit at
but, well), nej (no), næh (no). They can be single            the same time, thus making the semantic classifi-
words or multi-word expressions.                              cation very fine-grained. Table 1 shows how the
   Yes and No feedback expressions represent                  various types are distributed across the 466 feed-
about 9% of the approximately 47,000 running                  back expressions in our data.
words in the corpus. This is a rather high pro-
portion compared to other corpora, both spoken
                                                                           Dialogue Act
and written, and a reason why we decided to use                            Answer          70    15%
the DanPASS videos in spite of the fact that the                           RepeatRephrase  57    12%
gesture behaviour is relatively limited given the                          Accept         127    27%
                                                                           None           212    46%
fact that the two dialogue participants cannot see                         Agreement
each other. Furthermore, the restricted contexts                           Agree          166    36%
in which feedback expressions occur in these di-                           NonAgree        14     3%
                                                                           None           286    61%
alogues allow for a very fine-grained analysis of                          Turn Management
the relation of these expressions with prosody and                         TurnTake       113    24%
gestures. Feedback behaviour, both in speech and                           TurnElicit      85    18%
                                                                           None           268    58%
gestures, can be observed especially in the person
who is receiving the instructions (the follower).                 Table 1: Distribution of semantic categories
Therefore, we decided to focus our analysis only
on the follower’s part of the interaction. Because


                                                        319


3.1      Inter-coder agreement on feedback                        with how gestures contribute to the semantic in-
         expression annotation                                    terpretations of linguistic expressions. Therefore,
                                                                  only a subset of the MUMIN attributes has been
In general, dialogue act, agreement and turn anno-
                                                                  used, i.e. Smile, Laughter, Scowl, FaceOther for
tations were coded by an expert annotator and the
                                                                  facial expressions, and Nod, Jerk, Tilt, SideTurn,
annotations were subsequently checked by a sec-
                                                                  Shake, Waggle, Other for head movements.
ond expert annotator. However, one dialogue was
coded independently and in parallel by two expert                    A link was also established in ANVIL between
annotators to measure inter-coder agreement. A                    the gesture under consideration and the relevant
measure was derived for each annotated feature                    speech sequence where appropriate. The link was
using the agreement analysis facility provided in                 then used to extract gesture information together
ANVIL. Agreement between two annotation sets                      with the relevant linguistic annotations on which
is calculated here in terms of Cohen’s kappa (Co-                 to apply machine learning.
hen, 1960)1 and corrected kappa (Brennan and                         The total number of head gestures annotated is
Prediger, 1981)2 . Anvil divides the annotations in               264. Of these, 114 (43%) co-occur with feedback
slices and compares each slice. We used slices of                 expressions, with Nod as by far the most frequent
0.04 seconds. The inter-coder agreement figures                   type (70 occurrences) followed by FaceOther as
obtained for the three types of annotation are given              the second most frequent (16). The other tokens
in Table 2.                                                       are distributed more or less evenly, with a few oc-
                                                                  currences (2-8) per type. The remaining 150 ges-
         feature     Cohen’s k      corrected k                   tures, linked to different linguistic expressions or
         agreement   73.59          98.74                         to no expression at all, comprise many face ex-
         dial act    84.53          98.87                         pressions and a number of tilts. A rough prelim-
         turn        73.52          99.16                         inary analysis shows that their main functions are
                                                                  related to focusing or to different emotional atti-
Table 2: Inter-coder agreement on feedback ex-                    tudes. They will be ignored in what follows.
pression annotation
                                                                  4.1   Measuring inter-coder agreement on
   Although researchers do not totally agree on                         gesture annotation
how to measure agreement in various types of an-                  The head gestures in the DanPASS data have been
notated data and on how to interpret the resulting                coded by non expert annotators (one annotator
figures, see Artstein and Poesio (2008), it is usu-               per video) and subsequently controlled by a sec-
ally assumed that Cohen’s kappa figures over 60                   ond annotator, with the exception of one video
are good while those over 75 are excellent (Fleiss,               which was annotated independently and in parallel
1971). Looking at the cases of disagreement we                    by two annotators. The annotations of this video
could see that many of these are due to the fact                  were then used to measure inter-coder agreement
that the annotators had forgotten to remove some                  in ANVIL as it was the case for the annotations
of the features automatically proposed by ANVIL                   on feedback expressions. In the case of gestures
from the latest annotated element.                                we also measured agreement on gesture segmen-
                                                                  tation. The figures obtained are given in Table 3.
4       Gesture annotation
All communicative head gestures in the videos                      feature                 Cohen’s k     corrected k
were found and annotated with ANVIL using a                        face segment            69.89         91.37
subset of the attributes defined in the MUMIN an-                  face annotate           71.53         94.25
notation scheme (Allwood et al., 2007). The MU-                    head mov segment        71.21         91.75
MIN scheme is a general framework for the study                    head mov annotate       71.65         95.14
of gestures in interpersonal communication. In                    Table 3: Inter-coder agreement on head gesture
this study, we do not deal with functional classi-                annotation
fication of the gestures in themselves, but rather
    1                                                                These results are slightly worse than those ob-
     (P a − P e)/(1 − P e).
   2
     (P o − 1/c)/(1 − 1/c) where c is the number of cate-         tained in previous studies using the same annota-
gories.                                                           tion scheme (Jokinen et al., 2008), but are still sat-


                                                            320


isfactory given the high number of categories pro-            tation was only used in the training phase. The
vided by the scheme.                                          baseline for the evaluation are the results provided
   A distinction that seemed particularly difficult           by Weka’s ZeroR classifier, which always selects
was that between nods and jerks: although the                 the most frequent nominal class.
direction of the two movement types is different                 In Table 5 we provide results in terms of preci-
(down-up and up-down, respectively), the move-                sion (P), recall (R) and F-measure (F). These are
ment quality is very similar, and makes it difficult          calculated in Weka as weighted averages of the re-
to see the direction clearly. We return to this point         sults obtained for each class.
below, in connection with our data analysis.
                                                               dataset               Algor     P      R       F
5   Analysis of the data                                       YesNo                 ZeroR     27.8   52.8    36.5
                                                                                     HNB       47.2   53      46.4
The multimodal data we obtained by combining
                                                               +stress               HNB       47.5   54.1    47.1
the linguistic annotations from DanPASS with the
                                                               +stress+tone          HNB       47.8   54.3    47.4
gesture annotation created in ANVIL, resulted into
                                                               +stress+tone+hes      HNB       47.7   54.5    47.3
two different groups of data, one containing all Yes
and No expressions, and the other the subset of               Table 5: Classification results with prosodic fea-
those that are accompanied by a face expression               tures
or a head movement, as shown in Table 4.

        Expression             Count     %                       The results indicate that prosodic information
        Yes                      420     90                   improves the classification of dialogue acts with
        No                        46     10                   respect to the baseline in all four experiments with
        Total                    466    100                   improvements of 10, 10.6, 10.9 and 10.8%, re-
        Yes with gestures        102     90                   spectively. The best results are obtained using
        No with gestures          12     10                   information on stress and tone, although the de-
        Total with gestures      114    100                   crease in accuracy when hesitations are introduced
                                                              is not significant. The confusion matrices show
           Table 4: Yes and No datasets                       that the classifier is best at identifying Accept,
                                                              while it is very bad at identifying RepeatRephrase.
   These two sets of data were used for automatic             This result if not surprising since the former type
dialogue act classification, which was run in the             is much more frequent in the data than the latter,
Weka system (Witten and Frank, 2005). We exper-               and since prosodic information does not correlate
imented with various Weka classifiers, compris-               with RepeatRephrase in any systematic way.
ing Hidden Naive Bayes, SMO, ID3, LADTree                        The second group of experiments was con-
and Decision Table. The best results on most of               ducted on the dataset where feedback expressions
our data were obtained using Hidden Naive Bayes               are accompanied by gestures (102 Yes and 12 No).
(HNB) (Zhang et al., 2005). Therefore, here we                The purpose this time was to see whether ges-
show the results of this classifier. Ten-folds cross-         ture information improves dialogue act classifica-
validation was applied throughout.                            tion. We believe it makes sense to perform the
   In the first group of experiments we took into             test based on this restricted dataset, rather than the
consideration all the Yes and No expressions (420             entire material, because the portion of data where
Yes and 46 No) without, however, considering ges-             gestures do accompany feedback expressions is
ture information. The purpose was to see how                  rather small (about 20%). In a different domain,
prosodic information contributes to the classifica-           where subjects are less constrained by the techni-
tion of dialogue acts. We started by totally leav-            cal setting, we expect gestures would make for a
ing out prosody, i.e. only the orthographic tran-             stronger and more widespread effect.
scription (Yes and No expressions) was consid-                   The Precision, Recall and F-measure of the Ze-
ered; then we included information about stress               roR classifier on these data are 31.5, 56.1 and 40.4,
(stressed or unstressed); in the third run we added           respectively. For these experiments, however, we
tone attributes, and in the fourth information on             used as a baseline the results obtained based on
hesitation. Agreement and turn attributes were                stress, tone and hesitation information, the com-
used in all experiments, while Dialogue act anno-             bination that gave the best results on the larger


                                                        321


dataset. Together with the prosodic information,                dataset          Algor     P      R       F
Agreement and turn attributes were included just                YesNo            HNB       43.1   56.1    46.4
as earlier, while the dialogue act annotation was               +face            HNB       43.7   56.1    46.9
only used in the training phase. Face expression                +headm           HNB       47     57.9    51
and head movement attributes were disregarded                   +face+headm      HNB       51.6   57.9    53.9
in the baseline. We then added face expression
alone, head movement alone, and finally both ges-            Table 7: Classification results with fewer head
ture types together. The results are shown in Ta-            movements
ble 6.

    dataset         Algor    P      R       F                Danish. We have conducted three sets of experi-
    YesNo           HNB      43.1   56.1    46.4             ments, first looking at how prosodic features con-
    +face           HNB      43.7   56.1    46.9             tribute to the classification, then testing whether
    +headm          HNB      44.7   55.3    48.2             the use of head gesture information improved the
    +face+headm     HNB      49.9   57      50.3             accuracy of the classifier, finally running the clas-
                                                             sification on a dataset in which the head move-
Table 6: Classification results with head gesture            ment types were slightly more general. The re-
features                                                     sults indicate that prosodic features improve the
                                                             classification, and that in those cases where feed-
   These results indicate that adding head ges-              back expressions are accompanied by head ges-
ture information improves the classification of di-          tures, gesture information is also useful. The re-
alogue acts in this reduced dataset, although the            sults also show that using a more coarse-grained
improvement is not impressive. The best results              distinction of head movements improves classifi-
are achieved when both face expressions and head             cation in these data.
movements are taken into consideration.                         Slightly more than half of the head gestures in
   The confusion matrices show that although the             our data co-occur with other linguistic utterances
recognition of both Answer and None improve, it              than those targeted in this study. Extending our in-
is only the None class which is recognised quite             vestigation to those, as we plan to do, will provide
reliably. We already explained that in our annota-           us with a larger dataset and therefore presumably
tion a large number of feedback utterances have an           with even more interesting and reliable results.
agreement or turn label without necessarily having              The occurrence of gestures in the data stud-
been assigned to one of our task-related dialogue            ied here is undoubtedly limited by the technical
act categories. This means that head gestures                setup, since the two speakers do not see each other.
help distinguishing utterances with an agreement             Therefore, we want to investigate the role played
or turn function from other kinds. Looking closer            by head gestures in other types of video and larger
at these utterances, we can see that nods and jerks          materials. Extending the analysis to larger datasets
often occur together with TurnElicit, while tilts,           will also shed more light on whether our gesture
side turns and smiles tend to occur with Agree.              annotation categories are too fine-grained for au-
   An issue that worries us is the granularity of            tomatic classification.
the annotation categories. To investigate this, in
a third group of experiments we collapsed Nod                Acknowledgements
and Jerk into a more general category: the distinc-          This research has been done under the project
tion had proven difficult for the annotators, and we         VKK (Verbal and Bodily Communication) funded
don’t have many jerks in the data. The results, dis-         by the Danish Council for Independent Research
played in Table 7, show as expected an improve-              in the Humanities, and the NOMCO project, a
ment. The class which is recognised best is still            collaborative Nordic project with participating re-
None.                                                        search groups at the universities of Gothenburg,
                                                             Copenhagen and Helsinki which is funded by the
6   Conclusion
                                                             NOS-HS NORDCORP programme. We would
In this study we have experimented with the au-              also like to thank Nina Grønnum for allowing us to
tomatic classification of feedback expressions into          use the DanPASS corpus, and our gesture annota-
different dialogue acts in a multimodal corpus of            tors Josephine Bødker Arrild and Sara Andersen.


                                                       322


References                                                      Kristiina Jokinen, Costanza Navarretta, and Patrizia
                                                                  Paggio. 2008. Distinguishing the communica-
Jens Allwood, Loredana Cerrato, Kristiina Jokinen,                tive functions of gestures. In Proceedings of the
   Costanza Navarretta, and Patrizia Paggio. 2007.                5th MLMI, LNCS 5237, pages 38–49, Utrecht, The
   The MUMIN Coding Scheme for the Annotation of                  Netherlands, September. Springer.
   Feedback, Turn Management and Sequencing. Mul-
   timodal Corpora for Modelling Human Multimodal               Michael Kipp. 2004. Gesture Generation by Imita-
   Behaviour. Special Issue of the International Jour-            tion - From Human Behavior to Computer Charac-
   nal of Language Resources and Evaluation, 41(3–                ter Animation. Ph.D. thesis, Saarland University,
   4):273–287.                                                    Saarbruecken, Germany, Boca Raton, Florida, dis-
                                                                  sertation.com.
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,
  Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
  Stephen Isard, Jacqueline Kowtko, Jan McAllister,             Max M. Louwerse, Patrick Jeuniaux, Mohammed E.
  Jim Miller, Catherine Sotillo, Henry S. Thompson,              Hoque, Jie Wu, and Gwineth Lewis. 2006. Mul-
  and Regina Weinert. 1991. The HCRC Map Task                    timodal communication in computer-mediated map
  Corpus. Language and Speech, 34:351–366.                       task scenarios. In R. Sun and N. Miyake, editors,
                                                                 Proceedings of the 28th Annual Conference of the
Ron Artstein and Massimo Poesio. 2008. Inter-Coder               Cognitive Science Society, pages 1717–1722, Mah-
  Agreement for Computational Linguistics. Compu-                wah, NJ: Erlbaum.
  tational Linguistics, 34(4):555–596.
                                                                Max M. Louwerse, Nick Benesh, Mohammed E.
Paul Boersma and David Weenink, 2009. Praat: do-                 Hoque, Patrick Jeuniaux, Gwineth Lewis, Jie Wu,
  ing phonetics by computer. Retrieved May 1, 2009,              and Megan Zirnstein. 2007. Multimodal communi-
  from http://www.praat.org/.                                    cation in face-to-face conversations. In R. Sun and
                                                                 N. Miyake, editors, Proceedings of the 29th Annual
Robert L. Brennan and Dale J. Prediger. 1981. Co-                Conference of the Cognitive Science Society, pages
  efficient Kappa: Some uses, misuses, and alterna-              1235–1240, Mahwah, NJ: Erlbaum.
  tives. Educational and Psychological Measurement,
  41:687–699.                                                   Evelyn McClave. 2000. Linguistic functions of head
                                                                  movements in the context of speech. Journal of
Jacob Cohen. 1960. A coefficient of agreement                     Pragmatics, 32:855–878.
   for nominal scales. Educational and Psychological
   Measurement, 20(1):37–46.                                    Louis-Philippe Morency, Candace Sidner, Christopher
                                                                  Lee, and Trevor Darrell. 2005. Contextual Recog-
Joseph L. Fleiss. 1971. Measuring nominal scale                   nition of Head Gestures. In Proceedings of the In-
   agreement among many raters. Psychological Bul-                ternational Conference on Multi-modal Interfaces.
   lettin, 76(5):378–382.
                                                                Louis-Philippe Morency, Candace Sidner, Christopher
Shinya Fujie, Y. Ejiri, K. Nakajima, Y Matsusaka, and             Lee, and Trevor Darrell. 2007. Head gestures for
  Tetsunor Kobayashi. 2004. A conversation robot                  perceptual interfaces: The role of context in im-
  using head gesture recognition as para-linguistic in-           proving recognition. Artificial Intelligence, 171(8–
  formation. In Proceedings of the 13th IEEE Inter-               9):568–585.
  national Workshop on Robot and Human Interactive
  Communication, pages 159 – 164, september.
                                                                Louis-Philippe Morency, Iwan de Kok, and Jonathan
Agustin Gravano and Julia Hirschberg. 2009. Turn-                 Gratch. 2009. A probabilistic multimodal ap-
  yielding cues in task-oriented dialogue. In Pro-                proach for predicting listener backchannels. Au-
  ceedings of SIGDIAL 2009: the 10th Annual Meet-                 tonomous Agents and Multi-Agent Systems, 20:70–
  ing of the Special Interest Group in Discourse and              84, Springer.
  Dialogue, September 2009, pages 253–261, Queen
  Mary University of London.                                    Gabriel Murray and Steve Renals. 2008. Detecting
                                                                  Action Meetings in Meetings. In Proceedings of
Nina Grønnum. 2006. DanPASS - a Danish pho-                       the 5th MLMI, LNCS 5237, pages 208–213, Utrecht,
  netically annotated spontaneous speech corpus. In               The Netherlands, September. Springer.
  N. Calzolari, K. Choukri, A. Gangemi, B. Maegaard,
  J. Mariani, J. Odijk, and D. Tapias, editors, Pro-            Harm Rieks op den Akker and Christian Schulz. 2008.
  ceedings of the 5th LREC, pages 1578–1583, Genoa,               Exploring features and classifiers for dialogue act
  May.                                                            segmentation. In Proceedings of the 5th MLMI,
                                                                  pages 196–207.
Kristiina Jokinen and Anton Ragni. 2007. Cluster-
  ing experiments on the communicative prop- erties             Patrizia Paggio and Costanza Navarretta. 2010. Feed-
  of gaze and gestures. In Proceeding of the 3rd.                 back in Head Gesture and Speech. To appear in Pro-
  Baltic Conference on Human Language Technolo-                   ceedings of 7th Conference on Language Resources
  gies, Kaunas, Lithuania, October.                               and Evaluation (LREC-2010), Malta, May.


                                                          323


Dennis Reidsma, Dirk Heylen, and Harm Rieks op den
  Akker. 2009. On the Contextual Analysis of Agree-
  ment Scores. In Michael Kipp, Jean-Claude Mar-
  tin, Patrizia Paggio, and Dirk Heylen, editors, Multi-
  modal Corpora From Models of Natural Interaction
  to Systems and Applications, number 5509 in Lec-
  ture Notes in Artificial Intelligence, pages 122–137.
  Springer.

Vivek Kumar Rangarajan Sridhar, Srinivas Bangaloreb,
  and Shrikanth Narayanan. 2009. Combining lexi-
  cal, syntactic and prosodic cues for improved online
  dialog act tagging. Computer Speech & Language,
  23(4):407–422.

Ian H. Witten and Eibe Frank. 2005. Data Mining:
   Practical machine learning tools and techniques.
   Morgan Kaufmann, San Francisco, second edition.

Harry Zhang, Liangxiao Jiang, and Jiang Su. 2005.
  Hidden Naive Bayes. In Proceedings of the Twen-
  tieth National Conference on Artificial Intelligence,
  pages 919–924.




                                                           324

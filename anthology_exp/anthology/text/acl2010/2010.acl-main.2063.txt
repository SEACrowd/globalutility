         Simultaneous Tokenization and Part-of-Speech Tagging for Arabic
                       without a Morphological Analyzer

                                                 Seth Kulick
                                          Linguistic Data Consortium
                                          University of Pennsylvania
                                        skulick@seas.upenn.edu


                        Abstract                                    In contrast, other approaches have used a
                                                                 pipelined approach, with separate models to first
        We describe an approach to simultaneous                  do tokenization and then part-of-speech tagging
        tokenization and part-of-speech tagging                  (Diab et al., 2007; Diab, 2009). While these ap-
        that is based on separating the closed and               proaches have somewhat lower performance than
        open-class items, and focusing on the like-              the joint approach, they have the advantage that
        lihood of the possible stems of the open-                they do not rely on the presence of a full-blown
        class words. By encoding some basic lin-                 morphological analyzer, which may not always be
        guistic information, the machine learning                available or appropriate as the data shifts to differ-
        task is simplified, while achieving state-               ent genres or Arabic dialects.
        of-the-art tokenization results and compet-                 In this work we present a novel approach to
        itive POS results, although with a reduced               this problem that allows us to do simultaneous to-
        tag set and some evaluation difficulties.                kenization and core part-of-speech tagging with
                                                                 a simple classifier, without using a full-blown
1       Introduction                                             morphological analyzer. We distinguish between
                                                                 closed-class and open-class categories of words,
Research on the problem of morphological disam-                  and encode regular expressions that express the
biguation of Arabic has noted that techniques de-                morphological patterns for the former, and simple
veloped for lexical disambiguation in English do                 regular expressions for the latter that provide only
not easily transfer over, since the affixation present           the generic templates for affixation. We find that a
in Arabic creates a very different tag set than for              simple baseline for the closed-class words already
English, in terms of the number and complexity                   works very well, and for the open-class words we
of tags. In additional to inflectional morphology,               classify only the possible stems for all such ex-
the POS tags encode more complex tokenization                    pressions. This is however sufficient for tokeniza-
sequences, such as preposition + noun or noun +                  tion and core POS tagging, since the stem identi-
possessive pronoun.                                              fies the appropriate regular expression, which then
   One approach taken to this problem is to use                  in turn makes explicit, simultaneously, the tok-
a morphological analyzer such as BAMA-v2.0                       enization and part-of-speech information.
(Buckwalter, 2004) or SAMA-v3.1 (Maamouri et
al., 2009c)1 , which generates a list of all possi-              2       Background
ble morphological analyses for a given token. Ma-
                                                                 The Arabic Treebank (ATB) contains a full mor-
chine learning approaches can model separate as-
                                                                 phological analysis of each “source token”, a
pects of a solution (e.g., “has a pronominal clitic”)
                                                                 whitespace/punctuation-delimited string from the
and then combine them to select the most appro-
                                                                 source text. The SAMA analysis includes four
priate solution from among this list. A benefit
                                                                 fields, as shown in the first part of Table 1.2 TEXT
of this approach is that by picking a single solu-
                                                                 is the actual source token text, to be analyzed. VOC
tion from the morphological analyzer, the part-of-
                                                                 is the vocalized form, including diacritics. Each
speech and tokenization comes as a unit (Habash
                                                                 VOC segment has associated with it a POS tag and
and Rambow, 2005; Roth et al., 2008).
                                                                     2
                                                                     This is the analysis for one particular instance of ktbh.
    1
    SAMA-v3.1 is an updated version of BAMA, with many           The same source token may receive another analysis else-
significant differences in analysis.                             where in the treebank.


                                                           342
                          Proceedings of the ACL 2010 Conference Short Papers, pages 342–347,
                    Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


            ATB analysis for one source token:                   NOA                  173938   PART                288
 TEXT:                          ktbh                             PREP                  49894   RESTRIC PART        237
  VOC:       kutub           u                 hu                PUNC                  41398   DET                 215
  POS:      NOUN CASE NOM POSS PRON 3MS                          NOUN PROP             29423   RC PART             192
 GLOSS:      books      [def.nom.]           its/his             CONJ                  28257   FOCUS PART          191
             Results in two ATB tree tokens:                     PV                    16669   TYPO                188
  TEXT:               ktb                       h                IV                    15361   INTERROG PART       187
  VOC:             kutub+u                     hu                POSS PRON              9830   INTERROG ADV        169
   POS:      NOUN+CASE NOM            POSS PRON 3MS              SUB CONJ               8200   INTERROG PRON       112
                 Current work recovers:                          PRON                   6995   CV                  106
  TEXT:               ktb                       h                REL PRON               5647   VOC PART             74
   POS:              NOA                  POSS PRON              DEM                    3673   VERB                 62
                                                                 OBJ PRON               2812   JUS PART             56
                                                                 NEG PART               2649   FOREIGN              46
  Table 1: Example analysis of one source token                  PSEUDO VERB            1505   DIALECT              41
                                                                 FUT PART               1099   INTERJ               37
      NOUN, ADJ, NOUN.VN,                       NOA              ADV                    1058   EMPHATIC PART        19
  ADJ.VN, NOUN NUM, ADJ NUM,                  (Noun or           VERB PART               824   CVSUFF DO            15
 NOUN QUANT, ADJ COMP, ABBREV                Adjective)          REL ADV                 414   GRAMMAR PROB          4
           IV, IV PASS                           IV              CONNEC PART             405   LATIN                 1
           PV, PV PASS                           PV
     IVSUFF DO, PVSUFF DO                    OBJ PRON                Table 3: The 40 “reduced core tags”, and their fre-
Table 2: Collapsing of ATB core tags into reduced                    quencies in ATB3-v3.2. The total count is 402291,
core tags                                                            which is the number of tree tokens in ATB3-v3.2.

                                                                     has 339710 source tokens and 402291 tree tokens,
GLOSS. While “tokenization” can be done in dif-
                                                                     where the latter are derived from the former as dis-
ferent ways on top of this analysis, the ATB splits
                                                                     cussed above. Table 3 lists the 40 reduced tags
the VOC/POS/GLOSS segments up based on the
                                                                     we use, and their frequency among the ATB3-v3.2
POS tags to form the “tree tokens” necessary for
                                                                     tree tokens.
treebanking. As shown in the second part of Table
1, the first two segments remain together as one
                                                                     3   Description of Approach
tree token, and the pronoun is separated as a sep-
arate tree token. In addition, the input TEXT is                     Given a source token, we wish to recover (1) the
separated among the two tree tokens.3                                tree tokens (which amounts to recovering the ATB
   Each tree token’s POS tag therefore consists                      tokenization), and (2) the reduced core POS tag for
of what can be considered an “ATB core tag”,                         each tree token. For example, in Table 1, given the
together with inflectional material (case, gender,                   input source token TEXT ktbh, we wish to recover
number). For example, in Table 1, the “core tag”                     the tree tokens ktb/NOA and h/POSS PRON.
of the first tree token is NOUN. In this work, we aim                   As mentioned in the introduction, we use reg-
to recover the separation of a source token TEXT                     ular expressions that encode all the tokenization
into the corresponding separate tree token TEXTs,                    and POS tag possibilities. Each “group” (substring
together with a “reduced core tag” for each tree to-                 unit) in a regular expression (regex) is assigned an
ken. By “reduced core tag”, we mean an ATB core                      internal name, and a list is maintained of the pos-
tag that has been reduced in two ways:                               sible reduced core POS tags that can occur with
   (1) All inflectional material [infl] is stripped                  that regex group. It is possible, and indeed usu-
off six ATB core tags: PRON[infl], POSS PRON[infl],                  ally the case for groups representing affixes, that
DEM[infl], [IV|PV|CV]SUFF DO[infl]                                   more than one such POS tag is possible. How-
   (2) Collapsing of some ATB core tags, as listed                   ever, it is crucial for our approach that while some
in Table 2.                                                          given source token TEXT may match many regu-
   These two steps result in a total of 40 reduced                   lar expressions (regexes), when the POS tag is also
core tags, and each tree token has exactly one such                  taken into account, there can be only one match
reduced core tag. We work with the ATB3-v3.2 re-                     among all the (open or closed-class) regexes. We
lease of the ATB (Maamouri et al., 2009b), which                     say a source token “pos-matches” a regex if the
   3
                                                                     TEXT matches and POS tags match, and “text-
     See (Kulick et al., 2010) for a detailed discussion of
how this splitting is done and how the tree token TEXT field         matches” if the TEXT matches the regex regard-
(called INPUT STRING in the ATB releases) is created.                less of the POS. During training, the pos-matching


                                                               343


(REGEX #1) [w|f]lm                                            which is the name of the stem part of the regular
   w: [PART, CONJ, SUB CONJ, PREP]                            expression, along with the characters that match
   f: [CONJ, SUB CONJ, CONNEC PART, RC PART]                  the stem. The stem name encodes whether there
   lm: [NEG PART]                                             is a prefix or suffix, but does not include a POS
(REGEX #2) [w|f]lm                                            tag. However, the source token pos-matches ex-
   w: and f: same as above                                    actly one of the regular expressions, and the pos
   lm: [REL ADV,INTERROG ADV]                                 tag for the stem is appended to the named stem for
                                                              that expression to form the gold label for training
   Figure 1: Two sample closed-class regexes
                                                              and the target for testing.
                                                                 For example, Table 4 lists the matching regular
regex for a source token TEXT is stored as the                expression for three words. The first, yjry, text-
gold solution for closed-class patterns, or used to           matches the generic regular expressions for any
create the gold label for the open-class classifier.          string/NOA, any string/IV, etc. These are sum-
                                                              marized in one listing, yjry/all. The name of the
   We consider the open-class tags in Table 3 to be:
                                                              stem for all these expressions is the same, just
NOUN PROP, NOA, IV, CV, PV, VERB. A source
                                                              stem, and so they all give rise to the same feature,
token is considered to have an open-class solution
                                                              stem=yjry. It also matches the expression for
if any of the tree tokens in that solution have an
                                                              a NOA with a possessive pronoun4 , and in this
open-class tag. For example, ktbh in Table 1 has
                                                              case the stem name in the regular expression is
an open-class solution because one of the tree to-
                                                              stem_spp (which stands for “stem with a pos-
kens has an open-class tag (NOA), even though the
                                                              sessive pronoun suffix”), and this gives rise to the
other is closed-class (POSS PRON).
                                                              feature stem_spp=yjr. Similarly, for wAfAdt
   We encode the possible solutions for closed-
                                                              the stem of the second expression has the name
class source tokens using the lists in the ATB mor-
                                                              p_stem, for a prefix. The third example shows
phological guidelines (Maamouri et al., 2009a).
                                                              the different stem names that occur when there are
For example, Figure 1 shows two of the closed-
                                                              both prefix and suffix possibilities. For each exam-
class regexes. The text wlm can text-match either
                                                              ple, there is exactly one regex that not only text-
REGEX #1 or #2, but when the POS tag for lm is
                                                              matches, but also pos-matches. The combination
taken into account, only one can pos-match. We
                                                              of the stem name in these cases together with the
return to the closed-class regexes in Section 4.
                                                              gold tag forms the gold label, as indicated in col-
   We also encode regular expression for the open-            umn 3.
class source tokens, but these are simply generic                Therefore, for each source token TEXT, the
templates expressing the usual affix possibilities,           features include the ones arising from the named
such as:                                                      stems of all the regexes that text-match that TEXT,
[wf] [blk] stem NOA poss pronoun                              as shown in column 4, and the gold label is the
where there is no list of possible strings for                appropriate stem name together with the POS
stem_NOA, but which instead can match any-                    tag, as shown in column 3. We also include
thing. While all parts except for the stem are op-            some typical features for each stem, such as first
tional, we do not make such parts optional in a               and last two letters of each stem, etc. For ex-
single expression. Instead, we multiple out the               ample, wAfAdt would also have the features
possibilities into different expressions with differ-         stem_fl=w, p_stem_fl=A, indicating that the
ent parts (e.g., [wf]) being obligatory). The reason          first letter of stem is w and the first letter of
for this is that we give different names to the stem          p_stem is A. We also extract a list of proper
in each case, and this is the basis of the features           nouns from SAMA-v3.1 as a temporary proxy for
for the classifier. As with the closed-class regexes,         a named entity list, and include a feature for a
we associate a list of possible POS tags for each             stem if that stem is in the list (stem_in_list,
named group within a regular expression. Here                 p_stem_in_list, etc.)
the stem NOA group can only have the tag NOA.                    We do not model separate classifiers for prefix
   We create features for a classifier for the open-          possibilities. There is a dependency between the
class words as follows. Each word is run through                 4
                                                                   The regex listed is slightly simplified. It actually con-
all of the open-class regular expressions. For each           tains a reference to the list of all possessive pronouns, not
expression that text-matches, we make a feature               just y.


                                                        344


    source TEXT     text-matching regular expressions                        gold label        feature
        yjry        yjry/all (happens)                                       stem:IV           stem=yjry
                    yjr/NOA+y/POSS PRON                                                        stem spp=yjr
      wAfAdt        wAfAdt/all                                                                 stem=wAfAdt
                    w + AfAdt/all (and+reported)                             p stem:PV         p stem=AfAdt
    lAstyDAHhm      lAstyDAHhm/all                                                             stem=lAstyDAHhm
                    l/PREP + AstyDAHhm/NOA                                                     p stem=AstyDAHhm
                    l/PREP + AstyDAH/NOA + hm/POSS PRON                      p stem spp:NOA    p stem spp=AstyDAH
                    for + request for clarification + their
                    lAstyDAH/NOA + hm/POSS PRON                                                stem spp=lAstyDAH
                    lAstyDAH/IV,PV,CV + hm/OBJ PRON                                            stem svop=lAstyDAH
                    l/PREP,JUS PART + AstyDAH/IV,PV,CV +                                       p stem svop=AstyDAH
                                                          hm/OBJ PRON

Table 4: Example features and gold labels for three words. Each text-matching regex gives rise to one
feature shown in column 4, based on the stem of that regular expression. A p before a stem means that
it has a prefix, spp after means that it has a possessive pronouns suffix, and svop means that it has
a (verbal) object pronoun suffix. “all” in the matching regular expression is shorthand for text-matching
all the corresponding regular expressions with NOA, IV, etc. For each word, exactly one regex also
pos-matches, which results in the gold label, shown in column 3.


possibility of a prefix and the likelihood of the re-               Section 3. For a closed-class solution, “solution”
maining stem, and so we focus on the likelihood of                  is the name of the single pos-matching regex. In
the possible stems, where the open-class regexes                    addition, for every regex seen during training that
enumerate the possible stems. A gold label to-                      pos-matches some source token TEXT, we keep a
gether with the source token TEXT maps back to                      listing (List #2) of all ((regex-group-name, text),
a single regex, and so for a given label, the TEXT                  POS-tag) tuples. We use the information in List
is parsed by that regular expression, resulting in a                #1 to choose a solution for all words seen in train-
tokenization along with list of possible POS tags                   ing in the Baseline and Run 2 below, and in Run
for each affix group in the regex.5                                 3, for words text-matching a closed-class expres-
   During training and testing, we run each word                    sion. We use List #2 to disambiguate all remain-
through all the open and closed regexes. Text-                      ing cases of POS ambiguity, wherever a solution
matches for an open-class regex give rise to fea-                   comes from.
tures as just described. Also, if the word matches                     For example, if wlm is seen during testing, List
any closed-class regex, it receives the feature                     #1 will be consulted to find the most common so-
MATCHES CLOSED. During training, if the cor-                        lution (REGEX #1 or #2), and in either case, List
rect match for the word is one of the closed-class                  #2 will be consulted to determine the most fre-
expressions, then the gold label is CLOSED. The                     quent tag for w as a prefix. While there is certainly
classifier is used only to get solutions for the open-              room for improvement here, this works quite well
class words, although we wish to give the classifier                since the tags for the affixes do not vary much.
all the words for the sentence. The cross-product                      We score the solution for a source token in-
of the stem name and (open-class) reduced core                      stance as correct for tokenization if it exactly
POS tags, plus the CLOSED tag, yields 24 labels                     matches the TEXT split for the tree tokens derived
for a CRF classifier in Mallet (McCallum, 2002).                    from that source token instance in the ATB. It is
                                                                    correct for POS if correct for tokenization and if
4   Experiments and Evaluation                                      each tree token has the same POS tag as the re-
                                                                    duced core tag for that tree token in the ATB.
We worked with ATB3-v3.2, following the train-                         For a simple baseline, if a source token TEXT
ing/devtest split in (Roth et al., 2008) on a pre-                  is in List #1 then we simply use the most fre-
vious release of the same data. We keep a list-                     quent stored solution. Otherwise we run the TEXT
ing (List #1) of all (source token TEXT, solution)                  through all the regexes. If it text-matches any
pairs seen during training. For an open-class so-                   closed-class expression, we pick a random choice
lution, “solution” is the gold label as described in                from among those regexes and otherwise from the
   5
     In Section 4 we discuss how these are narrowed down to         open-class regexes that it text-matches. Any POS
one POS tag.                                                        ambiguities for a regex group are disambiguated


                                                              345


 Solution               Baseline                           Run 2                             Run 3
  Origin     # tokens      Tok     POS       # tokens       Tok          POS      # tokens    Tok       POS
   All         51664     96.0%     87.4%       51664       99.4%        95.1%       51664    99.3%      95.1%
  Stored       46072     99.8%     96.6%       46072       99.8%        96.6%       16145    99.6%      96.4%
  Open           5565    64.6%     11.6%           10      10.0%         0.0%           11   54.5%       0.0%
  Closed           27    81.5%     59.3%           27      81.5%        63.0%           27   81.5%      63.0%
  Mallet            0                            5555      96.0%        83.8%       35481    99.1%      94.5%

Table 5: Results for Baseline and two runs. Origin “stored” means that the appropriate regex came from
the list stored during training. Origins “open” and “closed” are random choices from the open or closed
regexes for the source token. “Mallet” means that it comes from the label output by the CRF classifier.


using List #2, as discussed above. The results                 tokenization for evaluation of POS results, which
are shown in Table 5. The score is very high for               we do not. The “MorphPOS” task in (Roth et al.,
the words seen during training, but much lower                 2008), 96.4%, is somewhat similar to ours in that
for open-class words that were not. As expected,               it scores on a “core tag”, but unlike for us there is
almost all (except 27) instances of closed-class               only one such tag for a source token (easier) but it
words were seen during training.                               distinguishes between NOUN and ADJ (harder).
    For run 2, we continue to use the stored solution             We would like to do a direct comparison by sim-
if the token was seen in training. If not, then if the         ply runing the above systems on the exact same
TEXT matches one or more closed-class regexes,                 data and evaluating them the same way. However,
we randomly choose one. Otherwise, if the CRF                  this unfortunately has to wait until new versions
classifier has produced an open-class match for                are released that work with the current version of
that token, we use that (and otherwise, in only 10             the SAMA morphological analyzer and ATB.
cases, use a random open-class match). There is a
                                                               5   Future Work
significant improvement in the score for the open-
class items, and therefore in the overall results.             Obvious future work starts with the need to in-
   For run 3, we put more of a burden on the clas-             clude determiner information in the POS tags and
sifier. If a word matches any closed-class expres-             the important NOUN/ADJ distinction. There are
sion, we either use the most frequent occurence                various possibilities for recovering this informa-
during training (if it was seen), or use a random              tion, such as (1) using a different module combin-
maching closed-class expression (if not). If the               ing NOUN/ADJ disambiguation together with NP
word doesn’t match a closed-class expression, we               chunking, or (2) simply including NOUN/ADJ in
use the mallet result. The mallet score goes up, al-           the current classifier instead of NOA. We will be
most certainly because the score is now including              implementing and comparing these alternatives.
results on words that were seen during training.               We also will be using this system as a preprocess-
The overall POS result for run 3 is slightly less              ing step for a parser, as part of a complete Arabic
than run 2. (95.099% compared to 95.147%).                     NLP pipeline.
   It is not a simple matter to compare results with
                                                               Acknowledgements
previous work, due to differing evaluation tech-
niques, data sets, and POS tag sets. With differ-              We thank Ann Bies, David Graff, Nizar Habash,
ent data sets and training sizes, Habash and Ram-              Mohamed Maamouri, and Mitch Marcus for
bow (2005) report 99.3% word accuracy on tok-                  helpful discussions and comments. This work
enization, and Diab et al. (2007) reports a score              was supported by the Defense Advanced Re-
of 99.1%. Habash and Rambow (2005) reported                    search Projects Agency, GALE Program Grant
97.6% on the LDC-supplied reduced tag set, and                 No. HR0011-06-1-0003 and by the GALE pro-
Diab et al. (2007) reported 96.6%. The LDC-                    gram, DARPA/CMO Contract No. HR0011-06-
supplied tag set used is smaller than the one in               C-0022. The content of this paper does not nec-
this paper (24 tags), but does distinguish between             essarily reflect the position or the policy of the
NOUN and ADJ. However, both (Habash and                        Government, and no official endorsement should
Rambow, 2005; Diab et al., 2007) assume gold                   be inferred.


                                                         346


References
Tim Buckwalter. 2004. Buckwalter Arabic morpho-
  logical analyzer version 2.0. Linguistic Data Con-
  sortium LDC2004L02.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
 2007. Automatic processing of Modern Standard
 Arabic text. In Abdelhadi Soudi, Antal van den
 Bosch, and Gunter Neumann, editors, Arabic Com-
 putational Morphology, pages 159–179. Springer.
Mona Diab. 2009. Second generation tools (AMIRA
 2.0): Fast and robust tokenization, pos tagging, and
 base phrase chunking. In Proceedings of 2nd Inter-
 national Conference on Arabic Language Resources
 and Tools (MEDAR), Cairo, Egypt, April.
Nizar Habash and Owen Rambow. 2005. Arabic to-
  kenization, part-of-speech tagging and morphologi-
  cal disambiguation in one fell swoop. In Proceed-
  ings of the 43rd Annual Meeting of the Association
  for Computational Linguistics (ACL’05), pages 573–
  580, Ann Arbor, Michigan, June. Association for
  Computational Linguistics.

Seth Kulick, Ann Bies, and Mohamed Maamouri.
  2010. Consistent and flexible integration of mor-
  phological annotation in the Arabic Treebank. In
  Language Resources and Evaluation (LREC).

Mohamed Maamouri, Ann Bies, Sondos Krouna,
 Fatma Gaddeche, Basma Bouziri, Seth Kulick, Wig-
 dane Mekki, and Tim Buckwalter. 2009a. Arabic
 Treebank Morphological and Syntactic guidelines,
 July. http://projects.ldc.upenn.edu/ArabicTreebank.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
 Krouna, Fatma Gaddeche, and Wajdi Zaghouani.
 2009b. Arabic treebank part 3 - v3.2. Linguistic
 Data Consortium LDC2010T08, April.
Mohammed Maamouri, Basma Bouziri, Sondos
 Krouna, David Graff, Seth Kulick, and Tim Buck-
 walter. 2009c. Standard Arabic morphological ana-
 lyzer (SAMA) version 3.1. Linguistic Data Consor-
 tium LDC2009E73.
Andrew McCallum. 2002. Mallet: A machine learning
  for language toolkit. http://mallet.cs.umass.edu.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
  and Cynthia Rudin. 2008. Arabic morphologi-
  cal tagging, diacritization, and lemmatization using
  lexeme models and feature ranking. In Proceed-
  ings of ACL-08: HLT, Short Papers, pages 117–120,
  Columbus, Ohio, June. Association for Computa-
  tional Linguistics.




                                                         347

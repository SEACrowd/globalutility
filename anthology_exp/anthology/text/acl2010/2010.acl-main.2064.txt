                Hierarchical A∗ Parsing with Bridge Outside Scores

                                    Adam Pauls and Dan Klein
                                     Computer Science Division
                                  University of California at Berkeley
                                   {adpauls,klein}@cs.berkeley.edu




                    Abstract                                 true outside costs, we expect them to reduce the
                                                             work of computing inside costs in finer grammars.
    Hierarchical A∗ (HA∗ ) uses of a hierarchy               At the same time, because bridge costs mix com-
    of coarse grammars to speed up parsing                   putation from coarser and finer levels of the hier-
    without sacrificing optimality. HA∗ pri-                 archy, they are more expensive to compute than
    oritizes search in refined grammars using                purely coarse outside costs. Whether the work
    Viterbi outside costs computed in coarser                saved by using tighter estimates outweighs the ex-
    grammars. We present Bridge Hierarchi-                   tra computation needed to compute them is an em-
    cal A∗ (BHA∗ ), a modified Hierarchial A∗                pirical question.
    algorithm which computes a novel outside                    In this paper, we show that the use of bridge out-
    cost called a bridge outside cost. These                 side costs substantially outperforms the HA∗ al-
    bridge costs mix finer outside scores with               gorithm when the coarsest levels of the hierarchy
    coarser inside scores, and thus consti-                  are very loose approximations of the target gram-
    tute tighter heuristics than entirely coarse             mar. For hierarchies with tighter estimates, we
    scores. We show that BHA∗ substan-                       show that BHA∗ obtains comparable performance
    tially outperforms HA∗ when the hierar-                  to HA∗ . In other words, BHA∗ is more robust to
    chy contains only very coarse grammars,                  poorly constructed hierarchies.
    while achieving comparable performance
    on more refined hierarchies.                             2     Previous Work

1   Introduction                                             In this section, we introduce notation and review
                                                             HA∗ . Our presentation closely follows Pauls and
The Hierarchical A∗ (HA∗ ) algorithm of Felzen-              Klein (2009), and we refer the reader to that work
szwalb and McAllester (2007) allows the use of a             for a more detailed presentation.
hierarchy of coarse grammars to speed up pars-
ing without sacrificing optimality. Pauls and                2.1    Notation
Klein (2009) showed that a hierarchy of coarse               Assume we have input sentence s0 . . . sn−1 of
grammars outperforms standard A∗ parsing for a               length n, and a hierarchy of m weighted context-
range of grammars. HA∗ operates by computing                 free grammars G1 . . . Gm . We call the most refined
Viterbi inside and outside scores in an agenda-              grammar Gm the target grammar, and all other
based way, using outside scores computed under               (coarser) grammars auxiliary grammars. Each
coarse grammars as heuristics which guide the                grammar Gt has a set of symbols denoted with cap-
search in finer grammars. The outside scores com-            ital letters and a subscript indicating the level in
puted by HA∗ are auxiliary quantities, useful only           the hierarchy, including a distinguished goal (root)
because they form admissible heuristics for search           symbol Gt . Without loss of generality, we assume
in finer grammars.                                           Chomsky normal form, so each non-terminal rule
   We show that a modification of the HA∗ algo-              r in Gt has the form r = At → Bt Ct with weight
rithm can compute modified bridge outside scores             wr .
which are tighter bounds on the true outside costs              Edges are labeled spans e = (At , i, j). The
in finer grammars. These bridge outside scores               weight of a derivation is the sum of rule weights
mix inside and outside costs from finer grammars             in the derivation. The weight of the best (mini-
with inside costs from coarser grammars. Because             mum) inside derivation for an edge e is called the
the bridge costs represent tighter estimates of the          Viterbi inside score β(e), and the weight of the


                                                       348
                      Proceedings of the ACL 2010 Conference Short Papers, pages 348–352,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


(a)            Gt                 (b)                   Gt
                                                                                 items), as well as a chart of items already pro-
                                                                                 cessed. The fundamental operation of the algo-
               VPt                                      VPt                      rithm is to pop the highest priority item φ from
                                                                                 the agenda, put it into the chart with its current
                                                                                 weight, and form using deduction rules any items
 s0 .. s2 s3        s4   s5 .. sn-1     s0 .. s2   s3         s4   s5 .. sn-1    which can be built by combining φ with items al-
                                                                                 ready in the chart. If new or improved, resulting
Figure 1: Representations of the different types of items                        items are put on the agenda with priority given by
used in parsing and how they depend on each other. (a)
In HA∗ , the inside item I(VPt , 3, 5) relies on the coarse                      p(·). Because all antecedents must be constructed
outside item O(πt (VPt ), 3, 5) for outside estimates. (b) In                    before a deduction rule is executed, we sometimes
BHA∗ , the same inside item relies on the bridge outside item                    refer to particular conclusion item as “waiting” on
Õ(VPt , 3, 5), which mixes coarse and refined outside costs.
The coarseness of an item is indicated with dotted lines.                        an other item(s) before it can be built.

                                                                                 2.3   HA∗
best derivation of G → s0 . . . si−1 At sj . . . sn−1                            HA∗ can be formulated in terms of two types of
is called the Viterbi outside score α(e). The goal                               items. Inside items I(At , i, j) represent possible
of a 1-best parsing algorithm is to compute the                                  derivations of the edge (At , i, j), while outside
Viterbi inside score of the edge (Gm , 0, n); the                                items O(At , i, j) represent derivations of G →
actual best parse can be reconstructed from back-                                s1 . . . si−1 At sj . . . sn rooted at (Gt , 0, n). See
pointers in the standard way.                                                    Figure 1(a) for a graphical depiction of these
   We assume that each auxiliary grammar Gt−1                                    edges. Inside items are used to compute Viterbi in-
forms a relaxed projection of Gt . A grammar Gt−1                                side scores under grammar Gt , while outside items
is a projection of Gt if there exists some many-                                 are used to compute Viterbi outside scores.
to-one onto function πt which maps each symbol                                      The deduction rules which construct inside and
in Gt to a symbol in Gt−1 ; hereafter, we will use                               outside items are given in Table 1. The IN deduc-
A0t to represent πt (At ). A projection is relaxed                               tion rule combines two inside items over smaller
if, for every rule r = At → Bt Ct with weight                                    spans with a grammar rule to form an inside item
wr the projection r0 = A0t → Bt0 Ct0 has weight                                  over larger spans. The weight of the resulting item
wr0 ≤ wr in Gt−1 . In other words, the weight of r0                              is the sum of the weights of the smaller inside
is a lower bound on the weight of all rules r in Gt                              items and the grammar rule. However, the IN rule
which project to r0 .                                                            also requires that an outside score in the coarse
                                                                                 grammar1 be computed before an inside item is
2.2   Deduction Rules
                                                                                 built. Once constructed, this coarse outside score
HA∗ and our modification BHA∗ can be formu-                                      is added to the weight of the conclusion item to
lated in terms of prioritized weighted deduction                                 form the priority of the resulting item. In other
rules (Shieber et al., 1995; Felzenszwalb and                                    words, the coarse outside score computed by the
McAllester, 2007). A prioritized weighted deduc-                                 algorithm plays the same role as a heuristic in stan-
tion rule has the form                                                           dard A∗ parsing (Klein and Manning, 2003).
                                                                                    Outside scores are computed by the OUT-L and
                             p(w1 ,...,wn )                                      OUT-R deduction rules. These rules combine an
  φ1 : w1 , . . . , φn : wn −−−−−−−−→ φ0 : g(w1 , . . . , wn )
                                                                                 outside item over a large span and inside items
                                                                                 over smaller spans to form outside items over
where φ1 , . . . , φn are the antecedent items of the
                                                                                 smaller spans. Unlike the IN deduction, the OUT
deduction rule and φ0 is the conclusion item. A
                                                                                 deductions only involve items from the same level
deduction rule states that, given the antecedents
                                                                                 of the hierarchy. That is, whereas inside scores
φ1 , . . . , φn with weights w1 , . . . , wn , the conclu-
                                                                                 wait on coarse outside scores to be constructed,
sion φ0 can be formed with weight g(w1 , . . . , wn )
                                                                                 outside scores wait on inside scores at the same
and priority p(w1 , . . . , wn ).
                                                                                 level in the hierarchy.
   These deduction rules are “executed” within
a generic agenda-driven algorithm, which con-                                       Conceptually, these deduction rules operate by
structs items in a prioritized fashion. The algo-                                   1
                                                                                      For the coarsest grammar G1 , the IN rule builds rules
rithm maintains an agenda (a priority queue of                                   using 0 as an outside score.


                                                                           349


                                                                  HA∗
                                                                                          w1 +w2 +wr +w3
         IN:       I(Bt , i, l) : w1      I(Ct , l, j) : w2       O(A0t , i, j) : w3      −−−−−−−−−−→        I(At , i, j)         :       w1 + w2 + wr
                                                                                          w1 +w3 +wr +w2
         OUT-L:    O(At , i, j) : w1      I(Bt , i, l) : w2       I(Ct , l, j) : w3       −−−−−−−−−−→        O(Bt , i, l)         :       w1 + w3 + wr
                                                                                          w +w2 +wr +w3
         OUT-R:    O(At , i, j) : w1      I(Bt , i, l) : w2       I(Ct , l, j) : w3       −−1−−−−−−−−→       O(Ct , l, j)         :       w1 + w2 + wr

      Table 1: HA∗ deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.

                                                                  BHA∗
                                                                                           w +w +wr +w
        B-IN:        I(Bt , i, l) : w1     I(Ct , l, j) : w2       Õ(At , i, j) : w3     −−1−−−
                                                                                               2     3
                                                                                                −−−−−→        I(At , i, j)            :   w1 + w2 + wr
                                                                                           w1 +wr +w2 +w3
        B-OUT-L:     Õ(At , i, j) : w1    I(Bt0 , i, l)   : w2    I(Ct0 , l, j)   : w3   −−−−−−−−−−→         Õ(Bt , i, l)           :   w1 + wr + w3
                                                                                           w1 +w2 +wr +w3
        B-OUT-R:     Õ(At , i, j) : w1    I(Bt , i, l) : w2       I(Ct0 , l, j)   : w3   −−−−−−−−−−→         Õ(Ct , l, j)           :   w1 + w2 + wr

  Table 2: BHA∗ deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.


                                                                                                                            Gt
first computing inside scores bottom-up in the
coarsest grammar, then outside scores top-down
                                                                                                             St                             NPt
in the same grammar, then inside scores in the                                                                                              Xt-1
next finest grammar, and so on. However, the cru-                                                 NPt                VPt
cial aspect of HA∗ is that items from all levels                                                               VPt               NPt
of the hierarchy compete on the same queue, in-                                                                                  Xt-1
                                                                                                                  NPt
terleaving the computation of inside and outside                                                  NNt       VPt   Xt-1 Xt-1 Xt-1 Xt-1
scores at all levels. The HA∗ deduction rules come
with three important guarantees. The first is a                                                    s0      s1 s2 s3         s4 s5 sn-1
monotonicity guarantee: each item is popped off
the agenda in order of its intrinsic priority p̂(·).                                Figure 2: A concrete example of a possible bridge outside
For inside items I(e) over edge e, this priority                                    derivation for the bridge item Õ(VPt , 1, 4). This edge is
p̂(I(e)) = β(e) + α(e0 ) where e0 is the projec-                                    boxed for emphasis. The spine of the derivation is shown
                                                                                    in bold and colored in blue. Rules from a coarser grammar
tion of e. For outside items O(·) over edge e, this                                 are shown with dotted lines, and colored in red. Here we have
priority is p̂(O(e)) = β(e) + α(e).                                                 the simple projection πt (A) = X, ∀A.
   The second is a correctness guarantee: when
an inside/outside item is popped of the agenda, its                                 the sequence of rules between e and the root edge
weight is its true Viterbi inside/outside cost. Taken                               (Gt , 0, n). A bridge outside derivation of e is a
together, these two imply an efficiency guarantee,                                  derivation d of G → s1 . . . si At sj+1 . . . sn such
which states that only items x whose intrinsic pri-                                 that every rule on or left of the spine comes from
ority p̂(x) is less than or equal to the Viterbi inside                             Gt , and all other rules come from Gt−1 . The score
score of the goal are removed from the agenda.                                      of the best such derivation for e is the bridge out-
                                                                                    side cost α̃(e).
2.4     HA∗ with Bridge Costs                                                          Like ordinary outside costs, bridge outside costs
The outside scores computed by HA∗ are use-                                         form consistent and admissible estimates of the
ful for prioritizing computation in more refined                                    true Viterbi outside score α(e) of an edge e. Be-
grammars. The key property of these scores is                                       cause bridge costs mix rules from the finer and
that they form consistent and admissible heuristic                                  coarser grammar, bridge costs are at least as good
costs for more refined grammars, but coarse out-                                    an estimate of the true outside score as entirely
side costs are not the only quantity which satisfy                                  coarse outside costs, and will in general be much
this requirement. As an alternative, we propose                                     tighter. That is, we have
a novel “bridge” outside cost α̃(e). Intuitively,
                                                                                                        α(e0 ) ≤ α̃(e) ≤ α(e)
this cost represents the cost of the best deriva-
tion where rules “above” and “left” of an edge e                                    In particular, note that the bridge costs become
come from Gt , and rules “below” and “right” of                                     better approximations farther right in the sentence,
the e come from Gt−1 ; see Figure 2 for a graph-                                    and the bridge cost of the last word in the sentence
ical depiction. More formally, let the spine of                                     is equal to the Viterbi outside cost of that word.
an edge e = (At , i, j) for some derivation d be                                       To compute bridge outside costs, we introduce


                                                                            350


                                                                                        40
bridge outside items Õ(At , i, j), shown graphi-




                                                              Items Pushed (Billions)
cally in Figure 1(b). The deduction rules which                                                                                                                        BHA*
                                                                                        30
                                                                                                                                                                       HA*
build both inside items and bridge outside items
                                                                                        20
are shown in Table 2. The rules are very simi-
lar to those which define HA∗ , but there are two                                       10
important differences. First, inside items wait for
bridge outside items at the same level, while out-                                       0
                                                                                         0-split                               1-split       2-split         3-split    4-split   5-split
side items wait for inside items from the previous
level. Second, the left and right outside deductions
                                                              Figure 3: Performance of HA∗ and BHA∗ as a function of
are no longer symmetric – bridge outside items                increasing refinement of the coarse grammar. Lower is faster.
can extended to the left given two coarse inside
items, but can only be extended to the right given                                                                       10
an exact inside item on the left and coarse inside




                                                                                               Edges Pushed (billions)
item on the right.                                                                                                       7.5


2.5    Guarantees                                                                                                         5

These deduction rules come with guarantees anal-                                                                         2.5
ogous to those of HA∗ . The monotonicity guaran-
tee ensures that inside and (bridge) outside items                                                                        0
                                                                                                                                         3             3-5             0-5
are processed in order of:
                                                              Figure 4: Performance of BHA∗ on hierarchies of varying
              p̂(I(e)) = β(e) + α̃(e)                         size. Lower is faster. Along the x-axis, we show which coarse
                                                              grammars were used in the hierarchy. For example, 3-5 in-
             p̂(Õ(e)) = α̃(e) + β(e0 )                       dicates the 3-,4-, and 5-split grammars were used as coarse
                                                              grammars.
   The correctness guarantee ensures that when an
item is removed from the agenda, its weight will
be equal to β(e) for inside items and α̃(e) for
                                                              builds both inside and bridge outside items under
bridge items. The efficiency guarantee remains the
                                                              the target grammar, where HA∗ only builds inside
same, though because the intrinsic priorities are
                                                              items. It is an empirical, grammar- and hierarchy-
different, the set of items processed will be differ-
                                                              dependent question whether the increased tight-
ent from those processed by HA∗ .
                                                              ness of the outside estimates outweighs the addi-
   A proof of these guarantees is not possible
                                                              tional cost needed to compute them. We demon-
due to space restrictions. The proof for BHA∗
                                                              strate empirically in this section that for hier-
follows the proof for HA∗ in Felzenszwalb and
                                                              archies with very loosely approximating coarse
McAllester (2007) with minor modifications. The
                                                              grammars, BHA∗ can outperform HA∗ , while
key property of HA∗ needed for these proofs is
                                                              for hierarchies with good approximations, perfor-
that coarse outside costs form consistent and ad-
                                                              mance of the two algorithms is comparable.
missible heuristics for inside items, and exact in-
                                                                 We performed experiments with the grammars
side costs form consistent and admissible heuris-
                                                              of Petrov et al. (2006). The training procedure for
tics for outside items. BHA∗ also has this prop-
                                                              these grammars produces a hierarchy of increas-
erty, with bridge outside costs forming admissi-
                                                              ingly refined grammars through state-splitting, so
ble and consistent heuristics for inside items, and
                                                              a natural projection function πt is given. We used
coarse inside costs forming admissible and consis-
                                                              the Berkeley Parser2 to learn such grammars from
tent heuristics for outside items.
                                                              Sections 2-21 of the Penn Treebank (Marcus et al.,
3     Experiments                                             1993). We trained with 6 split-merge cycles, pro-
                                                              ducing 7 grammars. We tested these grammars on
The performance of BHA∗ is determined by the                  300 sentences of length ≤ 25 of Section 23 of the
efficiency guarantee given in the previous sec-               Treebank. Our “target grammar” was in all cases
tion. However, we cannot determine in advance                 the most split grammar.
whether BHA∗ will be faster than HA∗ . In fact,
BHA∗ has the potential to be slower – BHA∗                                              2
                                                                                            http://berkeleyparser.googlecode.com


                                                        351


   In our first experiment, we construct 2-level hi-                ence of the North American Chapter of the Associa-
erarchies consisting of one coarse grammar and                      tion for Computational Linguistics (NAACL).
the target grammar. By varying the coarse gram-                   Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
mar from the 0-split (X-bar) through 5-split gram-                   Klein. 2006. Learning accurate, compact, and in-
mars, we can investigate the performance of each                     terpretable tree annotation. In Proccedings of the
algorithm as a function of the coarseness of the                     Association for Computational Linguistics (ACL).
coarse grammar. We follow Pauls and Klein                         Stuart M. Shieber, Yves Schabes, and Fernando C. N.
(2009) in using the number of items pushed as                        Pereira. 1995. Principles and implementation of
a machine- and implementation-independent mea-                       deductive parsing. Journal of Logic Programming,
                                                                     24:3–36.
sure of speed. In Figure 3, we show the perfor-
mance of HA∗ and BHA∗ as a function of the
total number of items pushed onto the agenda.
We see that for very coarse approximating gram-
mars, BHA∗ substantially outperforms HA∗ , but
for more refined approximating grammars the per-
formance is comparable, with HA∗ slightly out-
performing BHA∗ on the 3-split grammar.
   Finally, we verify that BHA∗ can benefit from
multi-level hierarchies as HA∗ can. We con-
structed two multi-level hierarchies: a 4-level hier-
archy consisting of the 3-,4-,5-, and 6- split gram-
mars, and 7-level hierarchy consisting of all gram-
mars. In Figure 4, we show the performance of
BHA∗ on these multi-level hierarchies, as well as
the best 2-level hierarchy from the previous exper-
iment. Our results echo the results of Pauls and
Klein (2009): although the addition of the rea-
sonably refined 4- and 5-split grammars produces
modest performance gains, the addition of coarser
grammars can actually hurt overall performance.

Acknowledgements
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.


References
P. Felzenszwalb and D. McAllester. 2007. The gener-
   alized A* architecture. Journal of Artificial Intelli-
   gence Research.

Dan Klein and Christopher D. Manning. 2003. A*
  parsing: Fast exact Viterbi parse selection. In
  Proceedings of the Human Language Technology
  Conference and the North American Association
  for Computational Linguistics (HLT-NAACL), pages
  119–126.

M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
  Building a large annotated corpus of English: The
  Penn Treebank. In Computational Linguistics.

Adam Pauls and Dan Klein. 2009. Hierarchical search
  for parsing. In Proceedings of The Annual Confer-


                                                            352

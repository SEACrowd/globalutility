      Using Parse Features for Preposition Selection and Error Detection


          Joel Tetreault                      Jennifer Foster                       Martin Chodorow
    Educational Testing Service                    NCLT                           Hunter College of CUNY
            Princeton                       Dublin City University                 New York, NY, USA
            NJ, USA                                Ireland                             martin.chodorow
       JTetreault@ets.org                 jfoster@computing.dcu.ie                     @hunter.cuny.edu




                      Abstract                                    We recreate a state-of-the-art preposition usage
                                                               system (Tetreault and Chodorow (2008), hence-
     We evaluate the effect of adding parse fea-               forth T&C08) originally trained with lexical fea-
     tures to a leading model of preposition us-               tures and augment it with parser output features.
     age. Results show a significant improve-                  We employ the Stanford parser in our experiments
     ment in the preposition selection task on                 because it consists of a competitive phrase struc-
     native speaker text and a modest increment                ture parser and a constituent-to-dependency con-
     in precision and recall in an ESL error de-               version tool (Klein and Manning, 2003a; Klein
     tection task. Analysis of the parser output               and Manning, 2003b; de Marneffe et al., 2006;
     indicates that it is robust enough in the face            de Marneffe and Manning, 2008). We com-
     of noisy non-native writing to extract use-               pare the original model with the parser-augmented
     ful information.                                          model on the tasks of preposition selection in well-
                                                               formed text (fluent writers) and preposition error
1    Introduction                                              detection in learner texts (ESL writers).
The task of preposition error detection has re-                   This paper makes the following contributions:
ceived a considerable amount of attention in re-                   • We demonstrate that parse features have a
cent years because selecting an appropriate prepo-                   significant impact on preposition selection in
sition poses a particularly difficult challenge to                   well-formed text. We also show which fea-
learners of English as a second language (ESL).                      tures have the greatest effect on performance.
It is not only ESL learners that struggle with En-
                                                                   • We show that, despite the noisiness of learner
glish preposition usage — automatically detecting
                                                                     text, parse features can actually make small,
preposition errors made by ESL speakers is a chal-
                                                                     albeit non-significant, improvements to the
lenging task for NLP systems. Recent state-of-the-
                                                                     performance of a state-of-the-art preposition
art systems have precision ranging from 50% to
                                                                     error detection system.
80% and recall as low as 10% to 20%.
   To date, the conventional wisdom in the error                   • We evaluate the accuracy of parsing and
detection community has been to avoid the use                        especially preposition attachment in learner
of statistical parsers under the belief that a WSJ-                  texts.
trained parser’s performance would degrade too
                                                               2    Related Work
much on noisy learner texts and that the tradi-
tionally hard problem of prepositional phrase at-              T&C08, De Felice and Pulman (2008) and Ga-
tachment would be even harder when parsing ESL                 mon et al. (2008) describe very similar preposi-
writing. However, there has been little substantial            tion error detection systems in which a model of
research to support or challenge this view. In this            correct prepositional usage is trained from well-
paper, we investigate the following research ques-             formed text and a writer’s preposition is com-
tion: Are parser output features helpful in mod-               pared with the predictions of this model. It is
eling preposition usage in well-formed text and                difficult to directly compare these systems since
learner text?                                                  they are trained and tested on different data sets


                                                         353
                        Proceedings of the ACL 2010 Conference Short Papers, pages 353–358,
                  Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


but they achieve accuracy in a similar range. Of             3.1   Baseline System
these systems, only the DAPPER system (De Fe-                The work of Chodorow et al. (2007) and T&C08
lice and Pulman, 2008; De Felice and Pulman,                 treat the tasks of preposition selection and er-
2009; De Felice, 2009) uses a parser, the C&C                ror detection as a classification problem. That
parser (Clark and Curran, 2007)), to determine               is, given the context around a preposition and a
the head and complement of the preposition. De               model of correct usage, a classifier determines
Felice and Pulman (2009) remark that the parser              which of the 34 prepositions covered by the model
tends to be misled more by spelling errors than              is most appropriate for the context. A model of
by grammatical errors. The parser is fundamental             correct preposition usage is constructed by train-
to their system and they do not carry out a com-             ing a Maximum Entropy classifier (Ratnaparkhi,
parison of the use of a parser to determine the              1998) on millions of preposition contexts from
preposition’s attachments versus the use of shal-            well-formed text.
lower techniques. T&C08, on the other hand, re-                 A context is represented by 25 lexical features
ject the use of a parser because of the difficulties         and 4 combination features:
they foresee in applying one to learner data. Her-
met et al. (2008) make only limited use of the               Lexical Token and POS n-grams in a 2 word
Xerox Incremental Parser in their preposition er-            window around the preposition, plus the head verb
ror detection system. They split the input sentence          in the preceding verb phrase (PV), the head noun
into the chunks before and after the preposition,            in the preceding noun phrase (PN) and the head
and parse both chunks separately. Only very shal-            noun in the following noun phrase (FN) when
low analyses are extracted from the parser output            available (Chodorow et al., 2007). Note that these
because they do not trust the full analyses.                 are determined not through full syntactic parsing
   Lee and Knutsson (2008) show that knowl-                  but rather through the use of a heuristic chun-
edge of the PP attachment site helps in the task             ker. So, for the phrase many local groups around
of preposition selection by comparing a classifier           the country, examples of lexical features for the
trained on lexical features (the verb before the             preposition around include: FN = country, PN =
preposition, the noun between the verb and the               groups, left-2-word-sequence = local-groups, and
preposition, if any, and the noun after the preposi-         left-2-POS-sequence = JJ-NNS.
tion) to a classifier trained on attachment features
which explicitly state whether the preposition is            Combination T&C08 expand on the lexical fea-
attached to the preceding noun or verb. They also            ture set by combining the PV, PN and FN fea-
argue that a parser which is capable of distinguish-         tures, resulting in features such as PN-FN and
ing between arguments and adjuncts is useful for             PV-PN-FN. POS and token versions of these fea-
generating the correct preposition.                          tures are employed. The intuition behind creat-
                                                             ing combination features is that the Maximum En-
3   Augmenting a Preposition Model with                      tropy classifier does not automatically model the
    Parse Features                                           interactions between individual features. An ex-
                                                             ample of the PN-FN feature is groups-country.
To test the effects of adding parse features to
a model of preposition usage, we replicated the              3.2   Parse Features
lexical and combination feature model used in                To augment the above model we experimented
T&C08, training on 2M events extracted from a                with 14 features divided among five main classes.
corpus of news and high school level reading ma-             Table 1 shows the features and their values for
terials. Next, we added the parse features to this           our around example. The Preposition Head and
model to create a new model “+Parse”. In 3.1 we              Complement feature represents the two basic at-
describe the T&C08 system and features, and in               tachment relations of the preposition, i.e. its head
3.2 we describe the parser output features used to           (what it is attached to) and its complement (what
augment the model. We illustrate our features us-            is attached to it). Relation specifies the relation
ing the example phrase many local groups around              between the head and complement. The Preposi-
the country. Fig. 1 shows the phrase structure tree          tion Head and Complement Combined features
and dependency triples returned by the Stanford              are similar to the T&C08 Combination features
parser for this phrase.                                      except that they are extracted from parser output.


                                                       354


                                                             Model                                      Accuracy
                                                             combination only                             35.2
                                                             parse only                                   60.6
                                                             combination+parse                            61.9
                      NP
                                                             lexical only                                 64.4
                                                             combination+lexical (T&C08)                  65.2
           NP                       PP                       lexical+parse                                68.1
                                                             all features (+Parse)                        68.5
                            IN             NP
    DT     JJ     NNS
                           around    DT         NN          Table 2: Accuracy on preposition selection task
   many   local   groups                                    for various feature combinations
                                     the    country
amod(groups-3,    many-1)
amod(groups-3,    local-2)                                  The Preposition Head and Complement Mixed
prep(groups-3,    around-4)
det(country-6,    the-5)                                    features are created by taking the first feature in
pobj(around-4,    country-6)                                the previous set and backing-off either the head
                                                            or the complement to its POS tag. This mix of
                                                            tags and tokens in a word-word dependency has
Figure 1: Phrase structure tree and dependency
                                                            proven to be an effective feature in sentiment anal-
triples produced by the Stanford parser for the
                                                            ysis (Joshi and Penstein-Rosé, 2009). All the fea-
phrase many local groups around the country
                                                            tures described so far are extracted from the set of
                                                            dependency triples output by the Stanford parser.
                                                            The final set of features (Phrase Structure), how-
                                                            ever, is extracted directly from the phrase structure
                                                            trees themselves.

                                                            4       Evaluation
                                                            In Section 4.1, we compare the T&C08 and +Parse
 Prep. Head & Complement
                                                            models on the task of preposition selection on
 1. head of the preposition: groups
                                                            well-formed texts written by native speakers. For
 2. POS of the head: NNS
                                                            every preposition in the test set, we compare the
 3. complement of the preposition: country
                                                            system’s top preposition for that context to the
 4. POS of the complement: NN
                                                            writer’s preposition, and report accuracy rates. In
 Prep. Head & Complement Relation
                                                            Section 4.2, we evaluate the two models on ESL
 5. Prep-Head relation name: prep
                                                            data. The task here is slightly different - if the
 6. Prep-Comp relation name: pobj
                                                            most likely preposition according to the model dif-
 Prep. Head & Complement Combined
                                                            fers from the likelihood of the writer’s preposition
 7. Head-Complement tokens: groups-country
                                                            by a certain threshold amount, a preposition error
 8. Head-Complement tags: NNS-NN
                                                            is flagged.
 Prep. Head & Complement Mixed
 9. Head Tag and Comp Token: NNS-country                    4.1      Native Speaker Test Data
 10. Head Token and Comp Tag: groups-NN
 Phrase Structure                                           Our test set consists of 259K preposition events
 11. Preposition Parent: PP                                 from the same source as the original training data.
 12. Preposition Grandparent: NP                            The T&C08 model performs at 65.2% and when
 13. Left context of preposition parent: NP                 the parse features are added, the +Parse model im-
 14. Right context of preposition parent: -                 proves performance by more than 3% to 68.5%.1
                                                            The improvement is statistically significant.
            Table 1: Parse Features
                                                                1
                                                                 Prior research has shown preposition selection perfor-
                                                            mance accuracy ranging from 65% to nearly 80%. The dif-
                                                            ferences are largely due to different test sets and also training
                                                            sizes. Given the time required to train large models, we report
                                                            here experiments with a relatively small model.


                                                      355


          Model                          Accuracy                                Method     Precision    Recall
          T&C08                            65.2                                  T&C08      0.461        0.215
          +Phrase Structure Only           67.1                                  +Parse     0.486        0.225
          +Dependency Only                 68.2
          +Parse                           68.5                             Table 4: ESL Error Detection Results
          +head-tag+comp-tag               66.9
          +left                            66.8                       native speakers for the Test of English as a Foreign
          +grandparent                     66.6                       Language (TOEFL R ). The prepositions were
          +head-token+comp-tag             66.6                       judged by two trained annotators and checked
          +head-tag                        66.5                       by the authors using the preposition annotation
          +head-token                      66.4                       scheme described in Tetreault and Chodorow
          +head-tag+comp-token             66.1                       (2008b). 4,881 of the prepositions were judged to
                                                                      be correct and the remaining 302 were judged to
Table 3: Which parse features are important? Fea-
                                                                      be incorrect.
ture Addition Experiment
                                                                         The writer’s preposition is flagged as an error by
                                                                      the system if its likelihood according to the model
   Table 2 shows the effect of various feature class                  satisfied a set of criteria (e.g., the difference be-
combinations on prediction accuracy. The results                      tween the probability of the system’s choice and
are clear: a significant performance improvement                      the writer’s preposition is 0.8 or higher). Un-
is obtained on the preposition selection task when                    like the selection task where we use accuracy as
features from parser output are added. The two                        the metric, we use precision and recall with re-
best models in Table 2 contain parse features. The                    spect to error detection. To date, performance
table also shows that the non-parser-based feature                    figures that have been reported in the literature
classes are not entirely subsumed by the parse fea-                   have been quite low, reflecting the difficulty of the
tures but rather provide, to varying degrees, com-                    task. Table 4 shows the performance figures for
plementary information.                                               the T&C08 and +Parse models. Both precision
   Having established the effectiveness of parse                      and recall are higher for the +Parse model, how-
features, we investigate which parse feature                          ever, given the low number of errors in our an-
classes contribute the most. To test each contri-                     notated test set, the difference is not statistically
bution, we perform a feature addition experiment,                     significant.
separately adding features to the T&C08 model
(see Table 3). We make three observations. First,
                                                                      5   Parser Accuracy on ESL Data
while there is overlapping information between                        To evaluate parser performance on ESL data,
the dependency features and the phrase structure                      we manually inspected the phrase structure trees
features, the phrase structure features are mak-                      and dependency graphs produced by the Stanford
ing a contribution. This is interesting because                       parser for 210 ESL sentences, split into 3 groups:
it suggests that a pure dependency parser might                       the sentences in the first group are fluent and con-
be less useful than a parser which explicitly pro-                    tain no obvious grammatical errors, those in the
duces both constituent and dependency informa-                        second contain at least one preposition error and
tion. Second, using a parser to identify the prepo-                   the sentences in the third are clearly ungrammati-
sition head seems to be more useful than using it to                  cal with a variety of error types. For each preposi-
identify the preposition complement.2 Finally, as                     tion we note whether the parser was successful in
was the case for the T&C08 features, the combina-                     determining its head and complement. The results
tion parse features are also important (particularly                  for the three groups are shown in Table 5. The
the tag-tag or tag/token pairs).                                      figures in the first row are for correct prepositions
                                                                      and those in the second are for incorrect ones.
4.2      ESL Test Data
                                                                         The parser tends to do a better job of de-
Our test data consists of 5,183 preposition events                    termining the preposition’s complement than its
extracted from a set of essays written by non-                        head which is not surprising given the well-known
   2
       De Felice (2009) observes the same for the DAPPER sys-         problem of PP attachment ambiguity. Given the
tem.                                                                  preposition, the preceding noun, the preceding


                                                                356


                                  OK                          6   Conclusion
                        Head               Comp
   Prep Correct    86.7% (104/120) 95.0% (114/120)            We have shown that the use of a parser can boost
  Prep Incorrect          -                   -
                            Preposition Error                 the accuracy of a preposition selection model
                        Head               Comp               tested on well-formed text. In the error detection
   Prep Correct     89.0% (65/73)      97.3% (71/73)          task, the improvement is less marked. Neverthe-
  Prep Incorrect    87.1% (54/62)      96.8% (60/62)
                             Ungrammatical                    less, examination of parser output shows the parse
                        Head               Comp               features can be extracted reliably from ESL data.
   Prep Correct    87.8% (115/131) 89.3% (117/131)               For our immediate future work, we plan to carry
  Prep Incorrect    70.8% (17/24)      87.5% (21/24)
                                                              out the ESL evaluation on a larger test set to bet-
                                                              ter gauge the usefulness of a parser in this context,
Table 5: Parser Accuracy on Prepositions in a
                                                              to carry out a detailed error analysis to understand
Sample of ESL Sentences
                                                              why certain parse features are effective and to ex-
                                                              plore a larger set of features.
verb and the following noun, Collins (1999) re-                  In the longer term, we hope to compare different
ports an accuracy rate of 84.5% for a PP attach-              types of parsers in both the preposition selection
ment classifier. When confronted with the same                and error detection tasks, i.e. a task-based parser
information, the accuracy of three trained annota-            evaluation in the spirit of that carried out by Miyao
tors is 88.2%. Assuming 88.2% as an approximate               et al. (2008) on the task of protein pair interaction
PP-attachment upper bound, the Stanford parser                extraction. We would like to further investigate
appears to be doing a good job. Comparing the                 the role of parsing in error detection by looking at
results over the three sentence groups, its ability           other error types and other text types, e.g. machine
to identify the preposition’s head is quite robust to         translation output.
grammatical noise.
   Preposition errors in isolation do not tend to             Acknowledgments
mislead the parser: in the second group which con-            We would like to thank Rachele De Felice and the
tains sentences which are largely fluent apart from           reviewers for their very helpful comments.
preposition errors, there is little difference be-
tween the parser’s accuracy on the correctly used
prepositions and the incorrectly used ones. Exam-             References
ples are                                                      Martin Chodorow, Joel Tetreault, and Na-Rae Han.
(S (NP I)                                                      2007. Detection of grammatical errors involv-
                                                               ing prepositions. In Proceedings of the 4th ACL-
   (VP had                                                     SIGSEM Workshop on Prepositions, Prague, Czech
       (NP (NP a trip)                                         Republic, June.
           (PP for (NP Italy))
                                                              Stephen Clark and James R. Curran. 2007. Wide-
       )
                                                                 coverage efficient statistical parsing with CCG
   )                                                             and log-linear models. Computational Linguistics,
)                                                                33(4):493–552.
in which the erroneous preposition for is correctly           Michael Collins. 1999. Head-driven Statistical Mod-
attached to the noun trip, and                                  els for Natural Language Parsing. Ph.D. thesis,
                                                                University of Pennsylvania.
(S (NP A scientist)
   (VP devotes                                                Rachele De Felice and Stephen G. Pulman. 2008. A
                                                                classifier-based approach to preposition and deter-
       (NP (NP his prime part)                                  miner error correction in L2 english. In Proceedings
           (PP of (NP his life))                                of the 22nd COLING, Manchester, United Kingdom.
       )
       (PP in (NP research))                                  Rachele De Felice and Stephen Pulman. 2009. Au-
                                                                tomatic detection of preposition errors in learning
   )                                                            writing. CALICO Journal, 26(3):512–528.
)
                                                              Rachele De Felice. 2009. Automatic Error Detection
in which the erroneous preposition in is correctly              in Non-native English. Ph.D. thesis, Oxford Univer-
attached to the verb devotes.                                   sity.


                                                        357


Marie-Catherine de Marneffe and Christopher D. Man-
 ning. 2008. The stanford typed dependencies repre-
 sentation. In Proceedings of the COLING08 Work-
 shop on Cross-framework and Cross-domain Parser
 Evaluation, Manchester, United Kingdom.
Marie-Catherine de Marneffe, Bill MacCartney, and
 Christopher D. Manning. 2006. Generating typed
 dependency parses from phrase structure parses. In
 Proceedings of LREC, Genoa, Italy.
Michael Gamon, Jianfeng Gao, Chris Brockett,
  Alexandre Klementiev, William B. Dolan, Dmitriy
  Belenko, and Lucy Vanderwende. 2008. Using con-
  textual speller techniques and language modelling
  for ESL error correction. In Proceedings of the In-
  ternational Joint Conference on Natural Language
  Processing, Hyderabad, India.
Matthieu Hermet, Alain Désilets, and Stan Szpakow-
 icz. 2008. Using the web as a linguistic resource
 to automatically correct lexico-syntactic errors. In
 Proceedings of LREC, Marrekech, Morocco.
Mahesh Joshi and Carolyn Penstein-Rosé. 2009. Gen-
 eralizing dependency features for opinion mining.
 In Proceedings of the ACL-IJCNLP 2009 Confer-
 ence Short Papers, pages 313–316, Singapore.
Dan Klein and Christopher D. Manning. 2003a. Ac-
  curate unlexicalized parsing. In Proceedings of the
  41st Annual Meeting of the ACL, pages 423–430,
  Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b. Fast
  exact inference with a factored model for exact pars-
  ing. In Advances in Neural Information Processing
  Systems, pages 3–10. MIT Press, Cambridge, MA.
John Lee and Ola Knutsson. 2008. The role of PP at-
  tachment in preposition generation. In Proceedings
  of CICling. Springer-Verlag Berlin Heidelberg.
Yusuke Miyao, Rune Saetre, Kenji Sagae, Takuya Mat-
  suzaki, and Jun’ichi Tsujii. 2008. Task-oriented
  evaluation of syntactic parsers and their representa-
  tions. In Proceedings of the 46th Annual Meeting of
  the ACL, pages 46–54, Columbus, Ohio.
Adwait Ratnaparkhi. 1998. Maximum Entropy Mod-
  els for natural language ambiguity resolution. Ph.D.
  thesis, University of Pennsylvania.
Joel Tetreault and Martin Chodorow. 2008. The ups
  and downs of preposition error detection in ESL
  writing. In Proceedings of the 22nd COLING,
  Manchester, United Kingdom.
Joel Tetreault and Martin Chodorow. 2008b. Na-
  tive Judgments of non-native usage: Experiments in
  preposition error detection. In COLING Workshop
  on Human Judgments in Computational Linguistics,
  Manchester, United Kingdom.




                                                          358

             An Active Learning Approach to Finding Related Terms
         David Vickrey                    Oscar Kipersztok                            Daphne Koller
       Stanford University           Boeing Research & Technology                   Stanford Univeristy
dvickrey@cs.stanford.edu                   oscar.kipersztok                  koller@cs.stanford.edu
                                             @boeing.com


                     Abstract                                “jaguar”. First, there is sense ambiguity; we could
                                                             be referring to either a “large cat” or a “car.” Be-
    We present a novel system that helps non-                yond this, we might have in mind various more (or
    experts find sets of similar words. The                  less) specific groups: “Mexican animals,” “preda-
    user begins by specifying one or more seed               tors,” “luxury cars,” “British cars,” etc.
    words. The system then iteratively sug-
                                                                We propose a system which addresses sev-
    gests a series of candidate words, which
                                                             eral shortcomings of many set expansion systems.
    the user can either accept or reject. Cur-
                                                             First, these systems can be difficult to use. As ex-
    rent techniques for this task typically boot-
                                                             plored by Vyas et al. (2009), non-expert users
    strap a classifier based on a fixed seed
                                                             produce seed sets that lead to poor quality expan-
    set. In contrast, our system involves
                                                             sions, for a variety of reasons including ambiguity
    the user throughout the labeling process,
                                                             and lack of coverage. Even for expert users, con-
    using active learning to intelligently ex-
                                                             structing seed sets can be a laborious and time-
    plore the space of similar words. In
                                                             consuming process. Second, most set expansion
    particular, our system can take advan-
                                                             systems do not use negative examples, which can
    tage of negative examples provided by the
                                                             be very useful for weeding out other bad answers.
    user. Our system combines multiple pre-
                                                             Third, many set expansion systems concentrate on
    existing sources of similarity data (a stan-
                                                             noun classes such as “US Presidents” and are not
    dard thesaurus, WordNet, contextual sim-
                                                             effective or do not apply to other kinds of sets.
    ilarity), enabling it to capture many types
                                                                Our system works as follows. The user initially
    of similarity groups (“synonyms of crash,”
                                                             thinks of at least one seed word belonging to the
    “types of car,” etc.). We evaluate on a
                                                             desired set. One at a time, the system presents can-
    hand-labeled evaluation set; our system
                                                             didate words to the user and asks whether the can-
    improves over a strong baseline by 36%.
                                                             didate fits the concept. The user’s answer is fed
1   Introduction                                             back into the system, which takes into account this
Set expansion is a well-studied NLP problem                  new information and presents a new candidate to
where a machine-learning algorithm is given a                the user. This continues until the user is satisfied
fixed set of seed words and asked to find additional         with the compiled list of “Yes” answers. Our sys-
members of the implied set. For example, given               tem uses both positive and negative examples to
the seed set {“elephant,” “horse,” “bat”}, the al-           guide the search, allowing it to recover from ini-
gorithm is expected to return other mammals. Past            tially poor seed words. By using multiple sources
work, e.g. (Roark & Charniak, 1998; Ghahramani               of similarity data, our system captures a variety of
& Heller, 2005; Wang & Cohen, 2007; Pantel                   kinds of similarity. Our system replaces the poten-
et al., 2009), generally focuses on semi-automatic           tially difficult problem of thinking of many seed
acquisition of the remaining members of the set by           words with the easier task of answering yes/no
mining large amounts of unlabeled data.                      questions. The downside is a possibly increased
   State-of-the-art set expansion systems work               amount of user interaction (although standard set
well for well-defined sets of nouns, e.g. “US Pres-          expansion requires a non-trivial amount of user in-
idents,” particularly when given a large seed set.           teraction to build the seed set).
Set expansions is more difficult with fewer seed                There are many practical uses for such a sys-
words and for other kinds of sets. The seed words            tem. Building a better, more comprehensive the-
may have multiple senses and the user may have in            saurus/gazetteer is one obvious application. An-
mind a variety of attributes that the answer must            other application is in high-precision query expan-
match. For example, suppose the seed word is                 sion, where a human manually builds a list of ex-


                                                       371
                      Proceedings of the ACL 2010 Conference Short Papers, pages 371–376,
                Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics


pansion terms. Suppose we are looking for pages                correspond to how well each row aligns with the
discussing “public safety.” Then synonyms (or                  goal set G. Thus, θi should be large and positive if
near-synonyms) of “safety” would be useful (e.g.               row i has large entries for positive but not negative
“security”) but also non-synonyms such as “pre-                examples; and it should be large and negative if
cautions” or “prevention” are also likely to return            row i has large entries for negative but not positive
good results. In this case, the concept we are inter-          examples. Suppose that we have already chosen
ested in is “Words which imply that safety is being            an appropriate weight vector θ. We wish to rank
discussed.” Another interesting direction not pur-             all possible words (i.e., the columns of M ) so that
sued in this paper is using our system as part of              the most promising word gets the highest score.
a more-traditional set expansion system to build               A natural way to generate a score zj for column
seed sets more quickly.                                        j is toPtake the dot product of θ with column j,
2   Set Expansion                                              zj = i θi mij . This rewards word wj for having
As input, we are provided with a small set of seed             high membership in rows with positive θ, and low
words s. The desired output is a target set of                 membership in rows with negative θ.
words G, consisting of all words that fit the de-                 Our system uses a “batch” approach to active
sired concept. A particular seed set s can belong              learning. At iteration i, it chooses a new θ based
to many possible goal sets G, so additional infor-             on all data labeled so far (for the 1st iteration,
mation may be required to do well.                             this data consists of the seed set s). It then
   Previous work tries to do as much as possible               chooses the column (word) with the highest score
using only s. Typically s is assumed to contain at             (among words not yet labeled) as the candidate
least 2 words and often many more. Pantel et al.               word wi . The user answers “Yes” or “No,” indicat-
(2009) discusses the issue of seed set size in detail,         ing whether or not wi belongs to G. wi is added
concluding that 5-20 seed words are often required             to the positive set p or the negative set n based
for good performance.                                          on the user’s answer. Thus, we have a labeled data
                                                               set that grows from iteration to iteration as the user
   There are several problems with the fixed seed
                                                               labels each candidate word. Unlike set expansion,
set approach. It is not always easy to think of
                                                               this procedure generates (and uses) both positive
even a single additional seed word (e.g., the user is
                                                               and negative examples.
trying to find “German automakers” and can only
think of “Volkswagen”). Even if the user can think                We explore two options for choosing θ. Recall
of additional seed words, time and effort might be             that each row i is associated with a label li . The
saved by using active learning to find good sug-               first method is to set θi = 1 if li ∈ p (that is, the
gestions. Also, as Vyas et al. (2009) show, non-               set of positively labeled words includes label li ),
expert users often produce poor-quality seed sets.             θi = −1 if li ∈ n, and θi = 0 otherwise. We
                                                               refer to this method as “Untrained”, although it is
3   Active Learning System                                     still adaptive — it takes into account the labeled
Any system for this task relies on information                 examples the user has provided so far.
about similarity between words. Our system takes                  The second method uses a standard machine
as input a rectangular matrix M . Each column                  learning algorithm, logistic regression. As be-
corresponds to a particular word. Each row cor-                fore, the final ranking over words is based on the
responds to a unique dimension of similarity; the              score zj . However, zj is passed through the lo-
j th entry in row i mij is a number between 0 and              gistic function to produce a score between 0 and
1 indicating the degree to which wj belongs to the             1, zj0 =       1
                                                                                   . We can interpret this score
                                                                            1+e−zj
ith similarity group. Possible similarity dimen-               as the probability that wj is a positive example,
sions include “How similar is word wj to the verb              Pθ (Y |wj ). This leads to the objective function
jump?” “Is wj a type of cat?” and “Are the words
which appear in the context of wj similar to those                           Y                    Y
                                                               L(θ) = log(          Pθ (Y |wj )       (1−Pθ (Y |wj ))).
that appear in the context of boat?” Each row ri
                                                                            wj ∈p                 wj ∈n
of M is labeled with a word li . This may follow
intuitively from the similarity axis (e.g., “jump,”            This objective is convex and can be optimized us-
“cat,” and “boat”, respectively), or it can be gen-            ing standard methods such as L-BFGS (Liu & No-
erated automatically (e.g. the word wj with the                cedal, 1989). Following standard practice we add
highest membership mij ).                                                                    T
                                                               an L2 regularization term − θ2σ2θ to the objective.
    Let θ be a vector of weights, one per row, which
                                                               This method does not use the row labels li .

                                                         372


    Data          Word        Similar words
    Moby          arrive      accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,...
    WordNet       factory     (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);...
    DistSim       watch       (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),...

                 Table 1: Examples of unprocessed similarity entries from each data source.
4       Data Sources                                                   converting this hierarchy into a similarity score;
We consider three similarity data sources: the                         we chose to use the Jiang-Conrath distance (Jiang
Moby thesaurus1 , WordNet (Fellbaum, 1998), and                        & Conrath, 1997) because it tends to be more ro-
distributional similarity based on a large corpus                      bust to the exact structure of WordNet. The num-
of text (Lin, 1998). Table 1 shows similarity lists                    ber of types of similarity in WordNet tends to be
from each. These sources capture different kinds                       less than that captured by Moby, because synsets
of similarity information, which increases the rep-                    in WordNet are (usually) only allowed to have a
resentational power of our system. For all sources,                    single parent. For example, “murder” is classified
the similarity of a word with itself is set to 1.0.                    as a type of killing, but not as a type of crime.
   It is worth noting that our system is not strictly                     The Jiang-Conrath distance gives scores for
limited to choosing from pre-existing groups. For                      pairs of word senses, not pairs of words. We han-
example, if we have a list of luxury items, and an-                    dle this by adding one row for every word sense
other list of cars, our system can learn weights so                    with the right part of speech (rather than for ev-
that it prefers items in the intersection, luxury cars.                ery word); each row measures the similarity of ev-
   Moby thesaurus consists of a list of word-                          ery word to a particular word sense. The label of
based thesaurus entries. Each word wi has a list of                    each row is the (undisambiguated) word; multiple
similar words simij . Moby has a total of about 2.5                    rows can have the same label. For the columns, we
million related word pairs. Unlike some other the-                     do need to collapse the word senses into words;
sauri (such as WordNet and thesaurus.com), en-                         for each word, we take a maximum across all of
tries are not broken down by word sense.                               its senses. For example, to determine how similar
   In the raw format, the similarity relation is not                   (the only sense of) “factory” is to the word “plant,”
symmetric; for example, there are many words                           we compute the similarity of “factory” to the “in-
that occur only in similarity lists but do not have                    dustrial plant” sense of “plant” and to the “living
their own entries. We augmented the thesaurus to                       thing” sense of “plant” and take the higher of the
make it symmetric: if “dog” is in the similarity en-                   two (in this case, the former).
try for “cat,” we add “cat” to the similarity entry
                                                                          The Jiang-Conrath distance is a number be-
for “dog” (creating an entry for “dog” if it does not
                                                                       tween −∞ and 0. By examination, we determined
exist yet). We then have a row i for every similar-
                                                                       that scores below −12.0 indicate virtually no sim-
ity entry in the augmented thesaurus; mij is 1 if
                                                                       ilarity. We cut off scores below this point and
wj appears in the similarity list of wi , and 0 other-
                                                                       linearly mapped each score x to the range 0 to
wise. The label li of row i is simply word wi . Un-
                                                                       1, yielding a final similarity of min(0,x+12)
                                                                                                               12     . This
like some other thesauri (including WordNet and
                                                                       greatly sparsified the similarity matrix M .
thesaurus.com), the entries are not broken down
by word sense or part of speech. For polysemic                            Distributional similarity. We used Dekang
words, there will be a mix of the words similar to                     Lin’s dependency-based thesaurus, available at
each sense and part of speech.                                         www.cs.ualberta.ca/˜lindek/downloads.htm.
   WordNet is a well-known dictionary/thesaurus/                       This resource groups words based on the words
ontology often used in NLP applications. It con-                       they co-occur with in normal text. The words
sists of a large number of synsets; a synset is a set                  most similar to “cat” are “dog,” “animal,” and
of one or more similar word senses. The synsets                        “monkey,” presumably because they all “eat,”
are then connected with hypernym/hyponym links,                        “walk,” etc. Like Moby, similarity entries are not
which represent IS-A relationships. We focused                         divided by word sense; usually, only the dominant
on measuring similarity in WordNet using the hy-                       sense of each word is represented. This type of
pernym hierarchy.2 . There are many methods for                        similarity is considerably different from the other
    1
                                                                       two types, tending to focus less on minor details
     Available at icon.shef.ac.uk/Moby/.                               and more on broad patterns.
    2
     A useful similarity metric we did not explore in this pa-
per is similarity between WordNet dictionary definitions                  Each similarity entry corresponds to a single


                                                                 373


word wi and is a list of scored similar words simij .             We evaluated our algorithms along three axes.
The scores vary between 0 and 1, but usually the               First, the method for choosing θ: Untrained and
highest-scored word in a similarity list gets a score          Logistic (U and L). Second, the data sources used:
of no more than 0.3. To calibrate these scores                 each source separately (M for Moby, W for Word-
with the previous two types, we divided all scores             Net, D for distributional similarity), and all three
by the score of the highest-scored word in that                in combination (MWD). Third, whether active
list. Since each row is normalized individually,               learning is used (+/-). Thus, logistic regression us-
the similarity matrix M is not symmetric. Also,                ing Moby and no active learning is L(M,-). For lo-
there are separate similarity lists for each of nouns,         gistic regression, we set the regularization penalty
verbs, and modifiers; we only used the lists match-            σ 2 to 1, based on qualitative analysis during devel-
ing the seed word’s part of speech.                            opment (before seeing the test data).
5   Experimental Setup                                            We also compared the performance of our
                                                               algorithms to the popular online thesaurus
Given a seed set s and a complete target set G, it is          http://thesaurus.com. The entries in this
easy to evaluate our system; we say “Yes” to any-              thesaurus are similar to Moby, except that each
thing in G, “No” to everything else, and see how               word may have multiple sense-disambiguated en-
many of the candidate words are in G. However,                 tries. For each seed word w, we downloaded the
building a complete gold-standard G is in practice             page for w and extracted a set of synonyms en-
prohibitively difficult; instead, we are only capa-            tries for that word. To compare fairly with our al-
ble of saying whether or not a word belongs to G               gorithms, we propose a word-by-word method for
when presented with that word.                                 exploring the thesaurus, intended to model a user
   To evaluate a particular active learning algo-              scanning the thesaurus. This method checks the
rithm, we can just run the algorithm manually, and             first 3 words from each entry; if none of these are
see how many candidate words we say “Yes” to                   labeled “Yes,” it moves on to the next entry. We
(note that this will not give us an accurate estimate          omit details for lack of space.
of the recall of our algorithm). Evaluating several
different algorithms for the same s and G is more              6   Experimental Results
difficult. We could run each algorithm separately,             We designed a test set containing different types
but there are several problems with this approach.             of similarity. Table 2 shows each category, with
First, we might unconsciously (or consciously)                 examples of specific similarity queries. For each
bias the results in favor of our preferred algo-               type, we tested on five different queries. For each
rithms. Second, it would be fairly difficult to be             query, the first author built the seed set by writ-
consistent across multiple runs. Third, it would be            ing down the first three words that came to mind.
inefficient, since we would label the same words               For most queries this was easy. However, for the
multiple times for different algorithms.                       similarity type Hard Synonyms, coming up with
   We solved this problem by building a labeling               more than one seed word was considerably more
system which runs all algorithms that we wish to               difficult. To build seed sets for these queries, we
test in parallel. At each step, we pick a random al-           ran our evaluation system using a single seed word
gorithm and either present its current candidate to            and took the first two positive candidates; this en-
the user or, if that candidate has already been la-            sured that we were not biasing our seed set in favor
beled, we supply that algorithm with the given an-             of a particular algorithm or data set.
swer. We do NOT ever give an algorithm a labeled                  For each query, we ran our evaluation system
training example unless it actually asks for it – this         until each algorithm had suggested 25 candidate
guarantees that the combined system is equivalent              words, for a total of 625 labeled words per algo-
to running each algorithm separately. This pro-                rithm. We measured performance using mean av-
cedure has the property that the user cannot tell              erage precision (MAP), which corresponds to area
which algorithms presented which words.                        under the precision-recall curve. It gives an over-
   To evaluate the relative contribution of active             all assessment across different stopping points.
learning, we consider a version of our system                     Table 3 shows results for an informative sub-
where active learning is disabled. Instead of re-              set of the tested algorithms. There are many con-
training the system every iteration, we train it once          clusions we can draw. Thesaurus.Com performs
on the seed set s and keep the weight vector θ fixed           poorly overall; our best system, L(MWD,+),
from iteration to iteration.                                   outscores it by 164%. The next group of al-


                                                         374


                                                                            Algorithm            MAP
                                                                            Thesaurus.Com .122
 Category Name            Example Similarity Queries                        U(M,-)               .176
 Simple Groups (SG)       car brands, countries, mammals, crimes            U(W,-)               .182
 Complex Groups (CG) luxury car brands, sub-Saharan countries               U(D,-)               .211
 Synonyms (Syn)           syn of {scandal, helicopter, arrogant, slay}      L(D,-)               .236
 Hard Synonyms (HS)       syn of {(stock-market) crash, (legal) maneuver}
                                                                            L(D,+)               .288
 Meronym/Material (M) parts of a car, things made of wood                   U(MWD,-)             .233
                 Table 2: Categories and examples                           U(MWD,+)             .271
                                                                            L(MWD,-)             .286
               SG     CG Syn HS                M                            L(MWD,+)             .322
 Thesaurus.Com .041 .060 .275 .173 .060                                   Table 3: Comparison of algorithms
 L(D,+)          .377   .344    .211   .329    .177         across the board. Neither logistic regression nor
 L(M,-)          .102   .118    .393   .279    .119         active learning always improved performance, but
 U(W,+)          .097   .136    .296   .277    .165         L(MWD,+) performs near the top for every cate-
 U(MWD,+)        .194   .153    .438   .357    .213         gory. The complex groups category is particularly
 L(MWD,-)        .344   .207    .360   .345    .173         interesting, because achieving high performance
 L(MWD,+)        .366   .335    .379   .372    .158         on this category required using both logistic re-
                                                            gression and active learning. This makes sense
          Table 4: Results by category
                                                            since negative evidence is particularly important
gorithms, U(*,-), add together the similarity en-           for this category.
tries of the seed words for a particular similarity
source. The best of these uses distributional simi-         7 Discussion and Related Work
larity; L(MWD,+) outscores it by 53%. Combin-               The biggest difference between our system and
ing all similarity types, U(MWD,-) improves by              previous work is the use of active learning, espe-
10% over U(D,-). L(MWD,+) improves over the                 cially in allowing the use of negative examples.
best single-source, L(D,+), by a similar margin.            Most previous set expansion systems use boot-
   Using logistic regression instead of the un-             strapping from a small set of positive examples.
trained weights significantly improves perfor-              Recently, the use of negative examples for set ex-
mance.      For example, L(MWD,+) outscores                 pansion was proposed by Vyas and Pantel (2009),
U(MWD,+) by 19%. Using active learning also                 although in a different way. First, set expansion is
significantly improves performance: L(MWD,+)                run as normal using a fixed seed set. Then, human
outscores L(MWD,-) by 13%. This shows that                  annotators label a small number of negative exam-
active learning is useful even when a reasonable            ples from the returned results, which are used to
amount of initial information is available (three           weed out other bad answers. Our method incorpo-
seed words for each test case). The gains from              rates negative examples at an earlier stage. Also,
logistic regression and active learning are cumula-         we use a logistic regression model to robustly in-
tive; L(MWD,+) outscores U(MWD,-) by 38%.                   corporate negative information, rather than deter-
   Finally, our best system, L(MWD,+) improves              ministically ruling out words and features.
over L(D,-), the best system using a single data               Our system is limited by our data sources. Sup-
source and no active learning, by 36%. We con-              pose we want actors who appeared in Star Wars. If
sider L(D,-) to be a strong baseline; this compari-         we only know that Harrison Ford and Mark Hamill
son demonstrates the usefulness of the main con-            are actors, we have little to go on. There has
tributions of this paper, the use of multiple data          been a large amount of work on other sources of
sources and active learning. L(D,-) is still fairly         word-similarity. Hughes and Ramage (2007) use
sophisticated, since it combines information from           random walks over WordNet, incorporating infor-
the similarity entries for different words.                 mation such as meronymy and dictionary glosses.
   Table 4 shows the breakdown of results by cat-           Snow et al. (2006) extract hypernyms from free
egory. For this chart, we chose the best set-               text. Wang and Cohen (2007) exploit web-page
ting for each similarity type. Broadly speaking,            structure, while Pasca and Durme (2008) exam-
the thesauri work reasonably well for synonyms,             ine query logs. We expect that adding these types
but poorly for groups. Meronyms were difficult              of data would significantly improve our system.


                                                      375


References
Fellbaum, C. (Ed.). (1998). Wordnet: An elec-
  tronic lexical database. MIT Press.

Ghahramani, Z., & Heller, K. (2005). Bayesian
 sets. Advances in Neural Information Process-
 ing Systems (NIPS).

Hughes, T., & Ramage, D. (2007). Lexical se-
 mantic relatedness with random graph walks.
 EMNLP-CoNLL.

Jiang, J., & Conrath, D. (1997). Semantic similar-
   ity based on corpus statistics and lexical taxon-
   omy. Proceedings of International Conference
   on Research in Computational Linguistics.

Lin, D. (1998). An information-theoretic defini-
  tion of similarity. Proceedings of ICML.

Liu, D. C., & Nocedal, J. (1989). On the lim-
  ited memory method for large scale optimiza-
  tion. Mathematical Programming B.

Pantel, P., Crestan, E., Borkovsky, A., Popescu,
  A., & Vyas, V. (2009). Web-scale distributional
  similarity and entity set expansion. EMNLP.

Pasca, M., & Durme, B. V. (2008). Weakly-
  supervised acquisition of open-domain classes
  and class attributes from web documents and
  query logs. ACL.

Roark, B., & Charniak, E. (1998). Noun-phrase
 co-occurrence statistics for semiautomatic se-
 mantic lexicon construction. ACL-COLING.

Snow, R., Jurafsky, D., & Ng, A. (2006). Seman-
  tic taxonomy induction from heterogenous evi-
  dence. ACL.

Vyas, V., & Pantel, P. (2009). Semi-automatic en-
 tity set refinement. NAACL/HLT.

Vyas, V., Pantel, P., & Crestan, E. (2009). Helping
 editors choose better seed sets for entity expan-
 sion. CIKM.

Wang, R., & Cohen, W. (2007). Language-
 independent set expansion of named entities us-
 ing the web. Seventh IEEE International Con-
 ference on Data Mining.




                                                       376
